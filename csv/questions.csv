id,user_id,location,tag,title,body,score,up_vote_count,down_vote_count,created_at,answers_count,view_count,is_answered
79681963,17614576,"Kyiv, Ukraine",sql,Mutually exclusive columns and violating 3NF,"<pre><code>CREATE TABLE foo
(
   id SERIAL PRIMARY KEY,
   a TEXT,
   b TEXT
)
</code></pre>
<p>Column <code>a</code> can take a value only when column <code>b</code> is empty and vice versa:</p>
<pre><code>id | a     | b
---+-------+------
1  | NULL  | 'abc'
2  | 'cba' | NULL
</code></pre>
<p>There is a dependency between columns <code>a</code> and <code>b</code>, because we can't just set any value in those columns without knowing what is stored in the other.</p>
<p>Is the table structure above a violation of the third normal form of the database?</p>
",1,3,2,2025-06-27T12:37:38+00:00,2,218,True
79682129,8189772,,sql,SQL query to find list of tables with 0 row counts in MS Fabric Warehouse,"<p>I want to find out list of table and schema's present in my Microsoft Fabric Warehouse which have 0 row count or basically which are empty.
I have 9000 tables. The following queries work fine in our Azure Data Warehouse but don't work in the Fabric Warehouse.</p>
<pre><code>SELECT 
    two_part_name, 
    SUM(row_count) AS row_count
FROM 
    dbo.vTableSizes
GROUP BY 
    two_part_name
ORDER BY 
    row_count DESC;
</code></pre>
<p>I tried running the below query as well using UNION ALL, but for 9K tables, it would take a lot of time. Can anyone please suggest the right query for Fabric Warehouse to get the list of tables with 0 row counts?</p>
<pre><code>SELECT 'AAT.ANA1' AS table_name, COUNT(*) AS row_count FROM [AAT].[ANA1]
UNION ALL
SELECT 'AAT.ANA2' AS table_name, COUNT(*) AS row_count FROM [AAT].[ANA2]
UNION ALL
SELECT 'AAT.ANA3' AS table_name, COUNT(*) AS row_count FROM [AAT].[ANA3]
</code></pre>
",1,1,0,2025-06-27T14:45:08+00:00,2,153,False
79682182,,,sql,Get highest value from different rows,"<p>In the screenshot attached, it shows that one <code>ID</code> has different parts.</p>
<p><code>SET</code> shows the combined parts (A+B+C). The <code>WEIGHT</code> is the total of the parts, but not the <code>LENGTH</code>.</p>
<p>The <code>SET LENGTH</code> is 0, and I need to get the highest <code>LENGTH</code> from the other parts.</p>
<p>Ex:</p>
<ul>
<li>ID = 1, length is 30 (highest length)</li>
<li>ID = 2, length is 40 (highest length)</li>
</ul>
<p><a href=""https://i.sstatic.net/Hlq8rXhO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Hlq8rXhO.png"" alt=""enter image description here"" /></a></p>
<p>Thank you!</p>
",-1,1,2,2025-06-27T15:37:14+00:00,2,87,True
79682193,29863426,,sql,How to join SQL tables with multiple joins with cross-table criteria?,"<p>Consider the following Tables</p>
<p>Table A</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>etc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>...</td>
</tr>
<tr>
<td>2</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>Table B</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A_ID</th>
<th>NAME</th>
<th>etc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>...</td>
</tr>
<tr>
<td>1</td>
<td>B</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>Table C</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A_ID</th>
<th>NAME</th>
<th>etc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>...</td>
</tr>
<tr>
<td>1</td>
<td>C</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>Where etc. denotes some unrelated columns that need to be selected as well.</p>
<p>How do I join these tables such that I get the following result? (NAME field is not a dimensional value with a common table)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A_ID</th>
<th>B_NAME</th>
<th>C_NAME</th>
<th>etc. B</th>
<th>etc. C</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>A</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>1</td>
<td>B</td>
<td>null</td>
<td>...</td>
<td>null</td>
</tr>
<tr>
<td>1</td>
<td>null</td>
<td>C</td>
<td>null</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>I tried:</p>
<pre><code>SELECT a.ID as 'A_ID', b.NAME as 'B_NAME', c.NAME as 'C_NAME', etc.
FROM A a
FULL JOIN B b on a.ID = b.A_ID
FULL JOIN C c on a.ID = c.C_ID and (c.NAME = b.NAME or b.NAME is null)
</code></pre>
<p>This produces the following table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A_ID</th>
<th>B_NAME</th>
<th>C_NAME</th>
<th>etc. B</th>
<th>etc. C</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>A</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>1</td>
<td>B</td>
<td>null</td>
<td>...</td>
<td>null</td>
</tr>
<tr>
<td>null</td>
<td>null</td>
<td>C</td>
<td>null</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>Note that A_ID is null where B does not have a matching NAME.<br />
Why is this so hard?  It seems like I'm missing something obvious, but I can't find any questions joining this way.</p>
",1,1,0,2025-06-27T15:49:50+00:00,2,95,True
79682355,5560898,"Salem, MA, USA",sql,PostgreSQL &quot;_uuid&quot; data type,"<p>I am looking at some table DDL and I noticed we have two different data types.</p>
<ul>
<li>uuid</li>
<li>_uuid (yes, it's literally &quot;_uuid&quot;)</li>
</ul>
<p>I have never seen this before, what is the difference between them?</p>
",4,5,1,2025-06-27T18:19:43+00:00,1,142,True
79682423,13630206,,sql,Oracle Date returning 20xx instead of 19xx when format is mmddyy,"<p>I'm having issue while converting date format, I'm receiving as a text in the format <code>mmddyy</code> ( 031537 ), when I try to convert with <code>to_date</code> it is returning 2037 instead 1937</p>
<p>Appreciate if anyone has work around and I'm using oracle version 19c</p>
<pre><code>select to_date('031537','mmddyy') from dual;
03/15/2037

select to_date('031537','mmddyrr') from dual;
03/15/2037
</code></pre>
<p>Tried <code>to_date</code> but no luck</p>
",0,1,1,2025-06-27T19:48:43+00:00,1,90,True
79683923,24888000,,sql,Pg SQL Output tree hierarchy as JSON,"<p>I need to return a JSON tree by departments from the database, having a table of departments. I have a table like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>parent_id</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>null</td>
<td>Отдел 1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>Отдел 1.1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Отдел 1.1.1</td>
</tr>
<tr>
<td>3</td>
<td>null</td>
<td>Отдел 2</td>
</tr>
<tr>
<td>4</td>
<td>3</td>
<td>Отдел 2.1</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>Отдел 1.1.1.1</td>
</tr>
</tbody>
</table></div>
<p>I need to write a recursive query on this table and get the result as JSON:</p>
<pre><code>      {[
  {&quot;departmentId&quot;: 0,
   &quot;departmentName&quot;: &quot;Отдел 1&quot;,
        &quot;children&quot;: [
          {&quot;departmentId&quot;: 1,
           &quot;departmentName&quot;: &quot;Отдел 1.1&quot;,
           &quot;children&quot;: [
               {
                 &quot;departmentId&quot;: 2,
                 &quot;departmentName&quot;: &quot;Отдел 1.1.1&quot;,
                 &quot;children&quot;: [
                     {
                       &quot;departmentId&quot;: 5,
                       &quot;departmentName&quot;: &quot;Отдел 1.1.1.1&quot;,
                       &quot;children&quot;: &quot;null&quot;
                     }
                   ]
               }
             ]
          }],
          {&quot;departmentId&quot;: 3,
           &quot;departmentName&quot;: &quot;Отдел 2&quot;,
           &quot;children&quot;:[
             {&quot;departmentId&quot;: 4,
              &quot;departmentName&quot;: &quot;Отдел 2.1&quot;,
              &quot;children&quot;: &quot;null&quot;
             }
          ]
         }
  }
]}
</code></pre>
<p>After looking at recursive queries on the internet, I made the following query:</p>
<pre><code>    WITH recursive parents AS (
 
  SELECT
    id, name, parent_id
  FROM departments
  WHERE
    id = '798c66092ef24ec488ef308291947913'
 
  UNION ALL
 
  SELECT
    dep.id, dep.name, dep.parent_id
  FROM departments dep
  JOIN parents ON parents.parent_id = dep.id
)
 
SELECT
  *
FROM parents
WHERE id != '798c66092ef24ec488ef308291947913'
order by id;
</code></pre>
<p>But this query returns only the parents of the department by id. That is, it searches for all the parents of the passed department.
How can I pass it so that it returns me such a JSON?
I tried to do it in different ways, looked through different examples, but I can’t get myself such a JSON.
Could you please tell me how to write such a query?</p>
",2,2,0,2025-06-29T17:37:21+00:00,3,77,True
79684282,928786,"Pune, Maharashtra, India",sql,Insert or Update - Understanding difference between SQL statements,"<p>We have a database utility in our product suite that handles both the creation of new databases and the upgrade of existing ones.</p>
<ul>
<li>For new tenants or customers, the creation flow is executed.</li>
<li>For existing customers being upgraded from an older version, the upgrade flow is executed.</li>
</ul>
<p>We maintain two separate .SQL files:</p>
<ol>
<li>Creation flow script</li>
<li>Upgrade flow script</li>
</ol>
<p>As part of the upgrade flow script, the following statement was added:</p>
<pre><code>IF NOT EXISTS (SELECT 1 FROM table1 WHERE col1 = 1 AND col2 = 'ABC')
 INSERT INTO table1(col1, col2) VALUES(1, 'ABC')
UPDATE table1 SET col2 = 'ABC' WHERE col1 = 1
</code></pre>
<p>However, during code review, I was advised to replace it with the following:</p>
<pre><code>IF NOT EXISTS (SELECT 1 FROM table1 WHERE col1 = 1 AND col2 = 'ABC')
    INSERT INTO table1(col1, col2) VALUES(1, 'ABC')
ELSE
    UPDATE table1 SET col2 = 'ABC' WHERE col1 = 1
</code></pre>
<p>From my understanding, both versions appear to be similar or achieve the same result.</p>
<p>Can someone help me understand the difference between the two statements?</p>
",-1,0,1,2025-06-30T05:26:47+00:00,2,182,True
79684289,1023554,,sql,Joining two tables with Pivot query,"<p>I have two tables. One table shows the incident resolution time, and
the other table shows the incident status time. Joining both would
allow me to see all of the statuses in columns.</p>
<pre><code>SELECT
    IRD.INC_ID,
    IRD.resolution_time,
    p.&quot;Open&quot; AS open_duration
FROM
         INC_RES_DUR IRD
    JOIN (
        SELECT
            ISD.INC_ID,
            ISD.case_status,
            ISD.time_duration
        FROM
            INC_STATUS_DUR ISD
    ) PIVOT (
        SUM(time_duration)
        FOR case_status
        IN ( 'Open' )
    )
    p ON IRD.INC_ID = p.INC_ID
</code></pre>
<blockquote>
<p>Error invalid ORA-00904: &quot;P&quot; is what I'm seeing.The identifier &quot;Open&quot; is invalid.</p>
</blockquote>
<p>Where is it going wrong?</p>
<p>How can the status &quot;In Progress,&quot; which is not a single word, be accommodated?</p>
<p>Additionally, each status's time length in INC_STATUS_DUR has already been determined. So, is SUM required? Using Oracle</p>
",0,0,0,2025-06-30T05:40:22+00:00,3,98,True
79684302,30926748,,sql,SQL code for identifying salary for a status,"<p>I have 2 tables:</p>
<p><code>DEPT</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>dept_no</th>
<th>C</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>200</td>
<td>100</td>
</tr>
<tr>
<td>2</td>
<td>300</td>
<td>150</td>
</tr>
<tr>
<td>3</td>
<td>400</td>
<td>200</td>
</tr>
</tbody>
</table></div>
<p>and <code>EMP</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>dept_no</th>
<th>employee_no</th>
<th>Task_status</th>
<th>Salary</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>C</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>N</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>C</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>C</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>with column Salary has to be calculated. Task status is C is for job completed and N for not completed.</p>
<p>I need to <strong>write a SQL query to get <code>Salary</code> column to be updated</strong> with values as per the table <code>DEPT</code></p>
",-1,0,1,2025-06-30T06:03:26+00:00,2,79,False
79684960,22786821,,sql,TimeScaleDb/Postgres: Materialized Views(COGG): GROUP BY: group by certain field values,"<p>What I'm currently doing is this:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    time_bucket('60 min', raw_data.timestamp) AS time_60min,
    COUNT(raw_data.vehicle_class) AS &quot;count&quot;,
    raw_data.vehicle_class AS &quot;vehicle_class&quot;
FROM bma_raw_data_ode AS raw_data
WHERE raw_data.vehicle_class IN ('car', 'bus', 'van', 'motorbike')
GROUP BY time_60min, raw_data.vehicle_class
ORDER BY time_60min
</code></pre>
<p>All entries in <code>raw_data.vehicle_class</code> have one of 12 given values (<code>car</code>, <code>bus</code>, <code>person</code>, <code>bicycle</code> and so on) - think of it as an enum, as it can only be one of these 12 values.</p>
<p>What the query above returns at the moment is the count of each vehicle_class in raw_data within an hour:</p>
<pre><code>&quot;2025-06-10 19:00:00+00&quot;    1   &quot;bus&quot;
&quot;2025-06-10 19:00:00+00&quot;    4   &quot;motorbike&quot;
&quot;2025-06-10 19:00:00+00&quot;    126 &quot;car&quot;
&quot;2025-06-10 19:00:00+00&quot;    3   &quot;van&quot;
</code></pre>
<p>Now, what I'd like to achieve is: Not having separate <code>car</code> and <code>van</code> rows per hour, but <strong>a single row for the sum of both counts</strong>.</p>
<p>So, the result I'm looking for would be someting like:</p>
<pre><code>&quot;2025-06-10 19:00:00+00&quot;    1   &quot;bus&quot;
&quot;2025-06-10 19:00:00+00&quot;    4   &quot;motorbike&quot;
&quot;2025-06-10 19:00:00+00&quot;    129 &quot;car + van&quot;
</code></pre>
<p>Can you point be in the right direction? Can I achieve this directly in <code>COUNT(raw_data.vehicle_class) AS &quot;count&quot;,</code>, can it be done by a SELECT in the GROUP BY statement or is some fancy join on a virtual table the right way?</p>
<p>Thanks</p>
",1,1,0,2025-06-30T15:04:08+00:00,1,76,True
79685280,18586873,,sql,Filtering by Dates in drizzle ORM,"<h3>How can I use Drizzle to query records within a specific time frame defined by two timestamps?</h3>
<p>I'm using PostgreSQL and I searched and it seems the only thing available is <code>between()</code> method, which only works for <strong>integers</strong>, and I can't use it for <strong>strings</strong> or <strong>dates</strong>. I want to say like:</p>
<pre class=""lang-ts prettyprint-override""><code>const customers = await db
        .select()
        .from(customers)
        .where(between(customers.created, startDate, endDate));
</code></pre>
<p>here is my schema of customers in drizzle:</p>
<pre class=""lang-ts prettyprint-override""><code>export const customers = pgTable(&quot;customers&quot;,{
    ...
    created: timestamp({ withTimezone: true, mode: &quot;string&quot; }).defaultNow(),
  },
);
</code></pre>
<p>I tried using raw SQL, but it's not a typesafe way to do this:</p>
<pre class=""lang-ts prettyprint-override""><code>await db.execute(sql`
      SELECT * 
      FROM customers 
      WHERE created BETWEEN ${startDate} AND ${endDate}
    `)  
</code></pre>
",1,2,1,2025-06-30T20:53:37+00:00,1,1122,False
79686305,6541696,,sql,IBM DB2 stored procedure transaction has a strange behavior,"<p>I have this code into an IBM DB2 stored procedure, when I send the <code>@action = 'I'</code>, then the <code>INSERT</code> statement is executed successfully, but when I send <code>@action = 'D'</code>, the <code>DELETE</code> statement remains in execution until it receives a blocking timeout.</p>
<pre><code>SET TRANSACTION ISOLATION LEVEL UR; 

MERGE INTO LIBTMP.T1TMP A 
USING (SELECT @ACTION ACTION, @CLDOC CLDOC FROM SYSIBM.SYSDUMMY1) B
ON A.CLDOC = B.CLDOC

WHEN NOT MATCHED AND @ACTION = 'I' 
    THEN 
        INSERT (A.CLDOC, A.USER, A.IP, A.LOCATION, A.LANGUAGE, A.CHANNEL, A.TENANT, A.PROCESS, A.CRDATE)
        VALUES (@CLDOC, @USER, @IP, @LOCATION, @LANGUAGE, @CHANNEL, @TENANT, @PROCESS, CURRENT_TIMESTAMP)

WHEN MATCHED AND @ACTION = 'D' 
    THEN
        DELETE WITH NC;  

COMMIT;
</code></pre>
<p>But if I put <code>WITH UR</code> below <code>DELETE</code>, the <code>DELETE</code> statement executes perfectly fine:</p>
<pre><code>SET TRANSACTION ISOLATION LEVEL UR; 

MERGE INTO LIBTMP.T1TMP A 

USING (SELECT @ACTION ACTION, @CLDOC CLDOC FROM SYSIBM.SYSDUMMY1) B
ON A.CLDOC = B.CLDOC

WHEN NOT MATCHED AND @ACTION = 'I' 
    THEN 
        INSERT (A.CLDOC, A.USER, A.IP, A.LOCATION, A.LANGUAGE, A.CHANNEL, A.TENANT, A.PROCESS, A.CRDATE)
        VALUES (@CLDOC, @USER, @IP, @LOCATION, @LANGUAGE, @CHANNEL, @TENANT, @PROCESS, CURRENT_TIMESTAMP)

WHEN MATCHED AND @ACTION = 'D' 
    THEN
        DELETE WITH UR;  

COMMIT;
</code></pre>
<p>I have solved the problem by placing <code>WITH UR</code> instead of <code>WITH NC</code>, but I still have questions about what causes this behavior.</p>
",2,2,0,2025-07-01T15:46:28+00:00,0,121,False
79686536,5281902,,sql,Ent ORM query with computed fields,"<p>I'm using Ent ORM and need help with a simple issue.</p>
<p>I run a query like:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT id, first_name, last_name, first_name || ' ' || last_name AS fullname 
FROM users 
ORDER BY fullname
</code></pre>
<p>Since fullname doesn't exist in the DB schema, <code>client.User.All(ctx)</code> fails with:</p>
<blockquote>
<p>unexpected column &quot;fullname&quot; for type User</p>
</blockquote>
<p>I tried adding a virtual field via template, but it didn't help.</p>
<p>Only adding the field to the DB/schema worked.</p>
<p>This is just an example—could be distance via PostGIS instead.</p>
<p>It should be a simple SQL query, but Ent makes it tricky. (edited)</p>
",1,1,0,2025-07-01T19:41:58+00:00,1,164,False
79686674,16241,United States,sql,Get the first Item from a &quot;group by&quot; without CTE or Temp Table,"<p>Say I have a list of orders that looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>OrderId</th>
<th>CustomerId</th>
<th>ItemOrdered</th>
<th>OrderedWhen</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>Orange</td>
<td>2024-08-01 16:33:00</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Apple</td>
<td>2022-01-28 01:00:00</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>Peach</td>
<td>2025-06-10 03:39:00</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>Banana</td>
<td>2022-01-28 01:00:00</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>Kiwi</td>
<td>2021-12-21 11:31:00</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
<td>Apple</td>
<td>2025-02-08 01:00:00</td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>Strawberry</td>
<td>2024-05-16 01:30:00</td>
</tr>
<tr>
<td>8</td>
<td>3</td>
<td>Banana</td>
<td>2025-02-01 05:29:00</td>
</tr>
</tbody>
</table></div>

Here is an insert script for this:
<pre><code>DECLARE @Orders TABLE (OrderId BIGINT NOT NULL IDENTITY(1,1), CustomerId BIGINT NOT NULL, 
                       ItemOrdered varchar(300) NOT NULL, OrderedWhen DATETIME NOT NULL)
INSERT INTO @Orders VALUES 
(1,'Orange', '2024-08-01 16:33'),
(1,'Apple', '2022-01-28 01:00'),
(1,'Peach', '2025-06-10 03:39'),
(1,'Banana', '2022-01-28 01:00'),
(2,'Kiwi', '2021-12-21 11:31'),
(2,'Apple', '2025-02-08 01:00'),
(3,'Strawberry', '2024-05-16 01:30'),
(3,'Banana', '2025-02-1 05:29')
</code></pre>

<p>For each customer, I need to be able to get the first <code>ItemOrdered</code>, as indicated by the <code>OrderedWhen</code>, then by <code>ItemOrdered</code> (alphabetically).  This is the output I am looking for:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>OrderId</th>
<th>CustomerId</th>
<th>ItemOrdered</th>
<th>OrderedWhen</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>1</td>
<td>Apple</td>
<td>2022-01-28 01:00:00</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>Kiwi</td>
<td>2021-12-21 11:31:00</td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>Strawberry</td>
<td>2024-05-16 01:30:00</td>
</tr>
</tbody>
</table></div>
<p>Notice that Apple was picked before Banana.  They both have the same order when, but Apple is before Banana alphabetically.</p>
<p>I tried this:</p>
<pre><code>SELECT  ord.CustomerId, MIN(ord.ItemOrdered) OVER (PARTITION BY ord.CustomerId ORDER BY MIN(ord.OrderedWhen)) AS MostRecentItemOrdered
FROM    @Orders ord
GROUP BY ord.CustomerId
ORDER BY ord.CustomerId
</code></pre>
<p>But it does not compile because of <code>OrderedWhen</code> not being in the GroupBy.</p>
<p>I next though of using a temporary table like this:</p>
<pre><code>SELECT  ord.CustomerId, MIN(ord.OrderedWhen) AS MostRecentOrderWhen
INTO    #MostRecentOrderWhen
FROM    @Orders ord
GROUP BY ord.CustomerId
ORDER BY ord.CustomerId

SELECT  ord.CustomerId, MIN(ord.ItemOrdered) AS ItemOrdered, ord.OrderedWhen
FROM    #MostRecentOrderWhen recent
        JOIN @Orders ord
            ON recent.CustomerId = ord.CustomerId
            AND recent.MostRecentOrderWhen = ord.OrderedWhen
GROUP BY ord.CustomerId, ord.OrderedWhen
ORDER BY ord.CustomerId
</code></pre>
<p>This works, but I was hoping to not need to break it up.  (Meaning I would like to avoid using a CTE or Temporary table.)</p>
<p>Can this be done without a CTE or temporary table?</p>
",-1,1,2,2025-07-01T23:07:27+00:00,3,164,True
79686817,14116433,,sql,Column reference &quot;id&quot; is ambiguous while filtering in a chart,"<p>I created a dataset in Superset SQL Lab by joining 2 tables on <code>session_id</code>.</p>
<pre><code>SELECT *
FROM t1.performance as perf
INNER JOIN t1.test_session as session
ON perf.session_id = session.session_id
</code></pre>
<p>Now, to filter the these values in a chart, we regularly select a column name and value for that column. Doing this for the session_id column fails with:</p>
<blockquote>
<p>Error: column reference &quot;session_id&quot; is ambiguous.</p>
</blockquote>
<p>I tried to write custom query, using the psuedo column name created by Superset, &quot;session_id__1&quot;, this fails with error:</p>
<blockquote>
<p>column &quot;session_id__1&quot; does not exist in virtual_table, use the column name with table name (&quot;perf.session_id&quot;).</p>
</blockquote>
<p>But nothing seems to work.</p>
",1,1,0,2025-07-02T04:15:31+00:00,1,145,True
79687541,12843736,,sql,How to delete large number of records from a table faster in plsql?,"<p>Table #1:</p>
<pre><code>buck number(10), 
sname varchar(20),
... 
total of 20 columns
</code></pre>
<p>Table #2:</p>
<pre><code>sname varchar(20), 
sdiv varchar(10),
... 
total of 15 columns
</code></pre>
<ul>
<li>Table1 has 30M rows</li>
<li>Table2 has 10M rows</li>
</ul>
<p>I want to delete records in <code>table1.sname</code> where</p>
<pre><code>table1.sname = table2.sname and sdiv = 'tre'

select count(1) 
from table1 
where table1.sname = table2.sname 
  and sdiv = 'tre';    -- 3.3M records
</code></pre>
<pre><code>declare 

cursor cur is 
select distinct t.buck from table1 t, table2 s 
where t.sname = s.sname and s.sdiv = 'tre'; 

begin 

for i in cur 

loop 

delete table1 t 
where t.buck = i.buck and t.sname in (select s.sname from table2 s where s.sdiv = 'tre'); 
dbms_output.put_line(i.buck || ' buck records deleted'); 

end loop; 

end; 
/
</code></pre>
<p>In the above code, I am deleting buck wise... because each buck has 92k records... so that the database will not hang.</p>
<p>But my query is long running.. even after 42 mins it is not getting completed... and the database is getting stuck.</p>
<p>Is there any way I can optimise it?</p>
",0,0,0,2025-07-02T13:50:12+00:00,1,181,False
79687948,16735583,,sql,Is Snowflake&#39;s UNION BY NAME more computationally expensive than UNION?,"<p>Snowflake recently released a new function <a href=""https://docs.snowflake.com/en/sql-reference/operators-query"" rel=""nofollow noreferrer"">UNION [ALL] BY NAME</a>, which is exciting!  While I imagine it's minor, I'm wondering if this works just as performantly as UNION [ALL], or if there's an extra step the processor has to take that would make adding BY NAME less performant</p>
<p>I'm expecting that it does take longer, but by not enough to be confident in my quick side-by-side comparison testing</p>
",-1,0,1,2025-07-02T19:00:56+00:00,1,169,True
79688269,29465719,,sql,Multiple Select from same table,"<p>Tablename: <code>tbl_test</code></p>
<pre class=""lang-none prettyprint-override""><code>Code     |amount |Date   |Mos   |Remarks 
R1        300     1/7/25  Jul    Collect
R2        150     1/7/25  Jul    collect
R3        150     1/7/25  Jul    release    
R2        200     2/7/25  Jul    release
R3        200     3/7/25  Jul    release
R3        200     3/7/25  Jul    Cashout
R1        200     4/7/25  Jul    CashOut
</code></pre>
<p>I want to sum the amount and group by code</p>
<pre><code>select
    code
    , sum(amount) as ttlcollect from tbl_test where remarks = 'collect'
    , (select sum(amount)) from tbl_test where remarks = 'release') as ttlrelease
    , (select sum(amount)) from tbl_test where remarks = 'cashout') as ttlcashout
group by code
</code></pre>
<p>How can I achieve the output below?</p>
<pre class=""lang-none prettyprint-override""><code>CODE    |ttlCollect     |ttlrelease     |ttlcashout
R1      |300            |0              |200
R2      |150            |200            |0
R3      |0              |450            |200 
</code></pre>
",-1,0,1,2025-07-03T03:29:22+00:00,2,91,True
79688851,22126079,,sql,Recursive SQL query to hierarchyId,"<p>I currently have the problem that I get performance problems when migrating a large amount of data, the more data is migrated. As I cannot query the assignments of children to the TeamIds directly via the table, as there is no direct relationship, I have had to create these views using CTE. Unfortunately, this recursive approach does not seem to work well in terms of performance. Does anyone have an idea how I can solve this using HierarchyId to make the recursions obsolete?</p>
<pre><code>SET STATISTICS TIME ON

SELECT t.Id as TeamId, t.Name as TeamName, c.Id as CountryId
FROM dbo.MemberCountries mc 
JOIN Countries c ON c.Id = mc.CountryId 
JOIN Teams t ON t.Id = mc.TeamId 
JOIN Members m ON m.Id = mc.MemberId 
WHERE mc.MemberId = 1 -- Some member id
AND mc.CountryId IN (1) -- Some country id

SET STATISTICS TIME OFF
</code></pre>
<pre><code>create or alter view [dbo].TeamHierarchy
as 
with cte as (
    select
        t.Id,
        t.Name,
        t.ParentId,
        Id as TeamId,
        0 as Distance
    from [dbo].Teams as t
    union all
    select
        t.Id,
        t.Name,
        t.ParentId,
        c.TeamId,
        c.Distance + 1
    from [dbo].Teams as t
    inner join cte as c on t.Id = c.ParentId
)
select
    t.Id as TeamId,
    t.Name as TeamName,
    cte.Id as ParentId,
    cte.Name as ParentName,
    cte.Distance
from cte
join [dbo].Teams t on t.Id = cte.TeamId
where t.Id != cte.Id
</code></pre>
<pre><code>create or alter view [dbo].TeamCountries
as
with cte as (
    select
        t.*,
        Id as BaseTeamId
    from
        [dbo].Teams as t
    union all
    select
        t.*,
        c.BaseTeamId
    from
        [dbo].Teams as t
    inner join cte as c on t.Id = c.ParentId
)
select
    t.Id as TeamId,
    cte.Id as SourceId,
    ct.CountriesId as CountryId,
    cast((case when cte.Id = t.Id then 0 else 1 end) as bit) as IsInherited
from cte
    join [dbo].Teams t on t.Id = cte.BaseTeamId
    join [dbo].CountryTeam ct on ct.TeamsId = cte.Id
    join [dbo].Countries co on co.Id = ct.CountriesId
</code></pre>
<pre><code>create or alter view [dbo].MemberCountries
as
select
    memberCountries.MemberId,
    memberCountries.TeamId,
    memberCountries.CountryId
from (
     select
         mt.MembersId as MemberId,
         tc.TeamId,
         tc.CountryId
     from
         [dbo].TeamCountries tc
         inner join
         [dbo].MemberTeam mt on mt.TeamsId = tc.TeamId
 ) as memberCountries
group by
    memberCountries.MemberId,
    memberCountries.TeamId,
    memberCountries.CountryId
</code></pre>
<p>DDL:</p>
<pre><code>-- DROP SCHEMA dbo;

CREATE SCHEMA dbo;

-- app.dbo.Countries definition
-- Drop table
-- DROP TABLE app.dbo.Countries;

CREATE TABLE app.dbo.Countries 
(
    Id int IDENTITY(1,1) NOT NULL,
    Name nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
    CONSTRAINT PK_Countries PRIMARY KEY (Id)
);

-- app.dbo.Members definition
-- Drop table
-- DROP TABLE app.dbo.Members;

CREATE TABLE app.dbo.Members 
(
    Id int IDENTITY(1,1) NOT NULL,
    Name nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
    CONSTRAINT PK_Members PRIMARY KEY (Id)
);

-- app.dbo.[__EFMigrationsHistory] definition
-- Drop table
-- DROP TABLE app.dbo.[__EFMigrationsHistory];

CREATE TABLE app.dbo.[__EFMigrationsHistory] 
(
    MigrationId nvarchar(150) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
    ProductVersion nvarchar(32) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
    CONSTRAINT PK___EFMigrationsHistory PRIMARY KEY (MigrationId)
);

-- app.dbo.Teams definition
-- Drop table
-- DROP TABLE app.dbo.Teams;

CREATE TABLE app.dbo.Teams 
(
    Id int IDENTITY(1,1) NOT NULL,
    Name nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
    ParentId int NULL,
    CONSTRAINT PK_Teams PRIMARY KEY (Id),
    CONSTRAINT FK_Teams_Teams_ParentId 
         FOREIGN KEY (ParentId) REFERENCES app.dbo.Teams(Id)
);

CREATE NONCLUSTERED INDEX IX_Teams_ParentId 
    ON app.dbo.Teams ( ParentId ASC )
       WITH (PAD_INDEX = OFF, FILLFACTOR = 100, SORT_IN_TEMPDB = OFF, 
             IGNORE_DUP_KEY = OFF, STATISTICS_NORECOMPUTE = OFF,
             ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY ] ;

-- app.dbo.CountryTeam definition
-- Drop table
-- DROP TABLE app.dbo.CountryTeam;

CREATE TABLE app.dbo.CountryTeam 
(
    CountriesId int NOT NULL,
    TeamsId int NOT NULL,
    CONSTRAINT PK_CountryTeam PRIMARY KEY (CountriesId, TeamsId),
    CONSTRAINT FK_CountryTeam_Countries_CountriesId 
         FOREIGN KEY (CountriesId) REFERENCES app.dbo.Countries(Id) 
                 ON DELETE CASCADE,
    CONSTRAINT FK_CountryTeam_Teams_TeamsId 
         FOREIGN KEY (TeamsId) REFERENCES app.dbo.Teams(Id) 
                 ON DELETE CASCADE
);

CREATE NONCLUSTERED INDEX IX_CountryTeam_TeamsId 
    ON app.dbo.CountryTeam (TeamsId ASC)
       WITH (PAD_INDEX = OFF, FILLFACTOR = 100, SORT_IN_TEMPDB = OFF, 
             IGNORE_DUP_KEY = OFF, STATISTICS_NORECOMPUTE = OFF,
             ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY];

-- app.dbo.MemberTeam definition
-- Drop table
-- DROP TABLE app.dbo.MemberTeam;

CREATE TABLE app.dbo.MemberTeam 
(
    MembersId int NOT NULL,
    TeamsId int NOT NULL,
    CONSTRAINT PK_MemberTeam PRIMARY KEY (MembersId, TeamsId),
    CONSTRAINT FK_MemberTeam_Members_MembersId 
         FOREIGN KEY (MembersId) REFERENCES app.dbo.Members(Id) 
                 ON DELETE CASCADE,
    CONSTRAINT FK_MemberTeam_Teams_TeamsId 
         FOREIGN KEY (TeamsId) REFERENCES app.dbo.Teams(Id) 
                 ON DELETE CASCADE
);

CREATE NONCLUSTERED INDEX IX_MemberTeam_TeamsId 
    ON app.dbo.MemberTeam (TeamsId ASC)
       WITH (PAD_INDEX = OFF, FILLFACTOR = 100, SORT_IN_TEMPDB = OFF, 
             IGNORE_DUP_KEY = OFF, STATISTICS_NORECOMPUTE = OFF, 
             ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY];

-- dbo.MemberCountries source

CREATE OR ALTER VIEW [dbo].MemberCountries
AS
    SELECT
        memberCountries.MemberId,
        memberCountries.TeamId,
        memberCountries.CountryId
    FROM
        (SELECT
             mt.MembersId AS MemberId,
             tc.TeamId,
             tc.CountryId
         FROM
             [dbo].TeamCountries tc
         INNER JOIN 
             [dbo].MemberTeam mt ON mt.TeamsId = tc.TeamId
        ) AS memberCountries
    GROUP BY
        memberCountries.MemberId, memberCountries.TeamId,
        memberCountries.CountryId;
</code></pre>
",2,2,0,2025-07-03T12:24:22+00:00,1,76,True
79688989,29935472,,sql,Is it bad to fork for input null or not null within a single query?,"<p>I have a table <code>users</code> with a column <code>department_id</code>. Every user has a <code>department_id</code>.</p>
<p>I also have a dashboard if an Admin logs in I check which department_id he has and then I make a call to show only the users that have the same department id like the admin.</p>
<p>So but if an admin has no department_id I want to show users from all departments. So the super admin has the role that he can see all users. How can I make it now that I say if there is no department_id then show all users?</p>
<p>Currently I solved like this:</p>
<pre><code>SELECT u.id 
FROM users u
INNER JOIN department_users du
ON du.user_id = u.id
WHERE CASE WHEN $2 IS NULL THEN (u.department_id IS NULL OR u.department_id &gt; 0) ELSE u.department_id = $2 END
GROUP BY u.id
</code></pre>
<p>Department_id is an integer. I want to show all users when he has no department_id wherever department_id is NULL or have a number. If he has an department_id I want to show the users only from the department_id.</p>
<p>It works, but is the way correct or can I do it better?<br />
I have like 50 queries like this.</p>
",1,1,0,2025-07-03T14:15:40+00:00,1,80,True
79689145,16753548,,sql,Query to find the product name which has a continuous increase in sales every year,"<p>Product table with columns - PRODUCT_ID  PRODUCT_NAME</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>PRODUCT_ID</th>
<th>PRODUCT_NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>NOKIA</td>
</tr>
<tr>
<td>200</td>
<td>IPHONE</td>
</tr>
<tr>
<td>300</td>
<td>SAMSUNG</td>
</tr>
<tr>
<td>400</td>
<td>OPPO</td>
</tr>
</tbody>
</table></div>
<p>Sales table with columns - SALE_ID PRODUCT_ID YEAR  QUANTITY Price</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SALE_ID</th>
<th>PRODUCT_ID</th>
<th>YEAR</th>
<th>QUANTITY</th>
<th>PRICE</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>100</td>
<td>2010</td>
<td>25</td>
<td>5000</td>
</tr>
<tr>
<td>2</td>
<td>100</td>
<td>2011</td>
<td>16</td>
<td>5000</td>
</tr>
<tr>
<td>3</td>
<td>100</td>
<td>2012</td>
<td>8</td>
<td>5000</td>
</tr>
<tr>
<td>4</td>
<td>200</td>
<td>2010</td>
<td>10</td>
<td>9000</td>
</tr>
<tr>
<td>5</td>
<td>200</td>
<td>2011</td>
<td>15</td>
<td>9000</td>
</tr>
<tr>
<td>6</td>
<td>200</td>
<td>2012</td>
<td>20</td>
<td>9000</td>
</tr>
<tr>
<td>7</td>
<td>300</td>
<td>2010</td>
<td>20</td>
<td>7000</td>
</tr>
<tr>
<td>8</td>
<td>300</td>
<td>2011</td>
<td>18</td>
<td>7000</td>
</tr>
<tr>
<td>9</td>
<td>300</td>
<td>2012</td>
<td>20</td>
<td>7000</td>
</tr>
<tr>
<td>10</td>
<td>400</td>
<td>2010</td>
<td>15</td>
<td>7000</td>
</tr>
<tr>
<td>11</td>
<td>400</td>
<td>2011</td>
<td>18</td>
<td>7000</td>
</tr>
<tr>
<td>12</td>
<td>400</td>
<td>2012</td>
<td>22</td>
<td>7000</td>
</tr>
<tr>
<td>13</td>
<td>400</td>
<td>2013</td>
<td>23</td>
<td>7000</td>
</tr>
</tbody>
</table></div>
<p>Here Quantity is the number of products sold each year. Price is the sale price of each product.
Write a SQL query to find the product name which has a continuous increase in sales every year?</p>
<p>Output:
PRODUCT_NAME</p>
<pre><code>IPHONE

OPPO
</code></pre>
<p>I tried the below query, it can solve if we have only one product_id with increasing value, but it fails when we have multiple product_ids with increasing sales values.</p>
<pre class=""lang-sql prettyprint-override""><code>WITH cte1 AS
(
    SELECT a.*,
        (a.QUANTITY * a.PRICE) AS total_sales,
        LAG(a.QUANTITY * a.PRICE, 1, a.QUANTITY * a.PRICE) 
          OVER(PARTITION BY a.product_id ORDER BY year) as next_sales,
        B.PRODUCT_NAME
    FROM Sales AS a
    INNER JOIN Products AS b
       ON a.PRODUCT_ID = b.PRODUCT_ID
), 
cte2 AS
(
    SELECT *,
        (total_sales - next_sales) AS diff,
        CASE WHEN (total_sales - next_sales) &gt; 0 THEN 1
             WHEN (total_sales - next_sales) &lt; 0 THEN 0
             ELSE 0
        END AS val
    FROM cte1
),
cte3 AS
(
    SELECT
        PRODUCT_ID,
        SUM(val) AS max_val
    FROM cte2
    GROUP BY
        PRODUCT_ID
)
SELECT
    PRODUCT_ID
FROM cte3
WHERE max_val = (SELECT MAX(max_val) FROM cte3);
</code></pre>
",4,4,0,2025-07-03T15:59:24+00:00,5,200,True
79689559,7371506,,sql,&quot;Invalid cast from BOOL to TIMESTAMP&quot; error in LookML/BigQuery,"<p>I am trying to use Templated Filters logic in LookML to filter a look/dashboard based on flexible dates i.e., whatever date value the user enters in the date filter (<code>transaction_date_filter</code> dimension in this case). This is my LookML:</p>
<pre><code>    view: orders {
    
    derived_table: {
    
    sql:
    select
    customer_id,
    price,
    haspaid,
    debit,
    credit,
    transactiondate,
    case when haspaid= true or cast(transactiondate as timestamp) &gt;= date_trunc(cast({% condition transaction_date_filter %} cast(transactiondate as timestamp) {% endcondition %} as timestamp),year) then debit- credit else 0 end as ytdamount
    FROM
    orders ;;
    }
    
    dimension: transaction_date_filter {
    type: date
    sql: cast(${TABLE}.transactiondate as timestamp) ;;
    }
}
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Invalid cast from BOOL to TIMESTAMP</p>
</blockquote>
<p>Below is the rendered BQ SQL code from the SQL tab in the Explore when I use the <code>transaction_date_filter</code> as the filter,</p>
<p><a href=""https://i.sstatic.net/zj4mrA5n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zj4mrA5n.png"" alt=""enter image description here"" /></a></p>
<pre><code>select
    customer_id,
    price,
    haspaid,
    debit,
    credit,
    transactiondate,
    case 
        when haspaid = true or cast(transactiondate as timestamp) &gt;= date_trunc(cast((cast(orders.transactiondate as timestamp) &lt; (timestamp('2024-12-31 00:00:00'))) as timestamp), year)  
            then debit - credit 
            else 0 
    end as ytdamount
from
    orders
</code></pre>
<p>Can someone please help?</p>
",-1,0,1,2025-07-03T23:32:33+00:00,1,92,True
79690317,26135876,,sql,Select groups of values that cover a date interval together,"<p>I have a database table with the following relevant columns:</p>
<pre><code>group_id, value, valid_from, valid_to
</code></pre>
<p>None of those columns are unique (this is not the whole table).</p>
<p>Data could look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>group_id</th>
<th>valid_from</th>
<th>valid_to</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>20250201</td>
<td>20250230</td>
</tr>
<tr>
<td>2</td>
<td>20250201</td>
<td>20250230</td>
</tr>
<tr>
<td>2</td>
<td>20250201</td>
<td>20250230</td>
</tr>
<tr>
<td>3</td>
<td>20250201</td>
<td>20250215</td>
</tr>
<tr>
<td>3</td>
<td>20250213</td>
<td>20250220</td>
</tr>
<tr>
<td>3</td>
<td>20250221</td>
<td>20250230</td>
</tr>
<tr>
<td>4</td>
<td>20250101</td>
<td>20250220</td>
</tr>
<tr>
<td>4</td>
<td>20250210</td>
<td>20250215</td>
</tr>
<tr>
<td>4</td>
<td>20250219</td>
<td>20250230</td>
</tr>
<tr>
<td>5</td>
<td>20250101</td>
<td>20250115</td>
</tr>
<tr>
<td>5</td>
<td>20250513</td>
<td>20250619</td>
</tr>
</tbody>
</table></div>
<p>So each group has one or more entries. Every entry has a date interval. The intervals can overlap and are not unique.</p>
<p>Now I have the impossible task to determine all groups which cover a date interval together.</p>
<p>Let's say I need to find all groups that cover the interval 20250201 - 20250230.</p>
<p>With the test data shown, I would expect to find groups 1, 2, 3 and 4.</p>
<p>(None of the rows of group 3 cover the interval by themselves. But they overlap and cover the interval together).</p>
<p>I have to solve this with SQL ONLY but I can use multiple selects.</p>
<p>I'm a SAP Dev and I try to solve this with a HANA AMDP Method / SQLScript but feel free to give me any SQL solution you can come up with.</p>
<p>My idea was to select all of the date intervals that overlap (not cover it completely) with my needed date interval. Then use window function LAG() to select the previous date intervals of the group and check if they overlap, or if there is a gap.</p>
<p>If they overlap, I would take the <code>valid_from</code> of the previous row to widen/enhance the interval. This way I was hoping to create a entry that has the maximum covered interval of the group.</p>
<p>Then I could make one more select and look for entries that cover the whole interval.</p>
<pre><code>tmp_1 = 
SELECT
 group_id,
 CASE
  WHEN valid_from &lt;= ( LAG ( valid_to )
                       OVER ( PARTITION BY group_id
                              ORDER BY group_id, valid_from, valid_to ) + 1 )
   THEN LAG ( valid_to )
        OVER ( PARTITION BY group_id
               ORDER BY group_id, valid_from, valid_to )
 END as valid_from_min,
 CASE
  WHEN valid_to &lt; LAG ( valid_to )
                  OVER ( PARTITION BY group_id
                         ORDER BY group_id, valid_from, valid_to )
   THEN LAG ( valid_to )
        OVER ( PARTITION BY group_id
               ORDER BY group_id, valid_from, valid_to )
 END as valid_to_max
FROM DB_GROUP
WHERE valid_from &lt;= 20250230
  AND valid_to   &gt;= 20250201;

tmp_2 =
SELECT
 group_id
 FROM tmp_1
 WHERE valid_from_min &lt;= 20250201
  AND  valid_to_max   &gt;= 20250230
GROUP BY group_id;
</code></pre>
<p>This only works when a group has less than 3 entries.</p>
<p>Second row <code>valid_from_min</code> will equal <code>valid_from</code> of the first row - good.</p>
<p>Third row <code>valid_from_min</code> will equal <code>valid_from</code> of the second row - bad.</p>
",4,4,0,2025-07-04T14:05:09+00:00,4,354,True
79690387,14882293,,sql,Get previous latest if required latest not available,"<p>I have some data like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>timestamp</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>2025-01-27 10:00:00</td>
</tr>
<tr>
<td>100</td>
<td>2025-01-26 10:00:00</td>
</tr>
<tr>
<td>100</td>
<td>2025-01-25 10:00:00</td>
</tr>
<tr>
<td>100</td>
<td>2024-04-20 10:00:00</td>
</tr>
<tr>
<td>100</td>
<td>2024-03-25 10:00:00</td>
</tr>
<tr>
<td>100</td>
<td>2023-05-05 10:00:00</td>
</tr>
<tr>
<td>100</td>
<td>2022-08-01 10:00:00</td>
</tr>
</tbody>
</table></div>
<p>If I want to see data between <code>start</code> (<code>current Timestamp</code>) and <code>end</code> (<code>2024-04-19 00:00:00</code>) timestamp range (in reverse chronological order), how can I get the previous latest entry for a <code>Id</code> if not available for specified <code>end</code> timestamp.</p>
<p>In this case output should be</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>timestamp</th>
<th>(comment)</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>2025-01-27 10:00:00</td>
<td>start (latest available)</td>
</tr>
<tr>
<td>100</td>
<td>2025-01-26 10:00:00</td>
<td></td>
</tr>
<tr>
<td>100</td>
<td>2025-01-25 10:00:00</td>
<td></td>
</tr>
<tr>
<td>100</td>
<td>2024-04-20 10:00:00</td>
<td>last entry of the selected range</td>
</tr>
<tr>
<td>100</td>
<td>2024-03-25 10:00:00</td>
<td>previous latest entry of end timestamp because there's no row corresponding to the end (2024-04-19 00:00:00)</td>
</tr>
</tbody>
</table></div>
<p>If there are no entries beyond end timestamp, then what ever available should be fine. I need to run this query in Athena.</p>
<p>I am looking for some efficient way of doing it.</p>
<p>I don't want to write a stored procedure to write if statements to check previous latest entries (<code>will do as last option</code>).</p>
<p>Is there any other way of doing it ?</p>
<p>Thanks in advance</p>
",1,1,0,2025-07-04T15:10:47+00:00,4,146,True
79690550,23305162,,sql,Insert record into many-to-many relationship,"<p>I want to create a database for my blog website where I have many posts and categories. A post can have many categories and a category can have many posts. I couldn't figure out how to implement it in PostgreSQL.</p>
<p>This is the <code>category</code> table:</p>
<pre><code>CREATE TABLE category (
   category_name VARCHAR(255) PRIMARY KEY
);
</code></pre>
<p>And this is the <code>Post</code> table:</p>
<pre><code>CREATE TABLE post (
   post_id INTEGER NOT NULL PRIMARY KEY,
   title VARCHAR(255),
   category_name VARCHAR(255) REFERENCES category (category_name)
                 ON UPDATE CASCADE ON DELETE CASCADE,
   date DATE
);
</code></pre>
<p>I find it difficult to <code>INSERT</code> a record.</p>
<p>The following doesn't work:</p>
<pre><code>INSERT INTO post(post_id, title, category_name, date)
VALUES (1, 'How to create a website',['technology','website'], '2025-06-04');
</code></pre>
<p>I could've inserted some data into the <code>category</code> table, but only 1 value works at a time. How do I add multiple values?</p>
",0,2,2,2025-07-04T17:54:16+00:00,2,162,True
79690741,12642829,,sql,SQL capturing group: prioritizing possible?,"<p>Is it possible to order or prioritize something in a capturing group? Like this:</p>
<p><code>SELECT * IN table WHERE name = &quot;(f|o|d)(b|a|z).*&quot; ORDER BY ?;</code></p>
<p>so the result is ordered by the order of the characters inside the capturing groups.
Like:</p>
<pre><code>fbx
fawq
fz0
dz4242
obcrf
oaty
oz4r
</code></pre>
<p>I don't know in beforehand what characters (or strings) might be in the groups or how many groups there are, these are generated on the fly. So the leftmost element in each group has the highest priority. And then the next to the right and so on...</p>
",2,2,0,2025-07-05T01:01:09+00:00,3,148,True
79691469,30959664,,sql,How do I get my SELECT COUNT() statement to count and return the result rows that have no values?,"<p>I have a <code>SELECT</code> statement utilizing <code>JOIN</code> to combine two tables in MySQL. The query I'm attempting to complete is to combine the two tables, <code>INSTRUCTOR</code> and <code>SECTION</code>, and count the number of sections that each instructor has. My current query returns all instructors with the number of queries they have, if they at least one section. There are instructors in the <code>INSTRUCTOR</code> table that do not have any sections in the <code>SECTION</code> table and I need those to be displayed in the table as well. Below is the <code>SELECT</code> statement that I currently have:</p>
<pre><code>SELECT CONCAT(INSTRUCTOR.INSTRUCTOR_ID, ' ', INSTRUCTOR.FIRST_NAME, ' ', INSTRUCTOR.LAST_NAME) AS Instructor,
CONCAT(SUBSTRING(INSTRUCTOR.INSTRUCTOR_SSN, 1, 3), '-', SUBSTRING(INSTRUCTOR.INSTRUCTOR_SSN, 4, 2), '-', SUBSTRING(INSTRUCTOR.INSTRUCTOR_SSN, 6, 4)) AS 'Instructor SSN',
COUNT(SECTION.INSTRUCTOR_ID) AS '# of Sections'
FROM INSTRUCTOR
JOIN SECTION
ON INSTRUCTOR.INSTRUCTOR_ID = SECTION.INSTRUCTOR_ID
GROUP BY SECTION.INSTRUCTOR_ID, '# of Sections';
</code></pre>
<p>I have tried swapping the order of the tables in the statement as well as the different <code>JOIN</code> types.</p>
",0,0,0,2025-07-06T03:17:34+00:00,1,83,True
79693395,901239,,sql,Inconsistent Results for Boolean Filters in Bigquery based on whether the table is aliased,"<p>I'm getting really really inconsistent results, and I can't tell why. Here is the most obvious example:</p>
<p>Two queries:</p>
<pre><code>select * from my_table
where invitation_deleted = true
-- returns 118 results

select * from my_table t
where t.invitation_deleted = true
-- returns 0 results
</code></pre>
<p>But there are other examples! Such as when defining the view in my select statement</p>
<pre><code> CASE
    WHEN my_field IS NULL THEN FALSE
    ELSE TRUE
 END as my_computed_field
...
-- This will allow me to do &quot;where my_computed_field = true&quot; and get results
-- where as this:

my_field IS NOT NULL AS my_computed_field
-- this will not work if I go &quot;where my_computed_field = true&quot;
</code></pre>
<p>There seem to be many flavors of this... where my computed boolean field is extremely finicky, and I can't tell when it will or will not work as expected. Am I fundamentally misunderstanding something about how BigQuery handles booleans?</p>
",1,2,1,2025-07-07T20:37:04+00:00,1,93,False
79693444,13079675,,sql,Does SQL&#39;s IN operator hash its input?,"<p>I'm wondering about the performance of Postgres' <code>IN</code> operator. Specifically, does it hash the values it's passed?</p>
<p>Example:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM table WHERE col IN (1, 2, 3)
</code></pre>
<p>versus</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM table WHERE col in (1, 1, 1, 2, 2, 3, 3, 3, 3)
</code></pre>
<p>Are these two similar in performance? That is, does <code>IN</code> hash both lists its given?</p>
<p>Another example with a subquery (note the <code>DISTINCT</code> keyword):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM table
WHERE col IN (SELECT DISTINCT col FROM another_table WHERE ...)
</code></pre>
<p>Is this any better (or worse) than doing:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM table
WHERE col IN (SELECT col FROM another_table WHERE ...)
</code></pre>
<p>From my understanding:</p>
<ul>
<li>Doing <code>SELECT DISTINCT ...</code> in the subquery is actually extra work, and not performant. I'm not sure why though, I was just told this.</li>
<li>Is it because when we do <code>DISTINCT ...</code> we also construct our own hash table (similar to <code>IN</code>)?</li>
</ul>
",3,3,0,2025-07-07T21:52:00+00:00,2,99,True
79693980,20981111,,sql,Query for every 10th row from a subsection of a whole table,"<p>I have a DB table that contains x million rows and a new row is created regularly (multiple per second).</p>
<p>There is a timestamp column but no date recorded. However, there is a unique ID assigned to each row entry and I'd like to be able to run a select query that allows me to select every 10th row where the unique ID is greater than the one entered so it gives me a snap shot of every 10th record greater than a unique ID value rather than the whole table (so using the unique ID value as a variable)</p>
<p>I have put together the query below but this pulls out every 10th row from the entire table and then gives me the ones where the UNIQUEID is greater than 33188498698899117300014</p>
<pre><code>SELECT ID, TRANSTIMESTAMP, GLOBTIME, REMTIME
from (select ID as rn, t.*
      from TEST_TRANS t
     ) t
where rn - trunc(rn/10)*10 = 0 AND UNIQUEID &gt; '33188498698899117300014'
ORDER BY ID DESC;
</code></pre>
<p>But this isn't exactly what I need as I need every 10th row greater than UNIQUEID '33188498698899117300014'</p>
<p>I'm sure this is straightforward but think I've hit a brick wall so any help would be much appreciated.</p>
",1,1,0,2025-07-08T09:16:53+00:00,1,113,True
79694076,14195596,,sql,SQL query to select data from a column that only has 5 characters,"<p>From column sample I need only data highlighted on green that is the cells or column having only 5 characters and exclude others.</p>
<p>I was trying query as which is not working kindly assist.</p>
<pre><code>select * from dump where Sample left(5); 
</code></pre>
<p><a href=""https://i.sstatic.net/9Qcd3D9K.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9Qcd3D9K.png"" alt=""Sample
1213SRTS
HHHWYER
QA TE1212
AVGLY
AVGLZ"" /></a></p>
<p>As an output I only want data/complete rows that column Sample has AVGLY and AVGLZ i.e. output should only have 5 characters data from column sample.</p>
",-1,2,3,2025-07-08T10:29:45+00:00,2,105,True
79694353,30992996,,sql,How to sum two columns and calculate their average in BigQuery?,"<p>I'm working with Google BigQuery and I have a table with two numeric columns: grade1 and grade2. I want to calculate the total sum of both columns combined (row-wise) and then find the average of those combined totals across all rows.</p>
<p>Here’s a simplified version of my table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>grade1</th>
<th>grade2</th>
</tr>
</thead>
<tbody>
<tr>
<td>183217</td>
<td>76000</td>
<td>52000</td>
</tr>
<tr>
<td>40105</td>
<td>97000</td>
<td>71000</td>
</tr>
<tr>
<td>252976</td>
<td>55000</td>
<td>35000</td>
</tr>
</tbody>
</table></div>
<p>Expected steps:</p>
<ol>
<li>For each row, add grade1 + grade2.</li>
<li>Then calculate the average of these sums across all rows.
What would be the correct SQL syntax in BigQuery to do this?
Any help is appreciated!</li>
</ol>
",0,1,1,2025-07-08T13:47:42+00:00,1,142,True
79695686,31002087,,sql,Consolidate date ranges using Teradata,"<p>I need to take a list of date ranges and consolidate them. In our existing table, sometimes the FROM_DT and TO_DT are the same and the FROM_DT in the following row is 1 day after the previous row's TO_DT. I want to make it just 1 row with the FROM_DT of row 1 and the TO_DT of row 2.</p>
<pre><code>CREATE TABLE ML_ORIGINAL_RANGES
(MEMBER_ID VARCHAR(3),
FROM_DT DATE FORMAT 'yyyy-mm-dd',
TO_DT DATE FORMAT 'yyyy-mm-dd');

INSERT INTO ML_ORIGINAL_RANGES ('001', '2023-08-11', '2023-08-11');
INSERT INTO ML_ORIGINAL_RANGES ('001', '2023-08-12', '2023-08-12');
INSERT INTO ML_ORIGINAL_RANGES ('001', '2023-08-13', '2023-08-15');
INSERT INTO ML_ORIGINAL_RANGES ('001', '2023-08-19', '2023-08-19');
INSERT INTO ML_ORIGINAL_RANGES ('001', '2023-08-20', '2023-08-23');
INSERT INTO ML_ORIGINAL_RANGES ('002', '2023-07-14', '2023-07-14');
INSERT INTO ML_ORIGINAL_RANGES ('002', '2023-07-14', '2023-07-17');

SELECT * FROM ML_ORIGINAL_RANGES ORDER BY MEMBER_ID, FROM_DT, TO_DT;
</code></pre>
<p>With that, I want:</p>
<p><a href=""https://i.sstatic.net/25KyTZM6.png"" rel=""nofollow noreferrer"">Range1</a></p>
<p>to consolidate to:</p>
<p><a href=""https://i.sstatic.net/4a27PraL.png"" rel=""nofollow noreferrer"">Range2</a></p>
",1,1,0,2025-07-09T13:12:11+00:00,2,76,True
79695857,1713450,,sql,"How to join two tables, concat_group, then group by count of that concat_group value?","<p>I have data that looks like this:</p>
<p><code>serialized_book</code>:</p>
<pre><code>id | author | title
---+--------+---------------------------
0  | foo    | i am foo.
1  | bar    | the derivabloviating . . .
...
</code></pre>
<p><code>chapter_pub_date</code>:</p>
<pre><code>id | chapter | year | month | day | book_id
---+---------+------+-------+-----+---------
0  | 1       | 2024 | 12    | 10  | 0
1  | 2       | 2025 | 1     | 5   | 0
...
27 | 1       | 2024 | 12    | 10  | 1
...
</code></pre>
<p>I want a query that yields unique authors-who-published-that-month (I know the data isn't fully normalized since I don't have a separate table for &quot;author&quot;).</p>
<p>So something like this:</p>
<pre><code>month   | unique_author_count
--------+--------------------
2024-12 | 2
2025-1  | 1
...
</code></pre>
<p>The best I've got so far is</p>
<pre class=""lang-sql prettyprint-override""><code>select 
    w.author, concat(p.year, &quot;-&quot;, p.month) as yr_mo 
from 
    serialized_book w
join 
    chapter_pub_date p on w.id = p.book_id
group by 
    w.author, yr_mo
order by 
    yr_mo desc
</code></pre>
<p>This returns a list of every unique <code>(author, year-month-they-published-something)</code>:</p>
<pre><code>author | yr_mo
-------+----------
foo    | 2024-12
foo    | 2025-1
bar    | 2024-12
...
</code></pre>
<p>But I'm stuck going from this to</p>
<pre><code>yr_mo   | count_unique_author_per_yr_mo
--------+-------------------------------
2024-12 | 2
...
</code></pre>
<p>I think it's either going to require some kind of <code>partition</code> or a sub-query, but when I look at examples in the docs and on SO, I can't find any that feature joins, which complicates the sub-query solution.</p>
",2,2,0,2025-07-09T15:04:00+00:00,1,60,True
79695863,28535186,,sql,How to set degree of parallelism dynamically when using explicit cursor?,"<p>In a procedure, I need to set degree of parallelism dynamically using an explicit cursor.</p>
<p>The reason is some customers have a version of the Oracle that throws a bug when parallels using an index.</p>
<p>I'm trying this code:</p>
<pre><code>[...]
DECLARE 
    vLicense NUMBER(5);
    vParallel VARCHAR2(50);

BEGIN
    SELECT LICENSE INTO vLicense FROM LICENSES;

    IF vLicense = 1 THEN    
        vParallel := '/*+ PARALLEL(8) */';
    ELSIF vLicense = 2 THEN
        vParallel := '/*+ PARALLEL(1) */';
    ELSE
        vParallel := '/*+ PARALLEL(4) */';
    END IF;

    DECLARE
   
    CURSOR C_EXAMPLE IS 
    SELECT vParallel COLUMN_A, COLUMN_B FROM TABLE;
[...]
</code></pre>
<p>And this code:</p>
<pre><code>[...]
    DECLARE
   
    EXECUTE IMMEDIATE 'CURSOR C_EXAMPLE IS 
    SELECT ' || vParallel || ' COLUMN_A, COLUMN_B FROM TABLE';
[...]
</code></pre>
<p>Both snippets of code don't work.</p>
",2,2,0,2025-07-09T15:06:06+00:00,2,85,True
79696026,20613378,,sql,"How do I get the last valid (non-null, non-zero) value per day in a time-series SQL query?","<p>I’m working with time-series data in SQL Server and need to retrieve the last valid value for each day. A valid value is defined as one that is non-null and not zero.</p>
<p>The challenge is that data points may continue to be recorded after the last meaningful value of the day — such as system-generated filler values (e.g., 0 or non-null placeholders). For example, if the last real value on a particular day is at 9:46 PM, followed by entries with 0 or null values, I want the 9:46 PM value to be selected as the final data point for that date.</p>
<p>My goal is to write a query that returns exactly one row per day, capturing only the final non-filler value, even if invalid entries follow. I need to apply this across an entire month or year, depending on the input date range.</p>
<pre><code>CREATE OR ALTER PROCEDURE dbo.Weekly_CSS1_LT_Coasting
AS
BEGIN
SET NOCOUNT ON;

    -- Define current month boundaries
    DECLARE @StartDate DATE = DATEFROMPARTS(YEAR(GETDATE()), MONTH(GETDATE()), 1);
    DECLARE @EndDate DATE = DATEADD(MONTH, 1, @StartDate);
    DECLARE @DummyDate DATE = DATEADD(DAY, -1, @StartDate);  -- Last day of previous month (optional row)
    
    -- Generate all dates in current month
    ;WITH DateList AS (
        SELECT @StartDate AS [Date]
        UNION ALL
        SELECT DATEADD(DAY, 1, [Date])
        FROM DateList
        WHERE DATEADD(DAY, 1, [Date]) &lt; @EndDate
    ),
    
    -- Get last NON-NULL value per day
    ValidHistory AS (
        SELECT 
            CAST(H.DateTime AS DATE) AS [Date],
            H.Value,
            H.DateTime,
            ROW_NUMBER() OVER (
                PARTITION BY CAST(H.DateTime AS DATE)
                ORDER BY 
                    CASE WHEN H.Value IS NOT NULL THEN 0 ELSE 1 END,
                    H.DateTime DESC
            ) AS rn
        FROM Runtime.dbo.History H
        WHERE 
            H.TagName = 'CSS1_Streetlight.LT_KWH_PerDay_Costing'
            AND H.wwRetrievalMode = 'Cyclic'
            AND H.wwCycleCount = 100
            AND H.wwQualityRule = 'Extended'
            AND H.wwVersion = 'Latest'
            AND H.DateTime &gt;= @StartDate
            AND H.DateTime &lt; @EndDate
    ),
    
    LastValidValue AS (
        SELECT [Date], Value
        FROM ValidHistory
        WHERE rn = 1
    )
    
    -- Final SELECT: Join directly without extra CTE
    SELECT @DummyDate AS [Date], 0 AS TotalCombinedValue
    UNION ALL
    SELECT 
        D.[Date],
        ISNULL(L.Value, 0) AS TotalCombinedValue
    FROM DateList D
    LEFT JOIN LastValidValue L ON D.[Date] = L.[Date]
    ORDER BY [Date]
    OPTION (MAXRECURSION 0);

END;
</code></pre>
",1,2,1,2025-07-09T17:30:41+00:00,2,106,True
79696464,1567453,,sql,Understanding SQL Update - Sub-Select Grouped Results,"<p>I'm working on migrating a retired data analyst's code AND processes into a repeatable procedure which is run via an interface. The results are important as they relate to WHS so I want to make sure I 100% understand what the SQL is doing when I rebuild the logic/queries.</p>
<p>There are quite a lot of statements which create these statistics but I don't understand the SQL mechanisms for them.
My main questions relate to understanding what the SQL is doing. They look like the following:</p>
<pre><code>-- Count the number of accidents at each site location. Separate count by the severity of the accident.
UPDATE work_location_stats
SET (fatal,
     hostpital,
     medical_treatment,
     minor_injury) =
    (SELECT SUM(DECODE (a.acc_severity, 1, 1, 0)),
            SUM(DECODE (a.acc_severity, 1, 2, 0)),
            SUM(DECODE (a.acc_severity, 1, 3, 0)),
            SUM(DECODE (a.acc_severity, 1, 4, 0))
     FROM accidents a, accident_locations l
     WHERE EXISTS (SELECT 'x'
                   FROM accident_criteria ac
                   WHERE ac.criteria = 1
                     AND a.accident_id = ac.accident_id)
       AND a.accident_id = l.accident_id
       AND work_location_stats.location_id = l.location_id
       AND a.accident_date BETWEEN to_date('01/JUL/2023', 'DD-MON-YYYY') AND to_date('30-JUL-2025', 'DD-MON-YYYY'));
</code></pre>
<p>My questions are:</p>
<ol>
<li>How can <code>SUM</code> be used here in a select statement without a <code>GROUP BY</code> clause?</li>
<li>Can the <code>EXISTS</code> clause turn into a <code>JOIN</code> statement? (I'm concerned about performance)</li>
<li><code>work_location_stats</code> isn't being joined right? My understanding is that it's just processing each row and providing limiting criteria for the aggregate query.</li>
</ol>
<p>I've read tons of documentation over the past few days trying to get some clear explanations and I've tried many SQL statements to play with the data (producing catastrophic differences in results). Since I'm having trouble myself, I'm reaching out for help - any pointers to documentation is appreciated, any information and knowledge you can share as well.</p>
<p>Some information on the tables mentioned if it helps explain the SQL mechanisms.</p>
<p><code>work_location_stats</code> table is a flat table which aggregates statistics about our work locations. For example, # of accidents and their severity at site, productivity numbers and so on.</p>
<p><code>accidents</code> table contains every accident we have recorded and the details around them.</p>
<p><code>accident_locations</code> has location information about the accidents.</p>
<p><code>accident_criteria</code>: accidents are rated and categorised as &quot;accepted&quot;  - if they appear in this table then they should be accounted for.</p>
",0,1,1,2025-07-10T03:49:56+00:00,1,88,True
79696924,21687867,,sql,Can we use the HAVING clause instead of WHERE to filter rows by a column like id in MySQL?,"<p>In MySQL, to fetch a record <code>where id = 1</code>, the common approach is:</p>
<pre><code>SELECT * FROM emp1 WHERE id = 1;
</code></pre>
<p>I’m curious whether it’s possible to use the <code>HAVING</code> clause for the same purpose, like:</p>
<pre><code>SELECT * FROM emp1 HAVING id = 1;
</code></pre>
<p>From what I understand, <code>HAVING</code> is usually used with <code>GROUP BY</code> to filter aggregated results. But in this case, I’m not using any aggregation or <code>GROUP BY</code>.</p>
",-2,0,2,2025-07-10T10:45:47+00:00,3,98,True
79697062,3233405,,sql,Result data that is grouped by year and a column showing previous year average price,"<p>I have simple raw data table containing monthly data for each customer and I desire to have a column in the result data that is grouped by year and a column showing previous year average price.</p>
<p>To obtain all records I use this SQL:</p>
<pre><code>SELECT
    LEFT(D3611_Transaktionsda, 4) AS MYPERIOD,
    D3631_Antal AS MYQTY1,
    D3653_Debiterbart AS SALES,
    D3653_Debiterbart/D3631_Antal AS PRICE
FROM PUPROTRA
WHERE PUPROTRA.D3605_Artikelkod = 'XYZ'
    AND PUPROTRA.D3601_Ursprung = 'O' 
    AND PUPROTRA.D3625_Transaktionsty = 'U'
    AND D3631_Antal &lt;&gt; 0
ORDER BY LEFT(D3611_Transaktionsda, 4)
</code></pre>
<p>Thereafter I group them by MYPERIOD and intruduce SUM() for the other columns to get a SQL and result looking like below:</p>
<pre><code>SELECT
    LEFT(D3611_Transaktionsda, 4) AS MYPERIOD,
    SUM(D3631_Antal) AS MYQTY1,
    SUM(D3653_Debiterbart) AS SALES,
    SUM(D3653_Debiterbart)/SUM(D3631_Antal) AS PRICE
FROM PUPROTRA
WHERE PUPROTRA.D3605_Artikelkod = 'XYZ'
    AND PUPROTRA.D3601_Ursprung = 'O'
    AND PUPROTRA.D3625_Transaktionsty = 'U'
    AND D3631_Antal &lt;&gt; 0
GROUP BY MYPERIOD
</code></pre>
<p><a href=""https://i.sstatic.net/MjhGZipB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MjhGZipB.png"" alt=""Image of grouped result"" /></a></p>
<p>In order to obtain a new column containing the difference between previous year price and current year price I have been trying to use the below SQL, but apparently it doesn't work.</p>
<pre><code>SELECT
    LEFT(D3611_Transaktionsda, 4) AS MYPERIOD,
    SUM(D3631_Antal) AS MYQTY1,
    SUM(D3653_Debiterbart) AS SALES,
    SUM(D3653_Debiterbart)/SUM(D3631_Antal) AS PRICE,
    LAG(SUM(D3653_Debiterbart)/SUM(D3631_Antal)) OVER 
        (PARTITION BY LEFT(D3611_Transaktionsda, 4)) AS PREV_PRICE
FROM PUPROTRA
WHERE PUPROTRA.D3605_Artikelkod = 'XYZ'
    AND PUPROTRA.D3601_Ursprung = 'O'
    AND PUPROTRA.D3625_Transaktionsty = 'U'
    AND D3631_Antal &lt;&gt; 0
GROUP BY MYPERIOD
</code></pre>
<p>I would like the result to look like below but I don't know SQL well enough to achieve this so any help is welcome.</p>
<p><a href=""https://i.sstatic.net/Qh136bnZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qh136bnZ.png"" alt=""Desired result"" /></a></p>
",0,1,1,2025-07-10T12:35:03+00:00,4,112,True
79697129,1510813,"Kochi, India",sql,Performance issue while deleting top 10K data from SQL Server table,"<p>I have 3 million rows in a table, and I need to delete 1.9 million rows with date. I tried the query #1, but it not even deleted one row. When I tried the query #2 by replacing the delete statement in query #1, then it started deleting. Changes are <code>@DeleteBatchSize</code> and <code>@TillDate</code> with hard coded value.</p>
<p>Datatype of <code>createdTimestamp</code> is <code>datetime2</code>.</p>
<p>Primary key is defined in the table.</p>
<p>Query #1:</p>
<pre><code>DECLARE @StartDate DATETIME = GETDATE()
DECLARE @EndDate DATETIME
DECLARE @TillDate DATETIME = '2025-01-01'
DECLARE @RowsAffected BIGINT=0
DECLARE @DeleteBatchSize BIGINT = 10000 --Set the batch size to 1000 so that everytime, we will delete 10000 rows together.
DECLARE @TotalRecordsDeleted BIGINT = 0

SET @RowsAffected = @DeleteBatchSize

SELECT @RowsAffected,@DeleteBatchSize
WHILE (@RowsAffected = @DeleteBatchSize)
BEGIN
    DELETE TOP(@DeleteBatchSize) FROM [dbo].[Transaction] 
    WHERE [createdTimestamp] &lt; @TillDate

    SET @RowsAffected = @@ROWCOUNT
    SET @TotalRecordsDeleted = @TotalRecordsDeleted + @RowsAffected

    PRINT 'TOTAL ROWS DELETED : ' + CAST(@TotalRecordsDeleted AS VARCHAR)
END

SET @EndDate = GETDATE()
PRINT 'Time taken to execute the script (in Min) : ' + CAST(DATEDIFF(MINUTE,@StartDate,@EndDate) as VARCHAR)
</code></pre>
<p>Query #2:</p>
<pre><code>DELETE TOP(10000) FROM [dbo].[Transaction] 
WHERE [createdTimestamp] &lt; '2025-01-01'
</code></pre>
<p>Note: I tried on another database, with less data, and did not face any issue.</p>
<p>Please help me understand why the data is not deleted when using the parameters.</p>
",0,0,0,2025-07-10T13:23:55+00:00,2,190,False
79697304,31011946,,sql,Calculate the calculated cumulative sum without recursion,"<p>I am trying to calculate the column F1, but it is based on summing up its own previous value. I have calculated the intermediate column F which has the value for the first record.</p>
<p>The second record has to be derived by summing up the first record, which works with the window function.</p>
<p>0.87889069 (E) - 0.80022360(F1) = 0.07866708</p>
<p>But the third row has to be a sum of previous two values. It fails here since, I am unable to calculate the running total for a calculated value in that batch.</p>
<p>Expected calculation:<br />
0.89611630(E) - (0.80022360+0.07866708) (F1) = 0.01722561</p>
<p>The fourth records should be:<br />
0.91105699(E) - (0.80022360+0.07866708+0.01722561)(F1)  = 0.01494068</p>
<p>Can this be achieved without recursion?</p>
<pre><code>SELECT A,B,C,D,E,F,
    E - SUM(F) OVER(PARTITION BY A,B,C ORDER BY D) AS F1
FROM  CTE
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col A</th>
<th>Col B</th>
<th>Col C</th>
<th>Col D</th>
<th>Col E</th>
<th>Col F</th>
<th>Col F1</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0.80022360</td>
<td>0.80022360</td>
<td>0.80022360</td>
<td>0.80022360</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>0.87889069</td>
<td>0</td>
<td>0.07866708</td>
<td>0.07866708</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>3</td>
<td>0.89611630</td>
<td>0</td>
<td>0.09589270</td>
<td>0.01722561</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>4</td>
<td>0.91105699</td>
<td>0</td>
<td>0.11083338</td>
<td>0.01494068</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>5</td>
<td>0.92868688</td>
<td>0</td>
<td>0.12846327</td>
<td>0.01762989</td>
</tr>
</tbody>
</table></div>
<p>I tried using window function, but it is calculating only the second row and unable to add up the previously calculated value.</p>
<p>Please let me know if this can be achieved in pyspark?</p>
<p>Input Table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col A</th>
<th>Col B</th>
<th>Col C</th>
<th>Col D</th>
<th>Col E</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0.80022360</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>0.87889069</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>3</td>
<td>0.89611630</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>4</td>
<td>0.91105699</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>5</td>
<td>0.92868688</td>
</tr>
</tbody>
</table></div>
<p>Output Table</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col A</th>
<th>Col B</th>
<th>Col C</th>
<th>Col D</th>
<th>Col E</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0.80022360</td>
<td>0.80022360</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>0.87889069</td>
<td>0.07866708</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>3</td>
<td>0.89611630</td>
<td>0.01722561</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>4</td>
<td>0.91105699</td>
<td>0.01494068</td>
</tr>
<tr>
<td>Base</td>
<td>3</td>
<td>1</td>
<td>5</td>
<td>0.92868688</td>
<td>0.01762989</td>
</tr>
</tbody>
</table></div>
<p>Logic to derive 'Required' column:</p>
<ol>
<li>If Col D = 1 THEN Col E</li>
<li>If Col D &lt;&gt; 1 THEN Col E - sum of previous 'Required' Column values</li>
</ol>
<p>*have explained the calculation above.</p>
",2,2,0,2025-07-10T15:39:46+00:00,2,108,True
79697374,15027137,,sql,"Get Counts, then Group By Values","<p>I have a table in my Oracle SQL database as follows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>DATE_A</th>
<th>DATE_B</th>
</tr>
</thead>
<tbody>
<tr>
<td>07-2025</td>
<td>07-2025</td>
</tr>
<tr>
<td>05-2025</td>
<td>05-2025</td>
</tr>
<tr>
<td>03-2025</td>
<td>03-2025</td>
</tr>
<tr>
<td>01-2025</td>
<td>02-2025</td>
</tr>
<tr>
<td>01-2025</td>
<td>01-2025</td>
</tr>
<tr>
<td>01-2025</td>
<td>01-2025</td>
</tr>
<tr>
<td>01-2025</td>
<td>03-2025</td>
</tr>
<tr>
<td>04-2025</td>
<td>03-2025</td>
</tr>
</tbody>
</table></div>
<p>I'd like to count the number of times that a date shows up in a group, to get a result as follows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">DATE</th>
<th style=""text-align: left;"">COUNT_A</th>
<th style=""text-align: left;"">COUNT_B</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">07-2025</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">05-2025</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">04-2025</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">03-2025</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">02-2025</td>
<td style=""text-align: left;"">0</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">01-2025</td>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">2</td>
</tr>
</tbody>
</table></div>
<p>How might I accomplish this as efficiently as possible?</p>
",2,3,1,2025-07-10T16:49:24+00:00,4,111,True
79697557,6319675,"Lima, Peru",sql,Confluent Cloud - Kafka Sink Postgres - type UUID,"<p>I am using CC and Postgres Sink connector.</p>
<p>I am facing the issue:</p>
<blockquote>
<p>Sink Connector for PostgreSQL fails to delete record if key type is UUID</p>
</blockquote>
<blockquote>
<p>java.sql.SQLException: Exception chain:<br />
java.sql.BatchUpdateException:<br />
Batch entry 0 DELETE FROM &quot;my_table&quot; WHERE &quot;my_id&quot; = ('e9157f76-543f-5230-b62f-807f3ea3af22') AND &quot;start_ts&quot; = ('2025-07-05 05:53:23.408+00')<br />
was aborted: ERROR: operator does not exist: uuid = character varying<br />
Hint: No operator matches the given name and argument types. You might need to add explicit type casts.</p>
<p>Position: 93  Call getNextException to see other errors in the batch.</p>
</blockquote>
<p>I found more people facing the same, <a href=""https://github.com/confluentinc/kafka-connect-jdbc/issues/1084"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Can't figure out how I can fix it. Any clues?</p>
",-1,0,1,2025-07-10T20:12:20+00:00,1,67,False
79697654,16817823,,sql,Make repeated expressions in a select query more efficient,"<p>I am trying to do a time series analysis project to learn more and test my own skills. For this time series analysis, I want to show moving averages and percent differences over the periods, daily, monthly, and annually.</p>
<p>I am doing this by making a temporary table containing a LAG function to get values to do the arithmetic for the moving averages and percent differences. I believe the issue of efficiency is from referencing the table too many times in the select query, and that's causing it to take forever to compile. Along with a large case function that is repeated three times. I am not the most familiar with SQL, but is there any way I can use recursion or make a function to call upon for a repeated expression?  Any help to make this more efficient is greatly appreciated!
I am also using MySQL for this project.</p>
<p>Code attached below.</p>
<pre><code>WITH debt_lag AS(
         SELECT *, LAG(`Debt Held by the Public`, 7) OVER (ORDER BY `Record Date`)   AS week_b4_dhpb,
                 LAG(`Debt Held by the Public`, 7) OVER (ORDER BY `Record Date`)   AS month_b4_dhpb,
                 LAG(`Debt Held by the Public`, 365) OVER (ORDER BY `Record Date`) AS year_b4_dhpb,
                 LAG(`Intragovernmental Holdings`, 7) OVER (ORDER BY `Record Date`)   AS week_b4_ig,
                 LAG(`Intragovernmental Holdings`, 7) OVER (ORDER BY `Record Date`)   AS month_b4_ig,
                 LAG(`Intragovernmental Holdings`, 365) OVER (ORDER BY `Record Date`) AS year_b4_ig,
                 LAG(`Total Public Debt Outstanding`, 7) OVER (ORDER BY `Record Date`)   AS week_b4_tdpo,
                 LAG(`Total Public Debt Outstanding`, 7) OVER (ORDER BY `Record Date`)   AS month_b4_tdpo,
                 LAG(`Total Public Debt Outstanding`, 365) OVER (ORDER BY `Record Date`) AS year_b4_tdpo

          FROM DebtPenny_19930401_20250623
)

SELECT `Record Date`, (((`Debt Held by the Public` - week_b4_dhpb) / week_b4_dhpb )* 100) AS DHPB_percent_change_weekly,
      ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),3) AS DHPB_WEEKLY_MOVING_AVERAGE,
       (((`Debt Held by the Public` - month_b4_dhpb) / month_b4_dhpb )* 100) AS DHPB_percent_change_monthly, 
CASE
    WHEN SUBSTRING(`Record Date`,6,2) = '01' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '02' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 27 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '03' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '04' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '05' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '06' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '07' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '08' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '09' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '10' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '11' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '12' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    
END AS DHPB_MONTHLY_MOVING_AVERAGE,
       (((`Debt Held by the Public` - year_b4_dhpb) / year_b4_dhpb )* 100) AS DHPB_percent_change_annual,
       ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 364 PRECEDING AND CURRENT ROW),3) AS DHPB_ANNUAL_MOVING_AVERAGE,
       

       (((`Intragovernmental Holdings` - week_b4_ig) / week_b4_ig )* 100) AS IG_percent_change_weekly,
      ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),3) AS IG_WEEKLY_MOVING_AVERAGE,
       (((`Intragovernmental Holdings` - month_b4_ig) / month_b4_ig)* 100) AS IG_percent_change_monthly, 
CASE
    
    WHEN SUBSTRING(`Record Date`,6,2) = '01' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '02' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 27 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '03' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '04' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '05' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '06' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '07' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '08' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '09' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '10' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '11' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '12' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    
END AS IG_MONTHLY_MOVING_AVERAGE,

       (((`Debt Held by the Public` - year_b4_ig) / year_b4_ig )* 100) AS IG_percent_change_annual,
       ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 364 PRECEDING AND CURRENT ROW),3) AS IG_ANNUAL_MOVING_AVERAGE,


       (((`Total Public Debt Outstanding` - week_b4_tdpo) / week_b4_tdpo )* 100) AS TDPO_percent_change_weekly,
      ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),3) AS TDPO_WEEKLY_MOVING_AVERAGE,
       (((`Total Public Debt Outstanding` - month_b4_ig) / month_b4_ig)* 100) AS TDPO_percent_change_monthly, 
CASE
    
    WHEN SUBSTRING(`Record Date`,6,2) = '01' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '02' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 27 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '03' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '04' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '05' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '06' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '07' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '08' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '09' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '10' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '11' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '12' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    
END AS TDPO_MONTHLY_MOVING_AVERAGE,

       (((`Total Public Debt Outstanding` - year_b4_tdpo) / year_b4_tdpo )* 100) AS TDPO_percent_change_annual,
       ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 364 PRECEDING AND CURRENT ROW),3) AS TDPO_ANNUAL_MOVING_AVERAGE
     
FROM debt_lag;
</code></pre>
",1,1,0,2025-07-10T22:32:09+00:00,2,107,True
79697920,13948791,,sql,Changing data structure from &quot;Date&quot; to &quot;From_Date&quot; and &quot;To_Date&quot; based on ID and Flag,"<p>I have IDs, their as at date (end of month format), and their corresponding flags. I only care if the flag is Y.</p>
<p>It is in the following format:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>as_at_date</th>
<th>Enabled_Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>31/01/2025</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>28/02/2025</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>31/03/2025</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>30/06/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>31/01/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>28/02/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>31/03/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>31/04/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>30/06/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>30/09/2025</td>
<td>Y</td>
</tr>
</tbody>
</table></div>
<p>As you can see, ID 1's flag is enabled from Jan-March. Because it does not have entries for April and May, it is disabled in those months, but is enabled again in June. Similar for ID 2.</p>
<p>I want to make the data in the following format using SQL (Teradata SQL preferably but any SQL is workable). Where the to_date is &quot;01/01/3000&quot;, it is to indicate it is the current/most recent record.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>from_date</th>
<th>to_date</th>
<th>Enabled_Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>31/01/2025</td>
<td>31/03/2025</td>
<td>Y</td>
</tr>
<tr>
<td>1</td>
<td>30/06/2025</td>
<td>01/01/3000</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>31/01/2025</td>
<td>31/04/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>30/06/2025</td>
<td>30/06/2025</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>30/09/2025</td>
<td>01/01/3000</td>
<td>Y</td>
</tr>
</tbody>
</table></div>
<p>Using a MIN()/MAX() on the data doesn't work because it'll just take the MAX() date but doesn't indicate to me if at any point an ID left the 'Y' population at any time.
Please help me</p>
",2,2,0,2025-07-11T06:28:24+00:00,3,138,True
79697943,30614229,,sql,Regex substring SQL,"<p>Can you please help me with this regex substr.  Thanks</p>
<p>Case 1: extract string between “/” and “-”</p>
<pre><code>SELECT REGEXP_SUBSTR(‘XXXXX-XXX#8#FF###TTTTTTT/SM123456789-JSMILEGGM 1.1####’, ‘[^/-]+$’) new_str 

—- Expected output “SM123456789”
</code></pre>
<p>Case 2: extract string after the first occurrence “/”</p>
<pre><code>SELECT REGEXP_SUBSTR(‘XXXXX-XXX#8#FF###TTTTTTT/SM123456789/JSMILEGGM 1.1####’, ‘[^/]+’) new_str 

-– Expected output “SM123456789/JSMILEGGM 1.1####”
</code></pre>
",-1,1,2,2025-07-11T06:57:23+00:00,1,78,True
79698143,8725180,,sql,Merging SCD-2 intervals from two tables,"<p>I have two historical (SCD-2) tables, that should be merged in a new one as shown below, using HiveQL.
<strong>The earliest dates does match on both tables!</strong></p>
<p><strong>1st table</strong></p>
<pre><code>PK  var1    start_dt    end_dt
123 false   2010-01-15  2015-01-15
123 true    2015-01-16  2025-05-02
123 false   2025-05-03  9999-12-31
456 false   2021-02-01  2025-02-01
</code></pre>
<p><strong>2nd table</strong></p>
<pre><code>PK  var2    start_dt    end_dt
123 AB      2010-01-15  2015-05-27
456 DE      2021-02-01  2025-02-01
456 CF      2025-02-02  9999-12-31
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>PK  var1    var2    start_dt    end_dt
123 false   AB      2010-01-15  2015-01-15
123 true    AB      2015-01-16  2025-05-02
123 false   AB      2025-05-03  2015-05-27
123 false   NULL    2025-05-28  9999-12-31
456 false   DE      2021-02-01  2025-02-01
456 NULL    CF      2025-02-02  9999-12-31
</code></pre>
<p>In this new table, overlaps of the original tables have spit as many rows as there are different combinations.<br />
e.g. for PK 123, period 2010-01-15 to 2015-05-27 from the table 2 (with <code>var2 = 'AB'</code>) overlaps the 3 periods from table 1 (with <code>var1</code> going <code>false</code>, then <code>true</code>, then <code>false</code>), so we have 3 periods in the resulting table (<code>false, 'AB'</code>, then <code>true, 'AB'</code>, then <code>false, 'AB'</code>), plus an additional period with <code>false, null</code> for the last period of table 1 continuing after 2015-05-27 (end of period from table 2).</p>
<p>Any ideas how to make this using HiveQL?</p>
<p>Here is the query I use in order to join the tables:</p>
<pre><code>with t1 as (
select 
    *,
    min(start_dt) over (partition by PK) as min_start_dt,
    max(end_dt) over (partition by PK) as max_end_dt
from table1
),
t2 as (
select 
    *,
    min(start_dt) over (partition by PK) as min_start_dt,
    max(end_dt) over (partition by PK) as max_end_dt
from table2
)
select *
from t1 
full join t2 on t1.PK=t2.PK and t1.min_start_dt = t2.min_start_dt and (t1.end_dt &lt;= t2.max_end_dt OR t2.end_dt &lt;= t1.max_end_dt)
</code></pre>
",2,2,0,2025-07-11T09:54:33+00:00,1,87,True
79698313,31018601,,sql,Apply a conditional rank to a temporary table,"<p>Here's an idea of what the temporary table contains:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>FullRank</th>
<th>Special</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>Yes</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>No</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>No</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>Yes</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>Yes</td>
</tr>
<tr>
<td>3</td>
<td>4</td>
<td>No</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>No</td>
</tr>
</tbody>
</table></div>
<p>And this is what I'm trying to achieve...</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>FullRank</th>
<th>Special</th>
<th>SpecialRank</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>No</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>Yes</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>No</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Yes</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>No</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>No</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>Yes</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>Yes</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>4</td>
<td>No</td>
<td>3</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>No</td>
<td>4</td>
</tr>
</tbody>
</table></div>
<p>Essentially, I want to insert another rank ('SpecialRank') that ranks from the first instance of 'Special' for an ID, based on the highest 'FullRank'.</p>
<p>I've tried variations of the following but without success:</p>
<pre><code>UPDATE #SpecialTemp AS STa
INNER JOIN (SELECT MIN(STb.FullRank) STb.ID
FROM #SpecialTemp AS STb
GROUP BY STb.ID
) AS b
ON STa.ID = b.ID
 AND STa = 'Yes'
SET STa.SpecialRank = 'Yes'
</code></pre>
<p>I'm getting some syntax errors with this, and unsure if I'm taking the right approach with this. Thanks!</p>
",2,2,0,2025-07-11T12:07:38+00:00,2,90,True
79698322,22722326,,sql,PostgreSQL Force Index Scan,"<p>I have a table with below columns in PostgreSQL.</p>
<pre><code>id,
location,
store,
flag1,
flag2
</code></pre>
<p>Here id, location, store are contributing to Primary Key. So by default postgreSQL creates an index on it.</p>
<p>I have created two more index apart from primary key index as below :</p>
<ol>
<li>id</li>
<li>location, id</li>
</ol>
<p>Now when I run the below query :</p>
<pre><code>EXPLAIN ANALYZE
SELECT *
FROM my_table
WHERE location = '1' AND id = '1';
</code></pre>
<p>It should have used the index with <code>location, id</code>, but it uses primary key index.</p>
<p>Now when I run below query, it uses <code>id</code> index, which is expected.</p>
<pre><code>EXPLAIN ANALYZE
SELECT *
FROM my_table
WHERE id = '1';
</code></pre>
<p>I am not sure on how can I force the indexing or the behaviour of PostgreSQL is correct &amp; efficient one. On paper, it looks like <code>location, id</code> would be better choice for the query.</p>
<p>Can some help on the issue/doubt I have ?
Also if this is the expected behaviour, can you provide some query which will use <code>location,id</code> index ?</p>
",0,1,1,2025-07-11T12:17:26+00:00,1,94,False
79698797,25474027,,sql,Dynamically loading row header,"<p>Here is the task description for MySQL:</p>
<p>Table called clients with columns id, mac_address</p>
<p>Table called streams with columns client_id, title, quality (Enum : 720p, 1080p, 2160p, 4320p), traffic</p>
<p>The user has provided an image containing a request to create a query for an online streaming service. The query should return a list of total stream traffic in HD and UHD quality for each client.</p>
<p>Here's a breakdown of the requirements:</p>
<p><strong>Output Columns:</strong></p>
<pre><code>quality: Quality group name (&quot;HD&quot; or &quot;UHD&quot;).

%client_1%...%client_N%: Traffic data for a specific client.

Column name format: MAC address of the client.

Columns sorted in ascending order by MAC address.

Record format: ## (##) where:

First ##: Total traffic amount for a specific quality group, rounded to two decimal places, with unit (MiB or GiB).

Second ## (in parentheses): Total number of streams for a specific quality group and client.
</code></pre>
<p><strong>Definitions:</strong></p>
<p>UHD quality group: 2160p and above.</p>
<p>1 MiB = 1024^2 bytes.</p>
<p>1 GiB = 1024^3 bytes.</p>
<p><strong>Expected Output</strong></p>
<pre><code> quality    | 9F-A1-2C-BD-55-77 | AB-4E-79-10-AC-22 | CF-91-08-DE-43-A2
---------------------------------------------------------------------
HD         | 2.11 GiB (2)      | 3.76 GiB (5)      | 1.89 GiB (3)
UHD        | 6.23 GiB (4)      | 975.45 MB (2)     | 4.12 GiB (3)
</code></pre>
<p>I am not clear how to select the mac_address as part of row header, till now I worked only on how to select the columns as row values.</p>
<p>What is the correct way to solve this problem?</p>
",-1,0,1,2025-07-11T20:23:40+00:00,1,67,False
79698938,1100221,,sql,"SQL Query to sum 2 columns from 1 table, and subtract result from a column in a different table","<p>I have 2 tables (not related, no primary/foreign keys, no nulls in any column)</p>
<p>Table: MonthlyEarnings</p>
<pre><code>| Month | Year | DebitAmount | CreditAmount |
| ----- | ---- | ----------- | ------------ |
| July  | 2025 | 0           | 250.45       |
| July  | 2025 | 0           | 468.35       |
| July  | 2025 | 110         | 0            |
| July  | 2025 | 0           | 130.87       |
| July  | 2025 | 28          | 0            |
</code></pre>
<p>Table: StartOfMonthBalance</p>
<pre><code>| Month | Year | BalanceAmount |
| ----- | ---- | ------------- |
| July  | 2025 | 35608.65      |
</code></pre>
<p>I want to write a query in SQL that returns the starting balance from StartOfMonthBalance, total expenses, total earnings from MonthlyEarnings and the ending balance calculated as starting balance + total earnings - total expenses.</p>
<p>The query I have in MS Access is as below:</p>
<pre><code>SELECT
    SUM(M.DebitAmount) AS TotalExpenses, 
    SUM(M.CreditAmount) AS TotalEarnings, 
    (SELECT S.BalanceAmount FROM StartOfMonthBalance S WHERE S.Month = &quot;July&quot; AND S.Year = 2025) AS StartingBalance, 
    (StartingBalance + TotalEarnings - TotalExpenses) AS EndingBalance 
FROM MonthlyEarnings M
WHERE M.Month = &quot;July&quot; AND M.Year = 2025
</code></pre>
<p>How can I write a similar query for SQLite?</p>
<p>When I try, this is the error I get:</p>
<blockquote>
<p>Execution finished with errors. Result: no such column: StartingBalance At line 1: SELECT SUM(M.DebitAmount) AS TotalExpenses, SUM(M.CreditAmount) AS TotalEarnings, (SELECT S.BalanceAmount FROM StartOfMonthBalance S WHERE S.Month = &quot;July&quot; AND S.Year = 2025) AS StartingBalance, (StartingBalance + TotalEarnings - TotalExpenses) AS EndingBalance FROM MonthlyEarnings M WHERE M.Month = &quot;July&quot; AND M.Year = 2025</p>
</blockquote>
",-1,0,1,2025-07-12T01:16:43+00:00,0,37,False
79698939,20906397,California,sql,Can HAVING clause conditions use aliases for the columns?,"<p>I am preparing for the Oracle database test, and I saw this choice from practice tests <a href=""https://examsnet.com/test/oracle-database-sql-12c-1z0-071-exam-practice-test-1/1"" rel=""nofollow noreferrer"">online</a>:</p>
<blockquote>
<p>The HAVING clause conditions can use aliases for the columns.</p>
</blockquote>
<p>And this choice is wrong. However, I wrote the following example:</p>
<pre><code>CREATE TABLE ALIASESTEST (AGE INT);

INSERT INTO ALIASESTEST VALUES (1),(1),(1),(10),(2),(5),(2);

SELECT age, COUNT(*) AS stu_age_count
FROM ALIASESTEST
GROUP BY age
HAVING stu_age_count &gt; 1;
</code></pre>
<p>It used aliases <code>stu_age_count</code> in the <code>HAVING</code> clause, while it also successfully executed and outputted the result (in both MySQL and Oracle SQL).</p>
<p>In this case, why that statement is wrong? Can HAVING clause conditions use aliases for the columns?</p>
",1,2,1,2025-07-12T01:19:04+00:00,1,80,True
79700435,16731543,,sql,SQL to list out duplicate value within different values excluding a combination,"<p>I am trying to write a query in Oracle SQL where same order might have multiple modes and I want list out the orders which only has T and I mode including duplicates. If order is duplicate which has mode O and T both then I want to exclude them it should not come it output.
For example in the below table, AAWER order has duplicate values with mode O in it, SQL out put should exclude complete AAWER having O and T mode.</p>
<p>Output should be only TTRTW / RRRRE &amp; RRRRE.</p>
<p>Sample data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Order</th>
<th>Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td>AAWER</td>
<td>O</td>
</tr>
<tr>
<td>AAWER</td>
<td>T</td>
</tr>
<tr>
<td>TTRTW</td>
<td>T</td>
</tr>
<tr>
<td>RRRRE</td>
<td>T</td>
</tr>
<tr>
<td>RRRRE</td>
<td>I</td>
</tr>
</tbody>
</table></div>
<p>Output I want as</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ORDER_ID</th>
<th>ORDER_MODE</th>
</tr>
</thead>
<tbody>
<tr>
<td>TTRTW</td>
<td>T</td>
</tr>
<tr>
<td>RRRRE</td>
<td>T</td>
</tr>
<tr>
<td>RRRRE</td>
<td>I</td>
</tr>
</tbody>
</table></div>
<pre><code>SELECT &quot;Order&quot;, &quot;Mode&quot;
FROM your_table
WHERE &quot;Order&quot; IN (
    SELECT &quot;Order&quot;
    FROM your_table
    GROUP BY &quot;Order&quot;
    HAVING 
        COUNT(DISTINCT CASE WHEN &quot;Mode&quot; IN ('T', 'I') THEN &quot;Mode&quot; END) &gt;= 1
        AND COUNT(DISTINCT CASE WHEN &quot;Mode&quot; NOT IN ('T', 'I') THEN &quot;Mode&quot; END) = 0
)
ORDER BY &quot;Order&quot;;
</code></pre>
<p>Getting error in line 4</p>
",4,4,0,2025-07-14T04:27:48+00:00,3,128,True
79700457,23253271,,sql,Get random rows in interval in T-SQL,"<p>I need to select 10 to 20 random rows from a table. (I only can find how to select just N random rows). I need to get something like this</p>
<pre><code>SELECT *
FROM table
WHERE ABS(CHECKSUM(NewId())) % 19 BETWEEN 10 and 20
</code></pre>
<p>I did not find a duplicate of my question.</p>
",-1,2,3,2025-07-14T05:18:10+00:00,1,190,True
79701027,4793550,,sql,Adding a column with default value incorrectly fills a different column,"<p>After executing the following script:</p>
<pre><code>alter table test_table 
add (test1 varchar2(1 char),
     test2 varchar2(1 char) default 'T' not null);
</code></pre>
<p>I got interesting results... both columns are filled with 'T'. Default value is correctly added to &quot;test2&quot; column. &quot;test1&quot; column correctly has no default value.</p>
<p>Also, executing this script:</p>
<pre><code>alter table test_table 
add (test1 varchar2(1 char),
     test2 varchar2(1 char),
     test3 varchar2(1 char) default 'T' not null);
</code></pre>
<p>resulting in &quot;test1&quot; and &quot;test3&quot; columns filled with 'T'.</p>
<p>Could anyone explain to me why this is happening?</p>
<p>I'm using SQLDeveloper to execute scripts.</p>
",1,1,0,2025-07-14T14:27:13+00:00,1,77,True
79701246,2703209,,sql,Set-returning functions are not allowed in WHERE,"<p>I am accessing a PostgreSQL database form Java using JDBC.</p>
<p>I would like to return rows from a table <code>messages</code> where <code>id</code> matches one of multiple values, which are determined in Java code. The list of IDs to select is not anywhere in the database, and the number of IDs may vary (can be as many as 300).</p>
<p>I had this working with HSQLDB, where I would run a parametrized query</p>
<pre><code>select * from message where id in (unnest(?))
</code></pre>
<p>and populate the parameter with a <code>java.sql.Array</code> of <code>String</code> values matching the IDs I need.</p>
<p>With PostgreSQL I get an <code>org.postgresql.util.PSQLException</code> exception saying</p>
<blockquote>
<p>ERROR: set-returning functions are not allowed in WHERE</p>
</blockquote>
<p>What is the correct way to write this statement so it will work on PostgreSQL, preferably with minimal changes to the surrounding Java code?</p>
",2,2,0,2025-07-14T18:26:17+00:00,4,182,True
79701279,5477223,"Narvon, PA, USA",sql,Update four fields of current spreadsheet with info from a SQL query - error #1004 &quot;Application-defined or Object-defined error&quot;,"<p>I'm trying to populate four fields with metadata of a PDF. The query works fine returning one instance with the four fields, I can access the current row of the spreadsheet confirming I'm looking at the right one.</p>
<p>I can pop up a msgbox with the four fields; <strong>BUT</strong> when trying to receive the fields using my normal method, an unspecified error occurred.</p>
<pre><code>wsFix.Range(&quot;G&quot; &amp; vRow).CopyFromRecordset recs
</code></pre>
<p>Next, tried the four fields individually and could display the values in a MsgBox.</p>
<p>When trying to insert the value as a string into the worksheet received the Application-defined or Object-defined error.</p>
<p>Any insight would be appreciated (I already looked at a number of the suggested answers)</p>
<pre><code>Function FillDD(DocID As String,) As String        '----work in progress
    Dim wbMacro As Workbook, wsFix As Worksheet
    Dim conn As ADODB.Connection, recs As ADODB.Recordset
    Dim vTestField As String
    Set conn = New ADODB.Connection
    Set recs = New ADODB.Recordset
    vx = ActiveWorkbook.Name
    Set wbMacro = Workbooks(vx)
    Set wsFix = wbMacro.Sheets(&quot;DocFixes&quot;)
    'This sets vRow to the row the function is being called from
    vRow = Application.ThisCell.Row
    'This grabs the a value of the prior row: In testing confirmed it returned the expected value of the prior row.
    vTestField = wsFix.Range(&quot;G&quot; &amp; vRow - 1).Value
    vServer = &quot;&lt;Server&gt;&quot;
    vDatabase = &quot;&lt;DB&gt;&quot;
    'Query returns four fields related to the DocID
    vQuery = &quot;&lt;SELECT &amp; FROM clauses WHERE DocumentID = '&quot; &amp; DocID &amp; &quot;'&quot;
    On Error GoTo EH
    vQuery = &quot;SET NOCOUNT ON &quot; &amp; Replace(Replace(vQuery, Chr(13), &quot;&quot;), Chr(10), &quot; &quot;)
    conn.ConnectionString = &quot;DRIVER=SQL Server;SERVER=&quot; &amp; vServer &amp; &quot;;Trusted_Connection=Yes;APP=Microsoft Office 2016;DATABASE=&quot; &amp; vDatabase
    conn.Open
    recs.CursorType = adOpenKeyset
    recs.Open vQuery, conn
EH:
    If 0 &lt; Len(Err.Description) Then MsgBox &quot;Error#: &quot; &amp; Err.Number &amp; &quot; Description: &quot; &amp; Err.Description
    'wsFix.Range(&quot;G&quot; &amp; vRow).CopyFromRecordset recs    &lt;--  '-2147467259': Automation error: Unspecified error
    vTestField = recs.Fields.Item(0).Value '&lt;-- returned expected value
    wsFix.Range(&quot;G&quot; &amp; vRow).CopyFromRecordset recs
    If Len(vTestField) = 9 Then
        vx = &quot;'&quot; &amp; vTestField
        vTestField = recs.Fields.Item(1).Value
        vx = vx &amp; &quot;,'&quot; &amp; vTestField
        vTestField = recs.Fields.Item(2).Value
        vx = vx &amp; &quot;,'&quot; &amp; vTestField
        vTestField = recs.Fields.Item(3).Value
        vx = vx &amp; &quot;,'&quot; &amp; vTestField
        'MsgBox worked
        'MsgBox vx
        'Next statement causes FillDD to fail on Error 1004 &quot;Application-defined or Object-defined error&quot;
        wsFix.Range(&quot;G&quot; &amp; vRow).Value = vx
        'wsFix.Range(&quot;H&quot; &amp; vRow).Value = recs.Fields.Item(1).Value
        'wsFix.Range(&quot;I&quot; &amp; vRow).Value = recs.Fields.Item(2).Value
        'wsFix.Range(&quot;J&quot; &amp; vRow).Value = recs.Fields.Item(3).Value
        FillDD = &quot;Success&quot;
    Else
        FillDD = &quot;Fail&quot;
    End If
    recs.Close
    conn.Close
    Set conn = Nothing
    Set recs = Nothing
End Function
</code></pre>
",0,0,0,2025-07-14T19:19:39+00:00,1,70,False
79701599,23512643,,sql,Count number of new colors in each year,"<p>I have this table in SQL:</p>
<pre><code>CREATE TABLE myt (
  color TEXT,
  year INTEGER
);

INSERT INTO myt (color, year) VALUES
  ('red',    2000),
  ('blue',   2000),
  ('green',  2000),
  ('cyan',   2000),
  ('white',  2000),

  ('blue',   2001),
  ('green',  2001),
  ('yellow', 2001),
  ('black',  2001),
  ('white',  2001),

  ('green',  2002),
  ('yellow', 2002),
  ('orange', 2002),
  ('black',  2002),
  ('pink',   2002),

  ('yellow', 2003),
  ('orange', 2003),
  ('purple', 2003),
  ('pink',   2003),
  ('grey',   2003);
</code></pre>
<p>In each pair of consecutive years, I want to find out: how many total colors, how many new colors compared to last year, how many colors dropped out from last year, how many remaining colors from last year.</p>
<p>I can only think of a way to do this where I explicitly make all year transitions (i.e. see the CTE):</p>
<pre><code>WITH distinct_colors AS (
      SELECT DISTINCT color, year FROM myt
  ),
  year_pairs AS (
      SELECT 2001 AS current_year, 2000 AS previous_year
      UNION ALL
      SELECT 2002, 2001
      UNION ALL
      SELECT 2003, 2002
  ),
  curr_prev_colors AS (
      SELECT 
          yp.current_year,
          yp.previous_year,
          curr.color AS curr_color,
          prev.color AS prev_color
      FROM year_pairs yp
      LEFT JOIN distinct_colors curr ON curr.year = yp.current_year
      FULL OUTER JOIN distinct_colors prev 
          ON prev.year = yp.previous_year AND curr.color = prev.color
  )
  SELECT 
      current_year,
      previous_year,
      COUNT(DISTINCT curr_color) AS total_colors,
      COUNT(DISTINCT CASE WHEN curr_color IS NOT NULL AND prev_color IS NOT NULL THEN curr_color END) AS from_last_year,
      COUNT(DISTINCT CASE WHEN curr_color IS NOT NULL AND prev_color IS NULL THEN curr_color END) AS new_colors,
      COUNT(DISTINCT CASE WHEN curr_color IS NULL AND prev_color IS NOT NULL THEN prev_color END) AS left_colors
  FROM curr_prev_colors
  GROUP BY current_year, previous_year
  ORDER BY current_year;
</code></pre>
<p>And this is still not giving the year formats correctly:</p>
<pre><code> current_year previous_year total_colors from_last_year new_colors left_colors
           NA            NA            0              0          0          11
         2001          2000            5              3          2           0
         2002          2001            5              3          2           0
         2003          2002            5              3          2           0
</code></pre>
<p>Can someone please show me how to fix this?</p>
",2,2,0,2025-07-15T05:34:41+00:00,5,240,True
79701721,21204623,,sql,Access IIF statement to choose values from multiple tables,"<p>I have 3 Access 2010 tables cont, cond and prop.
All tables have 3 fields: table cont the key ID_cont
and the fields cond_ID and prop_ID that are related
to the keys ID_cond and ID_prop of cond and prop
respectively. Table cond and prop have alse nome and
cognome fields. For each cont record only one field
between cond_ID and prop_ID has a usable value and
the other is -1. Table cont is:</p>
<pre><code>|ID_cont|prop_ID|cond_ID|
|   55  |   1   |  -1   |
|   12  |  -1   |   3   |
|   37  |  -1   |   1   |
|    7  |   2   |  -1   |
</code></pre>
<p>Tables cond and prop are:</p>
<pre><code>|ID_cond| nome  |cognome|
|   1   |Giulio |Rossi  |
|   2   |Sergio |Bianchi|
|   3   |Roberto|Mancini|

|ID_prop| nome  |cognome|
|   1   |Anna   |Verdi  |
|   2   |Luigi  |Sacchi |
|   3   |Antonio|Merli  |
</code></pre>
<p>I need a query that once I selected an ID_cont it
returns only nome and cognome choosing the table
cond or prop that has no -1 value in prop_ID and
cond_ID.
For example for ID_cont=37 it returns</p>
<pre><code>| nome  |cognome|
|Giulio |Rossi  |
</code></pre>
<p>I tried the solution proposed in the question
&quot;If statement to choose values from other tables&quot;
but it doesn't fit with my problem.</p>
",-3,0,3,2025-07-15T07:45:36+00:00,1,96,True
79701738,16546938,,sql,Take user input and use as a visibility filter in BIRT report writer,"<p>I have a simple SQL Script</p>
<pre><code>SELECT
    id,
    name
FROM
    traders
WHERE
    tradertype = 'C'
    AND id LIKE ?
</code></pre>
<p>I have added a parameter which is a static combo box with the values 'Toilet', 'Parts'.</p>
<p>Those values do not link to any data which is pulled by the SQL but what I am hoping is possible is that if the user was to select 'Toilet' then the visibility filters will show only the items relating to 'Toilet'.</p>
<p><img src=""https://i.sstatic.net/3KEYGpKl.png"" alt=""visibility expression"" /></p>
<p>Is what I am trying to do possible? or do all params NEED to link to the data?</p>
",1,1,0,2025-07-15T08:06:31+00:00,1,126,True
79701919,31045425,,sql,How to assign a unique rank to values in SQL,"<p>We are creating a report where we want a list of product names/numbers, and are ranking them based on the sales figures each day. We are facing an issue where it comes to tied sales - we need each product to have a distinct rank, i.e. if two products are joint first place with £100 in sales, then one of the products should be assigned rank 1, and the other rank 2 - but the assigned rank needs to be constant.</p>
<p>The view we created in Snowflake uses the below logic to assign a rank to the products:</p>
<pre class=""lang-sql prettyprint-override""><code>  row_number() over
    (partition by o.order_date, o.business_reporting_channel
     order by sum(o.sales_quantity) desc) as sales_quantity_daily_channel_rank
, sum(o.net_sales_gbp) as net_sales_gbp
, row_number() over
    (partition by o.order_date, o.business_reporting_channel
     order by sum(o.net_sales_gbp) desc) as net_sales_gbp_daily_channel_rank
</code></pre>
<p>However this is not working as expected, because the assigned rank is not consistent, as it seems to change based on the specific query I run. For example, if I run a query for all columns, I see that the 'Rsbry' product in rank 2, and 'Grn' product in rank 3:</p>
<p>However, when I run a query specifying the particular columns of interest, I am seeing 'Grn' in rank 2, and 'Rsbry' in rank 3:</p>
<p>Is there a way to prevent this from happening?</p>
",0,0,0,2025-07-15T10:30:36+00:00,1,119,True
79702717,1742777,"Gaithersburg, MD. USA",sql,Group by query to calculate percentage,"<p>I have an PostgreSQL table called <code>my_table</code> that looks like this:</p>
<pre><code>       col_a |                         col_b |
-------------+-------------------------------+
           A | 2025-04-28 16:23:55.961 -0400 |
             | 2024-03-27 17:17:08.100 -0400 |
           C | 2024-03-27 18:57:23.194 -0500 |
           B | 2025-04-28 17:44:51.647 -0500 |
             | 2023-04-28 10:47:30.667 -0400 |
</code></pre>
<p>I want to find out for each day in <code>col_b</code>, what percentage of rows have <code>col_a=null</code></p>
<p>So the result should look like this:</p>
<pre><code>      b_date | percentage |
-------------+------------+
  2023-04-28 |         66 |
  2024-03-27 |         50 |
</code></pre>
<p>This is the query I tried, but it is not quite right:</p>
<pre><code>select 
    col_b::date as b_date, 
    (count(col_a=null) * 100)/count(*) as percentage
from my_table
group by b_date
order by b_date asc
</code></pre>
",2,2,0,2025-07-15T22:19:27+00:00,2,87,True
79702768,18786773,,sql,Snowflake JOIN performance Issue,"<p>I have a large query that drives a business report - returns about 150k rows but has many steps. I am trying to join to a view that returns about 400k rows.</p>
<p>The view has a material that equals my main query material and a country that matches my main query country - however, the material can be either 10 or 11 digits in the view and thus match one or the other in the main query.</p>
<p>I am trying to join the data like this:</p>
<pre><code>LEFT JOIN PUBLISH_D.GSC_MTD_PUBLIC.V_PKG_LAB_REGDESK RD 
  ON (
       LENGTH(RD.MATERIAL_CTRY) = 12 AND
       SP.PHASE_IN_RESERVED_SAP_ID = RD.SKU AND
       COUNTRY_ABV = RD.COUNTRY_CODE
     )
  OR (
       LENGTH(RD.MATERIAL_CTRY) = 13 AND
       SP.&quot;Phase In Reserved Legacy ID&quot; = RD.SKU AND
       COUNTRY_ABV = RD.COUNTRY_CODE
     )
  OR (
       LENGTH(RD.MATERIAL_CTRY) = 13 AND
       SP.PHASE_IN_RESERVED_UU = RD.SKU AND
       COUNTRY_ABV = RD.COUNTRY_CODE
     )
</code></pre>
<p>However, it just runs endlessly at about 51% bytes scanned.</p>
<p>If I change it to this:</p>
<pre><code>LEFT JOIN PUBLISH_D.GSC_MTD_PUBLIC.V_PKG_LAB_REGDESK RD ON SP.PHASE_IN_RESERVED_SAP_ID = RD.SKU AND COUNTRY_ABV = RD.COUNTRY_CODE
</code></pre>
<p>it executes quickly. Am I missing something in the first JOIN that is causing such a delay?</p>
",1,1,0,2025-07-15T23:46:49+00:00,1,91,True
79702867,30988813,,sql,SELECT all columns except one in PostgreSQL,"<p>While learning, I thought of a use case where I might have to show all the columns and it's data except one particular column. For example I might want to hide the id column in a select query.</p>
<p>Is there a way where I can simply write <code>SELECT * (EXCEPT id) FROM table_name</code> instead of writing out all the column names and excluding the id column manually? Let's say I have a lot of columns.
I didn't find any solid current info on this, I saw SnowFlake allows the <code>EXCEPT</code> or <code>EXCLUDE</code> keyword. Is there any particular reason for that (if PostgreSQL doesn't allow)?</p>
",-1,1,2,2025-07-16T03:12:17+00:00,3,1036,True
79703443,23093624,,sql,Merge data between 2 tables using the same primary key but different record count,"<p>I have 2 tables - Car1:</p>
<p><a href=""https://i.sstatic.net/iasHVwj8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iasHVwj8.png"" alt=""enter image description here"" /></a></p>
<p>And Car2:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>S_No</th>
<th>Brand</th>
<th>Type</th>
<th>Cost</th>
<th>Tax</th>
<th>Discount</th>
<th>Launch Month</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>Maruti</td>
<td>Baleno</td>
<td>2000000</td>
<td>10</td>
<td>6</td>
<td>June</td>
</tr>
<tr>
<td>5</td>
<td>Maruti</td>
<td>Baleno</td>
<td>2500000</td>
<td>11</td>
<td>8</td>
<td>June</td>
</tr>
</tbody>
</table></div>
<p>Primary key is S.No. column. However, the information needs to be merged based on columns 'Brand' and 'Type'.</p>
<p>Requirement is: I want to merge the data from <code>Car1</code> into <code>Car2</code> for specific columns only (<code>Cost, Tax, Discount</code>). The other column's data is to remain the same.</p>
<p>So for the 2 records in the target table (<code>Car2</code>), the data should be updated and a new record should be inserted if the record count is more in source table.</p>
<p>The final data in <code>Car2</code> should look like this:</p>
<p><a href=""https://i.sstatic.net/4a4F2fgL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4a4F2fgL.png"" alt=""enter image description here"" /></a></p>
<p>How can I do this?</p>
<p>SQL queries are:</p>
<pre class=""lang-sql prettyprint-override""><code>create table car1
(
    S_No Number,
    brand varchar2(100),
    Type varchar2(100),
    Cost number,
    Tax number,
    Discount number,
    Launch_Month varchar2(100)
);

create table car2
(
    S_No Number,
    brand varchar2(100),
    Type varchar2(100),
    Cost number,
    Tax number,
    Discount number,
    Launch_Month varchar2(100)
);

Insert into CAR1 (S_NO,BRAND, TYPE, COST, TAX, DISCOUNT, LAUNCH_MONTH) 
values (1,'Maruti', 'Baleno', 1000000, 5, 5, 'May');
Insert into CAR1 (S_NO,BRAND,TYPE,COST,TAX,DISCOUNT,LAUNCH_MONTH) 
values (2,'Maruti', 'Baleno', 2000000, 6, 10, 'May');
Insert into CAR1 (S_NO, BRAND, TYPE, COST, TAX, DISCOUNT, LAUNCH_MONTH)  
values (3,'Maruti', 'Baleno', 3000000, 7, 11, 'May');

Insert into CAR2 (S_NO,BRAND, TYPE, COST, TAX, DISCOUNT, LAUNCH_MONTH) 
values (4,'Maruti', 'Baleno', 2000000, 10, 6, 'June');
Insert into CAR2 (S_NO, BRAND, TYPE, COST, TAX, DISCOUNT, LAUNCH_MONTH)  
values (5,'Maruti', 'Baleno', 2500000, 11, 8, 'June');
</code></pre>
",1,2,1,2025-07-16T12:53:04+00:00,3,94,True
79703637,16808223,,sql,Pointing old table to updated table in SQL,"<p>I am working in a data warehouse and I am looking for a way to have a query that reference a table, start referencing a different table, without having to change the query.  We have a set of tables that are brought into our operation data store that are the same as the source table.  The ETL process that brings these tables in was updated to version 2, and now we are ready to shut down the version 1 tables.</p>
<p>Currently, there are two sets of tables, version 1 and version 2.  An example pair of tables would be ods.STRING_LOCALE and ods.STRING_LOCALE_v2.  I want to remove the ods.STRING_LOCALE table, but allow any prebuilt queries referencing ods.STRING_LOCALE to automatically begin using ods.STRING_LOCALE_v2.  What would be a good strategy to accomplish this?</p>
",0,1,1,2025-07-16T15:09:30+00:00,1,89,True
79704017,26590820,,sql,Retrieving data from another table after using MIN function,"<p>I have the following query to pull the oldest batch we have shipped today:</p>
<pre><code>SELECT 
 TO_CHAR(PT.CREATE_DATE_TIME, 'MM/DD/YY'),
 PT.ITEM_ID, 
 MIN(PT.BATCH_NBR)
FROM 
 PIX_TRAN PT
WHERE 
 PT.TRAN_TYPE = '605'
AND
 PT.TRAN_CODE = '01'
AND
 TO_CHAR(PT.CREATE_DATE_TIME, 'MM/DD/YY') = '07/16/25'
GROUP BY
 PT.CREATE_DATE_TIME,
 PT.ITEM_ID
ORDER BY
 PT.ITEM_ID;
</code></pre>
<p>I would like to retrieve the expiration date associated to that batch that is in the table batch_master:</p>
<pre><code>SELECT 
 BM.ITEM_ID,
 BM.BATCH_NBR,
 BM.XPIRE_DATE
FROM
 BATCH_MASTER BM;
</code></pre>
<p>But when I try to join the tables, I don't get an exact match. I keep getting all the different batches (and expiration dates) for that item. I only need the expiration date for the one I am pulling as MIN.</p>
<pre><code>SELECT 
     TO_CHAR(PT.CREATE_DATE_TIME, 'MM/DD/YY'),
     PT.ITEM_ID, 
     MIN(PT.BATCH_NBR),
     BM.XPIRE_DATE
    FROM 
     PIX_TRAN PT
    JOIN BATCH_MASTER BM ON (BM.BATCH_NBR = PT.BATCH_NBR AND BM.ITEM_ID = PT.ITEM_ID)
    WHERE 
     PT.TRAN_TYPE = '605'
    AND
     PT.TRAN_CODE = '01'
    AND
     TO_CHAR(PT.CREATE_DATE_TIME, 'MM/DD/YY') = '07/16/25'
    GROUP BY
     PT.CREATE_DATE_TIME,
     PT.ITEM_ID,
     BM.XPIRE_DATE
    ORDER BY
     PT.ITEM_ID;
</code></pre>
<p>I would appreciate any suggestions on how to link the tables correctly - thanks!</p>
",1,2,1,2025-07-16T20:56:54+00:00,3,123,True
79704660,5172533,,sql,Add a 3rd table after an 1st table OUTER APPLY 2nd table,"<p>I have a SQL script written by someone that uses outer apply on a function. I think a code generator WYSIWYG was used , I thought that all the CAST's was making it more stable, but apparently just slowed it down .</p>
<p>But I want to add a 3rd table that has a value I want but can't understand what or how to do it.</p>
<p>With the current code (shown below) I get</p>
<blockquote>
<p>Incorrect syntax near ')'</p>
</blockquote>
<p>at the line containing the last <code>OUTER APPLY</code>.</p>
<p>This runs on SQL Server Express and I run the script in a vendor app and an Excel spreadsheet on a PC that can't have 3rd part apps installed.</p>
<p>I have made most of the changes suggested not because I think I know better but it is in my skill level and I want to keep using it to understand it better.</p>
<pre><code>declare @currentweek int
select @currentweek = datepart(week, getdate())
declare @currentyear int
select @currentyear = datepart(year, getdate())

select @currentweek as Current_Week , @currentyear as Current_Year

SELECT
        d.DocRef as DocRef,    
        h.GRN_Number AS Int_Wbill,
        h.DelNote AS Delivery_Notes,     
        h.CustomerCode AS [Producer ID],
        tbl_M_Customer.Customfield1 AS GGN, -- ← HERE how can I access a column of the 3rd table?
        h.CustomerName AS Name,
        h.Run_Date AS [Date],
        DATEPART( WEEK, h.Run_Date ) AS [Week],
        h.CultivarCode AS Commodity,
        h.VarietyCode AS Variety,
        h.Orchard AS [Orchard No],
        h.Containers_Tipped AS TotBins,    
        h.Kilograms_Tipped AS TotWeight,
        h.Container_Average AS AvgBinWeight,
        h.BinType AS [Bin Type] 
FROM
         tbl_Doc d 
         OUTER APPLY fn_Producer_Packout_Run_HEADER ( d.DocID ) h 
         OUTER APPLY ( select Customfield1 from tbl_M_Customer where tbl_M_Customer.CustomerCode = h.CustomerCode )
         -- ← HERE how can I join the 3rd table?

WHERE
        d.DocType = 'PHJ' 
       
        AND DATEPART( WEEK, h.Run_Date ) = @currentweek
        AND DATEPART(YEAR, h.Run_Date ) = @currentyear
</code></pre>
",-1,1,2,2025-07-17T10:30:38+00:00,1,105,True
79705016,31064543,,sql,Merging Count Distinct and Group By Range in MS Access SQL?,"<p>So, I've got a table with data and their respective acqusition time (in hour:minutes format) on it.</p>
<p>What I want to do with it is:</p>
<ol>
<li>Count the distinctive values</li>
<li>Display the number of distinctive values in terms of time intervals (ranges) of their acqusition</li>
</ol>
<p>First I wrote a query with <code>count(*)</code> and a subquery counting the distinct data (couldn't use <code>count(distinct Something)</code> for it since Access's SQL engine doesn't support this function).</p>
<p>For the second part, I wrote a query with the <code>format()</code> function which counts and displays the data grouped by the time.</p>
<p>However I've got a little problem merging the two queries together. Basically the second query should be modified so it counts every value once.</p>
<p>The count distinct query:</p>
<pre><code>SELECT 
    COUNT(*) AS Pcs, Inputtime
FROM 
    (SELECT DISTINCT
         Data
     FROM
         Datatable)
GROUP BY 
    Inputtime;
</code></pre>
<p>The count + time range query:</p>
<pre><code>SELECT 
    FORMAT(Inputtime,'hh:00:00 - hh:59:59') AS Timerange, 
    COUNT(Data) AS Aquired
FROM 
    Datatable
GROUP BY 
    FORMAT(Inputtime,'hh:00:00 - hh:59:59');
</code></pre>
<p>My goal is basically restructuring this second query so it counts every value only once (like the first query did).</p>
<p>Thanks for any help in advance!</p>
",2,2,0,2025-07-17T15:03:53+00:00,1,71,True
79705297,31069978,,sql,Using ORDER BY and FOR UPDATE in a single query,"<p>I'm trying to fetch new messages from a table <code>TXINCH</code> that have an entry in another table <code>TXINCH_FILE</code>, and return the output sorted by the column <code>M_PRIORITY</code> from the table <code>RTPE_CNB_FILES</code>. They are joined by a file id/file reference column.</p>
<p><code>RTPE_CNB_FILES</code> and <code>TXINCH_FILE</code> contain the imported batch files while <code>TXINCH</code> contains the individual transactions in each file, which can be a couple thousand.</p>
<pre><code>-- Get file with the highest priority that also has an entry in TXINCH_FILE

WITH next_file_id AS (
    SELECT *
    FROM (
        SELECT f.TXFILEID
        FROM TXINCH_FILE f
        LEFT OUTER JOIN RTPE_CNB_FILES cnbf
        ON f.TXFILEID = cnbf.M_FILE_REF
        WHERE f.TXSTATE = 'NW'
        ORDER BY cnbf.M_PRIORITY ASC
    )
    WHERE ROWNUM &lt;= 1
)

-- Process the entries in TXINCH using the previously fetched file id

SELECT *
FROM TXINCH i
WHERE i.TXSTATE = 'NW'
  AND i.TXFILEID = (
    SELECT TXFILEID
    FROM next_file_id
    WHERE ROWNUM &lt;= 1
  )
  AND ROWNUM &lt;= 750
FOR UPDATE SKIP LOCKED
</code></pre>
<p>This query fetches only one file reference at a time, which would be fine if each file had more than 750 transactions. However, if I get a high number of files with one transaction each, the framework causes a delay of 15 seconds per query.</p>
<p>I'm looking for a single query that can fetch entries while sorting them by priority (<code>'HIGH'</code>, <code>'LOW'</code> or <code>(null)</code>) while locking the rows as they are in use. I tried <code>ORDER BY</code> clauses but that causes the ORA-02014 error:</p>
<blockquote>
<p>cannot select <code>FOR UPDATE</code> from view with <code>DISTINCT</code>, <code>GROUP BY</code>, etc.</p>
</blockquote>
<p>I'd need to choose HIGH priorities if there are any, use LOW or empty otherwise <em>or</em> fetch as many rows as I can (ignoring the <code>ROWNUM &lt;= 750</code> at the end) while keeping the priority order.</p>
<pre><code>CREATE TABLE RTPE_CNB_FILES (
    M_FILE_REF VARCHAR2(32) NOT NULL,
    M_PRIORITY VARCHAR2(8) NOT NULL
);

CREATE TABLE TXINCH_FILE (
  TXFILEID VARCHAR2(32) NOT NULL,
  TXSTATE VARCHAR2(8)

);

CREATE TABLE TXINCH (
    TXREF VARCHAR2(32) NOT NULL,
    TXFILEID VARCHAR2(32) NOT NULL,
    TXSTATE VARCHAR2(8)
);

INSERT INTO RTPE_CNB_FILES (M_FILE_REF, M_PRIORITY) VALUES(&quot;0001&quot;,&quot;HIGH&quot;);
INSERT INTO RTPE_CNB_FILES (M_FILE_REF, M_PRIORITY) VALUES(&quot;0002&quot;,&quot;HIGH&quot;);
INSERT INTO RTPE_CNB_FILES (M_FILE_REF, M_PRIORITY) VALUES(&quot;0003&quot;,&quot;HIGH&quot;);
INSERT INTO RTPE_CNB_FILES (M_FILE_REF, M_PRIORITY) VALUES(&quot;0004&quot;,&quot;LOW&quot;);
INSERT INTO RTPE_CNB_FILES (M_FILE_REF, M_PRIORITY) VALUES(&quot;0005&quot;,&quot;LOW&quot;);
INSERT INTO RTPE_CNB_FILES (M_FILE_REF, M_PRIORITY) VALUES(&quot;0006&quot;,&quot;LOW&quot;);

INSERT INTO TXINCH_FILE (TXFILEID, TXSTATE) VALUES(&quot;0001&quot;, &quot;NW&quot;);
INSERT INTO TXINCH_FILE (TXFILEID, TXSTATE) VALUES(&quot;0002&quot;, &quot;NW&quot;);
INSERT INTO TXINCH_FILE (TXFILEID, TXSTATE) VALUES(&quot;0003&quot;, &quot;NW&quot;);
INSERT INTO TXINCH_FILE (TXFILEID, TXSTATE) VALUES(&quot;0004&quot;, &quot;NW&quot;);
INSERT INTO TXINCH_FILE (TXFILEID, TXSTATE) VALUES(&quot;0005&quot;, &quot;NW&quot;);
INSERT INTO TXINCH_FILE (TXFILEID, TXSTATE) VALUES(&quot;0006&quot;, &quot;NW&quot;);

INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;1&quot;,&quot;0001&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;2&quot;,&quot;0001&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;3&quot;,&quot;0001&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;4&quot;,&quot;0002&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;5&quot;,&quot;0002&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;6&quot;,&quot;0003&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;7&quot;,&quot;0003&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;8&quot;,&quot;0003&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;9&quot;,&quot;0003&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;10&quot;,&quot;0004&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;11&quot;,&quot;0004&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;12&quot;,&quot;0004&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;13&quot;,&quot;0004&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;14&quot;,&quot;0005&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;15&quot;,&quot;0005&quot;, &quot;NW&quot;);
INSERT INTO TXINCH (TXREF, TXFILEID, TXSTATE) VALUES(&quot;16&quot;,&quot;0006&quot;, &quot;NW&quot;);
</code></pre>
<p>With the database in this state the query returns only three rows; the three entries in <code>TXINCH</code> that have <code>M_PRIORITY</code> <code>'HIGH'</code>. It should also return the other rows with <code>M_PRIORITY</code> <code>'HIGH'</code> without being limited to a single file reference (a single entry in <code>RTPE_CNB_FILES</code>) per query.</p>
<p>Current output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>TXREF</th>
<th>TXFILEID</th>
<th>TXSTATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0001</td>
<td>NW</td>
</tr>
<tr>
<td>2</td>
<td>0001</td>
<td>NW</td>
</tr>
<tr>
<td>3</td>
<td>0001</td>
<td>NW</td>
</tr>
</tbody>
</table></div>
<p>Target output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>TXREF</th>
<th>TXFILEID</th>
<th>TXSTATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0001</td>
<td>NW</td>
</tr>
<tr>
<td>2</td>
<td>0001</td>
<td>NW</td>
</tr>
<tr>
<td>3</td>
<td>0001</td>
<td>NW</td>
</tr>
<tr>
<td>4</td>
<td>0002</td>
<td>NW</td>
</tr>
<tr>
<td>5</td>
<td>0002</td>
<td>NW</td>
</tr>
<tr>
<td>6</td>
<td>0003</td>
<td>NW</td>
</tr>
<tr>
<td>7</td>
<td>0003</td>
<td>NW</td>
</tr>
<tr>
<td>8</td>
<td>0003</td>
<td>NW</td>
</tr>
<tr>
<td>9</td>
<td>0003</td>
<td>NW</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table></div>
",1,2,1,2025-07-17T18:52:57+00:00,1,128,True
79705361,10691560,,sql,What is the equivalent to &quot;on existing skip&quot; in Sybase ASE?,"<p>I'm working with Sybase ASE (Adaptive Server Enterprise/16.0) and I need to insert certain values from table SOURCE_TABLE into table TARGET_TABLE.</p>
<p>SOURCE_TABLE already contains some values from TARGET_TABLE. Hence, the following statement:</p>
<pre><code>insert
  into TARGET_TABLE    (
       ID              ,
       TYPE            ,
       DATE            ,
       BALANCE         )
select ST.ID           ,
       ST.TYPE         ,
       ST.DATE         ,
       ST.BALANCE
  from SOURCE_TABLE ST,
       TABLE_X       X,
       TABLE_Y       Y
 where ST.COLUMN_5       = X.COLUMN_3        
   and X.COLUMN_7        = Y.COLUMN_8
   and Y.COLUMN_9        = 'some_value'     
</code></pre>
<p>... results in this error message:</p>
<blockquote>
<p>Attempt to insert duplicate key row in object 'TARGET_TABLE' with unique index 'IND'</p>
</blockquote>
<p>Unique index 'IND' contains the following columns: ID, TYPE, DATE</p>
<p>What I would like to do is something like the following:</p>
<pre><code>insert
  into TARGET_TABLE    (
       ID              ,
       TYPE            ,
       DATE            ,
       BALANCE         )
on existing skip -- THIS LINE DOES NOT WORK !!!
select ST.ID           ,
       ST.TYPE         ,
       ST.DATE         ,
       ST.BALANCE
  from SOURCE_TABLE ST,
       TABLE_X       X,
       TABLE_Y       Y
 where ST.COLUMN_5       = X.COLUMN_3        
   and X.COLUMN_7        = Y.COLUMN_8
   and Y.COLUMN_9        = 'some_value'  
</code></pre>
<p>However, &quot;on existing skip&quot; (which would work with SQL Anywhere) does not work in the case of Sybase ASE.</p>
<p>I was not able to find out the equivalent command for Sybase ASE.</p>
<p>Does anybody know the equivalent to &quot;on existing skip&quot; for Sybase ASE?</p>
",1,1,0,2025-07-17T20:16:28+00:00,0,63,False
79705490,31067324,,sql,Query count &amp; sum nested selects into a Grouped Category,"<p>My current project/issue is we have a list of permit types. With these types they are needing to be grouped by an overall category. Once grouped out the end goal is to count how many within specified date range along with a sum total of the estimated value. The initial group category element doesn't exist.</p>
<p>I'm trying to learn how to use nested select statements, and thought I might be able to do that for each grouped category. Although I can technically get it to work, all the resulted data displayed is on one single row. I figured I'm probably going to need to create a temporary table, but I don't fully understand how it works, and how I would apply it to my query.</p>
<pre><code>declare @CATEGORY1 nvarchar(40)
declare @CATEGORY2 nvarchar(40)
declare @CATEGORY3 nvarchar(40)
declare @CATEGORY4 nvarchar(40)
declare @CATEGORY5 nvarchar(40)
declare @CATEGORY6 nvarchar(40)

Set @CATEGORY1 = 'Commercial - New'
Set @CATEGORY2 = 'Commercial - Modifications'
Set @CATEGORY3 = 'Multi-Family - New'
Set @CATEGORY4 = 'Multi-Family - Modifications'
Set @CATEGORY5 = 'Residential - New'
Set @CATEGORY6 = 'Residential - Modifications'

select 
@CATEGORY4 as 'Group Category'

, query1.[Count]
, query1.[SUM]

,@CATEGORY5 as 'Group Category'
, query2.[Count]
, query2.[SUM]

from 
(
Select
    Count(DISTINCT P1.PermitNum) as 'Count'
    , Sum(P1.EstimatedValue) as 'SUM'
FROM
    Permit P1
where P1.PermitTypeMasterID in (57,60,59)
and
P1.CreatedDate &gt;= '2025-01-01' 
) as query1 , 

(
Select
    Count(DISTINCT P2.PermitNum) as 'Count'
    , Sum(P2.EstimatedValue) as 'SUM'
FROM
    Permit P2
where P2.PermitTypeMasterID in (1,46,78,79)
and
P2.CreatedDate &gt;= '2025-01-01' 
) as query2 ;
</code></pre>
<p>The results of the above looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Group Category</th>
<th>Count</th>
<th>SUM</th>
<th>Group Category</th>
<th>Count</th>
<th>SUM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-Family - Modifications</td>
<td>15</td>
<td>8360000</td>
<td>Residential - New</td>
<td>72</td>
<td>32360475</td>
</tr>
</tbody>
</table></div>
<p>What my end goal is something like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Group Category</th>
<th>Count</th>
<th>SUM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-Family - Modifications</td>
<td>15</td>
<td>8360000</td>
</tr>
<tr>
<td>Residential - New</td>
<td>72</td>
<td>32360475</td>
</tr>
</tbody>
</table></div>
",2,2,0,2025-07-17T22:50:53+00:00,3,129,True
79705869,9400754,,sql,PostgreSQL solution to work easier on a specific row with multiple columns,"<p>I have a table where I want to do some statistics on each rows different columns. For example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>NW2020</th>
<th>NW2021</th>
<th>NW2022</th>
<th>NW2023</th>
<th>NW2024</th>
<th>CA2020</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>-299</td>
<td>4000</td>
<td>21</td>
<td>-325</td>
<td>2544</td>
<td>55</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>54</td>
<td>244</td>
<td>-5</td>
<td>-54</td>
<td>325</td>
<td>874</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>How can I have a table with the frequency of negative values of each Net Worth?
Is there a way that I can ask for &quot;NW%&quot; columns? (a kind of GROUP BY but by columns and not by rows)</p>
<p>The only (UGLY) solution I had in mind was this one :</p>
<pre><code>SELECT *, 
((CASE NW2020 &lt; 0 THEN 1 ELSE 0 END) + (CASE WHEN NW2021 &lt; 0 THEN 1 ELSE 0 END) + (CASE WHEN NW2022 &lt; 0 THEN 1 ELSE 0 END) + ... )::double precision/
    (CASE WHEN NW2020 is not null THEN 1 ELSE 0 END) + (CASE WHEN NW2021 is not null THEN 1 ELSE 0 END) + (CASE WHEN NW2022  is not null THEN 1 ELSE 0 END) + ... )as freq_negative
FROM a
</code></pre>
<p>(I know <code>coalesce</code> but it doesn't short it enough for the amount of columns I have.</p>
<p>I had the same issue calculating the average and the standard deviation.</p>
<p>Is my DB and table organization bad?</p>
<p>Thanks in advance</p>
<p><strong>EDIT</strong> : The solution for future convenience was to implement Normalisation into my schema. See @richard-huxton 's comment.</p>
",0,0,0,2025-07-18T08:05:17+00:00,4,128,True
79707800,17685806,Planet Earth,sql,How to use two different expressions in one filter in Diesel ORM?,"<p>I am trying to run a very simple filter on a joined table using Diesel ORM.</p>
<p>The database has a simple many-to-many relationship set up using an associative table:</p>
<pre><code>CREATE TABLE items (
    id INT NOT NULL,
    name TEXT NOT NULL,
    PRIMARY KEY(id)
);

CREATE TABLE item_lists (
    id INT NOT NULL,
    name TEXT NOT NULL,
    PRIMARY KEY(id)
);

/* Associative table that links lists to their contents. */ 
CREATE TABLE list_items (
    id INT NOT NULL,
    list_id INT NOT NULL,
    item_id INT NOT NULL,
    PRIMARY KEY(id),
    FOREIGN KEY(list_id) REFERENCES item_lists(id),
    FOREIGN KEY(item_id) REFERENCES items(id)
);
</code></pre>
<p>I need to write a function that will return all items in a specific list, or in no list. Given an id or NULL represented as <code>Option&lt;i32&gt;</code>, the function needs to: if it's <code>None</code>, return items that aren't in any list; if it's <code>Some(id)</code>, return items in that list. It looks somewhat like this so far:</p>
<pre><code>pub type ItemInList = (Item, Option&lt;(ListItem, ItemList)&gt;);

pub fn get_items_in_list(
    db: &amp;mut SqliteConnection,
    list_id: Option&lt;i32&gt;,
) -&gt; Result&lt;Vec&lt;ItemInList&gt;&gt; {
    let result: Vec&lt;ItemInList&gt; = items::table
        .left_join(list_items::table.inner_join(item_lists::table))
        .filter(list_id
            .map(|id| item_lists::id.eq(id))
            .unwrap_or_else(|| item_lists::id.is_null()))
        .select(ItemInList::as_select())
        .load(db)?;

    Ok(result)
}
</code></pre>
<p>This code does not compile as the return types of <code>eq</code> and <code>is_null</code>, which are <code>dsl::Eq</code> and <code>dsl::IsNull</code>) respectively, aren't the same, so the <code>filter</code> call can't be compiled. I don't understand how the ORM expects me to type this otherwise, it seems that it should be trivial to create a filter call equivalent to <code>WHERE item_lists.id = in_id OR (item_lists.id IS NULL AND in_id IS NULL)</code>.</p>
<p>Is there a type that represents &quot;any filterable expression&quot;? How do I make this work without separate duplicate functions (or two separate implementations in a <code>match</code> statement) for the <code>None</code> and <code>Some</code> variants of <code>list_id</code>?</p>
",2,2,0,2025-07-20T07:04:05+00:00,1,111,True
79707979,23178679,,sql,Select only one in case of multiple occurrences,"<p>When looking at an <code>SZERZODES_ID</code>, there are records where <code>PARTNER_ID</code> is specified and there are records where it is not. In the result, I would like to get back only those records where the <code>PARTNER_ID</code> is also specified.</p>
<p>At the same time, if there are any <code>SZERZODES_ID</code> where only <code>CSAPATVEZETO_ID</code> is given, I want these to be included in the result.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">SZERZODES_ID</th>
<th style=""text-align: center;"">MUNKALAP_ID</th>
<th>CSAPATVEZETO_ID</th>
<th style=""text-align: right;"">PARTNER_ID</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000001</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000001</td>
<td>23,323</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000002</td>
<td>23,323</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000002</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000003</td>
<td>23,323</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000004</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000005</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000005</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000006</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">10000000</td>
<td style=""text-align: center;"">10000006</td>
<td>23,323</td>
<td style=""text-align: right;"">682</td>
</tr>
<tr>
<td style=""text-align: center;"">20000000</td>
<td style=""text-align: center;"">20000001</td>
<td>4,957</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
<tr>
<td style=""text-align: center;"">20000000</td>
<td style=""text-align: center;"">20000002</td>
<td>4,957</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
<tr>
<td style=""text-align: center;"">20000000</td>
<td style=""text-align: center;"">20000003</td>
<td>4,957</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
<tr>
<td style=""text-align: center;"">20000000</td>
<td style=""text-align: center;"">20000004</td>
<td>4,957</td>
<td style=""text-align: right;"">[NULL]</td>
</tr>
</tbody>
</table></div>
",2,3,1,2025-07-20T12:22:55+00:00,2,99,True
79708133,23512643,,sql,Tracking when colors appear for the first time under different conditions,"<p>I have this table in SQL (each color only appears once in each year):</p>
<pre><code>CREATE TABLE myt (
    color VARCHAR(20),
    year INTEGER,
    var INTEGER
);

INSERT INTO myt (color, year, var) VALUES
('red', 2020, 1),
('red', 2021, 1),
('red', 2022, 0),
('red', 2023, 1),

('blue', 2020, 0),
('blue', 2021, 0),
('blue', 2022, 0),
('blue', 2023, 1),

('green', 2021, 1),
('green', 2022, 1),
('green', 2023, 0),

('yellow', 2021, 0),
('yellow', 2022, 0),
('yellow', 2023, 0),

('purple', 2022, 1),
('purple', 2023, 1),

('orange', 2022, 0),
('orange', 2023, 1),

('pink', 2023, 1),

('black', 2023, 0),

('white', 2020, 0),
('white', 2022, 1),
('white', 2023, 0);
</code></pre>
<p>The table looks like this:</p>
<pre><code>| Color  | Year | Var |
|--------|------|-----|
| red    | 2020 | 1   |
| red    | 2021 | 1   |
| red    | 2022 | 0   |
| red    | 2023 | 1   |
| blue   | 2020 | 0   |
| blue   | 2021 | 0   |
| blue   | 2022 | 0   |
| blue   | 2023 | 1   |
| green  | 2021 | 1   |
| green  | 2022 | 1   |
| green  | 2023 | 0   |
| yellow | 2021 | 0   |
| yellow | 2022 | 0   |
| yellow | 2023 | 0   |
| purple | 2022 | 1   |
| purple | 2023 | 1   |
| orange | 2022 | 0   |
| orange | 2023 | 1   |
| pink   | 2023 | 1   |
| black  | 2023 | 0   |
| white  | 2020 | 0   |
| white  | 2022 | 1   |
| white  | 2023 | 0   |
</code></pre>
<p>I want to write SQL to answer the following question. In each year, I want to count: (note that all categories will sum to the total number of colors in that year):</p>
<ul>
<li>Category 1: First appearance in dataset AND var=1</li>
<li>Category 2: First appearance in dataset AND var=0</li>
<li>Category 3: Repeat appearance AND always had var=0</li>
<li>Category 4: Repeat appearance AND var=1 (could have var1=0 in the past and var1=1 in the past)</li>
<li>Category 5: Repeat appearance AND var=0 but had var=1 in past at least once</li>
<li>Category 6: Repeat appearance AND var=1 for the first time</li>
</ul>
<p>I think the final answer should look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>year</th>
<th>category1</th>
<th>category2</th>
<th>category3</th>
<th>category4</th>
<th>category5</th>
<th>category6</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2021</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2022</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2023</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>I tried to organize the SQL the following way. First, I transpose the data to make columns for whether a color appears in each year or not. Then, for each year, I do the analysis and perform a series of UNION ALLs:</p>
<pre><code>WITH pivoted_data AS (
    SELECT 
        color,
        MAX(CASE WHEN year = 2020 THEN var END) as in_2020,
        MAX(CASE WHEN year = 2021 THEN var END) as in_2021,
        MAX(CASE WHEN year = 2022 THEN var END) as in_2022,
        MAX(CASE WHEN year = 2023 THEN var END) as in_2023,
        MIN(year) as first_year,
        MAX(year) as last_year
    FROM myt
    GROUP BY color
),
categorized_by_year AS (
    SELECT 
        color,
        2020 as year,
        in_2020 as current_var,
        CASE 
            WHEN in_2020 IS NULL THEN NULL
            WHEN first_year = 2020 AND in_2020 = 1 THEN 'category1'
            WHEN first_year = 2020 AND in_2020 = 0 THEN 'category2'
            ELSE 'other'
        END as category
    FROM pivoted_data
    WHERE in_2020 IS NOT NULL
    
    UNION ALL
    
    SELECT 
        color,
        2021 as year,
        in_2021 as current_var,
        CASE 
            WHEN in_2021 IS NULL THEN NULL
            WHEN first_year = 2021 AND in_2021 = 1 THEN 'category1'
            WHEN first_year = 2021 AND in_2021 = 0 THEN 'category2'
            WHEN first_year &lt; 2021 AND in_2020 = 0 AND in_2021 = 0 THEN 'category3'
            WHEN first_year &lt; 2021 AND in_2021 = 1 AND in_2020 = 0 THEN 'category6'
            WHEN first_year &lt; 2021 AND in_2021 = 1 AND in_2020 = 1 THEN 'category4'
            WHEN first_year &lt; 2021 AND in_2021 = 0 AND in_2020 = 1 THEN 'category5'
            ELSE 'other'
        END as category
    FROM pivoted_data
    WHERE in_2021 IS NOT NULL
    
    UNION ALL
    
    SELECT 
        color,
        2022 as year,
        in_2022 as current_var,
        CASE 
            WHEN in_2022 IS NULL THEN NULL
            WHEN first_year = 2022 AND in_2022 = 1 THEN 'category1'
            WHEN first_year = 2022 AND in_2022 = 0 THEN 'category2'
            WHEN first_year &lt; 2022 AND 
                 COALESCE(in_2020, 0) = 0 AND COALESCE(in_2021, 0) = 0 AND in_2022 = 0 THEN 'category3'
            WHEN first_year &lt; 2022 AND in_2022 = 1 AND 
                 COALESCE(in_2020, 0) = 0 AND COALESCE(in_2021, 0) = 0 THEN 'category6'
            WHEN first_year &lt; 2022 AND in_2022 = 1 AND 
                 (COALESCE(in_2020, 0) = 1 OR COALESCE(in_2021, 0) = 1) THEN 'category4'
            WHEN first_year &lt; 2022 AND in_2022 = 0 AND 
                 (COALESCE(in_2020, 0) = 1 OR COALESCE(in_2021, 0) = 1) THEN 'category5'
            ELSE 'other'
        END as category
    FROM pivoted_data
    WHERE in_2022 IS NOT NULL
    
    UNION ALL
    
    SELECT 
        color,
        2023 as year,
        in_2023 as current_var,
        CASE 
            WHEN in_2023 IS NULL THEN NULL
            WHEN first_year = 2023 AND in_2023 = 1 THEN 'category1'
            WHEN first_year = 2023 AND in_2023 = 0 THEN 'category2'
            WHEN first_year &lt; 2023 AND 
                 COALESCE(in_2020, 0) = 0 AND COALESCE(in_2021, 0) = 0 AND COALESCE(in_2022, 0) = 0 AND in_2023 = 0 THEN 'category3'
            WHEN first_year &lt; 2023 AND in_2023 = 1 AND 
                 COALESCE(in_2020, 0) = 0 AND COALESCE(in_2021, 0) = 0 AND COALESCE(in_2022, 0) = 0 THEN 'category6'
            WHEN first_year &lt; 2023 AND in_2023 = 1 AND 
                 (COALESCE(in_2020, 0) = 1 OR COALESCE(in_2021, 0) = 1 OR COALESCE(in_2022, 0) = 1) THEN 'category4'
            WHEN first_year &lt; 2023 AND in_2023 = 0 AND 
                 (COALESCE(in_2020, 0) = 1 OR COALESCE(in_2021, 0) = 1 OR COALESCE(in_2022, 0) = 1) THEN 'category5'
            ELSE 'other'
        END as category
    FROM pivoted_data
    WHERE in_2023 IS NOT NULL
)
SELECT 
    year,
    COUNT(CASE WHEN category = 'category1' THEN 1 END) as category1,
    COUNT(CASE WHEN category = 'category2' THEN 1 END) as category2,
    COUNT(CASE WHEN category = 'category3' THEN 1 END) as category3,
    COUNT(CASE WHEN category = 'category4' THEN 1 END) as category4,
    COUNT(CASE WHEN category = 'category5' THEN 1 END) as category5,
    COUNT(CASE WHEN category = 'category6' THEN 1 END) as category6,
    COUNT(*) as total_rows,
    COUNT(DISTINCT color) as distinct_colors
FROM categorized_by_year
WHERE category IS NOT NULL
GROUP BY year
ORDER BY year;
</code></pre>
<p>Is there a different way to do this so that I don't have to manually UNION ALL the results of each year? This approach will get quite lengthy when there are many years.</p>
",0,0,0,2025-07-20T15:26:29+00:00,2,88,True
79708942,11480762,,sql,Query pivot with encryption,"<p>I need to make a query with a large dataset. The data is spread over multiple rows in a table, the goal is to have them in a single column.</p>
<p>Some columns are encrypted (Always Encrypted, in this example the 'Topic' and 'Rating' column).</p>
<p>On some other examples I tried (f.e. MAX() Function) I got the error message:</p>
<blockquote>
<p>Encryption scheme mismatch for columns/variables</p>
</blockquote>
<p>Example data - table #1 (<code>Person</code>):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ID</th>
<th style=""text-align: left;"">Person</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Testi McTest</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">Max Mustermann</td>
</tr>
</tbody>
</table></div>
<p>Table #2 (<code>Data</code>):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ID</th>
<th style=""text-align: left;"">PersonID</th>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: left;"">Topic (Encrypted)</th>
<th style=""text-align: left;"">Comment (Encrypted)</th>
<th style=""text-align: left;"">Rating (Encrypted)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Text 1</td>
<td style=""text-align: left;"">Testvalue 1</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">Text 2</td>
<td style=""text-align: left;"">Testvalue 2</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Text 3</td>
<td style=""text-align: left;"">Testvalue 3</td>
<td style=""text-align: left;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">Text 4</td>
<td style=""text-align: left;"">Testvalue 4</td>
<td style=""text-align: left;"">2</td>
</tr>
</tbody>
</table></div>
<p>I need to query and get the results in a single row:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Person</th>
<th style=""text-align: left;"">Index 1 Topic</th>
<th style=""text-align: left;"">Index 1 Rating</th>
<th style=""text-align: left;"">Index 2 Topic</th>
<th style=""text-align: left;"">Index 2 Rating</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Testi McTest</td>
<td style=""text-align: left;"">Text 1</td>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Text 2</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">Max Mustermann</td>
<td style=""text-align: left;"">Text 3</td>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">Text 4</td>
<td style=""text-align: left;"">2</td>
</tr>
</tbody>
</table></div>
<p>I tried it with joining the rows, but its way too slow.. This is the only solution I got that worked:</p>
<pre><code>WITH tbl AS
(
    SELECT  
        PersonID, Index, Topic, Rating
    FROM            
        Data
)
SELECT
    Person
    i1.Topic,
    i1.Rating,
    i2.Topic,
    i2.Rating
FROM 
    Data
LEFT JOIN 
    Person ON Data.PersonId = Person.Id
INNER JOIN 
    tbl AS i1 on Data.PersonId = i1.Id AND i1.Index = 1
INNER JOIN 
    tbl AS i2 on Data.PersonId = i2.Id AND i2.Index = 2
</code></pre>
<p>Other example with encyption error:</p>
<pre><code>SELECT
    p.Person,
    MAX(CASE WHEN d.Index = 1 THEN d.Topic END) AS &quot;Index 1 Topic&quot;,
    MAX(CASE WHEN d.Index = 1 THEN d.Rating END) AS &quot;Index 1 Rating&quot;,
    MAX(CASE WHEN d.Index = 2 THEN d.Topic END) AS &quot;Index 2 Topic&quot;,
    MAX(CASE WHEN d.Index = 2 THEN d.Rating END) AS &quot;Index 2 Rating&quot;
FROM
    Person p
LEFT JOIN
    Data d ON p.ID = d.PersonID
GROUP BY
    p.ID, p.Person
</code></pre>
",1,1,0,2025-07-21T10:41:36+00:00,1,81,True
79708964,673496,"Berlin, Germany",sql,Get the column NAME as a foreign key,"<p>I have a table <code>Languages</code>:</p>
<pre><code>ID | ISO639 | NAME
---+--------+---------
 1 | EN     | English
 2 | DE     | Deutsch
 3 | TR     | Türkçe
(etc.)
</code></pre>
<p>I have another table <code>Headlines</code>:</p>
<pre><code>ID | EN       | DE      | TR      | ...(etc.)
---+----------+---------+---------+---------
 1 | Shop     | Laden   | Mağaza  |
(etc.)
</code></pre>
<p>Is it possible to have column names as foreign key or to restrict them to the table entries, or do I have to check that myself? Or is the only recommended way like this:</p>
<pre><code>ID | SUB_ID | Foreign | Headline
---+--------+---------+-----------
 1 |    1   | EN      | Shop
 2 |    1   | DE      | Laden
 3 |    1   | TR      | Mağaza
 4 |    2   | (next headline)
(etc.)
</code></pre>
<p>?</p>
",-1,1,2,2025-07-21T11:04:29+00:00,1,125,True
79709014,25493371,,sql,Correct design for SQL table in SQLite3: how to optimize substring search?,"<p>I am using <code>SQLite3</code> in C++ where I need to store text data. There are 7 columns in the table. I need to filter data by 7 columns. I need to check for equality of 6 columns to a specific value, and I need to search for a substring by the last column. I use a regular index for the first 6 columns and the search (SELECT query) works quite fast. However, for the 7th column, I use <code>LIKE %value%</code> to search for a substring. However, <code>LIKE</code> does not use indexes. How can I efficiently implement searching by this field? Note that <code>WHERE</code> filtering by fields can contain all 7 columns, or just one (which uses LIKE).</p>
<p><strong>UPDATE</strong>
I found out that full text search works well in such cases, but full text search does not work with a regular table, as far as I know. What solution do you recommend?</p>
<p>My texts are not long (up to 100 characters) and there are no spaces. Example: ThisIsMyExampleString. In that case I'm not sure that full text search will work here</p>
",0,0,0,2025-07-21T11:49:00+00:00,0,172,False
79709161,673496,"Berlin, Germany",sql,SQL plain or PDO: how do I get the foreign key?,"<p>I have two tables, T1 and T2. Both have an ID. The second column of T2 (MyRefColumn) is a foreign key to the ID of T1. I <em>know</em> this. But how do I <em>get</em> this? Like:</p>
<pre><code>$link = GetForeignKey('T2', 'MyRefColumn');
/* this should return something like
[Table=&gt;'T1', Column =&gt; 'ID'] */
</code></pre>
<p>I already have a full copy of the Meta-Array or the Column...</p>
<pre><code>SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'T2';
</code></pre>
<p>...and the ['COLUMN_KEY'] of the column is <code>MUL</code> but I fail to see anything that tells me where column 2 links to. I dont want a join, I want to get the name of the Table and the column where the foreign key links to.</p>
",-1,0,1,2025-07-21T13:46:49+00:00,1,55,True
79709223,206567,,sql,SQL Rounded Months calculations,"<p>I am trying to get this <code>Rounded Months</code>. Below is what I been trying. How to get the desired value?</p>
<pre><code>SELECT 
    ROUND(CAST(DATEDIFF(DAY, '4/19/2024', '5/23/2024') AS FLOAT) / 7, 0),
    DATEDIFF(MONTH, '4/19/2024', '5/23/2024'),
    DATEDIFF(DAY, '4/19/2024', '5/23/2024'),
    CAST(DATEDIFF(DAY, '4/19/2024', '5/23/2024') AS DECIMAL(10, 2)) / 7 AS DateDifference,
    ROUND(SUM(DATEDIFF(ss,'4/19/2024', '5/23/2024') / 60.0 / 60.0), 2)
</code></pre>
<p>This is the results I'm getting:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>1</td>
<td>34</td>
<td>4.857142</td>
<td>816.0000000000</td>
</tr>
</tbody>
</table></div>
<p>And this is the result I'm looking to get:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date A</th>
<th>Date B</th>
<th>Rounded Months</th>
</tr>
</thead>
<tbody>
<tr>
<td>4/19/2024</td>
<td>5/23/2024</td>
<td>1.12</td>
</tr>
<tr>
<td>4/26/2024</td>
<td>7/1/2024</td>
<td>2.17</td>
</tr>
</tbody>
</table></div>
",0,1,1,2025-07-21T14:41:19+00:00,1,127,True
79709608,5829088,,sql,Combine two lists for INSERT into a table,"<p>I have a list of 35 state abbreviations ('AK', 'AL', 'AZ', etc.) and another list of 10 non-sequential 2-digit numbers as <code>CHAR(2)</code> ('02', '39', '44', etc.).</p>
<p>I need to combine the contents of these two lists so that when I <code>INSERT</code> into the table each state from the first list will have every possible number from the second list.</p>
<p>I know that I can manually <code>INSERT</code> as below:</p>
<pre><code>INSERT dbo.TableName (state, region, date, person)
VALUES ('AK', '02', GETDATE(), 'SampleName'),
       ('AK', '39', GETDATE(), 'SampleName'),
       ...
</code></pre>
<p>...but that's 350 <code>VALUES</code> to create by hand. For this MUCH simplified example, that'd be tedious but hardly worth the time to ask and answer the question. I've looked at a <code>WHILE</code> loop, but I don't see how I can make it work with non-sequential lists.</p>
<p>How can I <code>INSERT</code> two lists into one table without writing hundreds of <code>INSERT</code> statements?</p>
",2,3,1,2025-07-21T20:24:48+00:00,4,168,True
79709826,1721838,,sql,SQL query gives error Invalid object name,"<p>I have three tables:</p>
<ul>
<li>Table1: Account (UserID, FirstName, LastName)</li>
<li>Table2: Luncheon (LuncheonID, Restaurant, Date, Address)</li>
<li>Table3: SignUp (UserID, LuncheonID)</li>
</ul>
<p>I am trying to populate a GridView with columns:<br />
FirstName, LastName, Restaurant, Date, Address</p>
<p>I have tried several queries and struggled with</p>
<blockquote>
<p>exposed names. Use correlation names to distinguish them.</p>
</blockquote>
<p>and</p>
<blockquote>
<p>The multi-part identifier could not be bound.</p>
</blockquote>
<p>The latest query I have tried:</p>
<pre><code>SELECT a.FirstName AS Name, l.Restaurant AS Location, l.Date AS Date
FROM Account AS a, Luncheon AS l, SignUp AS s
JOIN
    a ON s.UserID = a.UserID
JOIN
    l ON s.UserID = l.LuncheonID;
</code></pre>
<p>which resolved those errors, but results in</p>
<blockquote>
<p>Error: Invalid object name 'a'.</p>
</blockquote>
<p>What is the correct query syntax? Also, I would like to combine FirstName and LastName into a single column and sort by Name or Date.</p>
",-3,0,3,2025-07-22T03:05:48+00:00,1,103,True
79710641,9616557,"S&#227;o Paulo, SP, Brasil",sql,How to ensure unique index names when creating indexes for FKs with PL/SQL?,"<p>I'm trying to write a procedure that automatically creates indexes for foreign keys that don't have one yet. I created a function called SNK_VERIFICA_NOME_IDX which is supposed to ensure the index name is unique by appending a numeric suffix if the name already exists.</p>
<p>Here's the function and procedure:</p>
<pre><code>CREATE OR REPLACE FUNCTION SNK_VERIFICA_NOME_IDX(P_INDICE VARCHAR2)
RETURN VARCHAR2
IS
  P_COUNT INT;
  P_QTD INT;
  P_NOVONOME VARCHAR2(100);
BEGIN
  P_NOVONOME := P_INDICE;
  SELECT COUNT(1) INTO P_COUNT FROM USER_INDEXES WHERE INDEX_NAME = P_INDICE;      
  IF P_COUNT &lt;&gt; 0 THEN
    P_QTD := 1;        
    WHILE (P_COUNT &lt;&gt; 0)
    LOOP
      P_NOVONOME := P_INDICE || P_QTD;
      SELECT COUNT(1) INTO P_COUNT FROM USER_INDEXES WHERE INDEX_NAME = P_INDICE || P_QTD;          
      P_QTD := P_QTD + 1;
    END LOOP;
  END IF;  
  RETURN P_NOVONOME;
END;
/

CREATE OR REPLACE PROCEDURE STP_GERA_INDICES_FKS
AS
BEGIN
DECLARE
  P_COUNT INT;
  P_COMANDO VARCHAR2(4000);
  P_TABELA VARCHAR2(50);
  P_INDICE VARCHAR2(50);
  P_COLUNAS VARCHAR2(1000);
  P_QTD INT;
  CURSOR CURIDX IS
  SELECT TABLE_NAME, SUBSTR(TABLE_NAME,1,10) || '_IDX_FK_' || SUBSTR(REPLACE(FK_COLUMNS,',','_'),1,10) AS INDEX_NAME, FK_COLUMNS AS INDEX_COLUMNS FROM
  (SELECT CASE WHEN B.TABLE_NAME IS NULL THEN 'unindexed' ELSE 'indexed' END AS STATUS,
          A.TABLE_NAME, A.CONSTRAINT_NAME, A.FK_COLUMNS, B.INDEX_NAME, B.INDEX_COLUMNS
     FROM (SELECT A.TABLE_NAME, A.CONSTRAINT_NAME,
                  LISTAGG(A.COLUMN_NAME, ',') WITHIN GROUP (ORDER BY A.POSITION) FK_COLUMNS
             FROM USER_CONS_COLUMNS A, USER_CONSTRAINTS B
            WHERE A.CONSTRAINT_NAME = B.CONSTRAINT_NAME
              AND B.CONSTRAINT_TYPE = 'R'
              AND A.TABLE_NAME LIKE 'T%'
         GROUP BY A.TABLE_NAME, A.CONSTRAINT_NAME) A,
          (SELECT TABLE_NAME, INDEX_NAME,
                  LISTAGG(C.COLUMN_NAME, ',') WITHIN GROUP (ORDER BY C.COLUMN_POSITION) INDEX_COLUMNS
             FROM USER_IND_COLUMNS C
            WHERE C.TABLE_NAME LIKE 'T%'
         GROUP BY TABLE_NAME, INDEX_NAME) B
    WHERE A.TABLE_NAME = B.TABLE_NAME(+)
      AND B.INDEX_COLUMNS(+) LIKE A.FK_COLUMNS || '%'
    ORDER BY 1 DESC, 2) X
  WHERE STATUS = 'unindexed';
BEGIN
  OPEN CURIDX;
  LOOP
    FETCH CURIDX INTO P_TABELA, P_INDICE, P_COLUNAS;
    EXIT WHEN CURIDX%NOTFOUND;
    P_INDICE := SNK_VERIFICA_NOME_IDX(P_INDICE);
    P_COMANDO := 'CREATE INDEX ' || P_INDICE || ' ON ' || P_TABELA || '(' || P_COLUNAS || ') tablespace SANKIND;';
    DBMS_OUTPUT.PUT_LINE(P_COMANDO);
  END LOOP;
  CLOSE CURIDX;
END;
END;
/
</code></pre>
<p>The problem is that even with this logic, the code still attempts to create two indexes with the same name:</p>
<pre><code>SQL&gt; CREATE INDEX XXXX_IDX_FK_XXXX ON XXXX(XXXXX);
Index created.

SQL&gt; CREATE INDEX XXXX_IDX_FK_XXXX ON XXXX(XXXXX);
ERROR at line 1:
ORA-00955: name is already used by an existing object
</code></pre>
<p>Clearly, the function is not correctly ensuring unique index names. Can someone help me identify what’s wrong with the logic in the SNK_VERIFICA_NOME_IDX function or the way it’s being used in the procedure?</p>
<p>Edit: the problem is that I am using dbms_output.put_line.</p>
<p>The reason why using DBMS_OUTPUT.PUT_LINE doesn’t work in my case is because it only prints the SQL command as a string — it does not execute the statement.</p>
<p>As a result, the index is never actually created, so the Oracle data dictionary (e.g., USER_INDEXES) is not updated. My function SNK_VERIFICA_NOME_IDX, which checks if the index name already exists, keeps returning that the name is available — even though you've already &quot;printed&quot; a command with that name — because the command wasn't executed.</p>
<p>In contrast, when I use EXECUTE IMMEDIATE, the index is actually created in the database, which updates the data dictionary immediately. This allows your name-checking logic to work correctly on the next iteration.</p>
",1,1,0,2025-07-22T14:32:21+00:00,1,126,True
79711547,29282840,,sql,Check a MySQL 8 database table to see if a user has logged,"<p>I need to check a MySQL 8 database table to see if a user has logged in for at least two days in January of the current year.</p>
<p>If there are at least two days recorded, I update a second MySQL table in the same database with the value OK, otherwise with the value KO.</p>
<p>This is my sproc.</p>
<p>I have this error because in fact the dates on which the user <em>foo</em> logged in are 2, the days:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>tDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-05</td>
</tr>
<tr>
<td>2025-01-02</td>
</tr>
</tbody>
</table></div>
<blockquote>
<p>Procedure execution failed</p>
<p>1172 - Result consisted of more than one row</p>
<p>Time: 0,518s</p>
</blockquote>
<pre><code>CREATE DEFINER=`root`@`%` PROCEDURE `dCreate_20250723`( )
BEGIN
    
DECLARE retval INT;
  
FLUSH HOSTS;
    
SET @s = CONCAT('SELECT DISTINCT COUNT(B.tDate) INTO @n 
             FROM `dotable_1` B 
             WHERE 
             YEAR ( B.tDate ) = 2025 
             AND MONTH ( B.tDate ) = 1 
             AND B.tUsers = ''foo''
             GROUP BY DATE(B.tDate);');

FLUSH HOSTS; 

PREPARE `stmt` FROM @`s`;
SET @`s` := NULL;
EXECUTE `stmt`;
DEALLOCATE PREPARE `stmt`;
     
SET retval := NULL;
 
SET retval = @n;     
 
IF retval &gt;= 2 THEN
        
UPDATE `dotable_2` SET tCheck = &quot;OK&quot; WHERE tUsers = 'foo';
     
ELSE

UPDATE `dotable_2` SET tCheck = &quot;KO&quot; WHERE tUsers = 'foo';  
    
END IF;
     
SET retval := NULL; 

END
</code></pre>
",3,3,0,2025-07-23T08:34:29+00:00,2,74,True
79711592,15445261,,sql,KWIC in Text database,"<p>In a text database every word of the text is in a single row an has it's unique Id, which determines the sequence. The DB is static, the text will not be changed, no insertions or deletions will be made. So it looks like this:</p>
<p>Tablename = &quot;text&quot;</p>
<pre><code>Id | Word
---------
1   | The
2   | texts
3   | articles
4   | and
5   | conversations
6   | are
7   | brief
8   | and
9   | appropriate
10  | for
11  | all
... | ...
</code></pre>
<p>Now, the user should be able to do a KWIC (keyword in context) search, the result should be, e.g. when the user searches for &quot;conversations&quot; and context is 2 (that is 2 words to the left and 2 word to the right) the result should be &quot;articles and conversations are brief&quot;.</p>
<p>When the user searches for a word, which occurs multiple times, they should get multiple rows as a result, e.g. if they search for &quot;and&quot; in the given example and the context is 2 again, they should get both &quot;texts articles and conversations are&quot; and &quot;are brief and appropriate for&quot;.</p>
<p>So far I have tried this:</p>
<pre><code>SELECT * FROM text 
   WHERE Id 
       BETWEEN ((SELECT Id FROM text WHERE Word='conversations') - 2) 
           AND ((SELECT Id FROM text WHERE Word='conversations') + 2)
</code></pre>
<p>This works fine, but only if the searched word occurs only once in the text. (Which is  the case with &quot;conversations&quot;.) If the search term occurs multiple times in the text (e.g. if &quot;conversations&quot; in the query is substituted with &quot;and&quot;), it crashes, probably because the result of the inner SELECTs in this case is not a single Id, but a list of IDs. I would appreciate if somebody could give me advice on how to deal with multiple occurrences of the search term. Thank you!</p>
<p>---- Edit -----</p>
<p>Thank you, Dale, for the hint with LEAD and LAG! I've tried this together with the CONCAT function and the query now looks like this:</p>
<pre><code>SELECT Word, temp FROM
     (SELECT Word, CONCAT ( 
          LAG(Word, 1) OVER (ORDER BY Id), 
          ' &lt;B&gt;', Word, '&lt;/B&gt; ', 
          LEAD(Word, 1) OVER (ORDER BY Id)) 
          AS temp FROM text) t 
          WHERE Word='and';
</code></pre>
<p>It now gives the results &quot;articels <strong>and</strong> conversations&quot; and &quot;brief <strong>and</strong> appropriate&quot; as expected. Furthermore, the searched word (&quot;and&quot;) can be bolded, which is a great byproduct.
Now, I am stuck with the problem, how to expand the range from one single word left and right of the searched word to a another number, eg. 10 or max. 20. This number should be determined by the user in the final application. Is there a way, how this could be accomplished?</p>
",4,4,0,2025-07-23T09:03:59+00:00,2,199,True
79711703,31123186,,sql,Check dates within same table,"<p>This is my table from SQL and the output that should be. What I need is to merge both dates and their entries.</p>
<p>I tried both queries, IN and OUT, but unable to merge.</p>
<p>IN:</p>
<pre><code>SELECT 
    MAX(one.TOTAL_IN) AS TOTAL_IN, one.Date_In 
FROM
    (SELECT 
         CAST(ST.DateIn AS DATE) AS Date_In, 
         COUNT(ST.ID) OVER (ORDER BY CAST(ST.Datein AS DATE)) AS TOTAL_IN 
     FROM 
         Table ST) AS one 
GROUP BY
    one.Date_In
</code></pre>
<p>OUT:</p>
<pre><code>SELECT 
    MAX(two.TOTAL_OUT) AS TOTAL_OUT, two.Date_Out
FROM
    (SELECT 
         CAST(ST.DateOut AS DATE) AS Date_Out,
         COUNT(ST.ID) OVER (PARTITION BY CAST(ST.DateOut AS DATE) 
                            ORDER BY CAST(ST.DateOut AS DATE)) AS TOTAL_OUT
     FROM 
         Table ST) AS two
GROUP BY
    two.Date_Out
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Date_IN</th>
<th>Date_OUT</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2025-06-02</td>
<td>2025-06-13</td>
</tr>
<tr>
<td>2</td>
<td>2025-06-02</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>2025-06-03</td>
<td>2025-06-07</td>
</tr>
<tr>
<td>4</td>
<td>2025-06-04</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>2025-06-04</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>2025-06-04</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>2025-06-06</td>
<td>2025-06-07</td>
</tr>
<tr>
<td>8</td>
<td>2025-06-07</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>2025-06-08</td>
<td>2025-06-08</td>
</tr>
<tr>
<td>10</td>
<td>2025-06-11</td>
<td>2025-06-11</td>
</tr>
<tr>
<td>11</td>
<td>2025-06-12</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>2025-06-13</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>2025-06-14</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Desired output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>TOTAL_IN</th>
<th>DATE</th>
<th>TOTAL_OUT</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>2025-06-02</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2025-06-03</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>2025-06-04</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2025-06-06</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2025-06-07</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>2025-06-08</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2025-06-11</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2025-06-12</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2025-06-13</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2025-06-14</td>
<td></td>
</tr>
</tbody>
</table></div>
",3,3,0,2025-07-23T10:21:27+00:00,3,124,True
79711835,402281,,sql,Stateless REST API to poll new records,"<p>I'm working on this application that records operations performed within the application as <em>events</em>. Each operation is recorded as a separate row in the table <code>events</code>. The row is written in the same serializable database transaction that's also performing the operation.</p>
<p>I would like to add an API that allows polling this table in a way that returns exactly the rows that have been added since the last call. The API should be stateless, meaning that the server does not need to keep track of which rows have already been returned. Instead, the client should pass some information (cookie/cursor) in the request that allows the server to figure out which rows are new. I'm having trouble figuring out a scheme that works reliably.</p>
<p>This is what I currently have:</p>
<p>The database schema currently looks like this (reduced example):</p>
<pre class=""lang-sql prettyprint-override""><code>create table events (
    id int generated by default as identity primary key,
    message varchar,
    timestamp timestamptz
);
</code></pre>
<p>And rows are inserted like this. The ID is generated automatically by the sequence object attached to the <code>id</code> column:</p>
<pre class=""lang-sql prettyprint-override""><code>insert into events (message, timestamp) values ('hello', now());
</code></pre>
<p>The API allows passing an <code>after_id</code> parameter, which limits the result to events with an ID larger than the passed value.</p>
<pre class=""lang-none prettyprint-override""><code>// GET /v1/events?after_id=123
{
  &quot;events&quot;: [
    {
      id: 124,
      message: &quot;...&quot;,
      timestamp: &quot;...&quot;,
    },
    // ...
  ]
}
</code></pre>
<p>The problem with this is that even though the ID of each row is generated by a sequence object, the database doesn't guarantee that transactions will commit in the order of the IDs that they got from the sequence. For example:</p>
<ul>
<li>Transaction 1 starts.</li>
<li>Transaction 1 gets ID 125 from the sequence.</li>
<li>Transaction 2 starts.</li>
<li>Transaction 2 gets ID 126 from the sequence.</li>
<li>Transaction 2 commits.</li>
<li>API is accessed with <code>after_id=124</code> and returns the row with ID 126.</li>
<li>Transaction 1 commits.</li>
<li>API is accessed with <code>after_id=126</code> and returns no rows.</li>
</ul>
<p>In this example, the row with ID 125 has silently been skipped.</p>
<p><strong>What schemes exist to solve this problem?</strong></p>
<p>I could of course force all transaction to run in sequence, e.g. by locking the <code>events</code> table, but that would impact performance by quite a lot, as often there are dozens of such transactions running in parallel.</p>
",1,1,0,2025-07-23T11:55:44+00:00,1,77,True
79711921,10424501,,sql,Using a script variable to generate a stored procedure in Postgresql,"<p>I would like to use a script variable to set the data type, so that I can generate a few functions with different types, like so:</p>
<p><code>\set bucket_data_type DATE</code></p>
<p>Then run a script to generate this function:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION insert(
    ids BIGINT[],
    types TEXT[],
    buckets :bucket_data_type[]
) RETURNS VOID
LANGUAGE PLPGSQL
AS $$
BEGIN
   EXECUTE 
    FORMAT('INSERT INTO myTable(id, type, buckets)
    SELECT _.id, _.type, _.bucket
    FROM(
        SELECT unnest(%L::bigint[]) AS monitor_id,
               unnest(%L::text[]) AS feature_type,
               unnest(%L::'|| :bucket_data_type ||'[]) AS bucket
        ) _
        ON CONFLICT DO NOTHING;',
           ids, types, buckets);
END
$$;
</code></pre>
<p>and then again with <code>\set bucket_data_type TIMESTAMP</code>, for example.</p>
<p>It works for the parameter definitions, but <em>not</em> the SQL inside the FORMAT block.</p>
<p>I am expecting something like</p>
<pre><code>\set bucket_data_type DATE

&lt;run create script as per above&gt;
</code></pre>
<p>to return</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION insert(
    ids BIGINT[],
    types TEXT[],
    buckets DATE[]
) RETURNS VOID
LANGUAGE PLPGSQL
AS $$
BEGIN
   EXECUTE 
    FORMAT('INSERT INTO myTable(id, type, buckets)
    SELECT _.id, _.type, _.bucket
    FROM(
        SELECT unnest(%L::bigint[]) AS monitor_id,
               unnest(%L::text[]) AS feature_type,
               unnest(%L::DATE[]) AS bucket
        ) _
        ON CONFLICT DO NOTHING;',
           ids, types, buckets);
END
$$;
</code></pre>
<p>but I'm unable to get <code>unnest(%L::DATE[]) AS bucket</code> to appear.</p>
<p>I either get format errors like:</p>
<pre><code>ERROR:  syntax error at or near &quot;:&quot;
LINE 15:                unnest(%L::'|| :bucket_data_type ||'[]) AS bu...
</code></pre>
<p>or I simply get <code>:bucket_data_type</code> as a string inside the execute block.</p>
<p>I've tried <code>SELECT set_config('tsp.bucket_data_type', :'bucket_data_type', false);</code> as well (as this works in DO BLOCKS where things are string literals).</p>
<p>Is this even possible?</p>
",3,3,0,2025-07-23T12:55:56+00:00,3,120,True
79711964,12035358,,sql,How can I get a list of referenced tables from a query?,"<p>I have stored queries in a table that run when I want to validate the model. I want to get a list of every table that query references so I can check if a query needs to run only if a certain table is updated:</p>
<pre class=""lang-sql prettyprint-override""><code>SET SHOWPLAN_XML ON;
GO

EXEC sp_executesql @code

GO
SET SHOWPLAN_XML OFF;
</code></pre>
<p>This doesn't work. It will show only that it's going to execute code in the query plan, even if you just pasted in the query instead of using <code>sp_executesql</code> as you can't use the result in forming a list because <code>SHOWPLAN</code> is turned on. It's possible to get query plans with <code>sys.dm_exec_query_plan()</code>, but that requires running the query and having a plan handle which I don't know how to find.</p>
<p>How can I get a list of accessed tables for a query?</p>
<p>Process action is not connected to flow:</p>
<pre class=""lang-sql prettyprint-override""><code>create table #links (
    process_flow_id varchar(100) collate database_default,
    process_action_id varchar(100) collate database_default,
    primary key (process_flow_id, process_action_id)
)

insert into #links
select
    a.process_flow_id,
    a.process_action_id
from process_action a
where a.model_id = @model_id
  and a.branch_id = @branch_id
  and a.process_action_type = 98

while @@ROWCOUNT &gt; 0
begin
    insert into #links
    select
        s.process_flow_id,
        s.next_process_action_id
    from process_step s
    join #links l
      on l.process_flow_id   = s.process_flow_id
     and l.process_action_id = s.last_process_action_id
    where s.model_id = @model_id
      and s.branch_id = @branch_id
      and s.last_process_action_id &lt;&gt; s.next_process_action_id
      and not exists(select 1
                     from #links n
                     where n.process_flow_id = s.process_flow_id
                       and n.process_action_id = s.next_process_action_id)
    group by
        s.process_flow_id,
        s.next_process_action_id
end

insert into validation_msg
(
    model_id,
    branch_id,
    validation_id,
    pk_col_1,
    pk_col_2,
    pk_col_3,
    pk_col_4,
    generated,
    insert_user,
    insert_date_time,
    update_user,
    update_date_time
)
select
    p.model_id,
    p.branch_id,
    @validation_id,
    p.model_id,
    p.branch_id,
    p.process_flow_id,
    p.process_action_id,
    1,
    dbo.tsf_user(),
    sysdatetime(),
    dbo.tsf_user(),
    sysdatetime()
from process_action p
where model_id    = @model_id
and branch_id  = @branch_id
and not exists (select 1
                  from #links l
                  where l.process_flow_id   = p.process_flow_id
                    and l.process_action_id = p.process_action_id)

drop table #links
</code></pre>
<p>Table has no primary key:</p>
<pre class=""lang-sql prettyprint-override""><code>insert into validation_msg
(
    model_id,
    branch_id,
    validation_id,
    pk_col_1,
    pk_col_2,
    pk_col_3,
    pk_col_4,
    pk_col_5,
    pk_col_6,
    pk_col_7,
    generated,
    insert_user,
    insert_date_time,
    update_user,
    update_date_time
)
select
    @model_id,
    @branch_id,
    @validation_id,
    @model_id as pk_col_1,
    @branch_id as pk_col_2,
    t.tab_id as pk_col_3,
    null as pk_col_4,
    null as pk_col_5,
    null as pk_col_6,
    null as pk_col_7,
    1,
    dbo.tsf_user(),
    sysdatetime(),
    dbo.tsf_user(),
    sysdatetime()
from tab t
where t.model_id      = @model_id
  and t.branch_id  = @branch_id
  and not exists(select 1
                 from col c
                 where c.model_id     = t.model_id
                   and c.branch_id = t.branch_id
                   and c.tab_id         = t.tab_id
                   and c.primary_key    = 1)
</code></pre>
",2,2,0,2025-07-23T13:22:23+00:00,3,189,True
79711988,17537072,Colombia,sql,Optionally SELECT TOP,"<p>I want to have a query that can bring the top N results in some cases, and in others bring everything. Basically I need to optionally <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql"" rel=""nofollow noreferrer"">select the top</a> N elements from the query.</p>
<p><a href=""https://stackoverflow.com/questions/175962/dynamic-select-top-var-in-sql-server"">I know you can do</a> <code>SELECT TOP (@Var)</code> where <code>@Var</code> can be any number, but once I put the TOP clause I always have to provided a value, I have not found a way to omit the TOP clause entirely</p>
<p>I can do</p>
<pre class=""lang-sql prettyprint-override""><code>IF (@Var IS NOT NULL)
    SELECT TOP (@Var) ...
ELSE
    SELECT ...
</code></pre>
<p>But I'd have to repeat the select statement with all the columns and filters, which is not ideal.</p>
<p>A cleaner solution might be to set <code>@Var</code> to the biggest possible number</p>
<pre><code>IF (@Var IS NULL) SET @Var = 9223372036854775807;
SELECT TOP (@Var) ...
</code></pre>
<p>but I don't know if there is a better way, or at least a keyword to referece this number not to use this &quot;magic constant&quot; like <code>SET @Var = INFINITY</code> or <code>SET @Var = bigint.max</code> or something similar to what other languages have.</p>
<p>Thank you in advance</p>
",2,2,0,2025-07-23T13:34:27+00:00,4,163,True
79712352,6579736,,sql,Case expression to set a variable that is then used in another case expression,"<p>I have the following SQL query (VERY simplified):</p>
<pre><code>SELECT
    Column1,
    Column2,
    Column3,
    Column4,
    CASE
        WHEN [Field1] = Condition1 THEN Value1
        WHEN [Field1] = Condition2 THEN Value2
        ELSE Value2
    END AS &quot;Label1&quot;, 
    CASE @Var
        WHEN Condition 1 THEN
            CASE [Field2]
                WHEN Condition1 THEN Value1
                WHEN Condition2 THEN Value2
                WHEN Condition3 THEN Value3
                WHEN Condition4 THEN Value4
                ELSE Value2
            END
        ELSE
            CASE [Field2]
                WHEN Condition5 THEN Value5
                WHEN Condition6 THEN Value6
                WHEN Condition7 THEN Value7
                WHEN Condition8 THEN Value8
                ELSE Value2
            END
    END AS &quot;Label2&quot;, 
</code></pre>
<p>I need to use the result of the first <code>CASE</code> expression to evaluate the second <code>CASE</code> expression. I thought that using a variable (<code>@Var</code>) would be the best way, however I can't get the result of the first <code>CASE</code> expression assigned to the variable.</p>
<p>I have tried:</p>
<ol>
<li><code>SET @Var</code> in front of the <code>CASE</code> clause</li>
<li><code>SET @Var =</code> in each <code>WHEN</code> clause</li>
<li>Removing the <code>AS Label1</code> from the end of the <code>CASE</code> expression</li>
<li>Making two copies of the <code>CASE</code> expression, using one of the variable and the other for the <code>Field1</code> value</li>
</ol>
<p>Every attempt to add the variable to the first <code>CASE</code> expression returns an error. I've been able to add the nested <code>CASE</code> expression (and test it) properly, but I need to be able to examine the result of the first <code>CASE</code> expression in order to properly set the second.</p>
<p>I'm not fixed on using a variable to accomplish this requirement.</p>
",0,0,0,2025-07-23T17:24:57+00:00,2,156,True
79712917,5548886,,sql,How can I improve speed of this query?,"<ul>
<li><p><code>TABLE_C</code> a list of masters, ~66k records.</p>
</li>
<li><p><code>TABLE_R</code> a list with detailed information, ~1m records.</p>
</li>
<li><p><code>TABLE_SUB</code> a subset list (the target list), ~23k records.</p>
</li>
</ul>
<p>When I <code>join</code> to or add any information located in <code>Table_C</code> it slows the query down from 1 second to 33 seconds:</p>
<pre><code>SELECT
    TS.MasterID,
    TS.RecordNr,
    TR.MasterName
    TR.Date1,
    C.MasterName
FROM
    TABLE_SUB TS
INNER JOIN
    TABLE_R TR ON TS.MasterID = TR.MasterID
               AND TS.RecordNr = TR.RecordNr
INNER JOIN
    TABLE_C C ON TS.MasterID = C.MasterID
</code></pre>
<p>I have tried:</p>
<ul>
<li><p>Deriving from <code>TABLE_C</code> - no change. I am deriving from <code>TABLE_SUB</code> so I can exclude <code>TABLE_C</code> as process of elimination to identify issue.</p>
</li>
<li><p>Joining <code>TABLE_C</code> directly onto <code>TABLE_R</code> via <code>MasterID</code> - no change.</p>
</li>
<li><p>Excluding <code>TABLE_C</code> - 1 second return. This is a smaller list as filtered via join from <code>TABLE_SUB</code> but <code>SELECT * from TABLE_R</code> takes 23 seconds; a <code>SELECT</code> from <code>MasterID</code> takes 10 seconds.</p>
</li>
<li><p>Adding a non-clustered index on all tables with just the <code>MasterID</code>.</p>
</li>
</ul>
<p>The execution plan shows they are all clustered- or index scans and two inner joins. The <code>RecordNr</code> has to be <code>varchar</code> as it's from systems I have no control over. I created a new unique key on <code>TABLE_R</code> to see if it made a difference for <code>int.MasterID</code> and <code>int.RecordID</code> which it did not.</p>
<p>I added a <code>OPTION(FORCE ORDER)</code> and it brought my initial query down to 2 seconds. I did an <code>update STATISTICS</code> with no outcome.</p>
<p><a href=""https://www.brentozar.com/pastetheplan/?id=ZR3OI2EyB6"" rel=""nofollow noreferrer"">Query plan</a>.</p>
<pre><code>CREATE TABLE [dbo].[Table_C](
    [MasterID] [int] IDENTITY(1,1) NOT NULL,
    [MasterGroupID] [int] NULL,
    [CODE] [varchar](20) NOT NULL,
    [StateID] [tinyint] NULL,
    [MasterName] [varchar](1000) NOT NULL,
    [Comment] [varchar](1000) NULL
CONSTRAINT [PK_Table_C] PRIMARY KEY CLUSTERED
(
    [MasterID] ASC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[TABLE_R](
    [MasterID] [int] NOT NULL,
    [RecordNr] [varchar](40) NOT NULL,
    [MatterID] [int] IDENTITY(1,1) NOT NULL,
    [StateID] [tinyint] NULL,
    [IsElectronicFile] [bit] NULL,
    [Opened] [date] NULL,
    [DepartmentID] [tinyint] NULL,
    [RecordDescription] [varchar](500) NULL,
    [Notes] [varchar](2000) NULL,
    [Completed] [date] NULL
 CONSTRAINT [PK_TABLE_R] PRIMARY KEY CLUSTERED
(
    [MasterID] ASC,
    [RecordNr] ASC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[TABLE_SUB](
    [MasterID] [int] NOT NULL,
    [RecordNr] [varchar](40) NOT NULL,
    [ProjectID] [int] NULL,
    [StateID] [tinyint] NULL,
    [DepartmentID] [tinyint] NULL,
    [Comment] [varchar](200) NULL
 CONSTRAINT [PK_Table_Sub_1] PRIMARY KEY CLUSTERED
(
    [MasterID] ASC,
    [RecordNr] ASC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]
) ON [PRIMARY]
</code></pre>
",-2,1,3,2025-07-24T07:14:21+00:00,2,195,False
79713334,6145729,,sql,Strip numbers from string after certain characters into a recursive table,"<p>I have unstructured, free-form data, which requires cleansing to produce a clean list of all PO numbers in a column. Unfortunately, the column can accommodate multiple PO numbers in various formats (nightmare!!).</p>
<p>See test example:-</p>
<pre><code>DECLARE @temp TABLE
(
    string VARCHAR(50)
)

INSERT INTO @temp (string)
VALUES 
    ('Po998 blah blah po1001'),
    ('PO 999'),
    ('PO 1000 12345 blah PO1002')
</code></pre>
<p>Ideally, I'm looking for a single table which holds the following:-</p>
<p>PO - All PO numbers will begin with the letters' PO' and vary in length.</p>
<ol>
<li>998</li>
<li>999</li>
<li>1000</li>
<li>1001</li>
<li>1002</li>
</ol>
<p><strong>Note 12345 is not a PO</strong> and should not appear in the table.</p>
<p>Here is my current attempt to isolate the numerical values after the PO, but it does not give the correct results.</p>
<pre><code>with
clean_stg as (
    select  *,
            ltrim(SUBSTRING(string,CHARINDEX('po',string)+2,len(string))) Val
    from @temp
),

clean_po as (
            select b.*,
            LEFT(b.Val,PATINDEX('%[^0-9]%', b.Val+'a')-1) PO
from clean_stg b
) 

select * from clean_po;
</code></pre>
<p>I would love to do this with REGEX in Python, but I need to keep this in the SQL Server environment. I think a recursive Common Table Expression (CTE) might be my answer, but I'm unsure how to structure it best.</p>
",4,4,0,2025-07-24T12:16:24+00:00,1,101,True
79713650,2402240,Cape Town,sql,Counting unique records across two tables,"<p>I have two tables</p>
<p>The first table is my <code>INVOICEHEADER</code> which contains one record per invoice, but multiple records per order. <code>InvoiceNumber</code> is the unique key on this table (there is no table for <code>OrderNumber</code>, the combination of <code>OrderNumber &amp; AccountNumber</code> would be unique per order)</p>
<p>Columns:</p>
<pre><code>invoiceDate, InvoiceNumber, AccountNumber, OrderNumber
</code></pre>
<p>The second table in my <code>INVOICELINES</code> which contains multiple lines, each line is for a specific combination of invoice, container &amp; product, so multiple lines per invoice.</p>
<p><code>InvoiceNumber, LineNumber &amp; Product</code> is the unique key for this table</p>
<p>Columns:</p>
<pre><code>InvoiceNumber, LineNumber, Product, Quantity
</code></pre>
<p>I want to do two counts based on occurrences, not quantity, for a period of 1 year:</p>
<ol>
<li>Count how many times any product was part of an invoice (same product could be on an invoice multiple times but should be counted once per invoice)</li>
<li>Count how many times any product was part of an order (product could be on multiple invoices, each with multiple lines of the same product)</li>
</ol>
<p>So if I have 100 orders and 500 invoices, but product 137 was on 8 of those orders and those orders split across 30 invoices with product 137 on 15 of the invoice, I would want count 1 to be 15 and count 2 to be 8</p>
<p>I have this so far for query 1:</p>
<pre><code>SELECT 
    COUNT(DISTINCT(IL.PRODUCT))
FROM 
    INVOICELINES IL with (nolock)
INNER JOIN 
    INVOICEHEADER IH with (nolock) ON IH.INVOICENO = IL.INVOICENO
WHERE 
    IL.PRODUCT = '137' 
    AND CONVERT(DATE, IH.[DATE]) &gt; CONVERT(DATE, DATEADD(YEAR, -1, GETDATE()));
</code></pre>
",-1,1,2,2025-07-24T16:02:05+00:00,1,122,True
79713664,19629592,,sql,Parse multiple JSON items from a table cell,"<p>My table in SQL Server with an <code>nvarchar(max)</code> column holds multiple JSON arrays in each cell. I am trying to parse the 'result' items into separate rows.</p>
<p>JSON array:</p>
<pre><code>[
    {&quot;successful&quot;:true,
    &quot;fields&quot;:
        [{&quot;index&quot;:0, &quot;publicPath&quot;: &quot;date&quot;},
        {&quot;index&quot;:1, &quot;publicPath&quot;: &quot;accountID&quot;},
        {&quot;index&quot;:2, &quot;publicPath&quot;: &quot;amount&quot;},
        {&quot;index&quot;:3, &quot;publicPath&quot;: &quot;description&quot;}],
    &quot;result&quot;:
        [{&quot;0&quot;:2025-07-25&quot;,
          &quot;1&quot;: &quot;1234&quot;,
          &quot;2&quot;: &quot;100.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;},
         {&quot;0&quot;:2025-04-22&quot;,
          &quot;1&quot;: &quot;4567&quot;,
          &quot;2&quot;: &quot;12540.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;},
         {&quot;0&quot;:2025-02-22&quot;,
          &quot;1&quot;: &quot;1234&quot;,
          &quot;2&quot;: &quot;700.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;}]},
    {&quot;successful&quot;:true,
    &quot;fields&quot;:
        [{&quot;index&quot;:0, &quot;publicPath&quot;: &quot;date&quot;},
        {&quot;index&quot;:1, &quot;publicPath&quot;: &quot;accountID&quot;},
        {&quot;index&quot;:2, &quot;publicPath&quot;: &quot;amount&quot;},
        {&quot;index&quot;:3, &quot;publicPath&quot;: &quot;description&quot;}],
    &quot;result&quot;:
        [{&quot;0&quot;:3025-07-25&quot;,
          &quot;1&quot;: &quot;2222&quot;,
          &quot;2&quot;: &quot;2222.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;},
         {&quot;0&quot;:1025-04-22&quot;,
          &quot;1&quot;: &quot;4567&quot;,
          &quot;2&quot;: &quot;88540.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;},
         {&quot;0&quot;:2025-02-22&quot;,
          &quot;1&quot;: &quot;1234&quot;,
          &quot;2&quot;: &quot;700.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;}]},
    {&quot;successful&quot;:true,
    &quot;fields&quot;:
        [{&quot;index&quot;:0, &quot;publicPath&quot;: &quot;date&quot;},
        {&quot;index&quot;:1, &quot;publicPath&quot;: &quot;accountID&quot;},
        {&quot;index&quot;:2, &quot;publicPath&quot;: &quot;amount&quot;},
        {&quot;index&quot;:3, &quot;publicPath&quot;: &quot;description&quot;}],
    &quot;result&quot;:
        [{&quot;0&quot;:2025-07-25&quot;,
          &quot;1&quot;: &quot;1234&quot;,
          &quot;2&quot;: &quot;100.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;},
         {&quot;0&quot;:2025-04-22&quot;,
          &quot;1&quot;: &quot;4567&quot;,
          &quot;2&quot;: &quot;12540.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;},
         {&quot;0&quot;:2025-02-22&quot;,
          &quot;1&quot;: &quot;1234&quot;,
          &quot;2&quot;: &quot;876.00&quot;,
          &quot;3&quot;: &quot;Some description here&quot;}
        ]
    }
]
</code></pre>
<p>I only care about the result array within the JSON string. Multiple JSON strings are contained in individual cells within table <code>JSONTable</code> in column <code>Data</code>. I've had limited success with:</p>
<pre><code>SELECT
    JSON_Value(x.value, '$.result[*].&quot;0&quot;') AS 'date',
    JSON_Value(x.value, '$.result[*].&quot;1&quot;') AS 'accountID',
    JSON_Value(x.value, '$.result[*].&quot;2&quot;') AS 'amount',
    JSON_Value(x.value, '$.result[*].&quot;3&quot;') AS 'description'
FROM
    [JSONTable] js
CROSS APPLY
    OPENJSON([Data]) AS X
</code></pre>
<p>It gets only the last-most result from the JSON table, returning NULL for all other calls and does not segment individual results within each JSON segment into their own rows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>accountID</th>
<th>amount</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
</tr>
<tr>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
</tr>
<tr>
<td>2025-02-22</td>
<td>1234</td>
<td>876.00</td>
<td>Some description here</td>
</tr>
</tbody>
</table></div>
<p>It should parse individual results within each of the concatenated calls as well as all of the 'result' fields within each call and split them into their own rows (I am constrained by the JSON structure):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>accountID</th>
<th>amount</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-25</td>
<td>1234</td>
<td>100.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>2025-04-22</td>
<td>4567</td>
<td>12540.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>2025-02-22</td>
<td>1234</td>
<td>700.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>3025-07-25</td>
<td>2222</td>
<td>2222.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>1025-04-22</td>
<td>4567</td>
<td>88540.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>2025-02-22</td>
<td>1234</td>
<td>700.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>2025-07-25</td>
<td>1234</td>
<td>100.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>2025-04-22</td>
<td>4567</td>
<td>12540.00</td>
<td>Some description here</td>
</tr>
<tr>
<td>2025-02-22</td>
<td>1234</td>
<td>876.00</td>
<td>Some description here</td>
</tr>
</tbody>
</table></div>
",6,6,0,2025-07-24T16:09:32+00:00,2,137,True
79714042,26533539,,sql,SQL query to return roles and groups a user has that are not related to a toplevel role,"<p>I have a set of tables in SQL Server that all join to each other and I'm trying to find a way to find a negative relationship. My tables also have recursion.</p>
<p>The data is here: <a href=""https://sqlfiddle.com/sql-server/online-compiler?id=6e751c79-9e50-456d-a31f-1757f05bcbcf"" rel=""nofollow noreferrer"">https://sqlfiddle.com/sql-server/online-compiler?id=6e751c79-9e50-456d-a31f-1757f05bcbcf</a></p>
<p>I have objects users, groups, and roles, as well as join tables. I put all of the tables below. Users can have groups and user can be in roles. Roles grant access to groups. So in completeness, users can have groups either direct OR because of a role.</p>
<p>Then the next complication is that a role can grant access to another role. This is where the recursion comes in. The parent role grants access to the child role. So if a user has the parent, they also get all of the children on down the link.</p>
<p>Finally, a role can be a toplevel role. Just a bit.</p>
<p>The question then is can we get in a single statement all of the roles and groups that a user has that are NOT part of a toplevel role that the user has access to.</p>
<p>I can get a list of the groups a user has assigned directly, as well as a listing of the roles a user has, with the groups.</p>
<p>But I can't figure out how to move beyond that. If a role is a parent/child, how do I include the groups in the child? Then, the most important part, is how do I exclude from these results anything that is in a toplevel role hierarchy?</p>
<p>In my sample data, let's take Alice, uid=1. Alice has roles 1 and 3. Alice has ALL groups assigned directly. Role 1 is a parent and grants role 2. Role 1 is a toplevel role. I want to show everything that Alice has that is NOT related to any of her toplevel roles. Alice has role 1 which grants role 2. Role 2 grants groups 1 and 2.</p>
<p>My result should looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>uid</th>
<th>name</th>
<th>roleid</th>
<th>rolename</th>
<th>gid</th>
<th>groupname</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>alice</td>
<td>3</td>
<td>Role C</td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>alice</td>
<td></td>
<td></td>
<td>3</td>
<td>group 3</td>
</tr>
</tbody>
</table></div>
<p>Alice has group3 and role 3 because they are not in the chain of anything that the toplevel role1 grants.</p>
<p>I can do this with queries and looping in code, but can it be done with SQL, even if with a CTE? In general, I could have 5 or more levels of a role within a role.</p>
<p>Thank you.</p>
<pre><code>CREATE TABLE [dbo].[groups]
(
    [gid] [int] NOT NULL,
    [groupname] [varchar](50) NOT NULL
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[roles]
(
    [roleid] [int] NOT NULL,
    [rolename] [varchar](50) NOT NULL,
    [toplevel] [bit] NULL
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[users]
(
    [uid] [int] NOT NULL,
    [name] [varchar](50) NULL
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[usergroup]
(
    [uid] [int] NOT NULL,
    [gid] [int] NOT NULL
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[userrole]
(
    [uid] [int] NOT NULL,
    [roleid] [int] NOT NULL
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[rolegroup]
(
    [roleid] [int] NOT NULL,
    [gid] [int] NOT NULL
) ON [PRIMARY]
GO

CREATE TABLE [dbo].[rolerole]
(
    [partentid] [int] NOT NULL,
    [childid] [int] NOT NULL
) ON [PRIMARY]
GO

INSERT INTO [users] VALUES (1,'alice')
INSERT INTO [users] VALUES (2,'bob')
INSERT INTO [users] VALUES (3,'chris')

INSERT INTO [groups] VALUES (1,'group 1')
INSERT INTO [groups] VALUES (2,'group 2')
INSERT INTO [groups] VALUES (3,'group 3')

INSERT INTO [roles] VALUES (1,'Role A',1)
INSERT INTO [roles] VALUES (2,'Role B',0)
INSERT INTO [roles] VALUES (3,'Role C',0)

INSERT INTO rolerole VALUES (1,2)

INSERT INTO rolegroup VALUES (2,1)
INSERT INTO rolegroup VALUES (2,2)
INSERT INTO rolegroup VALUES (3,1)
INSERT INTO rolegroup VALUES (3,2)

INSERT INTO userrole VALUES (1,1)
INSERT INTO userrole VALUES (1,3)
INSERT INTO userrole VALUES (2,1)
INSERT INTO userrole VALUES (3,2)

INSERT INTO usergroup VALUES (1,1)
INSERT INTO usergroup VALUES (1,2)
INSERT INTO usergroup VALUES (1,3)

INSERT INTO usergroup VALUES (3,1)
INSERT INTO usergroup VALUES (3,2)


SELECT * 
FROM groups
INNER JOIN usergroup ON usergroup.gid = groups.gid
INNER JOIN users ON users.uid = usergroup.uid
WHERE users.uid = 1


SELECT * 
FROM users
LEFT JOIN userrole ON userrole.uid = users.uid  
LEFT JOIN roles ON roles.roleid = userrole.roleid
LEFT JOIN rolegroup ON rolegroup.roleid = userrole.roleid
LEFT JOIN groups ON groups.gid = rolegroup.gid
WHERE users.uid = 1
</code></pre>
",4,5,1,2025-07-24T23:05:16+00:00,2,234,True
79714236,11202670,,sql,Query with limit offset - bad performance despite index,"<p>Hsqldb 2.7.4</p>
<p>If I use OFFSET in an index query, the index should be used for offset calculation, as far as I understand the documentation!?</p>
<p>That does not work for me. Any ideas?</p>
<pre><code>SELECT id FROM news ORDER BY id DESC LIMIT 25 offset 25;
</code></pre>
<p>--&gt; 17ms</p>
<pre><code>SELECT id FROM news ORDER BY id DESC LIMIT 25 offset 80000;
</code></pre>
<p>--&gt;1.31 seconds.</p>
<p>ID is the primary index.</p>
<pre><code>CREATE TABLE NEWS
(
   ID                            INTEGER                    GENERATED BY DEFAULT AS IDENTITY,
.... other fields  );

ALTER TABLE NEWS
   ADD PRIMARY KEY (ID);
</code></pre>
<p>Explain the plan:</p>
<pre><code>isDistinctSelect=[false]
isGrouped=[false]
isAggregated=[false]

columns=[  COLUMN: PUBLIC.NEWS.ID not nullable

]
[range variable 1
  join type=INNER
  table=NEWS
  cardinality=81400
  access=FULL SCAN
  join condition = [index=SYS_IDX_SYS_PK_10815_10822
  ]
  ]]
order by=[
COLUMN: PUBLIC.NEWS.ID
DESC
uses index]
offset=[
VALUE = 80000, TYPE = INTEGER]
limit=[
VALUE = 25, TYPE = INTEGER]
PARAMETERS=[]
SUBQUERIES[]
</code></pre>
<p>Thank you all for any help!</p>
<p>The documentation says: Indexes and ORDER BY, OFFSET and LIMIT
HyperSQL can use an index on an ORDER BY clause if all the columns in ORDER BY are in a single-column or multi-column index (in the exact order). This is important if there is a LIMIT n (or FETCH n ROWS ONLY) clause. In this situation, the use of an index allows the query processor to access only the number of rows specified in the LIMIT clause, instead of building the whole result set, which can be huge.</p>
<p>UPDATE:
If I do not use offset but a where clause with ID, that also doesn't help:</p>
<p>SELECT id FROM news where id&gt;80000 ORDER BY id DESC limit 25
--&gt; 12ms</p>
<p>SELECT id FROM news where id&gt;80 ORDER BY id DESC limit 25
--&gt; 1.5 seconds</p>
",0,0,0,2025-07-25T05:45:56+00:00,0,114,False
79714577,28448761,,sql,SQL Syntax: CASE &lt;COLUMN NAME&gt; for MULTIPLE values using IN - is it just not valid?,"<p>I have a question related to SQL syntax. Why does syntax A work, and syntax B does not?</p>
<pre><code>CREATE TABLE tblTest 
(
    Event VARCHAR(255),
    Description VARCHAR(255)
) 
</code></pre>
<p>Syntax A - this is working:</p>
<pre><code>SELECT
    Description, Event,
    CASE Description
        WHEN 'Value1' THEN 'Result1'
        WHEN 'Value2' THEN 'Result2'
        WHEN 'Value3' THEN 'Result3'
        ELSE 'DefaultResult'
    END AS NewColumn
FROM
    tblTest
</code></pre>
<p>Syntax B: this is <strong>not</strong> working:</p>
<pre><code>SELECT
    Description, Event,
    CASE Description
        WHEN IN ('Value1', 'Value2') THEN 'Result1 or 2'
        WHEN 'Value3' THEN 'Result3'
        ELSE 'DefaultResult'
    END AS NewColumn
FROM
    tblTest
</code></pre>
",-2,0,2,2025-07-25T10:31:03+00:00,1,102,True
79714624,8632124,,sql,WP error - Use placeholders and $wpdb-&gt;prepare(); found $sql,"<p>I created a WP plugin and I am testing it in Plugin Check (PCP) but it returns an error like this;</p>
<blockquote>
<p>Use placeholders and $wpdb-&gt;prepare(); found $sql</p>
</blockquote>
<p>This is my code:</p>
<pre><code>$db = STEPUP_Database::getInstance();
$table1 = $db-&gt;tb_lp_sections;
$sql = &quot;SELECT COUNT(*) FROM {$table1} WHERE section_course_id = %d&quot;;
$query = $db-&gt;wpdb-&gt;prepare( $sql, $post_id );
$result = $db-&gt;wpdb-&gt;get_var( $query );
</code></pre>
<p>Please help me to resolve this error. I tried to use some wordpress documentation like these:</p>
<p><a href=""https://developer.wordpress.org/reference/classes/wpdb/#protect-queries-against-sql-injection-attacks"" rel=""nofollow noreferrer"">https://developer.wordpress.org/reference/classes/wpdb/#protect-queries-against-sql-injection-attacks</a></p>
<p><a href=""https://ottopress.com/2013/better-know-a-vulnerability-sql-injection/"" rel=""nofollow noreferrer"">https://ottopress.com/2013/better-know-a-vulnerability-sql-injection/</a></p>
<p>I want to resolve my custom plugin error.</p>
",2,2,0,2025-07-25T11:07:30+00:00,2,108,True
79716239,17106708,,sql,Reverse order of split string parts,"<p>My simple Oracle SQL select statement:</p>
<pre><code>select id, my_column
from my_table;
</code></pre>
<p>returns the following output with over 1.000.000 rows in the result set:</p>
<pre><code>| id  | my_column   |
| 1   | abc.mno.xyz |
| 2   | ab.cd       |
| 3   | abcde       |
| ... | ...         |
</code></pre>
<p>But I need the output to look like this:</p>
<pre><code>| id  | my_column   |
| 1   | xyz.mno.abc |
| 2   | cd.ab       |
| 3   | abcde       |
| ... | ...         |
</code></pre>
<p>How can I reverse the order (of the <strong>string</strong> values in <code>my_column</code>) of the with &quot;.&quot; split parts, but not the order in the split parts in a performant way?</p>
<p>Thank you</p>
",-6,1,7,2025-07-27T08:05:51+00:00,2,119,True
79716450,4050048,,sql,Assign default date to row having maximum date1 and then assign other rows as date2-1 where date2 can have null or duplicate,"<p>I have below data :</p>
<p>Here as can be seen that Date2 is order by desc and</p>
<p>For id = 6 max(date2) = 07/02/2013 so enddate is assigned as 31/dec/2050 and then for previous rows having date1 = 13/04/2006 enddate = 20/12/2012(previous date1 -1)  then for next 2 previous rows dates are same as 22/12/2000 so endate = 12/04/2012 (previous date1 -1 ).</p>
<p>It will be same for both rows as date1 is same and next rows are null so it will again be same till the date changes.</p>
<p>So if date1 = 13-mar-2000 the enddate = 21/12/2000 (previous date1 -1 )</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>DATE1</th>
<th>DATE2</th>
<th>Enddate</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>11/01/2000</td>
<td>5/01/2001</td>
<td>12/03/2000</td>
</tr>
<tr>
<td>6</td>
<td>11/01/2000</td>
<td>10/01/2001</td>
<td>12/03/2000</td>
</tr>
<tr>
<td>6</td>
<td>11/01/2000</td>
<td>15/01/2001</td>
<td>12/03/2000</td>
</tr>
<tr>
<td>6</td>
<td>null</td>
<td>1/01/2001</td>
<td>21/12/2000</td>
</tr>
<tr>
<td>6</td>
<td>13/03/2000</td>
<td>8/01/2001</td>
<td>21/12/2000</td>
</tr>
<tr>
<td>6</td>
<td>13/03/2000</td>
<td>1/02/2001</td>
<td>21/12/2000</td>
</tr>
<tr>
<td>6</td>
<td>null</td>
<td>5/02/2001</td>
<td>12/04/2012</td>
</tr>
<tr>
<td>6</td>
<td>null</td>
<td>5/03/2001</td>
<td>12/04/2012</td>
</tr>
<tr>
<td>6</td>
<td>22/12/2000</td>
<td>5/04/2001</td>
<td>12/04/2012</td>
</tr>
<tr>
<td>6</td>
<td>22/12/2000</td>
<td>18/03/2004</td>
<td>12/04/2012</td>
</tr>
<tr>
<td>6</td>
<td>13/04/2006</td>
<td>13/04/2006</td>
<td>20/12/2012</td>
</tr>
<tr>
<td>6</td>
<td>21/12/2012</td>
<td>7/02/2013</td>
<td>31/12/2150</td>
</tr>
<tr>
<td>1</td>
<td>17/07/2014</td>
<td>17/07/2014</td>
<td>23/10/2014</td>
</tr>
<tr>
<td>1</td>
<td>17/07/2014</td>
<td>15/09/2016</td>
<td>23/10/2014</td>
</tr>
<tr>
<td>1</td>
<td>24/10/2017</td>
<td>2/11/2017</td>
<td>31/12/2150</td>
</tr>
<tr>
<td>1</td>
<td>24/10/2017</td>
<td>18/07/2019</td>
<td>31/12/2150</td>
</tr>
<tr>
<td>1</td>
<td>24/10/2017</td>
<td>30/04/2020</td>
<td>31/12/2150</td>
</tr>
<tr>
<td>1</td>
<td>24/10/2017</td>
<td>3/06/2021</td>
<td>31/12/2150</td>
</tr>
<tr>
<td>9</td>
<td>null</td>
<td>28/03/2024</td>
<td>31/12/2150</td>
</tr>
</tbody>
</table></div>
<p>I have tried using Analytical function, lag to access the previous row. My issues is I need to compare with the group of same effectivedate and givt it the same value of previous effectivedate-1 and if effectivedate is null then also last not null effectivdate -1</p>
",-3,1,4,2025-07-27T13:58:07+00:00,2,158,True
79716909,23512643,,sql,Summing between dates,"<p>I have these two tables in SQL:</p>
<pre><code>CREATE TABLE myt1 
(
    name VARCHAR(50),
    date DATE,
    hours_watched DECIMAL(4,2)
);

INSERT INTO myt1 VALUES
('name1', '2024-01-15', 2.5),
('name1', '2024-03-05', 1.5),
('name1', '2024-06-10', 3.0),
('name2', '2024-01-20', 3.0),
('name2', '2024-04-15', 4.5),
('name3', '2024-02-10', 2.0),
('name3', '2024-07-15', 5.5),
('name4', '2024-12-01', 4.0);

CREATE TABLE myt2 
(
    name VARCHAR(50),
    team VARCHAR(50),
    subteam INTEGER,
    date_from DATE,
    date_to DATE
);

INSERT INTO myt2 VALUES
('name1', 'Team A', 1, '2024-01-01', '2024-02-29'),
('name1', 'Team B', 2, '2024-03-01', '2024-12-31'),
('name2', 'Team A', 2, '2024-01-01', '2024-12-31'),
('name3', 'Team B', 1, '2024-01-01', '2024-06-30');
</code></pre>
<p>Individuals on these teams watch movies. I want to find out how many total hours were spent watching movies for each team. I want to make sure that if someone is moving between teams, the hours get added to the team the person was on when he was watching.</p>
<p>I tried to do this with a simple join:</p>
<pre><code>SELECT 
    t2.team,
    SUM(t1.hours_watched) AS total_hours
FROM 
    myt1 t1
LEFT JOIN 
    myt2 t2 ON t1.name = t2.name 
            AND t1.date &gt;= t2.date_from 
            AND t1.date &lt;= t2.date_to
WHERE 
    t2.team IS NOT NULL
GROUP BY 
    t2.team;
</code></pre>
<p>But this will not work in the case that someone's team can not be located.</p>
<p>Is it better to solve this problem using CTEs for individuals that can be located/not located, or is there some easier option here?</p>
<hr />
<pre><code>SELECT 
    COALESCE(t2.team, 'UNMAPPED') AS team,
    SUM(t1.hours_watched) AS total_hours
FROM myt1 t1
LEFT JOIN myt2 t2 
    ON t1.name = t2.name 
    AND t1.date &gt;= t2.date_from 
    AND t1.date &lt;= t2.date_to
GROUP BY COALESCE(t2.team, 'UNMAPPED');
</code></pre>
",-2,3,5,2025-07-28T05:11:43+00:00,1,131,True
79717384,23512643,,sql,Find out if a name has more than one distinct value in the same time period,"<p>I have this table in R:</p>
<pre><code>CREATE TABLE myt 
(
    name VARCHAR(50),
    start_date DATE,
    end_date DATE,
    var VARCHAR(10)
);

INSERT INTO myt (name, start_date, end_date, var) VALUES
('RED', '2020-01-01', '2021-06-01', 'a'),
('RED', '2021-01-01', '2022-01-01', 'b'),
('RED', '2022-06-01', '2023-01-01', 'a'),

('BLUE', '2020-01-01', '2021-01-01', 'x'),
('BLUE', '2020-06-01', '2021-06-01', 'y'),
('BLUE', '2021-02-01', '2022-01-01', 'x'),

('GREEN', '2020-01-01', '2021-01-01', 'p'),
('GREEN', '2021-01-01', '2022-01-01', 'q'),
('GREEN', '2022-01-01', '2023-01-01', 'p'),

('YELLOW', '2020-01-01', '2023-01-01', 'alpha'),
('YELLOW', '2021-01-01', '2022-01-01', 'beta'),

('PURPLE', '2020-01-01', '2021-06-01', 'same'),
('PURPLE', '2021-01-01', '2022-01-01', 'same'),

('ORANGE', '2020-01-01', '2021-01-01', 'red'),
('ORANGE', '2020-06-01', '2021-06-01', 'blue'),
('ORANGE', '2020-09-01', '2021-03-01', 'green');
</code></pre>
<p>I want to see if for any names, var has more than 1 distinct value in the same time period. I want to only display rows where this happens.</p>
<p>I think the result should look like this:</p>
<pre><code>   name   var start_date   end_date
   ---------------------------------
   BLUE     x 2020-01-01 2021-01-01
   BLUE     y 2020-06-01 2021-06-01
   BLUE     x 2021-02-01 2022-01-01
 ORANGE   red 2020-01-01 2021-01-01
 ORANGE  blue 2020-06-01 2021-06-01
 ORANGE green 2020-09-01 2021-03-01
    RED     a 2020-01-01 2021-06-01
    RED     b 2021-01-01 2022-01-01
 YELLOW alpha 2020-01-01 2023-01-01
 YELLOW  beta 2021-01-01 2022-01-01
</code></pre>
<p>I tried to do this with a self join:</p>
<pre><code>  SELECT DISTINCT t1.name, t1.var, t1.start_date, t1.end_date
  FROM myt t1
  JOIN myt t2 ON t1.name = t2.name
  WHERE t1.var != t2.var
    AND t1.start_date &lt; t2.end_date 
    AND t1.end_date &gt; t2.start_date
  ORDER BY t1.name, t1.start_date;
</code></pre>
<p>Is this the correct way to use a self-join?</p>
",-1,2,3,2025-07-28T13:09:39+00:00,4,163,True
79717830,28856060,,sql,Moving Pro-Rate based on dates for commitment in SQL,"<p>I have a dataset for contracts with a start and end date where they need to hit part number commitments. They also sign amendments during the period between the start and end date and need to create a Pro-Rated commitment.</p>
<p>The below table is how I would like my results to look. The first 6 columns are from my data set and the Percentage and Pro-Rate would be calculated.</p>
<p>For the first row (Original) I would take the date of the second row (amendment) 2/24/2025 minus start date 10/1/2024 to understand how long that part was active for. (40%, 220 Pro-rate)</p>
<p>For the second row (amendment) I would take the date of 2/25/2025 minus 2/24/2025 to get how long that was active for (0.3%, 1.64 Pro-rate)</p>
<p>I need to follow the same logic until the end where there is no future date to compare against, so I would use the End date which would be 10/1/2025 minus 7/17/2025 for (21%, 187.4 Prorate)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">ID</th>
<th style=""text-align: center;"">Part</th>
<th style=""text-align: center;"">Qty Commit</th>
<th style=""text-align: center;"">Date</th>
<th style=""text-align: center;"">StartDate</th>
<th style=""text-align: center;"">EndDate</th>
<th style=""text-align: center;"">Percentage</th>
<th style=""text-align: center;"">Pro_Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">550</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">40%</td>
<td style=""text-align: center;"">220</td>
</tr>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">600</td>
<td style=""text-align: center;"">2/24/2025</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">0%</td>
<td style=""text-align: center;"">1.64</td>
</tr>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">750</td>
<td style=""text-align: center;"">2/25/2025</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">8%</td>
<td style=""text-align: center;"">59.59</td>
</tr>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">750</td>
<td style=""text-align: center;"">3/26/2025</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">11%</td>
<td style=""text-align: center;"">82.19</td>
</tr>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">800</td>
<td style=""text-align: center;"">5/5/2025</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">7%</td>
<td style=""text-align: center;"">54.79</td>
</tr>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">800</td>
<td style=""text-align: center;"">5/30/2025</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">13%</td>
<td style=""text-align: center;"">105.21</td>
</tr>
<tr>
<td style=""text-align: center;"">10002</td>
<td style=""text-align: center;"">200750</td>
<td style=""text-align: center;"">900</td>
<td style=""text-align: center;"">7/17/2025</td>
<td style=""text-align: center;"">10/1/2024</td>
<td style=""text-align: center;"">10/1/2025</td>
<td style=""text-align: center;"">21%</td>
<td style=""text-align: center;"">187.40</td>
</tr>
</tbody>
</table></div>
<p>I am using SQL inside a databricks warehouse.</p>
<pre><code>SELECT DISTINCT
    ID,
    Part,
    Qty_Commit,
    Date,
    StartDate,
    EndDate,
    Percentage,
    Pro_Rate,
FROM Contract_Base 
</code></pre>
<p>My initial thoughts revolved around using a ranking and row partition by but I am a little lost on how to accomplish it. Ideally the query would assume the final date for prorating would be the EndDate until an amendment drops into the system.</p>
",3,3,0,2025-07-28T20:03:44+00:00,1,108,True
79718123,8908951,,sql,JdbcClient StatementSpec bind multiple placeholders with a single parameter,"<p>When making a SQL query with JdbcClient, using <code>.param(String name, Object value)</code> binds the value to a corresponding placeholder <code>:name</code> on the query string.</p>
<p>The question is, can a single parameter bind multiple placeholders with the same name?</p>
<p>Sample situation:</p>
<pre><code>String query, foo;
List&lt;Map&lt;String, Object&gt;&gt; result;

query = &quot;SELECT * FROM table WHERE colA = :foo OR colB = :foo&quot;;
foo = &quot;bar&quot;;

result = jdbcClient.
    .sql(query)
    .param(&quot;foo&quot;, bar)
    .query()
    .listOfRows();
</code></pre>
",-1,1,2,2025-07-29T04:54:44+00:00,1,74,True
79718362,31062501,,sql,How to achieve Python-like tuple unpacking in DolphinDB SQL to dynamically select columns?,"<p>In Python, I can unpack a list into individual variables:</p>
<pre><code>&gt;&gt;&gt; name,age,date = ['Bob',20,'2025-1-1']
&gt;&gt;&gt; name
'Bob'
&gt;&gt;&gt; age
20
&gt;&gt;&gt; date
'2025-1-1'
</code></pre>
<p>In DolphinDB, I want to achieve a similar “unpacking” operation for column names stored in a list. For example, given a table <code>T</code> and a <code>name_set</code> list containing column names, how can I dynamically unpack these names into a SQL query to select those columns?</p>
<pre class=""lang-none prettyprint-override""><code>T = table(1..3 as id, 2..4 as val1, 3..5 as val2, 4..6 as val3)
name_set = [&quot;val1&quot;, &quot;val2&quot;, &quot;val3&quot;]

// Goal: Equivalent to `select val1, val2, val3 from T`
select *name_set from T  // Hypothetical syntax (not working)
</code></pre>
<p><strong>I have tried</strong> directly using <code>select *name_set</code> fails because DolphinDB does not support unpacking syntax like Python. I’ve explored using meta programming but haven’t found a clear way to dynamically unpack column names from a list.<br />
How can I programmatically expand the <code>name_set</code> list into column names in a DolphinDB SQL query, similar to Python’s unpacking?</p>
",0,0,0,2025-07-29T09:07:58+00:00,1,64,True
79719885,11246434,,sql,How to Convert Number Strings with Thousand and Decimal Separators to Float in PL/SQL?,"<p>I have a list of number strings using different formats for thousands and decimal separators. Some values use a comma , as the decimal separator and a dot . as the thousand separator (e.g., '1.211,15'), while others already use a dot . as the decimal separator (e.g., '910.25'). I need to convert all of them into proper NUMBER (float) format in PL/SQL.</p>
<p>Here are some examples:</p>
<p>Original: '1.211,15' '910.25' '910' '213,25'</p>
<p>Converted: 1211.15, 910.25, 910, 213.25</p>
",0,0,0,2025-07-30T11:35:40+00:00,2,196,True
79720297,7613649,"Belgrade, Serbia",sql,Cursor pagination with conditional sorting,"<p>I need to implement <a href=""https://stackoverflow.com/a/70520457/7613649"">keyset pagination</a> (sometimes referenced as cursor-based or seek method) in Postgres with the following ordering rules:</p>
<ol>
<li>Rows where <code>group_name = :priorityGroup</code> come first (<code>:priorityGroup</code> is a dynamic parameter).</li>
<li>Then sort by <code>name</code>, then by <code>id</code> (<code>id</code> is an unique key).</li>
</ol>
<p>Here, the cursor is a reference to the last retrieved row, indicating the position from which the next page should be fetched.</p>
<p>Previously, a query without conditional sorting worked well:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT id, name, group_name 
FROM record
WHERE (name, id) &gt; (:cursorName, :cursorId)
ORDER BY name, id
LIMIT :limit
</code></pre>
<p>Latency was low, because the index on (<code>name</code>, <code>id</code>) was used by both <code>WHERE</code> and <code>ORDER</code>. <br />
Now, the requirement is to prioritize a specific group (<code>group_name = :priorityGroup</code>)</p>
<p>Details:</p>
<ol>
<li>Millions of rows. If index is not used, latency is low</li>
<li>Keyset pagination only (e.g. limit/offset or server-side cursors (<code>DECLARE … CURSOR</code>) are not options)</li>
<li>PostgreSQL 15</li>
</ol>
<h4>The test cases</h4>
<p>They could be useful to test suggestions.</p>
<p>DDL:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE record (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    group_name VARCHAR(255) NOT NULL
);
CREATE INDEX name_and_id_idx ON record (name, id);
</code></pre>
<p>Sample data:</p>
<pre><code>| id | name | group_name |
|----|------|------------|
|  1 | A    | Group 1    |
|  2 | A    | Group 2    |
|  3 | A    | Group 2    |
|  4 | B    | Group 1    |
|  5 | B    | Group 2    |
|  6 | C    | Group 2    |
</code></pre>
<p>Input: <code>priorityGroup='Group 2', cursorId=2, limit=2</code> -&gt; Expected: <code>[record(id=3), record(id=5)]</code>.<br />
Input: <code>priorityGroup='Group 2', cursorId=5, limit=2</code> -&gt; Expected: <code>[record(id=6), record(id=1)]</code>.<br />
Input: <code>priorityGroup='Group 2', cursorId=1, limit=2</code> -&gt; Expected: <code>[record(id=4)]</code>.</p>
",4,4,0,2025-07-30T16:44:21+00:00,1,178,True
79720598,31180361,,sql,Error with MS Excel power Query ODBC Context,"<p>I have a parameter coming in from the excel table that is formulated as date time.</p>
<p><code>7/28/2025 12:00:00 AM</code> name BeginDate</p>
<p>I am trying to filter the source data which has a field called TransDate before it loads. If I use the following syntax I can get it to work but I can't use the field/parameter BeginDate.</p>
<pre><code>let
   Source = ODBC.QUERY(&quot;dsn=SOTAMAS90&quot;,&quot;select * from cl_trans WHERE TransDate &gt;= {ts '2025-07-28 00:00:00'}&quot;)
</code></pre>
<p>I have tried all kinds of formates with Textto.Date Date.from etc.</p>
",2,2,0,2025-07-30T22:29:49+00:00,1,60,False
79720945,9852762,,sql,Proper way to locate the position of a single newly inserted row based on order by clause immediately after insert?,"<p>After inserting a single row of data into a table (always a single row), I'd like to return to the application code the relative position of that row based on an order by clause. Thus, the application code sends SQLite a row to insert and SQLite returns its position.</p>
<p>Using <code>last_insert_rowid()</code> and the window function <code>row_number()</code> appears to work but that's just my hackish guess. Are there better, more efficient methods?</p>
<p>If I copied this correctly, this example runs in the SQLite Fiddle.</p>
<pre><code>create table data (key,value);

insert into data 
values (5, 'e'), (3, 'c'), (8, 'h');

select rowid, * from data;

+-------+-----+-------+
| rowid | key | value |
+-------+-----+-------+
| 1     | 5   | e     |
| 2     | 3   | c     |
| 3     | 8   | h     |
+-------+-----+-------+

insert into data values (1, 'a');

select * from data order by value asc;

+-----+-------+
| key | value |
+-----+-------+
| 1   | a     |
| 3   | c     |
| 5   | e     |
| 8   | h     |
+-----+-------+

select d.*
from 
    (select rowid as row_id, row_number() over (order by value asc) pos, * 
     from data) d
where d.row_id = last_insert_rowid();

+--------+-----+-----+-------+
| row_id | pos | key | value |
+--------+-----+-----+-------+
| 4      | 1   | 1   | a     |
+--------+-----+-----+-------+

insert into data values (2,'b');

select * from data order by value desc;

+-----+-------+
| key | value |
+-----+-------+
| 8   | h     |
| 5   | e     |
| 3   | c     |
| 2   | b     |
| 1   | a     |
+-----+-------+

select d.*
from 
    (select rowid as row_id, row_number() over (order by value desc) pos, * 
     from data) d
where d.row_id = last_insert_rowid();

+--------+-----+-----+-------+
| row_id | pos | key | value |
+--------+-----+-----+-------+
| 5      | 4   | 2   | b     |
+--------+-----+-----+-------+
</code></pre>
",2,3,1,2025-07-31T07:45:28+00:00,2,114,True
79721893,686621,,sql,Include duplicate rows with postgresql &#39;WHERE IN&#39; clause with lists containing duplicate id&#39;s,"<p>I am wondering if there is a way to suppress the automatic deduping that happens with a postgresql SELECT statement using a <code>WHERE id IN (list)</code> clause, where <code>list</code> will contain duplicate ids. What I want is something like this statement that will return 4 results including duplicates:</p>
<pre><code>SELECT * FROM table WHERE id IN (102, 103, 103, 104)
</code></pre>
<p>But the actual result is 3 rows returned:
((102, a, b)(103, c, d)(104, e, f))</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th><code>table.id</code></th>
<th><code>table.column1</code></th>
<th><code>table.column2</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>102</td>
<td>a</td>
<td>b</td>
</tr>
<tr>
<td>103</td>
<td>c</td>
<td>d</td>
</tr>
<tr>
<td>104</td>
<td>e</td>
<td>f</td>
</tr>
</tbody>
</table></div>
<p>while I would like 103 to appear twice in the return from the Select statement:
((102, a, b)(103, c, d)(103, c, d)(104, e, f))</p>
<p>I know that I can just handle the duplicates in my python code but I assume there is a way to handle this in Postgres. Thanks for any help.</p>
",3,5,2,2025-07-31T22:22:46+00:00,2,183,True
79722066,6529480,,sql,How to dynamically generate SQL to Update/Insert a table in Azure Databricks Notebook,"<p>Its a sort of CDC ( Change Data Capture ) scenario in which I am trying to compare new data (in <strong>tblNewData</strong>) with old data (in <strong>tblOldData</strong>), and logging the changes into a log table (<strong>tblExpectedDataLog</strong>) including Column names of the changed values, new value itself and the old value in that column. Sample tables creation and data insertion script is as below.
Challenge to me is, that the actual new and old tables are having more than 250 columns (just shortened the things to give you a better understanding). and I wish to avoid 250 conditions for each column to compare/check whether the values have changed or not.</p>
<pre><code>Create Table tblNewData
(
    TransactionID   string,
    ReportDate  date,
    Comments    string,
    Premium     int
);

Insert Into tblNewData
Select '1116_0025' As TransactionID,    '2025-07-30' As ReportDate, 'ghi' As Comments,  105 As Premium
Union
Select '2540_0038' As TransactionID,    '2025-07-30' As ReportDate, 'jkl' As Comments,  100 As Premium
Union
Select '3459_0001' As TransactionID,    '2025-07-30' As ReportDate, 'pqr' As Comments,  80 As Premium
Union
Select '4870_0041' As TransactionID,    '2025-08-01' As ReportDate, 'bbbb' As Comments, 80 As Premium;


Create Table tblOldData
(
    TransactionID   string,
    ReportDate  date,
    Comments    string,
    Premium     int,
    ActiveFlag  boolean
);

Insert Into tblOldData
Select '1116_0025' As TransactionID,    '2025-07-30' As ReportDate, 'def' As Comments,  95 As Premium,  1 As ActiveFlag
Union
Select '1116_0025' As TransactionID,    '2025-07-30' As ReportDate, 'abc' As Comments,  90 As Premium,  0 As ActiveFlag
Union
Select '2540_0038' As TransactionID,    '2025-07-30' As ReportDate, 'jkl' As Comments,  100 As Premium, 1 As ActiveFlag
Union
Select '3459_0001' As TransactionID,    '2025-07-30' As ReportDate, 'mno' As Comments,  70 As Premium,  1 As ActiveFlag
Union
Select '4870_0041' As TransactionID,    '2025-07-01' As ReportDate, 'bbbb' As Comments, 80 As Premiums, 1 As ActiveFlag;

Create Table tblExpectedDataLog
(
    TransactionID                   string,
    Column_name_of_changed_value    string,
    PreviousValue                   string,
    NewValue                        string,
    LogDateTime                     timestamp
);
</code></pre>
<p>So I thought of getting the column names from Information_Schema.columns and comparing them dynamically. And to achieve this I tried using my  old way of using <strong>STUFF()</strong> and <strong>for xml path('')</strong> to stuff the query with 250 columns and generate the  SQL dynamically.
But unfortunately Azure Databricks is not supporting this XML Path() function.</p>
<p>Eventually coming here to seek any possible solution?</p>
",0,1,1,2025-08-01T04:39:43+00:00,1,115,False
79722140,1460828,"Cornwall, United Kingdom",sql,How to copy a resultset into an existing column,"<p>I have a working query</p>
<pre><code>select count(rider) as ridecount
from rides
inner join Participants on Participants.rideID = rides.rideID
group by rider
</code></pre>
<p>I have also created a new column 'ridecount' with a default value of zero.
How can I copy this resultset into my 'ridecount' column? I tried</p>
<pre><code>select count(rider)
into ridecount
from rides
</code></pre>
<p>but this gives</p>
<blockquote>
<p>undeclared variable: ridecount</p>
</blockquote>
<p>presumably because <code>SELECT INTO</code> is used for a new table rather than a new column.</p>
<p>I have now also looked at  <a href=""https://stackoverflow.com/questions/1262786/mysql-update-query-based-on-select-query"">suggested answers</a> but these appear to be dealing with two tables; I only have a single table to update and/or I was confused by the accepted answer.</p>
<p>Note: this will be a once-only operation to deal with historical data; the 'ridecount' column will be updated as time progresses.</p>
",2,2,0,2025-08-01T06:27:57+00:00,1,138,True
79722606,23889783,,sql,Get an output from a Python function in Snowflake SQL,"<p>I have the following code, which is supposed to return the value &quot;Procedure executed successfully&quot; into the my_var variable, but it's give me an error message that reads</p>
<blockquote>
<p>&quot;Syntax error: unexpected 'INTO'. (line 17)
syntax error line 7 at position 2 unexpected 'RETURN'. (line 17)&quot;</p>
</blockquote>
<pre><code>CREATE OR REPLACE TEMPORARY PROCEDURE my_python_proc()
RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = 3.11
HANDLER = 'run'
AS
$$
def run():
    return &quot;Procedure executed successfully&quot;
$$
;

DECLARE my_var STRING;

BEGIN
  //LET my_var := CALL my_python_proc();
  EXECUTE IMMEDIATE 'CALL my_python_proc()' INTO :my_var;
  -- Use the variable, for example:
  RETURN my_var;
END;

Show VARIABLES;
</code></pre>
<p>Am I doing something wrong?</p>
<p>I am expecting the <code>&quot;my_var&quot;</code> variable to be set to <code>&quot;Procedure executed successfully&quot;</code></p>
",2,2,0,2025-08-01T13:59:19+00:00,1,217,True
79725304,14449451,,sql,pg8000 conn.run argument error/ SQL syntax issues inserting data with AWS Lambda,"<p>I am using AWS Lambda with Python and pg8000 to insert records into a PostgreSQL RDS database.
When <code>conn.run()</code> is called, I get argument errors or syntax errors.</p>
<p>INSERT_SQL:</p>
<pre><code>INSERT INTO crypto_prices (coin, price_usd, timestamp, day_of_week)
VALUES ($1, $2, $3, $4)
ON CONFLICT (coin, timestamp) DO NOTHING;
</code></pre>
<p>'params' looks like:</p>
<pre><code>('bitcoin', 115251, '2025-08-04T18:30:40.926246+00:00', 'Monday')
</code></pre>
<p>Error in AWS logs (if I use <code>conn.run(INSERT_SQL, params)</code>):</p>
<blockquote>
<p>Failed to insert bitcoin: list index out of range</p>
</blockquote>
<p>Error in AWS logs (if I use <code>conn.run(INSERT_SQL, *params)</code>):</p>
<blockquote>
<p>Failed to insert bitcoin: Connection.run() takes from 2 to 4 positional arguments but 6 were given</p>
</blockquote>
<p>Any help is super appreciated, thank you.</p>
",0,0,0,2025-08-04T19:17:58+00:00,3,141,True
79725307,34747,"Rio de Janeiro, Brazil",sql,How to pass vector from Python to Duckdb for vector similarity search,"<p>I am using Duckdb with the VSS extension to store document embeddings, it works normally however when I try to do a similarity search to a vector passed from Python, I am getting a TypeError. The query looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>query = text(
    &quot;&quot;&quot;
    select document
    from embedding_duckdb
    where collection_name = :collection_name
    order by array_cosine_similarity(
        embedding, :embedding::FLOAT[1024])
    )
    limit :num_docs
    &quot;&quot;&quot;
)
result = self.session.execute(
    query,
    {
        'collection_name': collection,
        'embedding': array_type(response[&quot;embedding&quot;]),
        'num_docs': num_docs,
    }
)
pages = [row[0] for row in result.fetchall()]
</code></pre>
<p>here is TypeError I get:</p>
<pre><code>E               TypeError: array_type(): incompatible function arguments. The following argument types are supported:
E                   1. (type: duckdb.duckdb.typing.DuckDBPyType, size: int, *, connection: duckdb.DuckDBPyConnection = None) -&gt; duckdb.duckdb.typing.DuckDBPyType
E               
E               Invoked with: [0.8049294352531433, -0.5165860652923584, -0.25891393423080444, -0.8668404221534729, 0.050446540117263794, -0.060985639691352844, -0.2768419682979584, -0.3037632703781128, 0.7962498664855957, -0.30102455615997314, 0.13201330602169037, -0.35436326265335083, -0.2618299722671509, -0.030353419482707977, -0.009162794798612595, -0.37244415283203125, -0.505980372428894, -0.2313883751630783, -0.4354000389575958, 0.7349468469619751, 0.18996360898017883, 0.47891461849212646, -0.009091421961784363, ...
</code></pre>
<p>I tried other ways of casting the vector, for example using the <code>cast</code> function.</p>
<p>AFAICT I am doing exactly as the <a href=""https://duckdb.org/docs/stable/core_extensions/vss.html"" rel=""nofollow noreferrer"">docs</a> instruct. Don't know what else to try.</p>
<p>EDIT: original code to define <code>query</code> variable is below</p>
<pre class=""lang-py prettyprint-override""><code>query = text('select document from embedding_duckdb where collection_name = :collection_name order by array_cosine_similarity(embedding, :embedding::FLOAT[1024])) limit :num_docs')
</code></pre>
",0,0,0,2025-08-04T19:22:01+00:00,1,206,False
79725366,28698620,,sql,Check referential integrity between two GridDB containers,"<p>I have two GridDB  Cloud containers participating in a one-to-many relationship. The tables layout is shown below:</p>
<p>Table <code>Customer</code> (collection container)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Nullity</th>
<th>Key</th>
</tr>
</thead>
<tbody>
<tr>
<td>Customer_ID</td>
<td>INTEGER</td>
<td>NOT NULL</td>
<td>TRUE</td>
</tr>
<tr>
<td>Customer_Name</td>
<td>STRING</td>
<td>NOT NULL</td>
<td>FALSE</td>
</tr>
<tr>
<td>Customer_Email</td>
<td>STRING</td>
<td>NULL</td>
<td>FALSE</td>
</tr>
</tbody>
</table></div>
<p>Table <code>CustomerData</code> (timeseries container)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Nullity</th>
<th>Key</th>
</tr>
</thead>
<tbody>
<tr>
<td>CustomerData_TS</td>
<td>TIMESTAMP</td>
<td>NOT NULL</td>
<td>TRUE</td>
</tr>
<tr>
<td>Customer_ID</td>
<td>INTEGER</td>
<td>NOT NULL</td>
<td>FALSE</td>
</tr>
<tr>
<td>CustomerData_Val1</td>
<td>DOUBLE</td>
<td>NOT NULL</td>
<td>FALSE</td>
</tr>
<tr>
<td>CustomerData_Val2</td>
<td>DOUBLE</td>
<td>NOT NULL</td>
<td>FALSE</td>
</tr>
</tbody>
</table></div>
<p><code>CustomerData.Customer_ID</code> is a foreign key to the primary key of table <code>Customer</code>. I´d like use a single SQL statement that inserts a line in the <code>CustomerData</code> table only if the value for <code>Customer_ID</code> exists in table <code>Customer</code>. Additionally, I’d like to automatically provide the equivalent of ANSI's <code>CURRENT_TIMESTAMP</code> value, in order to populate the <code>CustomerData_TS</code> column.</p>
<p>I tried to create the <code>CustomerData</code> table specifying declarative defaults and referential integrity (below), but GridDB throws an error:</p>
<blockquote>
<p>The invalid query was specified</p>
</blockquote>
<p>This is my table structure:</p>
<pre><code>Create table CustomerData
(
    CustomerData_TS TIMESTAMP NOT NULL PRIMARY KEY DEFAULT CURRENT_TIMESTAMP,
    Customer_ID       INTEGER NOT NULL  
        FOREIGN KEY REFERENCES Customer (Customer_ID),
    CustomerData_Val1 DOUBLE  NOT NULL,
    CustomerData_Val2 DOUBLE  NOT NULL
)
</code></pre>
<p>Any ideas on how to accomplish that?</p>
",0,0,0,2025-08-04T20:34:04+00:00,1,52,True
79726254,17499061,,sql,SQL Calculate Running Total Against Positive Transactions,"<p>I have an instance where I need to be calculating the running total by positive transactions. This exercise is to <strong>determine which positive entries in our warehouse entries is still open</strong> as we need to determine the date of inventory based off of our item ledger entries. We are using NAV 2018 for reference.</p>
<p>I already have the running total (<code>[Qty (Base) Running Total]</code>) per item and bin (<code>[Item No_], [Location Code], [Bin Code]</code>):</p>
<pre><code>select
    we.[Entry No_],
    we.[Registering Date],
    we.[Item No_],
    we.[Location Code],
    we.[Bin Code],
    we.[Qty_ (Base)],
    sum(we.[Qty_ (Base)]) over (partition by we.[Item No_], we.[Location Code], we.[Bin Code]
                                order by we.[Entry No_]) as [Qty (Base) Running Total],
    (select sum(we2.[Qty_ (Base)])
     from [Warehouse Entry] we2 with(nolock)
     where we2.[Item No_] = we.[Item No_]
       and we2.[Location Code] = we.[Location Code]
       and we2.[Bin Code] = we.[Bin Code]) as [Total Qty Base In Bin]
from 
    [Warehouse Entry] we with(nolock)
</code></pre>
<p>In the example dataset below (where I have filtered the dataset down to 1 item and 1 bin already), I need to know when the initial 1280 (and all positive transactions) went to 0 <strong>so that I do not include that entry as &quot;still in inventory&quot;</strong>.<br />
In the dataset when I manually sum the <code>Qty (Base)</code> I can see that the initial 1280 was zeroed out by Entry No 927025. Similarly, Entry No 656541 for 600 is zeroed out by Entry No 1205470, etc.</p>
<p>Dataset (<code>Entry No., Registering Date, Qty (Base)</code> are my data, <code>Qty (Base) Running Total</code> is computed by the above query):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">Entry No.</th>
<th>Registering Date</th>
<th style=""text-align: right;"">Qty (Base)</th>
<th style=""text-align: right;"">Qty (Base) Running Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">427752</td>
<td>4/17/2025</td>
<td style=""text-align: right;"">1280</td>
<td style=""text-align: right;"">1280</td>
</tr>
<tr>
<td style=""text-align: right;"">577708</td>
<td>4/28/2025</td>
<td style=""text-align: right;"">-100</td>
<td style=""text-align: right;"">1180</td>
</tr>
<tr>
<td style=""text-align: right;"">593215</td>
<td>4/29/2025</td>
<td style=""text-align: right;"">-130</td>
<td style=""text-align: right;"">1050</td>
</tr>
<tr>
<td style=""text-align: right;"">611579</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-130</td>
<td style=""text-align: right;"">920</td>
</tr>
<tr>
<td style=""text-align: right;"">616674</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">910</td>
</tr>
<tr>
<td style=""text-align: right;"">616682</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">900</td>
</tr>
<tr>
<td style=""text-align: right;"">616686</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">890</td>
</tr>
<tr>
<td style=""text-align: right;"">616758</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">880</td>
</tr>
<tr>
<td style=""text-align: right;"">616913</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">870</td>
</tr>
<tr>
<td style=""text-align: right;"">622854</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">850</td>
</tr>
<tr>
<td style=""text-align: right;"">622863</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">830</td>
</tr>
<tr>
<td style=""text-align: right;"">622875</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">810</td>
</tr>
<tr>
<td style=""text-align: right;"">622885</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">790</td>
</tr>
<tr>
<td style=""text-align: right;"">622902</td>
<td>4/30/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">770</td>
</tr>
<tr>
<td style=""text-align: right;"">633070</td>
<td>5/1/2025</td>
<td style=""text-align: right;"">-120</td>
<td style=""text-align: right;"">650</td>
</tr>
<tr>
<td style=""text-align: right;"">656541</td>
<td>5/2/2025</td>
<td style=""text-align: right;"">600</td>
<td style=""text-align: right;"">1250</td>
</tr>
<tr>
<td style=""text-align: right;"">671925</td>
<td>5/5/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">1220</td>
</tr>
<tr>
<td style=""text-align: right;"">704308</td>
<td>5/7/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1200</td>
</tr>
<tr>
<td style=""text-align: right;"">736014</td>
<td>5/8/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1180</td>
</tr>
<tr>
<td style=""text-align: right;"">853955</td>
<td>5/19/2025</td>
<td style=""text-align: right;"">-50</td>
<td style=""text-align: right;"">1130</td>
</tr>
<tr>
<td style=""text-align: right;"">884492</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1120</td>
</tr>
<tr>
<td style=""text-align: right;"">884495</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1110</td>
</tr>
<tr>
<td style=""text-align: right;"">884499</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1100</td>
</tr>
<tr>
<td style=""text-align: right;"">884501</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1090</td>
</tr>
<tr>
<td style=""text-align: right;"">884505</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1080</td>
</tr>
<tr>
<td style=""text-align: right;"">884509</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1070</td>
</tr>
<tr>
<td style=""text-align: right;"">884515</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1060</td>
</tr>
<tr>
<td style=""text-align: right;"">884517</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1050</td>
</tr>
<tr>
<td style=""text-align: right;"">884519</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1040</td>
</tr>
<tr>
<td style=""text-align: right;"">884521</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">1030</td>
</tr>
<tr>
<td style=""text-align: right;"">887327</td>
<td>5/21/2025</td>
<td style=""text-align: right;"">-120</td>
<td style=""text-align: right;"">910</td>
</tr>
<tr>
<td style=""text-align: right;"">893571</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">900</td>
</tr>
<tr>
<td style=""text-align: right;"">893573</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">890</td>
</tr>
<tr>
<td style=""text-align: right;"">893578</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">880</td>
</tr>
<tr>
<td style=""text-align: right;"">893582</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">870</td>
</tr>
<tr>
<td style=""text-align: right;"">893584</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">860</td>
</tr>
<tr>
<td style=""text-align: right;"">893586</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">850</td>
</tr>
<tr>
<td style=""text-align: right;"">893591</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">840</td>
</tr>
<tr>
<td style=""text-align: right;"">893593</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">830</td>
</tr>
<tr>
<td style=""text-align: right;"">893598</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">820</td>
</tr>
<tr>
<td style=""text-align: right;"">893600</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">810</td>
</tr>
<tr>
<td style=""text-align: right;"">899565</td>
<td>5/22/2025</td>
<td style=""text-align: right;"">-160</td>
<td style=""text-align: right;"">650</td>
</tr>
<tr>
<td style=""text-align: right;"">913031</td>
<td>5/23/2025</td>
<td style=""text-align: right;"">640</td>
<td style=""text-align: right;"">1290</td>
</tr>
<tr>
<td style=""text-align: right;"">927025</td>
<td>5/23/2025</td>
<td style=""text-align: right;"">-60</td>
<td style=""text-align: right;"">1230</td>
</tr>
<tr>
<td style=""text-align: right;"">1042437</td>
<td>6/6/2025</td>
<td style=""text-align: right;"">-80</td>
<td style=""text-align: right;"">1150</td>
</tr>
<tr>
<td style=""text-align: right;"">1043369</td>
<td>6/6/2025</td>
<td style=""text-align: right;"">-60</td>
<td style=""text-align: right;"">1090</td>
</tr>
<tr>
<td style=""text-align: right;"">1120232</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1070</td>
</tr>
<tr>
<td style=""text-align: right;"">1120236</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1050</td>
</tr>
<tr>
<td style=""text-align: right;"">1125853</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1030</td>
</tr>
<tr>
<td style=""text-align: right;"">1125858</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1010</td>
</tr>
<tr>
<td style=""text-align: right;"">1125864</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">990</td>
</tr>
<tr>
<td style=""text-align: right;"">1125996</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">970</td>
</tr>
<tr>
<td style=""text-align: right;"">1126000</td>
<td>6/16/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">940</td>
</tr>
<tr>
<td style=""text-align: right;"">1205340</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">930</td>
</tr>
<tr>
<td style=""text-align: right;"">1205342</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">920</td>
</tr>
<tr>
<td style=""text-align: right;"">1205346</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">910</td>
</tr>
<tr>
<td style=""text-align: right;"">1205350</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">900</td>
</tr>
<tr>
<td style=""text-align: right;"">1205356</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">890</td>
</tr>
<tr>
<td style=""text-align: right;"">1205362</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">880</td>
</tr>
<tr>
<td style=""text-align: right;"">1205364</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">870</td>
</tr>
<tr>
<td style=""text-align: right;"">1205368</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-170</td>
<td style=""text-align: right;"">700</td>
</tr>
<tr>
<td style=""text-align: right;"">1205381</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">690</td>
</tr>
<tr>
<td style=""text-align: right;"">1205470</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">680</td>
</tr>
<tr>
<td style=""text-align: right;"">1205476</td>
<td>6/24/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">670</td>
</tr>
<tr>
<td style=""text-align: right;"">1222807</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-200</td>
<td style=""text-align: right;"">470</td>
</tr>
<tr>
<td style=""text-align: right;"">1225247</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">440</td>
</tr>
<tr>
<td style=""text-align: right;"">1225249</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">410</td>
</tr>
<tr>
<td style=""text-align: right;"">1225251</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">380</td>
</tr>
<tr>
<td style=""text-align: right;"">1225253</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">350</td>
</tr>
<tr>
<td style=""text-align: right;"">1225255</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">320</td>
</tr>
<tr>
<td style=""text-align: right;"">1225259</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">290</td>
</tr>
<tr>
<td style=""text-align: right;"">1225273</td>
<td>6/25/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">270</td>
</tr>
<tr>
<td style=""text-align: right;"">1241866</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">240</td>
</tr>
<tr>
<td style=""text-align: right;"">1241881</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">210</td>
</tr>
<tr>
<td style=""text-align: right;"">1242022</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">180</td>
</tr>
<tr>
<td style=""text-align: right;"">1242026</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">150</td>
</tr>
<tr>
<td style=""text-align: right;"">1242109</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">120</td>
</tr>
<tr>
<td style=""text-align: right;"">1242115</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">90</td>
</tr>
<tr>
<td style=""text-align: right;"">1242175</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">70</td>
</tr>
<tr>
<td style=""text-align: right;"">1246598</td>
<td>6/27/2025</td>
<td style=""text-align: right;"">-70</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: right;"">1265596</td>
<td>6/30/2025</td>
<td style=""text-align: right;"">320</td>
<td style=""text-align: right;"">320</td>
</tr>
<tr>
<td style=""text-align: right;"">1265604</td>
<td>6/30/2025</td>
<td style=""text-align: right;"">720</td>
<td style=""text-align: right;"">1040</td>
</tr>
<tr>
<td style=""text-align: right;"">1289533</td>
<td>7/2/2025</td>
<td style=""text-align: right;"">-100</td>
<td style=""text-align: right;"">940</td>
</tr>
<tr>
<td style=""text-align: right;"">1296320</td>
<td>7/2/2025</td>
<td style=""text-align: right;"">-90</td>
<td style=""text-align: right;"">850</td>
</tr>
<tr>
<td style=""text-align: right;"">1296323</td>
<td>7/2/2025</td>
<td style=""text-align: right;"">90</td>
<td style=""text-align: right;"">940</td>
</tr>
<tr>
<td style=""text-align: right;"">1324258</td>
<td>7/7/2025</td>
<td style=""text-align: right;"">220</td>
<td style=""text-align: right;"">1160</td>
</tr>
<tr>
<td style=""text-align: right;"">1396957</td>
<td>7/14/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">1130</td>
</tr>
<tr>
<td style=""text-align: right;"">1396964</td>
<td>7/14/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">1100</td>
</tr>
<tr>
<td style=""text-align: right;"">1396968</td>
<td>7/14/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">1070</td>
</tr>
<tr>
<td style=""text-align: right;"">1396973</td>
<td>7/14/2025</td>
<td style=""text-align: right;"">-30</td>
<td style=""text-align: right;"">1040</td>
</tr>
<tr>
<td style=""text-align: right;"">1467117</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">1020</td>
</tr>
<tr>
<td style=""text-align: right;"">1467161</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-100</td>
<td style=""text-align: right;"">920</td>
</tr>
<tr>
<td style=""text-align: right;"">1470236</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">910</td>
</tr>
<tr>
<td style=""text-align: right;"">1470348</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">900</td>
</tr>
<tr>
<td style=""text-align: right;"">1470422</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">890</td>
</tr>
<tr>
<td style=""text-align: right;"">1470497</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">880</td>
</tr>
<tr>
<td style=""text-align: right;"">1471788</td>
<td>7/21/2025</td>
<td style=""text-align: right;"">-10</td>
<td style=""text-align: right;"">870</td>
</tr>
<tr>
<td style=""text-align: right;"">1499118</td>
<td>7/23/2025</td>
<td style=""text-align: right;"">-300</td>
<td style=""text-align: right;"">570</td>
</tr>
<tr>
<td style=""text-align: right;"">1531689</td>
<td>7/25/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">550</td>
</tr>
<tr>
<td style=""text-align: right;"">1531692</td>
<td>7/25/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">530</td>
</tr>
<tr>
<td style=""text-align: right;"">1531697</td>
<td>7/25/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">510</td>
</tr>
<tr>
<td style=""text-align: right;"">1531699</td>
<td>7/25/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">490</td>
</tr>
<tr>
<td style=""text-align: right;"">1531703</td>
<td>7/25/2025</td>
<td style=""text-align: right;"">-20</td>
<td style=""text-align: right;"">470</td>
</tr>
</tbody>
</table></div>
",1,2,1,2025-08-05T14:19:19+00:00,3,163,True
79726460,12115322,,sql,Combine rows to remove nulls or swap row values if not null,"<p>Input table:</p>
<pre><code>ID  Account Contact
-----------------------
ID1 A11 C11
ID1 A12 NULL
ID2 A21 NULL
ID2 A22 C22
ID3 A31 C31
ID3 A32 C32
</code></pre>
<p>Output needs to be like this</p>
<pre><code>ID  Account Contact 
-----------------------
ID1 A12 C11
ID2 A21 C22
ID3 A31 C32
ID3 A32 C31
</code></pre>
<p>For IDs with one NULL and one non-NULL Contact: The output row should combine the Account from the row with the NULL Contact and the Contact from the row with the non-NULL Contact.</p>
<p>For IDs with two non-NULL Contacts: The output should show both records, but with the Contact values swapped.</p>
<p>I framed this query but I am not sure if this is correct</p>
<pre><code>-- For IDs with one NULL and one NOT NULL contact
SELECT
  t_null.ID,
  t_null.Account,
  t_not_null.Contact
FROM
  MYtable AS t_null
JOIN
  MYtable AS t_not_null ON t_null.ID = t_not_null.ID
WHERE
  t_null.Contact IS NULL
  AND t_not_null.Contact IS NOT NULL

UNION ALL

-- For IDs with two NOT NULL contacts
SELECT
  t1.ID,
  t1.Account,
  t2.Contact
FROM
  MYtable AS t1
JOIN
  MYtable AS t2 ON t1.ID = t2.ID
WHERE
  t1.Contact IS NOT NULL
  AND t2.Contact IS NOT NULL
  AND t1.ID IN (SELECT ID FROM MYtable WHERE Contact IS NOT NULL GROUP BY ID HAVING COUNT(*) = 2)

ORDER BY
  ID, Account;
</code></pre>
",5,6,1,2025-08-05T17:37:28+00:00,4,177,True
79726497,2174085,"East Coast, USA",sql,Grouping data using the LEAD() function,"<p>I've got this query:</p>
<pre><code>SELECT TOP (1000) [TRANS_ID]
      ,[VSN_VENDR_EFF_DT]
      ,LEAD([VSN_VENDR_EFF_DT], 1, '9999-12-31') OVER (ORDER BY [ACCT_NUM],[VSN_VENDR_EFF_DT]) AS NEXT_EFF_DT
      ,[VSN_VENDR_CD]
      ,[ACCT_NUM]
      ,[VSN_VENDR_CAN_DT]
  FROM [SOBI_REPORTING].[dbo].[AS_tblVSN_ACCT_VENDR]
  --where ACCT_NUM = '0607805'
  order by ACCT_NUM, VSN_VENDR_EFF_DT
</code></pre>
<p>It produces these results when I look at just a specific segment of data (the actual dataset returned is hundreds of thousand of records, I'm just zooming in on a specific ACCT_NUM):</p>
<pre><code>TRANS_ID    VSN_VENDR_EFF_DT    NEXT_EFF_DT VSN_VENDR_CD    ACCT_NUM    VSN_VENDR_CAN_DT
8463855     2013-07-01          2023-07-01           VSP    0607805             NULL
9075501     2023-07-01          2012-06-01          EYMD    0607805             NULL
8395520     2012-06-01          2024-06-01           VSP    0607811             NULL
9119702     2024-06-01          2012-06-01          EYMD    0607811             NULL
</code></pre>
<p>However, when I uncomment that WHERE statement, it produces these results, which are actually what I'm expecting:</p>
<pre><code>TRANS_ID    VSN_VENDR_EFF_DT    NEXT_EFF_DT VSN_VENDR_CD    ACCT_NUM    VSN_VENDR_CAN_DT
8463855     2013-07-01          2023-07-01           VSP     0607805            NULL
9075501     2023-07-01          9999-12-31          EYMD     0607805            NULL
</code></pre>
<p>Note the NEXT_EFF_DT values between the two.  How can I get my query to return proper results?  I need to group by ACCT_NUM and order by VSN_VENDR_EFF_DT, and my current query appears to be grabbing the next VSN_VENDR_EFF_DT without regard to ACCT_NUM.</p>
<p>To be clear, I'm expecting something like this, given the data above:</p>
<pre><code>TRANS_ID    VSN_VENDR_EFF_DT    NEXT_EFF_DT VSN_VENDR_CD    ACCT_NUM    VSN_VENDR_CAN_DT
8463855     2013-07-01          2023-07-01           VSP    0607805             NULL
9075501     2023-07-01          9999-12-31          EYMD    0607805             NULL
8395520     2012-06-01          2024-06-01           VSP    0607811             NULL
9119702     2024-06-01          9999-12-31          EYMD    0607811             NULL
</code></pre>
",-1,0,1,2025-08-05T18:08:32+00:00,1,109,True
79726744,21906453,,sql,Partial matching on strings,"<p>In SQL Server I have a table of bank transactions, and a table of payees. I want to link the transaction to the correct payee. I have used <code>CHARINDEX (PayeeName, TxnDescription,1) &gt; 0</code> to find the links.</p>
<p>It works OK for some transactions, but I have hit a problem with this situation:</p>
<p>There are 2 payees, &quot;Tesco&quot; and &quot;Tesco Pet Insurance&quot; – when the transaction description contains the words &quot;Tesco Pet Insurance&quot; it matches on both payees, and I only want a match on payee &quot;Tesco Pet Insurance&quot;.</p>
<p>Can someone suggest an efficient method of achieving this.</p>
<p>This is my SQL statement:</p>
<pre><code>SELECT
    tx.Description
    , py.Payee
FROM tblTxns TX
INNER JOIN vwPayeeNames py
    ON CHARINDEX(py.Payee, tx.Description, 1) &gt; 0
WHERE tx.Description LIKE 'Tesco%'
AND LEN(tx.Description) &gt;= LEN(py.Payee)
GROUP BY tx.Description
    , py.Payee
</code></pre>
",0,1,1,2025-08-06T01:51:26+00:00,3,221,True
79727413,1695672,Ireland,sql,Setting the counter of one table equal that of another doesn&#39;t work as expected,"<p>I have two tables in a SQLite database:</p>
<pre><code>CREATE TABLE IF NOT EXISTS &quot;DVDsBluRays&quot; (
        &quot;dbd_Title&quot;     TEXT,
        &quot;dbd_DateBought&quot;        TEXT,
        &quot;dbd_LastSeen&quot;  TEXT,
        &quot;dbd_Notes&quot;     TEXT,
        &quot;dbd_Category&quot;  TEXT,
        &quot;dbd_Type&quot;      TEXT,
        &quot;dbd_Length&quot;    INTEGER,
        &quot;dbd_MyRating&quot;  TEXT,
        &quot;dbd_Cost&quot;      TEXT,
        &quot;dbd_Extras&quot;    TEXT,
        &quot;dbd_Quality&quot;   TEXT,
        &quot;dbd_IsBluRay&quot;  TEXT,
        &quot;dbd_DiscType&quot;  TEXT,
        &quot;dbd_fnm_Counter&quot;       INTEGER,
        &quot;dbd_Counter&quot;   INTEGER NOT NULL UNIQUE,
        PRIMARY KEY(&quot;dbd_Counter&quot; AUTOINCREMENT)
);
CREATE TABLE IF NOT EXISTS &quot;FilmNames&quot; (
        &quot;fnm_FilmName&quot;  TEXT UNIQUE,
        &quot;fnm_dbd_Counter&quot;       INTEGER,
        &quot;fnm_Counter&quot;   INTEGER NOT NULL UNIQUE,
        PRIMARY KEY(&quot;fnm_Counter&quot; AUTOINCREMENT)
);
</code></pre>
<p>These are their contents:</p>
<p><code>FilmNames</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Christmas carol</td>
<td></td>
<td>795</td>
</tr>
<tr>
<td>13th warrior, the</td>
<td></td>
<td>904</td>
</tr>
<tr>
<td>16 blocks</td>
<td></td>
<td>1747</td>
</tr>
<tr>
<td>1408</td>
<td></td>
<td>2050</td>
</tr>
<tr>
<td>17 again</td>
<td></td>
<td>2323</td>
</tr>
<tr>
<td>2012</td>
<td></td>
<td>2364</td>
</tr>
<tr>
<td>21 Jump Street</td>
<td></td>
<td>3012</td>
</tr>
<tr>
<td>2 guns</td>
<td></td>
<td>3922</td>
</tr>
<tr>
<td>A bad idea gone wrong</td>
<td></td>
<td>4134</td>
</tr>
<tr>
<td>21 bridges</td>
<td></td>
<td>4623</td>
</tr>
<tr>
<td>1917</td>
<td></td>
<td>4986</td>
</tr>
<tr>
<td>A Christmas carol (1984)</td>
<td></td>
<td>5386</td>
</tr>
</tbody>
</table></div>
<p><code>DVDsBluRays</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>dbd_Title</th>
<th>dbd_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>13th warrior, the</td>
<td>5</td>
</tr>
<tr>
<td>Abyss, the</td>
<td>10</td>
</tr>
<tr>
<td>A bug's life</td>
<td>80</td>
</tr>
<tr>
<td>A few good men</td>
<td>156</td>
</tr>
<tr>
<td>A murder of crows</td>
<td>307</td>
</tr>
<tr>
<td>16 blocks</td>
<td>567</td>
</tr>
<tr>
<td>12:01</td>
<td>571</td>
</tr>
<tr>
<td>17 again</td>
<td>720</td>
</tr>
<tr>
<td>2001: A space odyssey</td>
<td>736</td>
</tr>
<tr>
<td>2012</td>
<td>836</td>
</tr>
<tr>
<td>A fish called Wanda</td>
<td>915</td>
</tr>
</tbody>
</table></div>
<p>For the film names that they have in common I want to set <code>fnm_dbd_Counter</code> to <code>dbd_Counter</code>.  This shows the film names in common:</p>
<pre><code>select fnm_FilmName, fnm_Counter, dbd_Counter
from (select * from FilmNames) fnm
Join DVDsBluRays dbd on fnm.fnm_FilmName = dbd.dbd_Title
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>13th warrior, the</td>
<td>904</td>
<td>5</td>
</tr>
<tr>
<td>16 blocks</td>
<td>1747</td>
<td>567</td>
</tr>
<tr>
<td>17 again</td>
<td>2323</td>
<td>720</td>
</tr>
<tr>
<td>2012</td>
<td>2364</td>
<td>836</td>
</tr>
</tbody>
</table></div>
<p>When I try to set the <code>fnm_dbd_Counter</code> equal to <code>dbd_Counter</code> the outcome is not what I expect:</p>
<pre><code>update FilmNames
set fnm_dbd_Counter = dbd_Counter 
from (select * from FilmNames) fnm
Join DVDsBluRays dbd on fnm.fnm_FilmName = dbd.dbd_Title
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Christmas carol</td>
<td>836</td>
<td>795</td>
</tr>
<tr>
<td>13th warrior, the</td>
<td>836</td>
<td>904</td>
</tr>
<tr>
<td>16 blocks</td>
<td>836</td>
<td>1747</td>
</tr>
<tr>
<td>1408</td>
<td>836</td>
<td>2050</td>
</tr>
<tr>
<td>17 again</td>
<td>836</td>
<td>2323</td>
</tr>
<tr>
<td>2012</td>
<td>836</td>
<td>2364</td>
</tr>
<tr>
<td>21 Jump Street</td>
<td>836</td>
<td>3012</td>
</tr>
<tr>
<td>2 guns</td>
<td>836</td>
<td>3922</td>
</tr>
<tr>
<td>A bad idea gone wrong</td>
<td>836</td>
<td>4134</td>
</tr>
<tr>
<td>21 bridges</td>
<td>836</td>
<td>4623</td>
</tr>
<tr>
<td>1917</td>
<td>836</td>
<td>4986</td>
</tr>
<tr>
<td>A Christmas carol (1984)</td>
<td>836</td>
<td>5386</td>
</tr>
</tbody>
</table></div>
<p>They're all set to the dbd_Counter of one of the DVDs/Blu-rays, even the ones that don't match.  I would have expected:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Christmas carol</td>
<td></td>
<td>795</td>
</tr>
<tr>
<td>13th warrior, the</td>
<td>5</td>
<td>904</td>
</tr>
<tr>
<td>16 blocks</td>
<td>567</td>
<td>1747</td>
</tr>
<tr>
<td>1408</td>
<td></td>
<td>2050</td>
</tr>
<tr>
<td>17 again</td>
<td>720</td>
<td>2323</td>
</tr>
<tr>
<td>2012</td>
<td>836</td>
<td>2364</td>
</tr>
<tr>
<td>21 Jump Street</td>
<td></td>
<td>3012</td>
</tr>
<tr>
<td>2 guns</td>
<td></td>
<td>3922</td>
</tr>
<tr>
<td>A bad idea gone wrong</td>
<td></td>
<td>4134</td>
</tr>
<tr>
<td>21 bridges</td>
<td></td>
<td>4623</td>
</tr>
<tr>
<td>1917</td>
<td></td>
<td>4986</td>
</tr>
<tr>
<td>A Christmas carol (1984)</td>
<td></td>
<td>5386</td>
</tr>
</tbody>
</table></div>
",2,2,0,2025-08-06T14:21:48+00:00,1,86,True
79727552,23873959,,sql,Why do I get an &quot;invalid identifier&quot; error when I use LEFT()?,"<p>From a 6-character string in <code>TERM_CODE_KEY</code> I need to produce a calculated result as column <code>academic_yr</code>:</p>
<pre><code>-- TERM_CODE_KEY is of type VARCHAR2(6) and returns values of the form YYYYMM
SELECT TERM_CODE_KEY,
CASE SUBSTR(TERM_CODE_KEY,5)                -- Get MM part
    WHEN '01' THEN LEFT(TERM_CODE_KEY,4)    -- start of a calculation on the YYYY part
    WHEN '05' THEN LEFT(TERM_CODE_KEY,4)    -- start of a calculation on the YYYY part
    ELSE LEFT(TERM_CODE_KEY,4)              -- start of a calculation on the YYYY part
END AS academic_yr
FROM DWH.SR_COURSES_FAC
WHERE (TERM_CODE_KEY BETWEEN '202001' AND '202409')
ORDER BY TERM_CODE_KEY, academic_yr ASC
</code></pre>
<p>Oracle returns an error on the <code>ELSE</code> clause:</p>
<blockquote>
<p>SQL Error [904] [42000]: ORA-00904: &quot;LEFT&quot;: invalid identifier</p>
</blockquote>
<p>Why is <code>LEFT()</code> identified as an identifier? If I change the <code>ELSE</code> clause to return a string instead of a calculation the same error moves up to the last <code>WHEN</code> clause. It should be possible to execute operations in the <code>THEN</code> and <code>ELSE</code> clauses. What am I doing wrong?</p>
",4,4,0,2025-08-06T16:00:46+00:00,1,123,True
79727721,,,sql,How to implement this SQL query with CTE in Exposed DSL?,"<p>I have a SQL query that uses a Common Table Expression (CTE) with window functions, and I need to implement it using Kotlin's Exposed DSL. Since Exposed doesn't have built-in CTE support, I'm looking for a way to create custom CTEs.</p>
<p>Query:</p>
<pre><code>WITH ranked_orders AS (
    SELECT
        customer_id,
        order_date,
        total_amount,
        DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as rank_num
    FROM orders
)
SELECT
    customer_id,
    order_date,
    total_amount
FROM ranked_orders
WHERE rank_num &lt;= 3;
</code></pre>
<p>Based on this JetBrains video (29:00) <a href=""https://www.youtube.com/watch?v=xgfeqj8UyVA&amp;lc=Ugz_N4GGefP6accqKbl4AaABAg"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=xgfeqj8UyVA&amp;lc=Ugz_N4GGefP6accqKbl4AaABAg</a>, it seems Exposed provides the tools to implement CTEs ourselves, but I haven't found concrete examples of how to do it. They created custom CTE, haven't provided code.
Any working code examples would be greatly appreciated!</p>
",3,3,0,2025-08-06T18:45:07+00:00,1,238,True
79727724,23873959,,sql,Invalid identifier when using aggregate count(*),"<p>I need to count the number of students who were registered in a given course over an academic year.</p>
<p>The following <code>SELECT</code> query returns a column of course numbers and a column of the academic year.</p>
<pre><code>SELECT COURSE_SUBJECT_CODE || ' ' || COURSE_NUMBER AS COURSE,
    CASE SUBSTR(TERM_CODE_KEY, 5, 2)
         WHEN '01' THEN TO_CHAR(TO_NUMBER(SUBSTR(TERM_CODE_KEY, 1, 4))-1)
           WHEN '05' THEN TO_CHAR(TO_NUMBER(SUBSTR(TERM_CODE_KEY, 1, 4))-1)
           ELSE SUBSTR(TERM_CODE_KEY, 1, 4)
    END AS ACAD_YR
FROM DWH.SR_COURSES_FAC
WHERE (COURSE_SUBJECT_CODE = 'BMDE')
ORDER BY COURSE, ACAD_YR;
</code></pre>
<p>For example:</p>
<pre class=""lang-none prettyprint-override""><code>COURSE       ACAD_YR
BMDE 1       2024
BMDE 1       2024
BMDE 2       2024
BMDE 2       2025
</code></pre>
<p>I would like the following result:</p>
<pre class=""lang-none prettyprint-override""><code>COURSE       ACAD_YR    NUM_STUDENTS
BMDE 1       2024       2
BMDE 2       2024       1
BMDE 2       2025       1
</code></pre>
<p>When I modify the query as follows aggregate counting:</p>
<pre><code>SELECT COURSE_SUBJECT_CODE || ' ' || COURSE_NUMBER AS COURSE,
       CASE SUBSTR(TERM_CODE_KEY, 5, 2)
           WHEN '01' THEN TO_CHAR(TO_NUMBER(SUBSTR(TERM_CODE_KEY, 1, 4))-1)
           WHEN '05' THEN TO_CHAR(TO_NUMBER(SUBSTR(TERM_CODE_KEY, 1, 4))-1)
           ELSE SUBSTR(TERM_CODE_KEY, 1, 4)
       END AS ACAD_YR,
COUNT(*) AS NUM_STUDENTS
FROM DWH.SR_COURSES_FAC
WHERE (COURSE_SUBJECT_CODE = 'BMDE')
GROUP BY COURSE, ACAD_YR
ORDER BY COURSE, ACAD_YR;
</code></pre>
<p>I get the following error: <code>ORA-00904: &quot;ACAD_YR&quot;: invalid identifier</code>
flagged on the <code>GROUP BY</code> line. I don't understand why.</p>
",2,4,2,2025-08-06T18:46:04+00:00,2,114,True
79728772,2521564,,sql,GROUP BY with &quot;?&quot; placeholders for Trino,"<p><strong>Problem description in Trino</strong><br />
If the <code>GROUP BY</code> clause contains a <code>?</code>, the query fails.<br />
ie:</p>
<pre class=""lang-sql prettyprint-override""><code>PREPARE stmt FROM
SELECT
    IF(origin = ?, 'LI', 'AB'),
    SUM(1)
FROM
    &lt;catalog&gt;.&lt;table&gt;
WHERE
    date = '2025-07-01'
GROUP BY 
    1;
EXECUTE stmt USING '&lt;value&gt;';
</code></pre>
<p>works, but</p>
<pre class=""lang-sql prettyprint-override""><code>PREPARE stmt FROM
SELECT
    IF(origin = ?, 'LI', 'AB'),
    SUM(1)
FROM
    &lt;catalog&gt;.&lt;table&gt;
WHERE
    date = '2025-07-01'
GROUP BY 
    IF(origin = ?, 'LI', 'AB');
EXECUTE stmt USING '&lt;value&gt;', '&lt;value&gt;';
</code></pre>
<p>throws this error:</p>
<pre class=""lang-none prettyprint-override""><code>SQL Error [73]: Query failed (#20250807_145054_11734_hjgyj): line 2:3: 'IF((origin = ?), 'LI', 'AB')' must be an aggregate expression or appear in GROUP BY clause
</code></pre>
<p><strong>Problem description with <code>jOOQ</code></strong><br />
I don't need the value of the <code>GROUP BY</code> to be variable</p>
<pre class=""lang-sql prettyprint-override""><code>GROUP BY 
    IF(origin = ?, 'LI', 'AB');
</code></pre>
<p>since I know it at compile time, but that's the way the <code>org.jooq.Condition</code> seems to be translated to the SQL <code>PreparedStatement</code>.<br />
This is how I create the <code>org.jooq.Condition</code>:</p>
<pre><code>DSL.field(&quot;origin&quot;).eq(&lt;static value&gt;)
</code></pre>
<p><strong>Possible solution</strong><br />
Is there a way for the <code>org.jooq.Condition</code> to be created without the <code>?</code> placeholder? So that the <code>PreparedStatement</code> is executed in Trino will be</p>
<pre class=""lang-sql prettyprint-override""><code>GROUP BY 
    IF(origin = &lt;static val&gt;, 'LI', 'AB');
</code></pre>
<p>and not</p>
<pre class=""lang-sql prettyprint-override""><code>GROUP BY 
    IF(origin = ?, 'LI', 'AB');
</code></pre>
",2,3,1,2025-08-07T15:00:37+00:00,2,107,True
79729011,11956484,,sql,Filter table based on substrings taken from column in another table,"<p>In my data (Table 1) I have IDs and a list of codes that are stored as one large string Example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>CD_LIST</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A;B;C;D</td>
</tr>
<tr>
<td>2</td>
<td>A;E;F;G</td>
</tr>
<tr>
<td>3</td>
<td>X;Z;W;F;H</td>
</tr>
<tr>
<td>4</td>
<td>C;D;E</td>
</tr>
<tr>
<td>5</td>
<td>P;Q;R;S;T;U</td>
</tr>
</tbody>
</table></div>
<p>In another table (Table 2) I have a column of codes and their descriptions</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>CD</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>foo</td>
</tr>
<tr>
<td>B</td>
<td>bar</td>
</tr>
<tr>
<td>C</td>
<td>baz</td>
</tr>
<tr>
<td>D</td>
<td>buz</td>
</tr>
</tbody>
</table></div>
<p>I need to filter Table 1 to keep only rows where the ID is in Table 2. In this example my desired result would be</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>CD_LIST</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A;B;C;D</td>
</tr>
<tr>
<td>2</td>
<td>A;E;F;G</td>
</tr>
<tr>
<td>4</td>
<td>C;D;E</td>
</tr>
</tbody>
</table></div>
<p>How can I do this without writing a bunch of LIKE or regex statements for each code? There are 132 codes in Table 2 and the maximum length of CD_LIST in Table 1 is 32, meaning that some IDs have as many as 32 different codes to check against Table 2</p>
<p>I tried</p>
<pre><code>WITH a AS (
    SELECT CD, Description
    FROM Table2
    WHERE Description IN ('foo', 'bar', 'baz', 'buz')
)
SELECT * 
FROM Table1
WHERE CD_LIST LIKE '%' || (SELECT CD FROM a) || '%'
</code></pre>
<p>But this gives me too many rows returned error</p>
",0,0,0,2025-08-07T18:35:10+00:00,3,91,True
79729059,18293772,,sql,Syntax error creating function in Snowflake,"<p>I am unable to fix a syntax error that I keep getting when trying to create a function in Snowflake. This works fine when I create a procedure, but whenever I try to create a function with a variable declaration, I keep getting the error below. I want this to be a function since it only returns a single, scalar value.</p>
<blockquote>
<p>Syntax error: unexpected 'l_emp_excess_balance'.t</p>
</blockquote>
<p>Here is the code:</p>
<pre><code>CREATE OR REPLACE FUNCTION MDT_PRESDEV_DB_ADMIN.PERS.begin_bal_emp_excess_comp(psid VARCHAR)
RETURNS NUMBER
AS
$$
DECLARE
  l_emp_excess_balance NUMBER;
BEGIN
  SELECT COALESCE(v.peec_excess_leave_hrs, 0)
  INTO l_emp_excess_balance
  FROM MDT_PRESDEV_DB_ADMIN.pers_excess_xmpt_comp_leave v
  WHERE v.peec_ps_id = psid
    AND v.peec_system = 'PERS'
    AND v.peec_year = (
      SELECT MAX(m.peec_year)
      FROM MDT_PRESDEV_DB_ADMIN.pers_excess_xmpt_comp_leave m
      WHERE m.peec_system = 'PERS'
    );
  RETURN COALESCE(l_emp_excess_balance, 0);
END;
$$;
</code></pre>
<p>I tried creating a procedure instead of a function. That worked, but I would much rather this just be a function.</p>
",2,3,1,2025-08-07T19:22:35+00:00,1,116,True
79729079,5070879,,sql,Enforcing specific join order,"<p>I am looking for a way to preserve a specific join order during query execution:</p>
<pre><code>SELECT *
FROM tab1
JOIN tab2
  ON tab1.t2_id = tab2.id
JOIN tab3
  ON tab1.t3_id = tab3.id;
</code></pre>
<p>Effectively an equivalent of a query hint like:</p>
<ul>
<li>SQL Server <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/option-clause-transact-sql?view=sql-server-ver17#g-force-the-join-order-to-match-the-order-in-the-query"" rel=""nofollow noreferrer"">OPTION (FORCE ORDER)</a></li>
<li>MySQL <a href=""https://dev.mysql.com/doc/refman/8.0/en/optimizer-hints.html#optimizer-hints-join-order"" rel=""nofollow noreferrer"">SELECT /*+ JOIN_FIXED_ORDER */ ... </a></li>
</ul>
",-1,0,1,2025-08-07T19:49:24+00:00,1,129,True
79729171,15435022,,sql,Generated column with data type timestamptz (timestamp with time zone),"<p>In PostgreSQL, I want to make a computed column, where end_datetime = start_datetime + minute_duration adding an interval to a timestamp.</p>
<p>I keep getting error, how can I fix?</p>
<blockquote>
<p>ERROR:  generation expression is not immutable
SQL state: 42P17</p>
</blockquote>
<p>Tried two options below:</p>
<pre><code>CREATE TABLE appt ( 
  appt_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  minute_duration INTEGER NOT NULL,
  start_datetime TIMESTAMPTZ NOT NULL,
  end_datetime TIMESTAMPTZ GENERATED ALWAYS AS (start_datetime + (minute_duration || ' minutes')::INTERVAL) STORED
 );


 CREATE TABLE appt ( 
  appt_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  minute_duration INTEGER NOT NULL,
  start_datetime TIMESTAMPTZ NOT NULL,
  end_datetime TIMESTAMPTZ GENERATED ALWAYS AS (start_datetime + make_interval(mins =&gt; minute_duration)) STORED
);
</code></pre>
<p>The only other option (I can think of) would be trigger, but trying to refrain trigger method for now.
If trigger is the only answer, feel free to give trigger solution.</p>
",2,2,0,2025-08-07T22:11:45+00:00,2,254,True
79729280,27323756,,sql,"SELECT LAST with non-time filter returns empty set, how to get the latest row per device where val=&#39;1&#39;?","<p>Environment  Information</p>
<ul>
<li>Apache IoTDB 1.3.1(1C1D, standalone)</li>
<li>JDK 17</li>
<li>CentOS 7</li>
</ul>
<p>My scenario:
Create a time-series with  SQL:</p>
<pre><code>CREATE TIMESERIES root.gn.ZS.JYJ.TJB_2.YX.val WITH DATATYPE=TEXT, ENCODING=PLAIN; 
</code></pre>
<p>Insert Sample data:</p>
<pre><code>INSERT INTO root.gn.ZS.JYJ.TJB_2.YX (timestamp,val)
VALUES
(2025-03-02T10:00:00,'1'),
(2025-03-02T11:00:00,'0'),
(2025-03-03T09:00:00,'1'); 
</code></pre>
<p>Try to query the data with time filter, I can get the expected result.</p>
<pre><code>select *
from root.gn.ZS.JYJ.TJB_2.YX
where time &gt;= 2025-03-01T00:00:00
and time &lt; 2025-04-01T00:00:00
and val = '1'
align by device;
</code></pre>
<p>The result:</p>
<pre><code>+-----------------------------+-------------------------------+-----+ 
|                         Time|                         Device|  val|
+-----------------------------+-------------------------------+-----+ 
|2025-03-02T10:00:00.000+08:00|root.gn.ZS.JYJ.TJB_2.YX        |    1| 
|2025-03-03T09:00:00.000+08:00|root.gn.ZS.JYJ.TJB_2.YX        |    1| 
+-----------------------------+-------------------------------+-----+ 
Total line number = 2 
</code></pre>
<p>Then I try to use Last query with the same time filter</p>
<pre><code>select last val
from root.gn.ZS.JYJ.TJB_2.YX
where time &gt;= 2025-03-01T00:00:00
and time &lt; 2025-04-01T00:00:00
and val = '1'
align by device; 
</code></pre>
<p>I got the empty set.</p>
<pre><code>IoTDB&gt; +----+------+----+ 
       |Time|Device| val|
       +----+------+----+
       +----+------+----+ 
       Empty set (0.03 sec)
</code></pre>
<p>Then I try to use Last * query with the same time filter, also empty.</p>
<p>My question is: How can I retrieve the latest row per device whose <code>val='1'</code> within the given time range?  For the sample data, the expected output is:</p>
<pre><code>+-----------------------------+-------------------------------+-----+
|                         Time|                         Device|  val|
+-----------------------------+-------------------------------+-----+
|2025-03-03T09:00:00.000+08:00|root.gn.ZS.JYJ.TJB_2.YX        |    1|
+-----------------------------+-------------------------------+-----+
</code></pre>
<p>I'm looking for a single SQL statement if possible, or a concise workaround using standard IoTDB syntax.</p>
",1,2,1,2025-08-08T02:58:57+00:00,1,144,False
79729450,5922395,,sql,Oracle SQL hierarchical query to trace cable segments,"<p>I have a dataset that can be represented as below network diagram where cable segments form the hierarchy:</p>
<p><a href=""https://i.sstatic.net/Yjgh4qIx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Yjgh4qIx.png"" alt=""enter image description here"" /></a></p>
<p>The first segment is the input cable for a box and is tagged 'I', and the same box (<code>addressId</code>) will have out cable (tagged 'O') that would go and connect to another box at a different location. The out cable is termed as In cable at the second box.</p>
<p>I have SQL that can give me segment-1 data, and if I add another layer with subquery I get segment-2 and union of these will give me segment-1 and segment-2 cable information.  But if there are N number of segments in the network then my query gets complicated too fast. I wanted to find out if there is a way to rewrite these below queries such a way that it can retrieve all the segments regardless of the segment size and without having to write unions for each segments.</p>
<pre><code>-- 1 Segment list with the subject cable and pair range as Incount
select region,cable,lowRange,highRange,address,addressId 
      from schema.TableName
      where upper(region) = 'COLORADO' and
            upper(cable) = 'ABCD' and
            to_number(lowRange) &gt;= 1 and
            to_number(highRange) &lt;= 600 and
            side = 'i';

-- 2 Segment list with 1 Segment cable and pair range Outcount as Incount
select c.region,c.cable,c.lowRange,c.highRange,c.address,c.addressId
      from schema.TableName c,
           --1 Segment list  which have Outcount of their own
           (select a.region,a.address as xbox,a.cable,a.lowRange,a.highRange 
            from schema.TableName a, 
                 --1 Segment list with the subject cable and pair range as Incount
                 (select region,addressId 
                          from schema.TableName
                          where upper(region) = 'COLORADO' and
                            upper(cable) = 'ABCD' and
                            to_number(lowRange) &gt;= 1 and
                            to_number(highRange) &lt;= 600 and
                                side = 'i') b
            where a.region = b.region and
                  a.addressId = b.addressId and
                  a.side = 'o') d
      where c.region = d.region and
            c.ca = d.ca and
            c.side = 'i' and
            ((c.lowRange between d.lowRange and d.highRange) or
             (c.highRange between d.lopr and d.highRange));
</code></pre>
<p>My knowledge on hierarchical joins is limited, I came up with below but of course it doesn't work as intended. And I suspect it might be due to <code>CONNECT BY PRIOR</code> cannot account and not account box location (<code>addressId</code>) selectively (cable that is input to a box and output from the same box will have same <code>addressId</code> whereas the same cable tagged as input at different box will have different <code>addressid</code>) or maybe this needs recursive query logic.</p>
<p>Much appreciated if anyone can provide me pointers.</p>
<pre><code>SELECT DISTINCT 
    t.region, t.cable, t.lowRange, t.highRange, t.address, t.addressId,
    LEVEL AS segment_level
FROM
    schema.TableName t
WHERE
    t.side = 'i'
CONNECT BY
    PRIOR t.region = t.region
    AND PRIOR t.cable = t.cable
    AND PRIOR t.side = 'o'
    AND t.addressId &lt;&gt; PRIOR t.addressId
    AND (
        (t.lowRange BETWEEN PRIOR t.lowRange AND PRIOR t.highRange)
        OR
        (t.highRange BETWEEN PRIOR t.lowRange AND PRIOR t.highRange)
    )
START WITH
    UPPER(t.region) = 'COLORADO'
    AND UPPER(t.cable) = 'ABCD'
    AND TO_NUMBER(t.lowRange) &gt;= 1
    AND TO_NUMBER(t.highRange) &lt;= 600
    AND t.side = 'i';
</code></pre>
<p>========================</p>
<p>Update: Please find the DB fiddle below. The expected result should have all the segments available on the table of side 'i'. I have included 'segmentlevel' as an additional column just to help with the identification here, but in reality this column wont exist on the table.</p>
<p>Also the query I have with the unions provide upto segment '2' only because I have added in just 2 levels of union.</p>
<p><a href=""https://dbfiddle.uk/WpmHfcpk"" rel=""nofollow noreferrer"">https://dbfiddle.uk/WpmHfcpk</a></p>
",4,4,0,2025-08-08T07:31:01+00:00,1,139,True
79729716,7836976,Ireland,sql,Use function on foreign key constraint,"<p>Can I have a foreign key constraint that takes a timestamp column but uses it as date only?</p>
<p>I'm trying something like this but it doesn't work:</p>
<pre class=""lang-sql prettyprint-override""><code>alter table 
  table1 
add 
  constraint my_constraint foreign key (
    date(created_date)
  ) references table2(
    date(transaction_timestamp)
  );

</code></pre>
",2,3,1,2025-08-08T11:42:22+00:00,2,153,True
79729807,11474988,,sql,2 date attributes stored as text strings need to be compared with earliest date selected,"<p>I have 2 date attributes stored as text strings that need to be compared with earliest date selected.</p>
<p>In the output shown here, you can see for ID 1 the earliest date has not been correctly identified using my code.</p>
<p>I have tried using casting and creating new columns but cannot find a way to compare the 2 date attributes in their current format, please can someone advise on how to do this.</p>
<p>Current query:</p>
<pre><code>SELECT DISTINCT 
    A.[ID],
    A.[Date field 1 ],
    B.[Date field 2],
    CASE 
        WHEN A.[Date field 1 ] &lt; B.[Date field 2] 
            THEN (A.[Date field 1 ])
        ELSE [Date field 2]
    END AS 'Earliest date'
FROM
    [TABLE A] A
JOIN 
    [TABLE B] B ON A.[ID] = B.[ID]
GROUP BY
    A.[ID], A.[Date field 1 ], B.[Date field 2]
</code></pre>
<p>Output:</p>
<pre><code>ID  Date field 1        Date field 2        Earliest date
------------------------------------------------------------
1   02/08/2025 04:00    25/07/2025 15:58    02/08/2025 04:00
2   14/07/2025 04:08    25/07/2025 16:02    14/07/2025 04:08
3   14/07/2025 04:03    25/07/2025 16:11    14/07/2025 04:03
4   14/07/2025 04:03    25/07/2025 16:03    14/07/2025 04:03
</code></pre>
",2,2,0,2025-08-08T13:20:58+00:00,3,103,True
79730061,17581716,,sql,Query for the auto insert,"<p>I have 2 tables (remember I don't want to use system version table option at this time).</p>
<p>Table #1: <code>Employee</code></p>
<pre><code> ID   | name  | salary | grade
 -----+-------+--------+------
 1001 | name1 |   1000 | L1
</code></pre>
<p>Table #2: <code>Employee_History</code></p>
<pre><code>ID (auto generated) | Emp_ID | name   | salary | grade
--------------------+--------+--------+--------+------
         1          |   1001 | name1  |   1000 | L1
</code></pre>
<p>For example: if I update salary of <code>Employee</code> table as below</p>
<pre><code>1001, name1, 2000, L1 
</code></pre>
<p>I want to insert a new row into the <code>Employee_History</code> table automatically as shown.</p>
<p>So resultant <code>Employee_History</code> table will be like this after above update of the <code>Employee</code> table:</p>
<pre><code>1, 1001, name1, 1000, L1
2, 1001, name1, 2000, L1
</code></pre>
",-2,1,3,2025-08-08T17:18:40+00:00,1,108,True
79730239,10923920,,sql,Using GROUP BY - check for multiple conditions with a WHERE or HAVING clause for a single ID,"<p>I have a table of customer data. I will be joining it to a location table. Customer ID is distinct but Location ID is not because multiple customers can belong to one location. Each customer is identified as having an &quot;Active&quot; or &quot;Inactive&quot; status. Each customer is also identified as whether he or she is the location's Billing Contact.</p>
<p>I need to find all the Location ID values where at least one of these three conditions is true:</p>
<ol>
<li>No customer is listed as the Billing Contact.</li>
<li>All customers listed as the Billing Contact have an &quot;Inactive&quot; status.</li>
<li>All customers have an &quot;Inactive&quot; status.</li>
</ol>
<p>I tried something like the query below (the examples have been simplified), and I know it can't be right, but I can't see exactly how because I get an error message about invalid columns &quot;in the HAVING clause because it is not ctonained in either an aggregate function or the GROUP BY clause.&quot; I can put those two additional columns into the GROUP BY clause, but then I'm doing three groups when I'm pretty sure I only want to do one group.</p>
<pre><code>SELECT DISTINCT
  vLocation.LocationName
  ,vLocation.LocationID
  ,vCustomer.LocationID
  ,vCustomer.CustomerID
  ,vCustomer.CustomerName
  ,vCustomer.Status
  ,vCustomer.BillingContact
  ,vCustomer.Email
  ,vCustomer.Status
FROM
  DB.vLocation
  LEFT OUTER JOIN DB.vCustomer
    ON vLocation.LocationID = vCustomer.LocationID
WHERE
  vLocation.Status = 'Active'
  AND (vCustomer.CustomerID IS NULL
    OR vCustomer.CustomerID IN (
  SELECT DISTINCT
    vCustomer.CustomerID
  FROM
    DB.vCustomer
  GROUP BY
    vCustomer.CustomerID
  HAVING
    vCustomer.BillingContact = 'False'
    OR (vCustomer.BillingContact = 'True'
      AND vCustomer.Status = 'Inactive')
    OR vCustomer.Status = 'Inactive'
  )
)
</code></pre>
<p>A coworker suggested asking an AI for help, and it suggested this, which allowed me to get data, but my results were wrong because locations with active billing contacts were included.</p>
<pre><code>HAVING 
  SUM(CASE WHEN BillingContact = 'False' THEN 1 ELSE 0 END) &gt; 0
  OR 
  SUM(CASE WHEN BillingContact = 'True' AND Status = 'Inactive' THEN 1 ELSE 0 END) &gt; 0
</code></pre>
<p>Here is a sample table.
<strong>Location ID 004</strong> has no active customers and should be included in the results. Same with <strong>Location ID 085</strong> even though one of the customers is the Billing Contact.
<strong>Location ID 091</strong> has an active customer who is not the Billing Contact and an inactive customer who is the Billing Contact. This Location ID should be included in the results.
<strong>Location ID 092</strong> has two active customers but neither one is the Billing Contact; it should be included in the results.
<strong>Location IDs 100 and 101</strong> should be entirely EXCLUDED from the results because AT LEAST ONE of them is active and is the Billing Contact.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Location ID</th>
<th>Customer ID</th>
<th>Customer Name</th>
<th>Status</th>
<th>Billing Contact</th>
</tr>
</thead>
<tbody>
<tr>
<td>004</td>
<td>003</td>
<td>Bri</td>
<td>Inactive</td>
<td>FALSE</td>
</tr>
<tr>
<td>004</td>
<td>004</td>
<td>Rac</td>
<td>Inactive</td>
<td>FALSE</td>
</tr>
<tr>
<td>004</td>
<td>005</td>
<td>Tam</td>
<td>Inactive</td>
<td>FALSE</td>
</tr>
<tr>
<td>085</td>
<td>157</td>
<td>Ari</td>
<td>Inactive</td>
<td>FALSE</td>
</tr>
<tr>
<td>085</td>
<td>158</td>
<td>Hel</td>
<td>Inactive</td>
<td>TRUE</td>
</tr>
<tr>
<td>085</td>
<td>159</td>
<td>Ran</td>
<td>Inactive</td>
<td>FALSE</td>
</tr>
<tr>
<td>091</td>
<td>167</td>
<td>Jud</td>
<td>Inactive</td>
<td>TRUE</td>
</tr>
<tr>
<td>091</td>
<td>168</td>
<td>Ste</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>092</td>
<td>169</td>
<td>Izz</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>092</td>
<td>170</td>
<td>Kur</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>100</td>
<td>183</td>
<td>Ash</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>100</td>
<td>184</td>
<td>Bre</td>
<td>Active</td>
<td>TRUE</td>
</tr>
<tr>
<td>101</td>
<td>185</td>
<td>Cec</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>101</td>
<td>186</td>
<td>Emi</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>101</td>
<td>187</td>
<td>Eri</td>
<td>Active</td>
<td>TRUE</td>
</tr>
<tr>
<td>101</td>
<td>188</td>
<td>Jen</td>
<td>Active</td>
<td>FALSE</td>
</tr>
<tr>
<td>101</td>
<td>189</td>
<td>Jud</td>
<td>Inactive</td>
<td>TRUE</td>
</tr>
<tr>
<td>101</td>
<td>190</td>
<td>Mel</td>
<td>Active</td>
<td>FALSE</td>
</tr>
</tbody>
</table></div>
",0,1,1,2025-08-08T21:24:59+00:00,2,124,True
79730445,8434311,"Traun, &#214;sterreich",sql,DataGrip: Create Trigger within IDE,"<p>System: Firebase5.0 - Docker</p>
<p><strong>Problem</strong>: I am trying to create a trigger, for assigning the <code>ID</code> in Firebase 5; my problem here is that I can not change the TERM for DataGrip.</p>
<p>Code:</p>
<pre><code>CREATE TRIGGER BI_IMAGE_ID FOR IMAGE
BEFORE INSERT
AS
BEGIN
    IF (NEW.ID IS NULL) THEN
        NEW.ID = NEXT VALUE FOR SEQ_IMAGE_ID;
END
</code></pre>
<p>When running this with DataGrip, I get the following error message:</p>
<blockquote>
<p>[2025-08-09 14:57:41] &gt; CREATE TRIGGER BI_IMAGE_ID FOR IMAGE<br />
[2025-08-09 14:57:41] [42000][335544851] Dynamic SQL Error; SQL error code = -104; Unexpected end of command - line 6, column 29<br />
[SQLState:42000, ISC error code:335544851]</p>
</blockquote>
<p>Is there any solution for this?</p>
<p>In console I would just use</p>
<pre><code>SET TERM ^;
...CODE...
SET TERM ;^
</code></pre>
<p>But in the IDE this isn't possible?</p>
",1,1,0,2025-08-09T06:06:11+00:00,1,93,True
79731798,21494710,,sql,How to fix ORA-01417: a table may be outer joined to at most one other table in Oracle SQL?,"<p>I'm working on a query in Oracle using the old-style outer join syntax (+), and I'm running into the following error:</p>
<blockquote>
<p>ORA-01417: a table may be outer joined to at most one other table</p>
</blockquote>
<p>I understand that Oracle doesn't allow a table to be outer joined to more than one other table using this syntax. However, I need to check for missing customer notification preferences across multiple relationships.</p>
<p>Here’s a my query:</p>
<pre><code>SELECT c.customer_id, nic.notification_item_id, ns.schedule_id
FROM customers c,
 notification_item_channels nic,
 notification_sub_items nsi,
 notification_channels nc,
 subitem_schedules ss,
 notification_schedules ns,
 notification_items ni,
 cust_notif_prefs cnp
WHERE nic.sub_item_id = nsi.sub_item_id
 AND nic.channel_id = nc.channel_id
 AND nsi.sub_item_id = ss.sub_item_id
 AND ss.schedule_id = ns.schedule_id
 AND ni.item_id = nsi.item_id
 AND ni.item_type IN (1, 2)
 AND ni.item_id = 11
 AND c.customer_id = cnp.customer_id(+)
 AND nic.notification_item_id = cnp.notification_item_id(+)
 AND ns.schedule_id = cnp.schedule_id(+)
 AND cnp.preference_id IS NULL;
</code></pre>
<p>Data</p>
<ol>
<li><p>Table Creation</p>
<pre><code>CREATE TABLE customers (
    customer_id NUMBER PRIMARY KEY,
    customer_name VARCHAR2(50)
);
CREATE TABLE notification_items (
    item_id NUMBER PRIMARY KEY,
    item_name VARCHAR2(50),
    item_type NUMBER
);
CREATE TABLE notification_sub_items (
    sub_item_id NUMBER PRIMARY KEY,
    item_id NUMBER
);
CREATE TABLE notification_channels (
    channel_id NUMBER PRIMARY KEY,
    channel_name VARCHAR2(50)
);
CREATE TABLE notification_item_channels (
    notification_item_id NUMBER PRIMARY KEY,
    sub_item_id NUMBER,
    channel_id NUMBER
);
CREATE TABLE subitem_schedules (
    sub_item_id NUMBER,
    schedule_id NUMBER
);
CREATE TABLE notification_schedules (
    schedule_id NUMBER PRIMARY KEY,
    schedule_desc VARCHAR2(50)
);
CREATE TABLE cust_notif_prefs (
    preference_id NUMBER PRIMARY KEY,
    customer_id NUMBER,
    notification_item_id NUMBER,
    schedule_id NUMBER
);
</code></pre>
</li>
<li><p>Insert date</p>
<pre><code> -- Customers
 INSERT INTO customers VALUES (1, 'Alice');
 INSERT INTO customers VALUES (2, 'Bob');
 -- Notification Items
 INSERT INTO notification_items VALUES (11, 'Trade Alert', 1);
 INSERT INTO notification_items VALUES (12, 'Balance Update', 2);
 -- Notification Sub Items
 INSERT INTO notification_sub_items VALUES (101, 11);
 INSERT INTO notification_sub_items VALUES (102, 12);
 -- Notification Channels
 INSERT INTO notification_channels VALUES (201, 'Email');
 INSERT INTO notification_channels VALUES (202, 'SMS');
 -- Notification Item Channels
 INSERT INTO notification_item_channels VALUES (301, 101, 201);
 INSERT INTO notification_item_channels VALUES (302, 102, 202);
 -- Subitem Schedules
 INSERT INTO subitem_schedules VALUES (101, 401);
 INSERT INTO subitem_schedules VALUES (102, 402);
 -- Notification Schedules
 INSERT INTO notification_schedules VALUES (401, 'Daily');
 INSERT INTO notification_schedules VALUES (402, 'Weekly');
 -- Customer Notification Preferences
 INSERT INTO cust_notif_prefs VALUES (1, 1, 11, 401);
</code></pre>
</li>
</ol>
<p>I want to find customers who do not have a notification preference set for a specific item and schedule. The customer_notification_preferences table links customers, notification items, and schedules. I need to check for nulls across all three relationships.</p>
<p>I know I can use ANSI-style joins, but the current codebase uses legacy join standards. ANSI is planned for later, but right now we need a quick solution that's safe for legacy code. We need a fix that works without refactoring everything.</p>
<p>How can I rewrite this query using old Oracle join syntax to avoid the ORA-01417 error? Is there a workaround that still allows me to check for missing preferences across multiple columns?</p>
",0,0,0,2025-08-11T08:02:28+00:00,2,131,True
79732187,14375437,,sql,How to dynamically query from multiple tables?,"<p>Let's suppose the following:</p>
<ul>
<li>Schema 1 has multiple tables: table1_a, table2, table3, etc</li>
<li>Schema 2 has one table that contains the name of schema1 tables, so: table1_b with column1 that contains the values table1_a, table2, table3, etc..</li>
</ul>
<p>The base query is as follows:</p>
<pre><code>SELECT *
FROM &quot;schema1&quot;.&quot;table1&quot;
</code></pre>
<p>But instead of having &quot;table1&quot;, I want to have multiple tables from that schema <strong>without specifying their name</strong>, so basically I want to get the table names from schema2 and access the columns of the tables with that name in schema1 without hardcoding them.</p>
<p>In a more &quot;python-esque code&quot; way, it would be something like:</p>
<pre><code>x = [table_name] in schema2.table1_b.column1

SELECT max(column2)
FROM &quot;schema1&quot;.x
</code></pre>
<p>I want to avoid using PL/SQL if it's possible.</p>
",3,4,1,2025-08-11T14:59:02+00:00,3,155,True
79732369,2236793,,sql,Allow the answer of multiple question rows and multiple answers without null unless the question isn&#39;t answered,"<p>I have a table that holds questions and answers for each person. The question can be answered multiple times and can be null. If the question is answered I want to not show the null, but if they didn't answer the question I want to show the null.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Question</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>What did you eat today</td>
<td>eggs</td>
</tr>
<tr>
<td>What did you eat today</td>
<td>tost</td>
</tr>
<tr>
<td>What did you eat today</td>
<td>null</td>
</tr>
<tr>
<td>What did you eat yesterday</td>
<td>null</td>
</tr>
</tbody>
</table></div>
<p>In the example above I would the first 2 rows, not show the 3rd row, but show the 4th row.</p>
<p>Obviously</p>
<pre><code>select * 
from table 
where answer is not null
</code></pre>
<p>won't work because then it doesn't show the last row.</p>
<p>I was thinking something like this, I know it won't work but something similar is what I was thinking:</p>
<pre><code>select *
from table
where question in (select question 
                   from table 
                   where answer is not null and answer is null)
</code></pre>
",5,5,0,2025-08-11T18:25:32+00:00,3,151,True
79732436,7893406,,sql,Databricks SQL Query,"<p>I have a table like below in databricks. Each company can have more than 1 meas with two possible Volume (Exist,New) and quarterly amount. I would like to aggregate this data as below expected tabular format.</p>
<h4>Input Table:</h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Company</th>
<th>Meas</th>
<th>Volume</th>
<th>Q1</th>
<th>Q3</th>
<th>Q4</th>
<th>Q5</th>
<th>Q6</th>
<th>Q7</th>
<th>Q8</th>
<th>Q9</th>
<th>Q10</th>
<th>Q11</th>
<th>Q12</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>Interest</td>
<td>Existing</td>
<td>100</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>100</td>
<td>200</td>
</tr>
<tr>
<td>C1</td>
<td>Interest</td>
<td>New</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
</tr>
<tr>
<td>C1</td>
<td>Principal</td>
<td>Existing</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>100</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
</tr>
<tr>
<td>C1</td>
<td>Principal</td>
<td>New</td>
<td>0</td>
<td>1</td>
<td>100</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>C2</td>
<td>Interest</td>
<td>Existing</td>
<td>100</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>100</td>
<td>200</td>
</tr>
<tr>
<td>C2</td>
<td>Interest</td>
<td>New</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>10</td>
<td>5</td>
<td>80</td>
<td>500</td>
<td>200</td>
<td>10</td>
<td>5</td>
<td>80</td>
</tr>
</tbody>
</table></div>
<h4>Desired Output:</h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Company</th>
<th>1YR_<strong>Interest</strong>_Existing</th>
<th>2YR_<strong>Interest</strong>_Existing</th>
<th>3YR_<strong>Interest</strong>_Existing</th>
<th>1YR_<strong>Interest</strong>_New</th>
<th>2YR_<strong>Interest</strong>_New</th>
<th>3YR_<strong>Interest</strong>_New</th>
<th>1YR_<strong>Principal</strong>_Existing</th>
<th>2YR_<strong>Principal</strong>_Existing</th>
<th>3YR_<strong>Principal</strong>_Existing</th>
<th>1YR_<strong>Principal</strong>_New</th>
<th>2YR_<strong>Principal</strong>_New</th>
<th>3YR_<strong>Principal</strong>_New</th>
<th>1YR_<strong>Amort</strong>_Existing</th>
<th>2YR_<strong>Amort</strong>_Existing</th>
<th>3YR_<strong>Amort</strong>_Existing</th>
<th>1YR_<strong>Amort</strong>_New</th>
<th>2YR_<strong>Amort</strong>_New</th>
<th>3YR_<strong>Amort</strong>_New</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>315</td>
<td>590</td>
<td>880</td>
<td>295</td>
<td>...</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>C2</td>
<td>315</td>
<td>...</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>1YR_* represents sum of Q1+Q2+Q3+Q4, 2YR represents sum of Q5+Q6+Q7+Q8 and 3YR represents sum of Q9+Q10+Q11+Q12.  In the final results, there has to be only 1 record for each company.</p>
<p>I got the query below so far - for each meas and company, I got 2 records. now I would like to move meas and volume as part of each agg colume like 1YR__... group by Company;</p>
<pre class=""lang-sql prettyprint-override""><code>select company, meas,volume, sum(Q1+Q2+Q3+Q4) as 1YR, sum(Q5+Q6+Q7+Q8) as 2YR, sum(Q9+Q10+Q11+Q12) as 3YR 
from table
</code></pre>
<p>How can I achieve the desired results?</p>
",-3,0,3,2025-08-11T19:34:07+00:00,1,127,True
79732490,30455159,,sql,How to insert large amounts of data from CSV into Valentina Studio?,"<p>I want to import (relatively) large amounts of data from CSV files into a PostgreSQL database but ran into some problems.</p>
<p>I have learned the basics of SQL in PostgreSQL - if I create a table (I do it in PowerShell after a SQL bootcamp), the foreign key gets a &quot;UUID REFERENCES&quot; part. I wanted to import large amounts of data (350k records) from CSV files in Valentina Studio 15 into this DB but the (integer) foreign key column threw the error:</p>
<blockquote>
<p>invalid input syntax for type uuid &quot;92&quot;</p>
</blockquote>
<p>where the first foreign key value is 92.</p>
<p>Why is that and how do I insert/import my pre-existing data? If I do this with a simple integer, how will the link come into existence? (I did this in Valentina Studio 15 before but had to create the link manually).
I also have no idea how to import this amount of data (CSVs) from PowerShell.</p>
",0,0,0,2025-08-11T20:45:08+00:00,0,78,False
79733314,18874732,,sql,Read data from a table in a PL/SQL procedure and insert into another table while one hot encoding,"<p>I am trying to read data from a table in PL/SQL through a cursor where I am trying insert the data back to another table but one hot encoded.</p>
<p>So the data is like this:</p>
<p>ORIG_TABLE</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Orange</td>
</tr>
<tr>
<td>2</td>
<td>Apple</td>
</tr>
<tr>
<td>3</td>
<td>Mango</td>
</tr>
<tr>
<td>3</td>
<td>Orange</td>
</tr>
<tr>
<td>4</td>
<td>Apple</td>
</tr>
</tbody>
</table></div>
<p>And what I am trying to achieve is the following table</p>
<p>ONE_HOT</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Orange</th>
<th>Apple</th>
<th>Mango</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>I already have the new table created with the necessary columns, because the range of the values (in this case {Orange, Apple, Mango}) is already known.</p>
<pre><code>CREATE TABLE ONE_HOT
(   
ID        VARCHAR(11 BYTE),
ORANGE    NUMBER,
APPLE     NUMBER,
MANGO     NUMBER
)
</code></pre>
<p>I tried reading the cursor and tried inserting through case expression. Code is below, but is not working.
I was expecting the code to read a single line at a time and based on the fruit that the current line hold corresponding to the id the current line hold, and then insert 1 or 0 in the appropriate column. And expected the code to keep doing so until there are values in the table.</p>
<pre><code>CURSOR FRUIT_CUR IS SELECT * FROM ORIG_TABLE;
FRUIT_REC FRUIT_CUR%ROWTYPE

OPEN FRUIT_CUR

LOOP

FETCH FRUIT_CUR INTO FRUIT_REC;
EXIT WHEN FRUIT_CUR%NOTFOUND;

INSERT INTO ONE_HOT
VALUES(
FRUIT_REC.ID,
CASE WHEN FRUIT_REC.FRUIT = 'Orange' THEN 1 ELSE 0 END AS ORANGE,
CASE WHEN FRUIT_REC.FRUIT = 'Apple' THEN 1 ELSE 0 END AS APPLE,
CASE WHEN FRUIT_REC.FRUIT = 'Mango' THEN 1 ELSE 0 END AS MANGO

);

  
END LOOP;

CLOSE FRUIT_CUR;
</code></pre>
",1,1,0,2025-08-12T14:19:05+00:00,2,148,True
79733654,22701356,,sql,How to make the WHERE clause in a SQL UNION ALL query dynamic?,"<p>Is there a way make the <code>WHERE</code> clause in an <code>UNION ALL</code> query dynamic?</p>
<pre><code>SELECT 
    *
FROM
    (SELECT StoreName, GroceryItem FROM STORE) s
LEFT OUTER JOIN 
    (SELECT Name, Item, StoreName 
     FROM CART) c ON s.StoreName = c.StoreName        
                  AND s.GroceryItem = c.GroceryItem
WHERE 
    GroceryItem = 'Apple' 

UNION ALL

SELECT 
    *
FROM
    (SELECT StoreName, GroceryItem FROM STORE) s
LEFT OUTER JOIN 
    (SELECT Name, Item, StoreName 
     FROM CART) c ON s.StoreName = c.StoreName 
                  AND s.GroceryItem = c.GroceryItem
WHERE 
    GroceryItem = 'Grape'
</code></pre>
<p><code>CART</code> table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>StoreName</th>
<th>Item</th>
</tr>
</thead>
<tbody>
<tr>
<td>Greg</td>
<td>Walcart</td>
<td>Apple</td>
</tr>
<tr>
<td>Greg</td>
<td>Walcart</td>
<td>Banana</td>
</tr>
<tr>
<td>Greg</td>
<td>Walcart</td>
<td>Grape</td>
</tr>
<tr>
<td>Greg</td>
<td>Walcart</td>
<td>Cheese</td>
</tr>
<tr>
<td>Peter</td>
<td>Walcart</td>
<td>Cheese</td>
</tr>
<tr>
<td>John</td>
<td>Walcart</td>
<td>Apple</td>
</tr>
</tbody>
</table></div>
<p><code>Store</code> table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>StoreName</th>
<th>GroceryItem</th>
</tr>
</thead>
<tbody>
<tr>
<td>Walcart</td>
<td>Apple</td>
</tr>
<tr>
<td>Walcart</td>
<td>Banana</td>
</tr>
<tr>
<td>Walcart</td>
<td>Grape</td>
</tr>
<tr>
<td>Walcart</td>
<td>Cheese</td>
</tr>
</tbody>
</table></div>
<p>Desired results:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>StoreName</th>
<th>StoreItem</th>
<th>Name</th>
<th>StoreName</th>
<th>Item</th>
</tr>
</thead>
<tbody>
<tr>
<td>Walcart</td>
<td>Apple</td>
<td>Greg</td>
<td>Walcart</td>
<td>Apple</td>
</tr>
<tr>
<td>Walcart</td>
<td>Banana</td>
<td>Greg</td>
<td>Walcart</td>
<td>Banana</td>
</tr>
<tr>
<td>Walcart</td>
<td>Grape</td>
<td>Greg</td>
<td>Walcart</td>
<td>Grape</td>
</tr>
<tr>
<td>Walcart</td>
<td>Cheese</td>
<td>Greg</td>
<td>Walcart</td>
<td>Cheese</td>
</tr>
<tr>
<td>Walcart</td>
<td>Apple</td>
<td>Peter</td>
<td>Walcart</td>
<td></td>
</tr>
<tr>
<td>Walcart</td>
<td>Banana</td>
<td>Peter</td>
<td>Walcart</td>
<td></td>
</tr>
<tr>
<td>Walcart</td>
<td>Grape</td>
<td>Peter</td>
<td>Walcart</td>
<td></td>
</tr>
<tr>
<td>Walcart</td>
<td>Cheese</td>
<td>Peter</td>
<td>Walcart</td>
<td>Cheese</td>
</tr>
<tr>
<td>Walcart</td>
<td>Apple</td>
<td>John</td>
<td>Walcart</td>
<td>Apple</td>
</tr>
<tr>
<td>Walcart</td>
<td>Banana</td>
<td>John</td>
<td>Walcart</td>
<td></td>
</tr>
<tr>
<td>Walcart</td>
<td>Grape</td>
<td>John</td>
<td>Walcart</td>
<td></td>
</tr>
<tr>
<td>Walcart</td>
<td>Cheese</td>
<td>John</td>
<td>Walcart</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>The reason I can't do this with a <code>JOIN</code> or just include Null values, is because of the <code>Cart</code> table. If the item is in their cart, it shows in the <code>Cart</code> table. If it's not in their cart, it's completely missing from the list. I need something that shows each person &amp; what they have in their cart &amp; what's missing compared to the inventory in the store table.</p>
<p>However, since store inventory can change, I don't want to hardcode the item names.</p>
",1,1,0,2025-08-12T20:48:38+00:00,4,130,True
79733732,1927638,,sql,Spring JPA insertion order for a List,"<p>I seem to get mixed answers with this, so I just wanted to clarify the behavior of JpaRepository. Given a JPA entity such as:</p>
<pre><code>public class ItemEntity {
  @Id
  @SequenceGenerator(name = &quot;i_seq&quot;, sequenceName = &quot;i_seq, allocationSize = 1)
  @@GeneratedValue(stragegy = GenerationType.SEQUENCE, generator = &quot;i_seq&quot;)
  @Column(name = &quot;id&quot;, columnDefinition = &quot;bigint&quot;)
  private Long id;

  ...
}
</code></pre>
<p>The id is generated by a sequence on the database side. My goal is to insert the items in the order of the List.</p>
<pre><code>public void saveAll(List&lt;ItemEntity&gt; items) {
  ...
}
</code></pre>
<p>The problem is, I don't believe that JpaRepository.saveAll actually enforces the insertion order. My question is, can I &quot;force&quot; JPA to insert the items in the order of the List by flushing every ItemEntity after the call to save?</p>
<pre><code>public void saveAll(List&lt;ItemEntity&gt; items) {
  for (ItemEntity item : items) {
    jpaRepository.save(item);
    jpaRepository.flush();
  }
}
</code></pre>
<p>Or is the flush not required?</p>
",3,3,0,2025-08-12T23:51:49+00:00,1,158,True
79734092,1307987,"Amsterdam, Netherlands",sql,Android Room 2.7.2 - Key 1855838 is missing in the map,"<p>I get this error in Room:</p>
<pre><code>Fatal Exception: java.util.NoSuchElementException
Key 1855838 is missing in the map.
kotlin.collections.MapsKt__MapWithDefaultKt.getOrImplicitDefaultNullable (MapWithDefault.kt:24)

kotlin.collections.MapsKt__MapsKt.getValue (Maps.kt:370)
android.persistence.phonelines.ConnectedUserDao_Impl.getFullConnectedUsersFlowByOrderNr$lambda$6 (ConnectedUserDao_Impl.kt:342)
android.persistence.phonelines.ConnectedUserDao_Impl.$r8$lambda$43VE92PoOM1yQT3OPmuzeuKB5FM
android.persistence.phonelines.ConnectedUserDao_Impl$$ExternalSyntheticLambda2.invoke (D8$$SyntheticClass)
androidx.room.util.DBUtil__DBUtil_androidKt$performSuspending$lambda$1$$inlined$internalPerform$1.invokeSuspend (DBUtil.kt:68)
</code></pre>
<p>This is my Full object:</p>
<pre><code>data class FullConnectedUser(
    @Embedded
    val user: ConnectedUser,

    @Relation(
        parentColumn = &quot;phoneLineOid&quot;,
        entityColumn = &quot;oid&quot;
    )
    private val _phoneLines: List&lt;PhoneLine&gt;,

    @Relation(
        parentColumn = &quot;associatedAddressableOid&quot;,
        entityColumn = &quot;oid&quot;
    )
    private val _addressables: List&lt;Addressable&gt;
    )
</code></pre>
<p>My ConnectedUser:</p>
<pre><code>@Entity(tableName = &quot;connected_users&quot;)
data class ConnectedUser(
@PrimaryKey
val oid: String,

val phoneLineOid: String,
val associatedAddressableOid: String,

val isIncoming: Boolean,
val isOutgoing: Boolean,

val editable: String,
val orderNumber: Int
)
</code></pre>
<p>My phoneline:</p>
<pre><code>@Entity(tableName = &quot;phone_lines&quot;)
data class PhoneLine(
// Phone line OID
@PrimaryKey
val oid: String,

val associatedAddressableOid: String,
val userPhoneLineOid: String,

val commonName: String,
val externalNumber: String,
val callerName: String,
val unreadRecentContactsCount: Int,

val fetchedUntil: String 
)
</code></pre>
<p>It crashes at the last line here:</p>
<pre><code>      _tmpUser = ConnectedUser(_tmpOid,_tmpPhoneLineOid,_tmpAssociatedAddressableOid,_tmpIsIncoming,_tmpIsOutgoing,_tmpEditable,_tmpOrderNumber)
      val _tmpPhoneLinesCollection: MutableList&lt;PhoneLine&gt;
      val _tmpKey_2: String
      _tmpKey_2 = _stmt.getText(_columnIndexOfPhoneLineOid)
      _tmpPhoneLinesCollection = _collectionPhoneLines.getValue(_tmpKey_2)
</code></pre>
<p>I assume because they are fetched separately:</p>
<pre><code>   viewModel.viewModelScope.launch(coroutineExceptionHandler) {
        kotlin.runCatching { viewModel.fetchPhoneLines() }
        kotlin.runCatching { viewModel.fetchConnectedUsers() }
    }
</code></pre>
<p>How can I avoid this?
Note I can replicate easier if I open fragment and press back fast, I assume because in my compose I have this:</p>
<pre><code>val connectedUsersListState: List&lt;FullConnectedUser&gt; by viewModel.getFullConnectedUsersFlowByOrderNr().collectAsState(initial = emptyList())
</code></pre>
<p>Which does:</p>
<pre><code>@Query(&quot;SELECT * FROM connected_users ORDER BY orderNumber&quot;)
fun getFullConnectedUsersFlowByOrderNr(): Flow&lt;List&lt;FullConnectedUser&gt;&gt;
</code></pre>
",1,2,1,2025-08-13T09:07:26+00:00,0,139,False
79734105,25312006,,sql,Mariadb query to find a field change working backwards in a table,"<p>I am trying to write some sql code to embed in Qt to find out when it last rained.
Table</p>
<pre><code>CREATE TABLE `weather_data` (
  `time` bigint(20) NOT NULL,
  `temp_1` float DEFAULT NULL,
  `humid_1` float DEFAULT NULL,
  `wind_avg` float DEFAULT NULL,
  `wind_max` float DEFAULT NULL,
  `wind_dir` smallint(6) DEFAULT NULL,
  `rain_raw` float DEFAULT NULL,
  `pressure` float DEFAULT NULL,
  `temp_2` float DEFAULT NULL,
  `humid_2` float DEFAULT NULL,
  `temp_3` float DEFAULT NULL,
  `humid_3` float DEFAULT NULL,
  `temp_4` float DEFAULT NULL,
  `humid_4` float DEFAULT NULL,
  `temp_5` float DEFAULT NULL,
  PRIMARY KEY (`time`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1
</code></pre>
<p>I have some code that finds the last entry in my table that returns a time stamp and the current rain total:</p>
<pre><code>MariaDB [weatherV2_db]&gt; SELECT time,rain_raw FROM weather_data ORDER BY time DESC LIMIT 1;
+------------+----------+
| time       | rain_raw |
+------------+----------+
| 1755075032 |  23.6529 |
+------------+----------+
1 row in set (0.000 sec)
</code></pre>
<p>I am now looking for an efficient way to search backwards in the table to find when the &quot;rain_raw&quot; value changes and return the associated &quot;time&quot; value. My current method uses a simple &quot;top down(?)&quot; search which is eating Mb's of bandwith to my remote mariadb server. This method also shows up as &quot;the&quot; major use of cpu cycles when I profile my application. The database is ordered by the &quot;time&quot; field and the &quot;time&quot; field values are unique.</p>
<p>Any help would be appreciated.</p>
<p>Typical table</p>
<pre><code>...
| 1754642997 |  23.6409 |
| 1754643045 |  23.6409 |
| 1754643093 |  23.6409 |
| 1754643141 |  23.6409 |
| 1754643189 |  23.6409 |
| 1754643237 |  23.6409 |
| 1754643285 |  23.6409 |
| 1754643333 |  23.6409 |
| 1754643381 |  23.6409 |
| 1754643429 |  23.6409 |
| 1754643477 |  23.6529 | &lt;------ I want this time stamp
| 1754643525 |  23.6529 |
| 1754643573 |  23.6529 |
| 1754643621 |  23.6529 |
| 1754643669 |  23.6529 |
...
</code></pre>
<p>My current solution is to run the following query with an incrementing value for i until the value of rain_raw changes from the current last value in the table :</p>
<pre><code>QString query=QString().asprintf(&quot;SELECT time,rain_raw FROM weather_data ORDER BY time DESC LIMIT 1 OFFSET %d&quot;,i);
</code></pre>
<p>The remote mariadb server is:</p>
<blockquote>
<p>mysql  Ver 15.1 Distrib 10.6.7-MariaDB, for Linux (x86_64) using readline 5.1</p>
</blockquote>
<p>The time stamps are seconds from the epoch.</p>
<p>Using the suggested solution:</p>
<pre><code>SELECT time, rain_raw FROM   (SELECT time, rain_raw, LAG(rain_raw) OVER (ORDER BY time) AS lag_rain_raw FROM weather_data) t WHERE  rain_raw &lt;&gt; lag_rain_raw;

</code></pre>
<p>Gives me :</p>
<pre><code>...

| 1754581175 |  23.5469 |
| 1754581319 |  23.5469 |
| 1754581367 |  23.5589 |
| 1754581463 |  23.5589 |
| 1754582039 |  23.5699 |
| 1754582663 |  23.5939 |
| 1754582759 |  23.6059 |
| 1754582855 |  23.6179 |
| 1754582951 |  23.6289 |
| 1754583383 |  23.6409 |
| 1754643477 |  23.6529 | &lt;---- Required answer.
| 1754643525 |  23.6529 |
+------------+----------+
4094 rows in set (1.240 sec)
</code></pre>
<p>The last entry is the current last row in the table.
The row above that is the value I am looking for.
The rest appear to be all the other rain transition markers.
Thanks but I expected to just get the last rain transition not all of them.</p>
<p>On a second look the last entry from the result is not the end of the table could it be a floating point error ?</p>
<p>The solution supplied answers my question. Thank you.</p>
<pre><code>SELECT time, rain_raw FROM   (SELECT time, rain_raw, LAG(rain_raw) OVER (ORDER BY time) AS lag_rain_raw         FROM   weather_data) t WHERE  ROUND(rain_raw,3) &lt;&gt; ROUND(lag_rain_raw,3) ORDER BY time DESC LIMIT 1;
</code></pre>
",1,1,0,2025-08-13T09:21:32+00:00,2,162,True
79734587,11474988,,sql,Ranking dataset based upon two conditions,"<p>In SQL Server the following query needs to rank records with the same ID by the latest 'date completed' output column (derived from a coalesce from date columns from two tables specified in the join).</p>
<p>Also output needs to rank populated records over empty records for 'response' column.</p>
<p>Adding below the current and expected results if someone could advise how to re-work the query</p>
<pre><code>WITH RankedDates AS (
    SELECT
       DE.[ID],
       DE.[response],
        MAX(COALESCE(TRY_CONVERT(DATE, DE.[closed_on], 103), 
                     TRY_CONVERT(DATE, OE.[closed_at], 103))) AS [Date Completed],
        ROW_NUMBER() OVER (PARTITION BY DE.[ID] ORDER BY MAX(COALESCE(TRY_CONVERT(DATE,    DE.[closed_on], 103), TRY_CONVERT(DATE, OE.[closed_at], 103))) DESC, CASE WHEN DE.[response] IS NULL THEN 1 ELSE 0 END DESC) AS Rank
    FROM
        [Live].[Import_D_2025-08-13] DE
    LEFT OUTER JOIN
        [Live].[Import_O_2025-08-13] OE ON DE.[ID] = OE.[ID]
    WHERE
        DE.[ID] IN ('100')
    GROUP BY
        DE.[ID], DE.[response]
)
SELECT *
FROM RankedDates
ORDER BY [Date Completed] ASC, Rank;
</code></pre>
<p>current output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Response</th>
<th>Date Completed</th>
<th>Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td></td>
<td>08/08/2025</td>
<td>1</td>
</tr>
<tr>
<td>100</td>
<td>Effective</td>
<td>08/08/2025</td>
<td>2</td>
</tr>
<tr>
<td>100</td>
<td>design</td>
<td>24/07/2025</td>
<td>3</td>
</tr>
</tbody>
</table></div>
<p>expected output</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Response</th>
<th>Date Completed</th>
<th>Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td></td>
<td>08/08/2025</td>
<td>2</td>
</tr>
<tr>
<td>100</td>
<td>Effective</td>
<td>08/08/2025</td>
<td>1</td>
</tr>
<tr>
<td>100</td>
<td>design</td>
<td>24/07/2025</td>
<td>3</td>
</tr>
</tbody>
</table></div>
",1,1,0,2025-08-13T17:37:37+00:00,1,124,True
79735286,16599493,,sql,How can I count the occurrences of a value in between two specific values?,"<p>I have a table in SQL that shows different rule firings with timestamp ordered from oldest to newest - like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Timestamp</th>
<th>Customer ID</th>
<th>Rule Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-21 11:21:20</td>
<td>customer 1</td>
<td>quick_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:22:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:23:20</td>
<td>customer 1</td>
<td>slow_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:24:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:25:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:25:40</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:26:20</td>
<td>customer 2</td>
<td>quick_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:27:20</td>
<td>customer 3</td>
<td>quick_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:28:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:29:20</td>
<td>customer 1</td>
<td>slow_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:30:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:30:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:31:20</td>
<td>customer 3</td>
<td>decline_rule</td>
</tr>
</tbody>
</table></div>
<p>I'd like to group by date and <code>customer id</code> to create the following table that counts the number of times 'decline_rule' occurs after a rule containing '_hit' occurs but before the next '_hit' rule occurs for that customer. This is to know how many 'decline_rule' is caused from the most recent hit rule.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Hit Rule Name</th>
<th style=""text-align: right;"">Hit Rule Count</th>
<th style=""text-align: right;"">Count of Transactions on Decline Rule</th>
<th>Count of Customers on Decline Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td>1 (customer 1)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>slow_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">4</td>
<td>1 (customer 1 has 4 decline_rule after this slow_hit_rule but before the next slow_hit_rule)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
<td>0 (customer 2 didn't have any decline_rule after the hit so it's zero)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td>1 (customer 3)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>slow_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td>1 (customer 1)</td>
</tr>
</tbody>
</table></div>
<p>Then the final table is a group by the date and rulename:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Hit Rule Name</th>
<th>Hit Rule Count</th>
<th>Count of Transactions on Decline Rule</th>
<th>Count of Customers on Decline Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>slow_hit_rule</td>
<td>2</td>
<td>6</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>I've tried with <code>LAG</code>/<code>LEAD</code> functions, but I need help on how to approach this logic.</p>
<p>For the LEAD attempt:</p>
<pre><code>with hitrule as (
  select timestamp, rulename, customerid 
  from table_a 
  where rulename != &quot;decline_rule&quot;
) 
select distinct a.timestamp, 
  t.rulename, 
  a.customerid, 
  lead(t.timestamp) over (partition by t.customerid order by t.timestamp) as next_timestamp, 
  row_number() over (partition by t.customerid order by t.timestamp) as rn 
from hitrule a 
left join table_a on date(a.timestamp) = date(b.timestamp) 
  and a.customerid = b.customerid 
where t.rulename LIKE '%_hit%'
</code></pre>
<p>This was to get the next hit rule timestamp</p>
",1,1,0,2025-08-14T10:43:32+00:00,2,154,True
79735462,14962228,,sql,How to handle Grafana &quot;All&quot; option with QuestDB without expanding all variable values?,"<p>I'm using Grafana with QuestDB and have a multi-value variable element with &quot;All&quot; option enabled. When &quot;All&quot; is selected, Grafana expands all values in the WHERE clause:</p>
<pre class=""lang-sql prettyprint-override""><code>-- Current behavior when &quot;All&quot; is selected:
AND element IN ('value1', 'value2', 'value3', ... 'value1000')  -- Performance issue
</code></pre>
<p>This causes very long queries and performance problems with large datasets.</p>
<p>I want to skip the filter entirely or use a more efficient approach when &quot;All&quot; is selected, instead of listing every value. I've tried: Complex conditional logic checking for magic values, but it feels hacky:</p>
<pre class=""lang-sql prettyprint-override""><code>AND (
    ${element} = '_ALL_GRAFANA_MAGIC_'
    AND element IS NOT NULL AND element &lt;&gt; '' OR
    (
        ${element:sqlstring} != '_ALL_GRAFANA_MAGIC_' 
        AND element IN ('${element}') 
    )
)

</code></pre>
<p>What's the best practice to handle Grafana's &quot;All&quot; option with QuestDB queries efficiently?</p>
",1,1,0,2025-08-14T13:20:56+00:00,1,72,True
79736051,19875313,,sql,How to run a single boolean check once per query execution in a view (not per row)?,"<p>I have a large SQL Server view where I need to perform a boolean check only once per query execution, not once per row.</p>
<p>In other words: when a user queries the view, I want to call a function or check a condition a single time before returning rows.</p>
<ul>
<li>If the check returns 1 → return all rows</li>
<li>If the check returns 0 → return no rows</li>
</ul>
<p>The challenge is that SQL Server sometimes evaluates the condition per row, which kills performance.</p>
<p>The function is independent of logic inside the views itself.</p>
<p>I've tried a couple of things:</p>
<p><strong>Option 1</strong>: scalar function in <code>WHERE</code> clause</p>
<pre><code>create view vw_data 
as
    select col_a, col_b
    from big_table
    where dbo.fn_CheckPermission(convert(uniqueidentifier, session_context(N'UserId')), 42) = 1;
</code></pre>
<p>This sometimes behaves as desired (function is evaluated once), but for some views it seems to be executed for every row, causing slow performance.</p>
<p><strong>Option 2</strong>: join with table-valued function (TVF)</p>
<p>Convert the scalar function to a TVF:</p>
<pre><code>create view vw_data 
as
    select t.col_a, t.col_b
    from big_table t
    join dbo.fn_CheckPermissionTVF(42) p on p.is_allowed = 1;
</code></pre>
<p><strong>Option 3</strong>: <code>EXISTS</code> subquery</p>
<pre><code>create view vw_data 
as
    select col_a, col_b
    from big_table
    where exists (select 1 from dbo.fn_CheckPermissionTVF(42));
</code></pre>
<p>Which one to choose so that there is only a boolean check once per query execution and not for all rows within it?</p>
<p>And are there other SQL Server patterns to force “evaluate-once” behavior for a function call in a view?</p>
",3,5,2,2025-08-15T03:15:46+00:00,2,203,True
79736082,1478146,,sql,Find the Active Item for a Given Date With Time Intervals and Item Criteria,"<p>I have a problem that's similar to this one: <a href=""https://stackoverflow.com/questions/75289621/merge-overlapping-time-intervals-based-on-priority-and-type-sql-server"">Merge overlapping time intervals based on priority and type - SQL Server</a></p>
<p>But it's not quite the same.</p>
<p>I have items that have a type, a weight and an active/inactive status. I have a table that lists when a specifinc instance of a type becomes active or inactive.
And I can have multiple items for a given type, each with a different weight.</p>
<p>When given a type and a date, I need to determine the item with the highest weight for that item's type at that given date.</p>
<p>My data looks something like:</p>
<p>Items:</p>
<pre><code>
Id | Name | Type | Weight 
-------------------------
1    I1      T1       2
2    I2      T1       1
3    I3      T1       3
4    I4      T2       3
5    I5      T3       3
6    I6      T3       2
</code></pre>
<p>We can assume that weight is unique across all items of a given type: There won't be two items of type T1 with weight = 3, for example</p>
<p>Status History:</p>
<pre><code>ItemId | StatusDate | IsActive
------------------------------
1         1/1/2025       1
2         1/15/2025      1
3         2/1/2025       1
4         1/1/2025       1
4         2/1/2025       0
5         1/1/2025       1
6         1/15/2025      1
5         2/1/2025       0
</code></pre>
<p>The result should be something like:</p>
<pre><code>Type | Date    | Matching Item Id
---------------------------------
T1      1/2/2025     1
T1      1/20/2025    1  
T1      2/15/2025    3
T2      1/2/2025     4
T2      2/15/2025    NULL (since the only item of this type is inactive at this date)
T3      1/2/2025     5
T3      2/15/2025    6
</code></pre>
<p>I tried adapting what I saw in the above post, but it doesn't deal with the deactivation.
It seems to me like I want a to create a CTE that has effective Item Ids for a given type and date, something like:</p>
<pre><code>Type | FromDate | ToDate   | ItemId  (NULL ToDate means still active)
-----------------------------------
T1     1/1/2025   2/1/2025    1
T1     2/1/2025    NULL       3
T2     1/1/2025   2/1/2025    4
T3     1/1/2025   2/1/2025    5
T3     2/1/2025    NULL       6
</code></pre>
<p>I can construct the status intervals for each Item, but I'm having problems merging these for a given type:</p>
<pre><code>WITH OrderedStatusHistory AS
(
    SElECT SH.Id,
        SH.ItemId,
        I.Name,
        I.Type,
        I.Weight,
        SH.StatusDate,
        SH.IsActive,
        ROW_NUMBER() OVER (PARTITION BY I.Type
                           ORDER BY SH.StatusDate, Weight) RowNum
    FROM StatusHistory SH
    INNER JOIN Items I
        ON SH.ItemId = I.Id
),
StatusHistoryIntervals AS 
(
    SELECT SH1.ItemId,
           SH1.Name,
           SH1.Type,
           SH1.Weight,
           SH1.IsActive,
           SH1.StatusDate FromDate,
           SH2.StatusDate ToDate,
           SH1.RowNum
    FROM OrderedStatusHistory SH1
    LEFT JOIN OrderedStatusHistory SH2
        ON SH1.ItemId = SH2.ItemId
        AND SH1.RowNum = SH2.RowNum-1
)
</code></pre>
<p>But I can't get any farther.  It seems like it's a gap-and-island problem, but I just can't work it out.<br />
I'm using this information in a JOIN with another table, not sure if it's relevant enough to include that here, except I'll mention that my first attempt involved a subquery in the JOIN to get the item's weight - something like:</p>
<pre><code>SELECT *
FROM MyTable T
INNER JOIN Items I
  ON T.Type = I.Type 
  AND T.Weight = 
    (SELECT MAX(Weight) 
        FROM Item I2
        INNER JOIN StatusHistoryIntervals SH
            ON I2.Id = SH.ItemId
        WHERE I2.Type = SH.Type
            AND SH.IsActive = 1
            AND ((SH.RowNum = 1 AND T.SomeDate &lt; SH.FromDate) -- Assume that the first status is effective for all events prior to that status date
                OR (T.SomeDate BETWEEN SH.FromDate AND SH.ToDate) -- The event falls into the status interval
                OR (T.SomeDate &gt; SH.FromDate AND SH.ToDate IS NULL)))) 
</code></pre>
<p>This doesn't work when there are multiple items of the same type with different weights with different, overlapping active status intervals, though - the highest is always returned, even though it's active after SomeDate.</p>
<p>Here's a fiddle with the table creation and population, and the CTEs for the Status History Intervals:</p>
<p><a href=""https://sqlfiddle.com/sql-server/online-compiler?id=83ea1514-23ca-4ede-8eaf-8ce6360e8390"" rel=""nofollow noreferrer"">https://sqlfiddle.com/sql-server/online-compiler?id=83ea1514-23ca-4ede-8eaf-8ce6360e8390</a></p>
",5,5,0,2025-08-15T04:17:45+00:00,1,140,True
79736356,19315674,,sql,Snowflake Scripting. Using one declared NUMBER variable within the WHERE clause of an second declared NUMBER variable as SELECT statement,"<p>I'm getting the following error when trying to construct the following Snowsight scripting block:</p>
<blockquote>
<p>002003 (42S02): Uncaught exception of type 'EXPRESSION_ERROR' on line 7 at position 6 : SQL compilation error:</p>
</blockquote>
<p>What am I doing wrong w.r.t these variables please? I'm coming from T-SQL and I'm having to re-learn this Snowsight scripting!</p>
<p>This is my code:</p>
<pre><code>DECLARE 
    MaxPriorYear NUMBER;
    MaxPriorMonth NUMBER;

BEGIN
      MaxPriorYear :=
      (SELECT MAX(FISCAL_YEAR)
       FROM TBL_MAIN);

      MaxPriorMonth :=
      (SELECT MAX(ACCOUNTING_PERIOD) 
       FROM TBL_MAIN 
       WHERE FISCAL_YEAR = MaxPriorYear);
      
      RETURN MaxPriorMonth;
END;
</code></pre>
",0,0,0,2025-08-15T10:54:16+00:00,1,89,False
79736466,294657,,sql,Why does my query get slower and not faster when I constrain the data set?,"<p>I have a TimescaleDB table that records activities of various devices. This is a huge table and any interactions with it are normally expensive. Devices can be enabled as a group, and activation has a planned start and end time. I want to see which currently enabled group has recently stopped producing activities.</p>
<p>If I write a query like this:</p>
<pre class=""lang-sql prettyprint-override""><code>select *
from group g
where g.state = 'enabled'
    --there was activity within the past 2 days
    and exists(
               select 1
               from activity act
                 join device d on act.device_id = d.id
               where d.group_id = g.id
                 and act.date &gt;= current_timestamp - interval '2 day')
    --there was no activity in the past day
    and not exists(
                   select 1
                   from activity act
                     join device d on act.device_id = d.id
                   where d.group_id = g.id
                     and act.date &gt;= current_timestamp - interval '1 day');

</code></pre>
<p>it executes in 3s.</p>
<p>Yet if I further constrain the groups to only those that are still within their planned activation period, or out of it by less than a week:</p>
<pre class=""lang-sql prettyprint-override""><code>select *
from group g
where g.state = 'enabled'
    --adding this condition slows things down???
    and g.planned_end &gt;= current_timestamp - interval '7 days'
    and exists( 
               select 1
               from activity act
                 join device d on act.device_id = d.id
               where d.group_id = g.id
                 and act.date &gt;= current_timestamp - interval '2 day')
    and not exists( 
                   select 1
                   from activity act
                     join device d on act.device_id = d.id
                   where d.group_id = g.id
                     and act.date &gt;= current_timestamp - interval '1 day');

</code></pre>
<p>the execution time balloons to 8s!</p>
<p>Why would this happen? One would assume further filtering the groups prior to the expensive <code>exists</code> subqueries would speed it up, not slow it down. What gives?</p>
<p>Here are the execution plans for both cases:</p>
<pre><code>Nested Loop Anti Join  (cost=9604961.92..19571124.95 rows=3 width=661)
  Join Filter: (d_1.group_id = g.id)
  -&gt;  Merge Join  (cost=9603623.70..9603624.28 rows=17 width=661)
        Merge Cond: (g.id = d.group_id)
        -&gt;  Sort  (cost=3.32..3.37 rows=20 width=661)
              Sort Key: g.id
              -&gt;  Seq Scan on group r  (cost=0.00..2.89 rows=20 width=661)
                    Filter: (state = 'active'::group_state)
        -&gt;  Sort  (cost=9603620.38..9603620.54 rows=62 width=16)
              Sort Key: d.group_id
              -&gt;  HashAggregate  (cost=9603617.92..9603618.54 rows=62 width=16)
                    Group Key: d.group_id
                    -&gt;  Gather  (cost=1338.22..9570071.15 rows=13418705 width=16)
                          Workers Planned: 2
                          -&gt;  Hash Join  (cost=338.22..8227200.65 rows=5591127 width=16)
                                Hash Cond: (act.device_id = d.id)
                                -&gt;  Parallel Custom Scan (ChunkAppend) on activity act  (cost=0.00..8211590.04 rows=5814701 width=16)
                                      Chunks excluded during startup: 200
                                      -&gt;  Parallel Seq Scan on _hyper_1_263_chunk act_2  (cost=0.00..152549.80 rows=198561 width=16)
                                            Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
                                      -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_3  (cost=0.00..35876.12 rows=782444 width=16)
                                            Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
                                -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                                      -&gt;  Seq Scan on device a  (cost=0.00..246.43 rows=7343 width=32)
  -&gt;  Materialize  (cost=1338.22..9571273.34 rows=12234558 width=16)
        -&gt;  Gather  (cost=1338.22..9450360.55 rows=12234558 width=16)
              Workers Planned: 2
              -&gt;  Hash Join  (cost=338.22..8225904.75 rows=5097732 width=16)
                    Hash Cond: (act_1.device_id = d_1.id)
                    -&gt;  Parallel Custom Scan (ChunkAppend) on activity act_1  (cost=0.00..8211590.04 rows=5321306 width=16)
                          Chunks excluded during startup: 201
                          -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_4  (cost=0.00..35876.12 rows=487285 width=16)
                                Filter: (date &gt;= (CURRENT_TIMESTAMP - '1 day'::interval))
                    -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                          -&gt;  Seq Scan on device d_1  (cost=0.00..246.43 rows=7343 width=32)
</code></pre>
<p>and</p>
<pre><code>Nested Loop Semi Join  (cost=2676.58..19021891.33 rows=1 width=661)
  Join Filter: (g.id = d.group_id)
  -&gt;  Nested Loop Anti Join  (cost=1338.37..9451818.12 rows=1 width=661)
        Join Filter: (d_1.group_id = g.id)
        -&gt;  Index Scan using complex_key on group r  (cost=0.15..8.77 rows=2 width=661)
              Index Cond: (ended_at &gt;= (CURRENT_TIMESTAMP - '7 days'::interval))
              Filter: (state = 'active'::group_state)
        -&gt;  Gather  (cost=1338.22..9450372.27 rows=12234722 width=16)
              Workers Planned: 2
              -&gt;  Hash Join  (cost=338.22..8225900.07 rows=5097801 width=16)
                    Hash Cond: (act_1.device_id = d_1.id)
                    -&gt;  Parallel Custom Scan (ChunkAppend) on activity act_1  (cost=0.00..8211585.18 rows=5321374 width=16)
                          Chunks excluded during startup: 201
                          -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_4  (cost=0.00..35871.27 rows=487353 width=16)
                                Filter: (date &gt;= (CURRENT_TIMESTAMP - '1 day'::interval))
                    -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                          -&gt;  Seq Scan on device d_1  (cost=0.00..246.43 rows=7343 width=32)
  -&gt;  Gather  (cost=1338.22..9570071.66 rows=13418758 width=16)
        Workers Planned: 2
        -&gt;  Hash Join  (cost=338.22..8227195.86 rows=5591149 width=16)
              Hash Cond: (act.device_id = d.id)
              -&gt;  Parallel Custom Scan (ChunkAppend) on activity act  (cost=0.00..8211585.18 rows=5814723 width=16)
                    Chunks excluded during startup: 200
                    -&gt;  Parallel Seq Scan on _hyper_1_263_chunk act_2  (cost=0.00..152549.80 rows=198689 width=16)
                          Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
                    -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_3  (cost=0.00..35871.27 rows=782338 width=16)
                          Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
              -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                    -&gt;  Seq Scan on device a  (cost=0.00..246.43 rows=7343 width=32)
</code></pre>
",3,3,0,2025-08-15T13:21:24+00:00,1,99,True
79736516,7836976,Ireland,sql,Syntax error when trying to create PostgreSQL function,"<p>I'm trying to create this PostgreSQL function:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION sales.fn_drop_old_partitions(
    )
    RETURNS boolean
    LANGUAGE 'plpgsql'
    COST 100
    VOLATILE PARALLEL UNSAFE
AS $BODY$   
DECLARE
    quote_table_partition character varying(255);
BEGIN

    quote_table_partition := select pt.relname as partition_name
    from pg_class base_tb 
    join pg_inherits i on i.inhparent = base_tb.oid 
    join pg_class pt on pt.oid = i.inhrelid
    where base_tb.oid = 'quote_table'::regclass
    ORDER BY pt.relname asc
    LIMIT 1;
    
    RAISE NOTICE 'Value of : %', quote_table_partition;
    
    ALTER TABLE sales.quote_table DETACH PARTITION quote_table_partition;
    DROP TABLE quote_table_partition;
    
    ANALYZE sales.quote_table;
    RETURN true;
END;
$BODY$
</code></pre>
<p>When I run the code I get this error:</p>
<pre class=""lang-none prettyprint-override""><code>ERROR:  syntax error at or near &quot;select&quot;
LINE 13:  quote_table_partition := select pt.relname a...
                                                  ^
SQL state: 42601
Character: 361
</code></pre>
<p>The <code>SELECT</code> works on its own and beyond ensuring semicolons are at the end of each statement, I've no idea what the issue is. Can anyone help?</p>
",-1,0,1,2025-08-15T14:30:43+00:00,1,79,False
79736798,5692227,,sql,How can I store a list of IDs in a variable to re-use multiple times in a SQL worksheet?,"<p>I regularly have to do support at work using various database tables, checking and comparing values between different tables but always from a starting point of a set of ID values, as strings.</p>
<p>I've set up and saved a SQL Worksheet that I use with all of the various queries I run to do this support, but it takes a lot of copy/pasting of these ID values (in different orders) into all of the different queries. So I'm trying to automate and optimize my time a little.</p>
<p>What I'm hoping to achieve is, at the top of this worksheet, to paste in my ID values as a comma-separated list, store it into a variable somehow, and then run individual queries on the same worksheet and have them use the current value of that variable in their <code>WHERE</code> clauses.</p>
<p>For example,</p>
<pre class=""lang-sql prettyprint-override""><code>select 'A1234', 'B5678' into ids from dual;

select id, status from tableA where id in ids;

select id_2, status_2 from tableB where id in ids;
</code></pre>
<p>Since I'm not running this as a script from top to bottom, I don't think this method would directly work but is there something similar or can I be pointed in the right direction?</p>
<p>As a bonus if possible, I'm also hoping to not need to add the quotation marks (<code>'</code>) myself, and rather somehow just use, for example, <code>A1234,B5678</code> and have it do the work for me but this isn't necessary, just saves me a little bit of time.</p>
",0,0,0,2025-08-15T20:04:45+00:00,4,189,True
79737122,31275014,,sql,Counting number of rows up to a specified value by type and id,"<p>I have a dataset with <code>id</code>, <code>test_date</code>, <code>test_type</code>, <code>test_result</code> - like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">ID</th>
<th>Test_Date</th>
<th style=""text-align: center;"">Test_Type</th>
<th>Test_Result</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td>2024-03-21</td>
<td style=""text-align: center;"">A</td>
<td>Fail</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2024-04-21</td>
<td style=""text-align: center;"">A</td>
<td>Fail</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2024-04-30</td>
<td style=""text-align: center;"">A</td>
<td>Pass</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2025-05-15</td>
<td style=""text-align: center;"">B</td>
<td>Fail</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2025-05-31</td>
<td style=""text-align: center;"">B</td>
<td>Pass</td>
</tr>
</tbody>
</table></div>
<p>I need to create a variable counting the number of attempts to pass for each <code>Test_Type</code> by <code>id</code>.</p>
<p>The desired output would be:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">ID</th>
<th>Test_Date</th>
<th style=""text-align: center;"">Test_Type</th>
<th>Test_Result</th>
<th style=""text-align: right;"">Attempts</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td>2024-03-21</td>
<td style=""text-align: center;"">A</td>
<td>Fail</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2024-04-21</td>
<td style=""text-align: center;"">A</td>
<td>Fail</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2024-04-30</td>
<td style=""text-align: center;"">A</td>
<td>Pass</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2025-05-15</td>
<td style=""text-align: center;"">B</td>
<td>Fail</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td>2025-05-31</td>
<td style=""text-align: center;"">B</td>
<td>Pass</td>
<td style=""text-align: right;"">2</td>
</tr>
</tbody>
</table></div>
<p>Could someone help with some SQL code?</p>
",1,2,1,2025-08-16T10:38:01+00:00,3,124,True
79738076,17405488,,sql,dbt Model to Standardized Silver Layer - Aliasing Data,"<p>In the past, if I wanted to push landed data into a standardized layer, I would use an insert statement, like so:</p>
<pre><code>Insert into standardizedtable 
  (claim_num, claim_status, loss_date)
select claimnumber, status, date_of_loss from landedtable;
</code></pre>
<p>This would ensure that the correct raw fields were being mapped into the correct standardized fields.</p>
<p>With dbt, you don't use insert into statements.  Are aliases the only way to push data into the correct field in the standardized table?  Like so:</p>
<pre><code>select
  claimnumber as &quot;claim_num&quot;,
  status as &quot;claim_status&quot;,
  date_of_loss as &quot;loss_date&quot;
from {{ source('raw_data', 'raw_claim_data') }}
</code></pre>
",0,0,0,2025-08-17T19:51:50+00:00,0,75,False
79738459,1136807,,sql,Create multi table index/composite key,"<p>I have a query which contains 3 tables. If the fetched data length is small (about 50,000) then it works very fast (nearly under a second), but when the data starts exceeding it becomes slower (9 seconds for &lt;150,000).</p>
<p>I have Indexing enabled for all the columns (except text datatype) for all the tables along with composite keys (while researching for the cure), but could not overcome this issue. Even though, I have pagination enabled and fetching 50 records per page.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE `users` (
  `id` BIGINT UNSIGNED AUTO_INCREMENT,
  `user_name` VARCHAR(20) DEFAULT NULL,
  ...
  PRIMARY KEY (`id`),
  KEY `user_name`(`user_name`),
  ...
);

CREATE TABLE `transactions` (
  `user_id` BIGINT UNSIGNED,
  `UniquePaymentRequestId` VARCHAR(100),
  `TransactionNumber` VARCHAR(100),
  `CustomerNumber` VARCHAR(100),
  `OrderId` VARCHAR(40), 
  `Amount` DECIMAL(12,6),
  `UserCharge` DECIMAL(12,6),
  `DepartmentCharge` DECIMAL(12,6),
  `ServiceCode` INT UNSINGED,
  `payment_date` date GENERATED ALWAYS AS (cast(`creation_time` as date)) VIRTUAL,
  `creation_time` DATETIME,
  ...

  KEY `UniquePaymentRequestId`(`UniquePaymentRequestId`),
  KEY `TransactionNumber`(`TransactionNumber`),
  KEY `CustomerNumber`(`CustomerNumber`),
  KEY `OrderId`(`OrderId`),
  KEY `Amount`(`Amount`),
  KEY `UserCharge`(`UserCharge`),
  KEY `ServiceCode`(`ServiceCode`),
  KEY `DepartmentCharge`(`DepartmentCharge`),
  KEY `creation_time`(`creation_time`),
  KEY `payment_date` (`payment_date`),
  KEY `transactions` (
    `UniquePaymentRequestId`, `TransactionNumber`, `CustomerNumber`, `OrderId`, 
    `Amount`, `UserCharge`, `DepartmentCharge`, `creation_time`
  )
);

CREATE TABLE `service_departments` (
  `id` BIGINT UNSIGNED AUTO_INCREMENT,
  `title` VARCHAR(255),
  `type` VARCHAR(3) DEFAULT 'non',
  ...
  PRIMARY KEY (`id`),
  UNIQUE `title`(`title`),
  KEY `type`(`type`),
  ...
);

CREATE TABLE `services` (
  `service_department_id` BIGINT UNSIGNED,
  `title` VARCHAR(255),
  ...
  KEY `service_department_id`(`service_department_id`),
  UNIQUE `title`(`title`),
  ...
);
</code></pre>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
  U.user_name, S.department, S.service_code, S.service_name, 
  T.UniquePaymentRequestId, T.TransactionNumber, T.CustomerNumber, T.OrderId, 
  T.Amount, T.UserCharge, T.DepartmentCharge, T.creation_time 
FROM transactions AS T 
INNER JOIN ( 
  SELECT user_id, user_name FROM users 
  WHERE user_type = 5 
) AS U ON U.user_id = T.user_id 
LEFT JOIN ( 
  SELECT SD.title AS department, S.service_code, S.title AS service_name FROM services AS S 
  INNER JOIN service_departments AS SD ON SD.type = &quot;non&quot; AND SD.id = S.service_department_id 
) AS S ON S.service_code = T.ServiceCode 
WHERE T.payment_date &gt;= '2025-07-19' 
ORDER BY T.creation_time DESC 
</code></pre>
<p>EXPLAIN:</p>
<pre><code>    id  select_type  table   type    possible_keys                       key                    key_len  ref                            rows    Extra                        
------  -----------  ------  ------  ----------------------------------  ---------------------  -------  -----------------------------  ------  -----------------------------
     1  SIMPLE       T       ALL     user_index,payment_date             (NULL)                 (NULL)   (NULL)                         206192  Using where; Using filesort  
     1  SIMPLE       users   eq_ref  PRIMARY,user_type                   PRIMARY                4        gramaone_production.T.user_id  1       Using where                  
     1  SIMPLE       SD      ref     PRIMARY,type                        type                   63       const                          8       Using where                  
     1  SIMPLE       S       ref     service_code,service_department_id  service_department_id  5        gramaone_production.SD.id      3       Using where                  
</code></pre>
<p>How do I resolve this issue?</p>
",2,2,0,2025-08-18T08:54:17+00:00,3,229,True
79739025,583271,"Belfast, UK",sql,Query to set a value to null if it contains non-numeric characters,"<p>I currently have this query, where <code>response</code> a column defined as  <code>varchar(255)</code>:</p>
<pre><code>SELECT 
    CAST((NULLIF(REGEXP_REPLACE(response, '[^0-9]+', '', 'g'), ''), '0') AS INTEGER) 
FROM
    my_table;
</code></pre>
<p>The idea is that we only want to pull out those rows where the value of response is numeric. So this strips out non-numeric values, sets to null if the result is 0, and casts to an integer.</p>
<p>This is working fine for most columns, except when the column is a date - this removes all the ':' and '+' etc. and ends up with a number which it tries and fails to parse (because it's too long for an integer).</p>
<p>I'm not sure that this is the best way of handling this case anyway - is there a way to return a number if the column contains a string representing a number, and null otherwise? E.g. try to cast to an integer and return null if it fails or something like that?</p>
",3,4,1,2025-08-18T16:32:35+00:00,7,303,True
79739186,29276834,,sql,Insert environment username into SQL in a MS Access form,"<p>I have a MS Access form with an edit button.</p>
<p>Before it edits the record, it runs this code to backup to a changelog table:</p>
<pre><code>DoCmd.RunSQL &quot;INSERT INTO PlanningChangeLog SELECT * FROM Planning &quot; &amp; _
        &quot;WHERE Bestelbon = &quot; &amp; Me.txtSearch &amp; &quot;&quot;
</code></pre>
<p>I have created two new columns in this changelog table called <code>Timestamp</code> which uses the <code>=Now()</code> function, and <code>UserAccount</code> where I would like the SQL code to insert the environment username.</p>
<p>But I can't make sense of how to write this statement.</p>
<p>Any help would be appreciated.</p>
<p>Thanks</p>
<p>EDIT: This sql code seems to work, except it won't recognise my user variable. It always gives me a prompt to manually type it in.</p>
<pre><code>Private Sub CommandEdit_Click()

Dim db As DAO.Database
Dim rs As DAO.Recordset
Dim sql As String
Dim user As String

user = Environ(&quot;username&quot;)

'eerst een check of het zoekveld niet leeg is
If Nz(Me.txtSearch.Value, &quot;&quot;) = &quot;&quot; Then
    MsgBox &quot;Geef een bestelbon in !!&quot;, vbExclamation
    Exit Sub
Else

'maakt een backup van de originele record
'DoCmd.RunSQL &quot;INSERT INTO PlanningChangeLog SELECT * FROM Planning &quot; &amp; _
    '&quot;WHERE Bestelbon = &quot; &amp; Me.txtSearch &amp; &quot;&quot;
    
'DoCmd.RunSQL &quot;INSERT INTO PlanningChangeLog SELECT [ID], Now() as [TimeStampEdit], user as [UserAccount], [Datum], [Bestelbon], [Transporteur], [Productnaam], [Tank] FROM Planning &quot; &amp; _
    '&quot;WHERE Bestelbon = &quot; &amp; Me.txtSearch &amp; &quot;&quot;
    
DoCmd.RunSQL &quot;INSERT INTO PlanningChangeLog(&quot; &amp; _
    &quot;ID, TimeStampEdit, UserAccount, Datum, Bestelbon, Transporteur, Productnaam, Tank) &quot; &amp; _
    &quot;SELECT ID, Now() as TimeStampEdit, user as UserAccount, Datum, &quot; &amp; _
    &quot;Bestelbon, Transporteur, Productnaam, Tank FROM Planning &quot; &amp; _
    &quot;WHERE Bestelbon = &quot; &amp; Me.txtSearch &amp; &quot;&quot;
</code></pre>
<p>Any suggestions on how to have it recognise the variable?</p>
",0,1,1,2025-08-18T19:51:42+00:00,3,222,False
79739561,9848156,,sql,DENODO LISTAGG is not executable,"<p>I'm running some code in DENODO VQL that is erroring out and I'm a bit stumped.</p>
<p>PseudoCode (renamed to protect IP):</p>
<pre><code>SELECT ToolGroup, LISTAGG(ToolID, ', ') WITHIN GROUP (ORDER BY ToolID ASC) AS ActiveTools

FROM DB

GROUP BY ToolGroup
ORDER BY ToolGroup
</code></pre>
<p>DB is as follows:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ToolID</th>
<th>ToolGroup</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>B</td>
</tr>
<tr>
<td>2</td>
<td>E</td>
</tr>
<tr>
<td>3</td>
<td>E</td>
</tr>
<tr>
<td>4</td>
<td>C</td>
</tr>
<tr>
<td>5</td>
<td>A</td>
</tr>
<tr>
<td>6</td>
<td>D</td>
</tr>
</tbody>
</table></div>
<p>The error is as follows:</p>
<blockquote>
<p>Error executing view: Function listagg is not executable</p>
</blockquote>
<p>Expected Output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ToolGroup</th>
<th>Column B</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>5</td>
</tr>
<tr>
<td>B</td>
<td>1</td>
</tr>
<tr>
<td>C</td>
<td>4</td>
</tr>
<tr>
<td>D</td>
<td>6</td>
</tr>
<tr>
<td>E</td>
<td>2, 3</td>
</tr>
</tbody>
</table></div>
",0,1,1,2025-08-19T07:07:15+00:00,1,383,True
79739565,31259988,,sql,Get the average of an order by customer,"<p>I have tables <code>Customer</code>, <code>Order</code> and <code>Orderline</code>.</p>
<p>The relationship between between <code>Customer</code> and <code>Order</code> is based on <code>CustId</code> and between <code>Order</code> and <code>Orderline</code>on <code>OrderId</code>. In the <code>Orderline</code> table, I have the columns <code>Qty</code> and <code>Price</code>.</p>
<p>I would like a result that shows me by <code>Custid</code>:</p>
<ul>
<li><p><code>OrderId</code></p>
</li>
<li><p>the numbers of orders (COUNT)</p>
</li>
<li><p>the sum value of the order (SUM),</p>
</li>
<li><p>the sum of all orderlines (SUM(Qty*Price))</p>
</li>
<li><p>the average value of all orders</p>
<p><em>You get a <code>Custid</code> per order.</em></p>
</li>
</ul>
<p>I can get everything to work but the average.</p>
<p>Some variable(s) might be the solution.</p>
<p><code>Customer</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>CustID</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>10073</td>
<td>Test</td>
</tr>
<tr>
<td>10074</td>
<td>Test2</td>
</tr>
</tbody>
</table></div>
<p><code>Order</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>OrderID</th>
<th>CustID</th>
<th>OrderDate&amp;Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>VOR0060751</td>
<td>10073</td>
<td>2025-05-07 00:00:00.000</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>10073</td>
<td>2025-06-04 00:00:00.000</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>10073</td>
<td>2025-06-18 00:00:00.000</td>
</tr>
</tbody>
</table></div>
<p><code>OrderLine</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>orderid</th>
<th>Orderline</th>
<th>OrderSubline</th>
<th>Qty</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>VOR0060751</td>
<td>010</td>
<td>000</td>
<td>1.000</td>
<td>15.60</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>010</td>
<td>005</td>
<td>1.000</td>
<td>12.27</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>020</td>
<td>000</td>
<td>1.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>020</td>
<td>005</td>
<td>1.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>030</td>
<td>000</td>
<td>1.000</td>
<td>16.19</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>030</td>
<td>005</td>
<td>1.000</td>
<td>8.87</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>040</td>
<td>000</td>
<td>1.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>040</td>
<td>005</td>
<td>1.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>050</td>
<td>000</td>
<td>1.000</td>
<td>23.49</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>050</td>
<td>005</td>
<td>1.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>010</td>
<td>000</td>
<td>1.000</td>
<td>8.88</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>020</td>
<td>000</td>
<td>1.000</td>
<td>12.12</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>020</td>
<td>005</td>
<td>1.000</td>
<td>21.27</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>030</td>
<td>000</td>
<td>4.000</td>
<td>16.68</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>040</td>
<td>000</td>
<td>1.000</td>
<td>16.68</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>050</td>
<td>000</td>
<td>1.000</td>
<td>10.35</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>050</td>
<td>005</td>
<td>1.000</td>
<td>6.29</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>060</td>
<td>000</td>
<td>1.000</td>
<td>10.35</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>060</td>
<td>005</td>
<td>1.000</td>
<td>8.36</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>070</td>
<td>000</td>
<td>2.000</td>
<td>14.29</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>070</td>
<td>005</td>
<td>2.000</td>
<td>8.87</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>080</td>
<td>000</td>
<td>2.000</td>
<td>15.60</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>080</td>
<td>005</td>
<td>2.000</td>
<td>10.23</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>090</td>
<td>000</td>
<td>2.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>090</td>
<td>005</td>
<td>2.000</td>
<td>17.10</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>100</td>
<td>000</td>
<td>1.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>100</td>
<td>005</td>
<td>1.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>110</td>
<td>000</td>
<td>1.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>120</td>
<td>000</td>
<td>1.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>120</td>
<td>005</td>
<td>1.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>130</td>
<td>000</td>
<td>1.000</td>
<td>16.19</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>130</td>
<td>005</td>
<td>1.000</td>
<td>10.23</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>140</td>
<td>000</td>
<td>4.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>140</td>
<td>005</td>
<td>4.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>150</td>
<td>000</td>
<td>2.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>160</td>
<td>000</td>
<td>2.000</td>
<td>28.00</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>160</td>
<td>005</td>
<td>2.000</td>
<td>38.45</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>010</td>
<td>000</td>
<td>1.000</td>
<td>6.58</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>010</td>
<td>005</td>
<td>1.000</td>
<td>6.07</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>020</td>
<td>000</td>
<td>1.000</td>
<td>6.58</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>020</td>
<td>005</td>
<td>1.000</td>
<td>6.07</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>030</td>
<td>000</td>
<td>1.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>040</td>
<td>000</td>
<td>2.000</td>
<td>10.35</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>040</td>
<td>005</td>
<td>2.000</td>
<td>8.36</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>050</td>
<td>000</td>
<td>1.000</td>
<td>14.29</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>050</td>
<td>005</td>
<td>1.000</td>
<td>8.87</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>060</td>
<td>000</td>
<td>1.000</td>
<td>15.60</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>060</td>
<td>005</td>
<td>1.000</td>
<td>10.23</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>070</td>
<td>000</td>
<td>1.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>070</td>
<td>005</td>
<td>1.000</td>
<td>17.10</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>080</td>
<td>000</td>
<td>2.000</td>
<td>16.19</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>080</td>
<td>005</td>
<td>2.000</td>
<td>8.36</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>090</td>
<td>000</td>
<td>2.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>090</td>
<td>005</td>
<td>2.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>100</td>
<td>000</td>
<td>2.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>100</td>
<td>005</td>
<td>2.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>105</td>
<td>000</td>
<td>1.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>110</td>
<td>000</td>
<td>1.000</td>
<td>23.49</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>110</td>
<td>005</td>
<td>1.000</td>
<td>14.07</td>
</tr>
</tbody>
</table></div>
<p>Desired result data set:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Custid</th>
<th>OrderID</th>
<th>SUMOrder</th>
<th>AVGAllOrders</th>
<th>Countorders</th>
<th>SumAllOrders</th>
</tr>
</thead>
<tbody>
<tr>
<td>10073</td>
<td>VOR0060751</td>
<td>161.29000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
</tr>
<tr>
<td>10073</td>
<td>VOR0061499</td>
<td>702.04000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
</tr>
<tr>
<td>10073</td>
<td>VOR0061918</td>
<td>318.19000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
</tr>
</tbody>
</table></div>
<pre><code>SELECT 
    C.Custid, O.OrderID, TotVO.VOSUM AS N'SUMOrder',
    ISNULL((SELECT AVG(OL.Qty * OL.CurrPrice 
            FROM Orderline AS OL, Order AS O2 
            WHERE OL.OrderID = O2.OrderID 
              AND O2.CustId = O.CustId), 0) AS N'AVGAllOrders',
    (SELECT COUNT(O2.orderid) 
     FROM Order AS O2 
     WHERE O2.Custid = O.custId) AS N'CountOrders',
    ISNULL((SELECT SUM(OL.Qty * OL.Price) 
            FROM Orderline AS OL, Order AS O2 
            WHERE OL.OrderID = O2.OrderID 
              AND O2.CustId = O.CustId), 0) AS N'SumAllOrders'
FROM 
    Customer AS C
INNER JOIN
    Order AS O ON O.CustId = c.CustId
OUTER APPLY 
    (SELECT 
         SUM(OL2.Qty * OL2.Price) AS VOSUM 
     FROM
         Orderline AS OL2 
     WHERE 
         OL2.OrderID = O.OrderID) AS TotVO
WHERE 
    C.CustId = O.CustId
ORDER BY 
    custid
</code></pre>
<p>I get a really low value, the average of all the <code>orderline</code> totals. But I need averages of the orders.</p>
<p>How can I get the desired result?</p>
",0,3,3,2025-08-19T07:08:14+00:00,2,154,True
79740248,1100221,,sql,Update multiple rows in a SQLite DB - add a number to the value in multiple rows,"<p>I have a table in SQLite:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Id</th>
<th style=""text-align: left;"">Month</th>
<th style=""text-align: left;"">Year</th>
<th style=""text-align: left;"">StartingBalance</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Jan</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">50000.00</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">Feb</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">53105.67</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: left;"">Mar</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">53405.32</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">Apr</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">54216.89</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: left;"">May</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">57020.93</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td style=""text-align: left;"">Jun</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">59002.23</td>
</tr>
<tr>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">...</td>
</tr>
<tr>
<td style=""text-align: left;"">99</td>
<td style=""text-align: left;"">Jul</td>
<td style=""text-align: left;"">2025</td>
<td style=""text-align: left;"">141215.38</td>
</tr>
<tr>
<td style=""text-align: left;"">100</td>
<td style=""text-align: left;"">Aug</td>
<td style=""text-align: left;"">2025</td>
<td style=""text-align: left;"">145002.68</td>
</tr>
</tbody>
</table></div>
<p>I need to add a value of 55 to every StartingBalance value from Apr 2007 to Aug 2025. How can I do this in a single query?</p>
<p>Thanks.</p>
",0,1,1,2025-08-19T17:52:53+00:00,1,78,True
79740642,6507100,,sql,Use bind variable directly in snowflake SQL worksheet,"<p>I wanted to try this SQL to use bind variables as input (not in a stored procedure or anonymous block), but it's throwing an error.</p>
<pre><code>LET VARIABLE VARCHAR := 'RPT';
SELECT * FROM TABLE_NAME WHERE TYPE= :VARCHAR 
</code></pre>
<p>I see this message:</p>
<blockquote>
<p>Syntax error: unexpected 'LET'. (line 26)</p>
</blockquote>
<p>How can I resolve this?</p>
",0,0,0,2025-08-20T05:18:29+00:00,1,131,True
79740906,11790979,,sql,Modify only specific rows based on values from another column,"<p>I need to modify a relatively small subset of rows that - for whatever reason I still need to investigate - did not get their data populated appropriately.</p>
<p>Working on health data, to determine prescription end dates, if no value is provided, we perform some logic (see below) to populate this. Unfortunately, some rows did not populate (&lt; 100 out of tens of millions of rows) but NA values for the end date are disallowed by convention. I've already loaded everything into our staging DB, but to upload it to prod, I have to sort out these invalid dates. I'm unsure how to do this in SQL Server as I've never really ventured past selecting, filtering and insertion of data, very basic stuff.</p>
<p>One thing I <strong>DO NOT</strong> want to do is apply any logic to the entire table, I only want to modify the &lt; 100 rows that are invalid.</p>
<p>If I was to do this in R's <code>dplyr</code> library it looks something like this:</p>
<pre class=""lang-r prettyprint-override""><code>#semi-pseudocode

library(dplyr)

table &lt;- read_csv(&quot;my_data.csv&quot;) |&gt;
filter(is.na(drug_exposure_end_date)) |&gt;
mutate(drug_exposure_end_date = as_date(coalesce(drug_exposure_end_datetime,
                                                       drug_exposure_start_datetime + (days_supply - 1L),
                                                       drug_exposure_start_datetime + ddays(29L))))
</code></pre>
<p>I'm not sure how I can get a similar result in SQL, and really the only operation I want to do is to just add 29 days, in these invalid rows, <code>days_supply</code> and <code>drug_exposure_end_datetime</code> are both <code>NULL</code>.</p>
<p>This is how the data is in my database:</p>
<pre class=""lang-sql prettyprint-override""><code>drug_exposure_id    drug_concept_id drug_exposure_start_date    drug_exposure_start_datetime    drug_exposure_end_date  drug_exposure_end_datetime  verbatim_end_date   drug_type_concept_id    stop_reason refills quantity    days_supply sig route_concept_id    lot_number  provider_id visit_occurrence_id visit_detail_id drug_source_value   drug_source_concept_id  route_source_value  dose_unit_source_value
13903618    21059132    2015-03-24  2015-03-24 00:00:00.000 NULL    NULL    NULL    32833   NULL    NULL    NULL    NULL    NULL    0   NULL    NULL    NULL    NULL    [180840] PHENYLEPHRINE  2.5% EYE DROPS PRESERVA...  0   NULL    NULL
14097493    21177049    2015-04-01  2015-04-01 00:00:00.000 NULL    NULL    NULL    32833   NULL    NULL    NULL    NULL    NULL    0   NULL    NULL    NULL    NULL    [183044] CYCLOPENTOLATE  1% EYE DROPS PRESERVAT...  0   NULL    NULL
14109189    21059132    2015-01-16  2015-01-16 00:00:00.000 NULL    NULL    NULL    32833   NULL    NULL    NULL    NULL    NULL    0   NULL    NULL    NULL    NULL    [180840] PHENYLEPHRINE  2.5% EYE DROPS PRESERVA...  0   NULL    NULL
14338276    21177049    2015-04-01  2015-04-01 00:00:00.000 NULL    NULL    NULL    32833   NULL    NULL    NULL    NULL    NULL    0   NULL    NULL    NULL    NULL    [183044] CYCLOPENTOLATE  1% EYE DROPS PRESERVAT...  0   NULL    NULL
</code></pre>
",0,1,1,2025-08-20T09:42:54+00:00,1,97,True
79741044,19758505,,sql,Expense_ClaimMileage failed target table DML statement cannot have any enabled triggers if the statement contains an OUTPUT clause without INTO clause,"<p>I am creating two identical applications, one is authored in PowerApps and the other is a Blazor server application. I created a common stored procedure for both.</p>
<p>Everything is working fine in the Blazor sever side, but whenever I try to add row from PowerApps, I get this error:</p>
<blockquote>
<p>Expense_ClaimMileage failed: The target table 'dbo.Expense_ClaimMileage' of the DML statement cannot have any enabled triggers if the statement contains an OUTPUT clause without INTO clause.<br />
clientRequestId: ABCD&quot;</p>
</blockquote>
<p>My SQL Server procedure looks like this:</p>
<pre><code>CREATE OR ALTER PROCEDURE [dbo].[RecalcType3ForHeader]
(
    @ClaimHeaderId INT
)
AS
BEGIN
    SET NOCOUNT ON;

    ;WITH S AS (
        SELECT
            t3.ClaimMileage_id,
            SUM(t2.Net_miles) AS BusinessMiles
        FROM dbo.Expense_ClaimMileage t3
        LEFT JOIN dbo.Expense_ClaimMileage t2 ON
            t2.ClaimHeader_id = t3.ClaimHeader_id
            AND t2.Mileage_type = '2'
            AND t2.Active = 1
            AND t2.Date BETWEEN t3.From_date AND t3.To_date
            AND t2.ClaimMileage_id != t3.ClaimMileage_id
        WHERE t3.Mileage_type = '3'
          AND t3.Active = 1
          AND t3.ClaimHeader_id = @ClaimHeaderId
        GROUP BY t3.ClaimMileage_id
    )
    UPDATE t3
    SET
        Private_Miles =
            CASE
                WHEN (t3.End_Odometer - t3.Start_Odometer) - ISNULL(S.BusinessMiles, 0) &lt; 0
                    THEN 0
                ELSE (t3.End_Odometer - t3.Start_Odometer) - ISNULL(S.BusinessMiles, 0)
            END,
        Amount_To_Repay =
            (
                CASE
                    WHEN (t3.End_Odometer - t3.Start_Odometer) - ISNULL(S.BusinessMiles, 0) &lt; 0
                        THEN 0
                    ELSE (t3.End_Odometer - t3.Start_Odometer) - ISNULL(S.BusinessMiles, 0)
                END
            ) * ISNULL(t3.HMRC_Fuel_Cost, 0) / 100.0,
        Last_modified_at = GETDATE()
    FROM dbo.Expense_ClaimMileage t3
    LEFT JOIN S ON t3.ClaimMileage_id = S.ClaimMileage_id
    WHERE t3.Mileage_type = '3'
      AND t3.Active = 1
      AND t3.ClaimHeader_id = @ClaimHeaderId;

    -- Dummy select to satisfy Power Apps
    SELECT 'Done' AS Status;
END
</code></pre>
<p>And the patch function is:</p>
<pre><code>UpdateContext(
    {
        varReturn_MileageClaim: Patch(
            Expense_ClaimMileage,
            Defaults(Expense_ClaimMileage),
            {
                ClaimHeader_id: ExpenseHeaderForMileage,
                Reason_visit: AddMile_Reason_Input.Text,
                Active: true,
                Date: AddMile_Date_Input.SelectedDate,
                Mileage_type: AddMileage_MileageType_Dropdown.Selected.Id,
                UserVehicle_Id: AddMileage_Uservehicles_Dropdown.Selected.UserVehicle_id,
                From: AddMile_SPlace_Input.Text,
                From_postcode: AddMile_SPostCode_Input.Text,
                To: AddMile_LPlace_Input.Text,
                To_postcode: AddMile_LPostCode_Input.Text,
                Total_miles: Value(AddMiles_TravelledMiles_Input.Text),
                Net_miles: Value(AddMiles_NetMiles_Input.Text),

                Business_Mileage_Rate: If(AddMileage_MileageType_Dropdown.Selected.Id = 2, 0, Value(MapBusinessFuelRate.Business_Mileage_Rates)),
                HMRC_Fuel_Cost: If(AddMileage_MileageType_Dropdown.Selected.Id = 2, 0, Value(QuarterlyRates.FuelRate)),
                Mileage_VAT: If(AddMileage_MileageType_Dropdown.Selected.Id = 2, 0, Value(AddMiles_TotalFuelRate_Input.Text) - (Value(AddMiles_TotalFuelRate_Input.Text)/1.2)),
                Custom_Mileage_VAT: 0,
                Mileage_NET_Cost: If(AddMileage_MileageType_Dropdown.Selected.Id = 2, 0, Value(AddMiles_TotalFuelRate_Input.Text) / 1.2),
                Total_Fuel_Cost: If(AddMileage_MileageType_Dropdown.Selected.Id = 2, 0, Value(AddMiles_TotalFuelRate_Input.Text)),

                Business_Mileage_Cost: If(
                    AddMileage_MileageType_Dropdown.Selected.Id = 2,
                    0,
                    If(
                        AddMileage_MileageType_Dropdown.Selected.Id &lt;&gt; 4,
                        (QuarterlyRates.FuelRate * Value(AddMiles_NetMiles_Input.Text)) / 100,
                        Value(MapBusinessFuelRate.Business_Mileage_Rates) * Value(AddMiles_NetMiles_Input.Text)
                    )
                ),

                Business_MileageNET_Cost: If(
                    AddMileage_MileageType_Dropdown.Selected.Id = 2,
                    0,
                    If(
                        AddMileage_MileageType_Dropdown.Selected.Id &lt;&gt; 4,
                        Value(AddMiles_TotalFuelRate_Input.Text) / 1.2,
                        Value(MapBusinessFuelRate.Business_Mileage_Rates) * Value(AddMiles_NetMiles_Input.Text) - (Value(AddMiles_TotalFuelRate_Input.Text) - Value(AddMiles_TotalFuelRate_Input.Text) / 1.2)
                    )
                ),

                MileageImageAttached: If(
                    IsBlank(UploadedMileageImage.Image),
                    1,
                    2
                ),

                Last_modified_at: Now(),
                Last_modified_by: User().Email
            },
            If(
                AddMileage_MileageType_Dropdown.Selected.Id = 3,
                {
                    Start_Odometer: Value(AddMiles_StartOdometerMiles_Input.Text),
                    End_Odometer: Value(AddMiles_EndOdometerMiles_Input.Text),
                    Amount_To_Repay: Value((QuarterlyRates.FuelRate * TotalPrivateMiles)/100),
                    Private_Miles: Value(TotalPrivateMiles)
                },
                {} // else return blank record
            )
        )
    }
);


ClearCollect(
    result_RecalcType3,
    db_dev_test.dboRecalcType3ForHeader({
        ClaimHeaderId: varReturn_MileageClaim.ClaimHeader_id
    })
);
Navigate(
    Expense_Card_Screen,
    ScreenTransition.Fade,
    {ExpenseHeaderID: varReturn_MileageClaim.ClaimHeader_id}
);
</code></pre>
<p>And trigger is:</p>
<pre><code>CREATE TRIGGER dbo.trg_RecalcType3_OnType2Change
ON dbo.Expense_ClaimMileage
AFTER INSERT, UPDATE, DELETE
AS
BEGIN
    SET NOCOUNT ON;
 
    IF TRIGGER_NESTLEVEL() &gt; 1 RETURN;
  
    DECLARE @ids dbo.IntList;

    INSERT INTO @ids (Id)
    SELECT DISTINCT ClaimHeader_id
    FROM (
        SELECT ClaimHeader_id FROM inserted WHERE Mileage_type = '2'
        UNION
        SELECT ClaimHeader_id FROM deleted WHERE Mileage_type = '2'
    ) AS Changed
    WHERE ClaimHeader_id IS NOT NULL;
    
    IF EXISTS (SELECT 1 FROM @ids)
    BEGIN
        EXEC dbo.RecalcType3ForHeaders @ids;
    END
END
</code></pre>
",0,0,0,2025-08-20T12:04:53+00:00,0,120,False
79741486,28698620,,sql,Issue with lower SQL function in GridDB,"<p>I'm using the SQL statement below to convert the first character of a string column stored in a GridDB collection container to lowercase. I'm using the <code>lower</code> function. However, when the first letter is a character with an accent, the lower function won´t convert it, thus returning the original character.</p>
<pre><code>Select 
    Customer_Name, 
    substr(Customer_Name, 1, 1) as &quot;First Letter&quot;, 
    lower (substr(Customer_Name, 1, 1)) as  &quot;First Letter in Lowercase&quot;
From 
    Customer
</code></pre>
<p><a href=""https://i.sstatic.net/MBB5tDTp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MBB5tDTp.png"" alt=""Result set"" /></a></p>
<p>Any ideas on how to accomplish that?</p>
",0,0,0,2025-08-20T19:00:54+00:00,1,70,True
79741570,3377484,,sql,Performance issues when combining queries with multiple selects,"<p>I have 2 queries with the same 3 related tables and each one works fine separately. But I would ideally like to combine them into one query. So this one:</p>
<pre><code>select me.id, me.title, me.start_date_time, 
count(mti.checkin) from mco_event me 
left outer join mco_ticket_instance as mti on mti.event_id = me.id and mti.checkin is not null
where me.start_date_time &gt; '2024-01-01'
group by me.id order by me.start_date_time asc;
</code></pre>
<p>gets me the correct count of checkins.</p>
<pre><code>select me.id, me.title, me.start_date_time, 
count(mol.status) from mco_event as me
join mco_ticket_instance mti1 on mti1.event_id=me.id
join mco_order_line mol on mol.id = mti1.order_line_id and mol.status='order_paid'
where me.start_date_time &gt; '2024-01-01'
group by me.id order by me.start_date_time asc;
</code></pre>
<p>Second one gives me the valid (order_paid) count. But if I combine these:</p>
<pre><code>select me.id, me.title, me.start_date_time, 
(select sum(mol.quantity) from mco_order_line mol
inner join mco_ticket_type mtt on mol.ticket_type_id = mtt.id and mtt.event_id = me.id
where mol.status='order_paid') as quantity,
(select count(mti.checkin) from mco_ticket_instance mti
where mti.event_id = me.id and mti.checkin is not null) as checkins
from mco_event me
where   me.start_date_time &gt; '2024-01-01'
group by me.id order by me.start_date_time asc;
</code></pre>
<p>This gives correct values, but it spins for 15 seconds instead of a few microseconds like the individual, separate queries do.
Backstory: mco_event is the &quot;main&quot; table, mco_ticket_instance has an event_id which has a mco_event.id and an order_line_id which is mco_order_line.id.</p>
<p>So right now my app code is running the 2 separate queries and then stitching the 2 arrays together which just seems inefficient (and also does take some time).</p>
",-1,0,1,2025-08-20T20:43:14+00:00,0,43,False
79741584,10524769,,sql,Creating a table inside this SQL when running from BigQuery,"<pre><code>WITH
CATALOG_SKU_CTE AS (     
  SELECT A.sku    
  FROM `catalog-stage.catalog_enterprise_views.sku_hierarchy_view` A, `catalog-stage.catalog_enterprise_views.sku_v` B     
  WHERE A.sku = B.SKU  AND UPPER(B.group) = &quot;MERCHANDISING&quot;      
  AND UPPER(B.specification) = &quot;NONCONFIGURABLE&quot;      
  AND UPPER(B.classification) IN (&quot;LUMBER&quot;,&quot;NORMAL&quot;) 

  -- ====================================

), 

-- Get all current stores and their markets 

STORE_CTE AS (     
  SELECT STR_NBR      
  FROM `assrtmgmt-stage.assrtmgmt.STR_CRDB_VIEW`     
  WHERE str_typ_ind = &quot;N&quot;     
  AND MKT_NBR &lt;&gt; 401 
), 

-- Create a list of SKU-store combinations for this triangulation 

SKU_STORE_CTE AS ( 
  SELECT A.SKU, B.STR_NBR    
  FROM CATALOG_SKU_CTE A, STORE_CTE B     
), 

-- ==================================== 

AC_CTE AS (     
  SELECT SKU, ASSORTED, LOCATION, STRATEGY     
  FROM `assrtmgmt-stage.AC_SHARED.product_assortment_commons_realtime`      
  WHERE UPPER(LOCATION_TYPE) = &quot;STR&quot; 
  AND SELLING_CHANNEL = 1     
), 
POG_PRODUCT_CTE AS (     
  SELECT DISTINCT SKU_NBR, DBKEY     
  FROM `spaceplanning-stage.SPACE_COMMONS.IX_SPC_PRODUCT_VIEW`       
  WHERE ACTV_FLG = true     
), MYASSORT_CTE AS (     
  SELECT DISTINCT A.sku_id, B.str_id     
    FROM `assrtmgmt-stage.myassort_strategy.stock_set_sku` A,      
    `assrtmgmt-stage.myassort_strategy.stock_set_str` B    
  WHERE A.set_id = B.set_id     
), 

-- Get list of SKU-store combinations from POG where the SKU-store is either on live a POG or a Pending POG 

POG_VIEW_CTE AS (     
  SELECT DISTINCT POG_SKUS.SKU_NBR, POG_STORES.STR_NBR     
  FROM POG_PRODUCT_CTE POG_SKUS,      
      `spaceplanning-stage.SPACE_COMMONS.IX_SPC_PLANOGRAM_VIEW` POG,     
      `spaceplanning-stage.SPACE_COMMONS.IX_SPC_POSITION_VIEW` POG_POS,     
      `spaceplanning-stage.SPACE_COMMONS.THD_STORE_POG_ASSIGN_VIEW` POG_STORES     
  WHERE POG_SKUS.DBKEY = POG_POS.DBPARENTPRODUCTKEY     
  AND POG_POS.ACTV_FLG = true     
  AND POG_POS.DBPRNTPLANOGRAMKEY = POG.DBKEY     
  AND POG.ACTV_FLG = true     
  AND POG.DBKEY = POG_STORES.DBPARENTPLANOGRAMKEY     
  AND POG_STORES.FINISHEDDATE IS NULL     
  AND POG_STORES.ACTV_FLG = true 
)
         
-- Get list of SKU-store combinations from POG where the SKU-store is either on live a POG or a Pending POG     

-- =========================== 

-- MAIN QUERY 

-- =========================== 

SELECT     
  SSC.SKU AS `SKU`,     
  SSC.STR_NBR AS `Store`,     
  CASE      
    WHEN MA.sku_id is NULL THEN &quot;N&quot;     
    ELSE &quot;Y&quot;    
  END AS `MyAssortment`,     
  CASE      
    WHEN PVC.SKU_NBR is NULL THEN &quot;N&quot;     
    ELSE &quot;Y&quot;     
  END AS `POG`,     
  CASE      
    WHEN AC.ASSORTED IS NULL THEN &quot;N&quot;     
    WHEN UPPER(AC.ASSORTED) = &quot;OFF&quot; THEN &quot;N&quot;     
    WHEN UPPER(AC.STRATEGY) LIKE &quot;%INACTIVE%&quot; THEN &quot;N&quot;     
    WHEN UPPER(AC.STRATEGY) LIKE &quot;%DELETE%&quot; THEN &quot;N&quot;     
    WHEN UPPER(AC.STRATEGY) = &quot;CLEARANCE&quot; THEN &quot;N&quot;     
    ELSE &quot;Y&quot;       
  END AS `Assortment_Commons`     
FROM SKU_STORE_CTE SSC     
LEFT OUTER JOIN MYASSORT_CTE MA     
  ON SSC.SKU = MA.SKU_ID      
  AND SSC.STR_NBR = MA.STR_ID     
LEFT OUTER JOIN POG_VIEW_CTE PVC     
  ON SSC.SKU = PVC.SKU_NBR     
  AND SSC.STR_NBR = CAST(PVC.STR_NBR AS INT64)     
LEFT OUTER JOIN AC_CTE AC      
  ON SSC.SKU = AC.SKU     
  AND SSC.STR_NBR = AC.LOCATION 
  ORDER BY SSC.SKU, SSC.STR_NBR) ;
</code></pre>
<p>On the above query, I would like to create a new table from the below main select query.</p>
<p>When I add &quot;CREATE or REPLACE TABLE <code>stage.myassort.company_report</code> as, it shows unrecognized keyword &quot;CREATE&quot;. How do I create a new table from this select query?</p>
<pre><code>-- =========================== 

-- MAIN QUERY 

-- =========================== 
CREATE or REPLACE TABLE `stage.myassort.company_report` as
SELECT     
  SSC.SKU AS `SKU`,     
  SSC.STR_NBR AS `Store`,    
  CASE      
    WHEN MA.sku_id is NULL THEN &quot;N&quot;     
    ELSE &quot;Y&quot;     
  END AS `MyAssortment`,     
  CASE      
    WHEN PVC.SKU_NBR is NULL THEN &quot;N&quot;     
    ELSE &quot;Y&quot;     
  END AS `POG`,     
  CASE      
    WHEN AC.ASSORTED IS NULL THEN &quot;N&quot;     
    WHEN UPPER(AC.ASSORTED) = &quot;OFF&quot; THEN &quot;N&quot;     
    WHEN UPPER(AC.STRATEGY) LIKE &quot;%INACTIVE%&quot; THEN &quot;N&quot;     
    WHEN UPPER(AC.STRATEGY) LIKE &quot;%DELETE%&quot; THEN &quot;N&quot;     
    WHEN UPPER(AC.STRATEGY) = &quot;CLEARANCE&quot; THEN &quot;N&quot;     
    ELSE &quot;Y&quot; 
</code></pre>
",-3,0,3,2025-08-20T21:02:41+00:00,1,88,True
79741939,31316051,,sql,SELECT query over 4 tables including MAX function in JOIN,"<p>I have 4 tables in a <a href=""/questions/tagged/mysql"" class=""s-tag post-tag"" title=""show questions tagged &#39;mysql&#39;"" aria-label=""show questions tagged &#39;mysql&#39;"" rel=""tag"" aria-labelledby=""tag-mysql-tooltip-container"" data-tag-menu-origin=""Unknown"">mysql</a> database:</p>
<p><strong>Results</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Result_ID</th>
<th>Season_ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>37</td>
<td>5</td>
</tr>
<tr>
<td>38</td>
<td>5</td>
</tr>
</tbody>
</table></div>
<p><strong>Players</strong></p>
<pre><code>Player_ID | Player | Player_Type    | Player_Order
1           Fred     Defender-Left   1
2           Bert     Defender-Left   1
3           Joe      Defender-Right  2
4           Harry    Midfielder      3
5           Simon    Midfielder      3
6           Tate     Striker         4
7           Graeme   Striker         4
8           Jeff     Keeper          5
…           …        …               …
</code></pre>
<p><strong>PointTypes</strong></p>
<pre><code>PointType_ID | L1_Value   | L2_Value    | Point_Value | L1_Order
1              Appearance   Keeper        1             1
2              Appearance   Outfield      1             1
3              Goal         Keeper        4             1
4              Goal         Outfield      4             1
5              Assist       Keeper        3             1
6              Assist       Outfield      3             2
…              …            …             …             …
</code></pre>
<p><strong>Points</strong></p>
<pre><code>Point_ID | Player_ID | Result_ID | PointType_ID
1          1           1           1
2          2           1           2
3          3           1           2
…          …           …           …
1          1           37          1
2          2           37          2
3          3           37          2
2          4           37          2
3          6           37          2
4          4           37          6
6          6           37          4
1          1           38          1
2          2           38          2
3          3           38          2
2          4           38          2
3          5           38          2
4          2           38          4
6          5           38          6
</code></pre>
<p>I'm trying to run a query that produces the following result set:</p>
<pre><code>Player_Order | Player_Type   | Player_ID | Player_Name | Sum(Point_Value) | Season_ID
1              Defender-Left   1           Fred          2                  5
1              Defender-Left   2           Bert          6                  5
2              Defender-Right  3           Joe           2                  5
3              Midfielder      4           Harry         5                  5
3              Midfielder      5           Simon         4                  5
4              Striker         6           Tate          5                  5
…              …               …           …             …                  …
</code></pre>
<p>The following query provides the all data I want (excluding the Season_ID field).</p>
<pre><code>SELECT Players.Player_Order, Players.Player_Type, Players.Player_ID, Players.Player, SUM(PointTypes.Point_Value) AS Points
FROM Points, Players, PointTypes AS PT
WHERE Players.Player_ID = Points.Player_ID AND Points.PointType_ID = PointType.PointType_ID
GROUP BY Player_ID
ORDER BY Player_Order, Points DESC, Player;
</code></pre>
<p>How do I update the query (or even use a different type of query) to filter the data to show the same fields but adding a filter to show only the results where <code>Results.Season_ID = Max(Results.Season_ID)</code>.</p>
",2,2,0,2025-08-21T06:54:24+00:00,1,104,True
79742613,1682353,Dubai,sql,Calculating @StartRow and @EndRow for pagination,"<p>I have a stored procedure, I am calling this from a .NET Core backend web page.</p>
<pre><code>WITH PagedResults AS (
    SELECT
        Column1,
        Column2,
        ROW_NUMBER() OVER (ORDER BY SortColumn ASC) AS RowNum
    FROM
        YourTable
)
SELECT
    Column1,
    Column2
FROM
    PagedResults
WHERE
    RowNum BETWEEN @StartRow AND @EndRow;
</code></pre>
<p>I am using ADO.NET to call the stored procedure.</p>
<p>How do I calculate <code>@StartRow</code> and <code>@EndRow</code> for pagination?</p>
<p>Please note that I have <code>@PageNumber</code> and <code>@PageSize</code> parameters.</p>
",-2,0,2,2025-08-21T17:18:15+00:00,2,125,True
79742747,12940601,,sql,How to locate duplicate rows without having to name every column in table?,"<p>Due to flawed design, a table was created without a unique identifier for each row. Due to another issue, a number of those rows have been completely duplicated. I figured out how to determine the duplicated rows, but I had to include every column name in the group by clause. Is there a way for me to determine those full duplicates without naming every column in the group by? For this table it's not a huge deal, as there are only 20 or so columns, but if I  had a 50 column table, etc., it could get a bit onerous.</p>
<p>I'm on Oracle 19c, just FYI, and I have no way of upgrading. My end goal will be to delete the duplicate records by referencing the rowid, but for now I'm focusing on improving this query.</p>
<p>Below is basically the query I used and for which I am seeking improvement:</p>
<pre><code>select col1, col2,...col20
from table
where col3 in ('x','y')
group by col1, col2...col20
having count(*) &gt; 1;
</code></pre>
",5,6,1,2025-08-21T20:24:53+00:00,4,220,True
79742851,3179716,,sql,How to add days to timestamp with daylight savings time change,"<p>When adding 90 days to a timestamp that is in PST, I expected the result to be in PDT, but I'm getting a result that I think is incorrect. In my example, I start with a timestamp that is near midnight on 2025-01-30 and I got the same time but on 2025-04-30, but because of the start of daylight savings in the US on March 9, I expected the result to be 2025-05-01 at midnight.</p>
<pre><code>ALTER SESSION SET TIMEZONE = 'America/Los_Angeles';
SELECT
    TO_TIMESTAMP_TZ('2025-01-30 23:19:45.000') as ts
    ,DATEADD(day, 90, ts) as ts_plus_90;

-- ts: 2025-01-30 23:19:45.000 -0800
-- ts_plus_90: 2025-04-30 23:19:45.000 -0700
-- Expected ts_plus_90: 2025-05-01 00:19:45.000 -0700
</code></pre>
<p>Is there a workaround to get the correct DST-adjusted timestamp?</p>
",1,2,1,2025-08-22T00:03:28+00:00,2,98,True
79743442,1030576,"Colorado Springs, CO",sql,Finding records with no joined records or joined records that have been marked as deleted,"<p>Two tables, <code>s</code> and <code>c</code>, are in a many-to-many relationship with <code>cs</code> as the middle table. Each table has a primary key called <code>id</code>, and the <code>cs</code> and <code>c</code> tables have a column for deleted (0 means not deleted, 1 means deleted). Note: I cannot delete records from the tables. My only option is to set <code>del</code> to <code>0</code> or <code>1</code>.</p>
<pre><code>s    cs      c
--   -----   -------
id   c_id    id
     s_id    del
     del
</code></pre>
<p>The goal is to find all <code>s</code> records that do not have any <strong>active</strong> c records, which means:</p>
<ul>
<li>There may be no <code>cs</code> records, or there may be only deleted <code>cs</code> records</li>
<li>There may be active <code>cs</code> records that do not join to any <code>c</code> records or join to only deleted <code>c</code> records.</li>
</ul>
<p>Here is an example:</p>
<pre><code>s.id
----
1
2
3

cs.del cs.s_id cs.c_id
------ ------- -------
0      1       @      
0      1       $      
1      1       #      
0      3       @
0      3       #

c.id c.del
---- -----
@    1    
$    0    
#    1    
</code></pre>
<p>Results:</p>
<ul>
<li>s.id = 1 should <strong>not be found</strong> because there is an active c record through cs.id = 2</li>
<li>s.id = 2 should <strong>be found</strong> because it is not joined any cs records</li>
<li>s.id = 3 should <strong>be found</strong> because it is joined to only deleted c records</li>
</ul>
<p>Here is my attempt</p>
<pre><code>SELECT
  s.id

FROM
  s
  LEFT JOIN cs on s.id = cs.s_id
  LEFT JOIN c  on c.id = cs.c_id

WHERE
      (cs.del = 0 OR cs.del IS NULL)
  AND (c.del  = 0 OR c.del  IS NULL)

GROUP BY
  s.id

HAVING
  COUNT(s.id) &gt; 0
</code></pre>
<p>Unfortunately, it does not work (no records are found). I have tried many times to no avail, so I am not sure what is wrong. Any help would be greatly appreciated.</p>
",1,1,0,2025-08-22T13:21:30+00:00,3,78,True
79743563,31330011,,sql,Assign unique values in a set-based approach,"<p>Simplifying, I have the following data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col1</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>X</td>
</tr>
<tr>
<td>A</td>
<td>Y</td>
</tr>
<tr>
<td>A</td>
<td>Z</td>
</tr>
<tr>
<td>B</td>
<td>X</td>
</tr>
<tr>
<td>B</td>
<td>Y</td>
</tr>
<tr>
<td>B</td>
<td>Z</td>
</tr>
<tr>
<td>C</td>
<td>Z</td>
</tr>
</tbody>
</table></div>
<p>I need to receive the following result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col1</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>X</td>
</tr>
<tr>
<td>B</td>
<td>Y</td>
</tr>
<tr>
<td>C</td>
<td>Z</td>
</tr>
</tbody>
</table></div>
<p>In other words: For each value in the left column, I need to assign the minimum UNUSED value from the right column (no duplicates).
This was easy to do iteratively, i.e., with cursors. However, I would like something that's a thousand times faster.</p>
<p><strong>What I've tried</strong></p>
<p>Unsurprisingly, <code>select L,min(R)</code> gives the wrong result. I've tried partitioning over several window functions, but I can't get the right combination. I always get the following incorrect result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Col1</th>
<th>Col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>X</td>
</tr>
<tr>
<td>B</td>
<td>X</td>
</tr>
<tr>
<td>C</td>
<td>Z</td>
</tr>
</tbody>
</table></div>
<p>I've loaded some of the data into <a href=""https://dbfiddle.uk/6HbpdlYd"" rel=""nofollow noreferrer"">https://dbfiddle.uk/6HbpdlYd</a>.</p>
<p>Here are 142 rows, created from 139 distinct L values, and 139 distinct R values.</p>
<p>Since the input data is produced by a join, there is always exactly one correct solution.</p>
",-1,1,2,2025-08-22T15:13:41+00:00,1,173,True
79743764,6507100,,sql,SHOW PARAMETERS All level in single query,"<p>We can check parameters of SESSION/ACCOUNT/DATABASE level individually using <code>SHOW</code> command in Snowflake:</p>
<pre><code>SHOW PARAMETERS IN ACCOUNT;   —- shows parameters defined at the user level
SHOW PARAMETERS IN USER;      –- shows parameters defined at the session level
SHOW PARAMETERS IN SESSION;   -– shows parameters defined at the session level
</code></pre>
<p>Is there any common view to list everything at once in Snowflake?</p>
",2,2,0,2025-08-22T18:55:15+00:00,1,121,True
79743827,8200386,,sql,Use Output to populate multiple values and select when not already existing,"<p>I am trying to add information to two tables.  The first table will issue an ID number that must be included in the second table, and data is only to be added to the second table if it is not already there.</p>
<p>Right now, I have this (needless to say, it doesn't work). The DECLARE bit is fine and returns the ID and I can use the ID if I just select it and don't want to add multiple rows or check for previous existance.  Any push in the right direction would be most welcome!</p>
<pre><code>DECLARE @ID Table (ID int);
INSERT INTO Table1 (FIELD1, FIELD2, FIELD3) 
OUTPUT Inserted.IDFIELD INTO @ID
VALUES(1, 2, 3);

INSERT INTO Table2 (Other1_theID, Other2, Other3) 
Select (@ID.Sequence, New.Other2, New.Other3)
from @ID, NEW
(VALUES(Sequence,'A','B'),  
(Sequence,'C', 'D'),
(Sequence,'E', 'F'),
(Sequence,'G', 'H')) as NEW
where not exists (SELECT 'X' FROM Table2 TT WHERE TT.Other3 IN ('B','D','F','H');
</code></pre>
<p>So, in the above case, I'm adding a new row to Table1 with the first four lines and that much works fine.:</p>
<pre><code>ID  FIELD1  FIELD2  FIELD3  
1     5       8       6    &lt;existing data
2     9       3       2    &lt;existing data
3     4       1       7    &lt;existing data
4     1       2       3    &lt;my new row
^ the new ID automatically assigned by the system when the new row is added to the table
</code></pre>
<p>In the second bit, I need to add to a second table some data associated with the first (so I need to bring the ID along). If I use this code to bring in one row of data, it works:</p>
<pre><code>INSERT INTO Table2 (Other1_theID, Other2, Other3) 
Select ID, A, B from @ID

Other1_theID    Other2  Other3  
    4             A       B       &lt;my new row
    ^ the new ID I go from the Table1 insertion.
</code></pre>
<p>All of that's great, but I really want to add several rows to Table 2 AND I only want to add the if they don't already exist (if A, B exist in Other2 and Other3 with a different ID, then don't add them).</p>
",-3,0,3,2025-08-22T20:25:20+00:00,1,110,False
79745144,15568887,,sql,How to make query with the same result but without conditions after &#39;ON&#39;?,"<p>Here is the query:</p>
<pre><code>SELECT *
FROM TEMP_AVIA_EVENT AE
LEFT JOIN TEMP_AVIA_EVENT A 
    ON AE.pnr_id = A.pnr_id
    AND AE.operation_type_id IN (4, 6) 
    AND A.operation_type_id IN (17, 9);
</code></pre>
<p>How to make query with same result without <code>AND AE.operation_type_id IN (4, 6) AND A.operation_type_id IN (17, 9)</code> part after ON? Probably, I should use subqueries.</p>
<p>I tried</p>
<pre><code>WITH AE AS (
    SELECT *
    FROM TEMP_AVIA_EVENT WHERE operation_type_id IN (4, 6)
), A AS (
    SELECT *
    FROM TEMP_AVIA_EVENT WHERE operation_type_id IN (17, 9)
),
SELECT *
FROM AE
LEFT JOIN A 
    ON AE.pnr_id = A.pnr_id;
</code></pre>
<p>It returns a lot less records than the original query.</p>
<p>Also, I tried</p>
<pre><code>WITH AE AS ( SELECT *
FROM TEMP_AVIA_EVENT),
A AS ( SELECT *
FROM TEMP_AVIA_EVENT WHERE operation_type_id IN (17, 9) AND operation_type_id IN (4, 6)),
SELECT *
FROM AE
LEFT JOIN A 
    ON AE.pnr_id = A.pnr_id;
</code></pre>
<p>It returns a little bit less records.</p>
<p>Why didn't my attempts work?</p>
",1,1,0,2025-08-24T19:58:17+00:00,2,111,True
79745180,30848174,,sql,Why does SQL meta-programming using macro variables fail,"<p>I have an in-memory table defined as follows in DolphinDB:</p>
<pre><code>t = table(2025.01.01 as date, 1 as M01, 2 as M02, 3 as M03, 4 as M04, 5 as M05, 6 as M06, 1 as M07, 2 as M08, 3 as M09, 4 as M10, 5 as M11, 6 as M12)
/*
date        M01 M02 M03 M04 M05 M06 M07 M08 M09 M10 M11 M12
2025.01.01  1   2   3   4   5   6   1   2   3   4   5   6
*/
</code></pre>
<p>Here, date is a date field, and M01 to M12 represent monthly counts for each month of the year.</p>
<p>I am trying to use meta-programming to calculate the differences between consecutive months for each row. My code is as follows:</p>
<pre><code>leg1Month = t.columnNames()[1: (11 + 1)]  // Returns vector [M01, M02, ..., M11]
leg2Month = t.columnNames()[2: (11 + 2)]  // Returns vector [M02, M03, ..., M12]
columns = leg1Month + &quot; - &quot; + leg2Month   // Returns vector [M01 - M02, M02 - M03, ..., M11 - M12]
columnAlias = &quot;\&quot;&quot; + leg1Month + &quot;-&quot; + leg2Month + &quot;\&quot;&quot;
sqlCode = &lt;select date, _$$columns as _$$columnAlias from t&gt;
sqlCode.eval()
</code></pre>
<p>However, when executing the last line, I get the following error:
<code>eval(sqlCode) =&gt; Unrecognized column name [M01 - M02].</code></p>
<p>I checked the value of sqlCode:</p>
<pre><code>sqlCode
// Output:
code('&lt; select date, M01 - M02 as &quot;M01-M02&quot;, M02 - M03 as &quot;M02-M03&quot;, M03 - M04 as &quot;M03-M04&quot;, M04 - M05 as &quot;M04-M05&quot;, M05 - M06 as &quot;M05-M06&quot;, M06 - M07 as &quot;M06-M07&quot;, M07 - M08 as &quot;M07-M08&quot;, M08 - M09 as &quot;M08-M09&quot;, M09 - M10 as &quot;M09-M10&quot;, M10 - M11 as &quot;M10-M11&quot;, M11 - M12 as &quot;M11-M12&quot; from t &gt;')
</code></pre>
<p>The generated meta-code appears correct. If I directly execute the following SQL:</p>
<pre><code>select date, M01 - M02 as &quot;M01-M02&quot;, M02 - M03 as &quot;M02-M03&quot;, M03 - M04 as &quot;M03-M04&quot;, M04 - M05 as &quot;M04-M05&quot;, M05 - M06 as &quot;M05-M06&quot;, M06 - M07 as &quot;M06-M07&quot;, M07 - M08 as &quot;M07-M08&quot;, M08 - M09 as &quot;M08-M09&quot;, M09 - M10 as &quot;M09-M10&quot;, M10 - M11 as &quot;M10-M11&quot;, M11 - M12 as &quot;M11-M12&quot; from t
</code></pre>
<p>I get the desired result. Why does sqlCode.eval() throw an error?</p>
",1,1,0,2025-08-24T21:18:30+00:00,0,58,False
79745378,972647,,sql,Self-join on table to find missing entries with conditions,"<p>I have a table in which I want to find missing rows in a specific column but with conditions.</p>
<p>I want to see if rows that have an id starting with &quot;A&quot; have a corresponding row (= same value in join column) that has an id starting with &quot;B&quot;.</p>
<p>I then want to see matches and such that do not have an entry in B. So all rows starting with &quot;A&quot; should be in the results set.</p>
<p>This is my current SQl statement:</p>
<pre><code>SELECT
    a.id,
    b.id
FROM
    abc a
LEFT JOIN
    abc b on a.join_column = b.join_column
WHERE
    a.id LIKE 'A%'
    AND (b.id LIKE 'B%' OR b.id is null)
</code></pre>
<p>I need to specific that the id can start with &quot;B&quot; or is null (does not exists). The issue is I'm only getting matches like if I did an inner join and I'm not getting why. somehow I seem to misunderstand the boolean logic.</p>
<p>How can I get the desired result, all entires of A and in case there is a entry with B with same join column also the id of B?</p>
",1,2,1,2025-08-25T05:32:13+00:00,2,151,True
79745661,1103606,,sql,Date field value out of range,"<p>I'm trying to migrate this Oracle SQL query to PostgreSQL:</p>
<pre><code>CASE 
    WHEN (:dateOfBirthYear IS NULL 
          OR :dateOfBirthMonth IS NULL 
          OR :dateOfBirthDay IS NULL) 
        THEN NULL 
        ELSE to_date(to_char(:dateOfBirthYear) || 
                     to_char(:dateOfBirthMonth, '00') || 
                     to_char(:dateOfBirthDay, '00'), 'YYYY mm DD')                                  
END, 
</code></pre>
<p>I tried this one:</p>
<pre><code>CASE 
    WHEN (:dateOfBirthYear IS NULL 
          OR :dateOfBirthMonth IS NULL 
          OR :dateOfBirthDay IS NULL) 
        THEN NULL 
        ELSE make_date(:dateOfBirthYear, :dateOfBirthMonth, :dateOfBirthDay) 
END, 
</code></pre>
<p>But I get this error because I send empty date value.</p>
<blockquote>
<p>org.postgresql.util.PSQLException: ERROR: date field value out of range: 0-00-00</p>
<p>at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.5.jar:42.7.5]</p>
</blockquote>
<p>Is there any solution to modify the SQL code to work with empty value?</p>
",0,1,1,2025-08-25T11:06:24+00:00,2,203,True
79746177,9627444,,sql,Query with WHERE clause on a UNION query results in a table scan without using the indexes,"<p>An MS Access query with a WHERE clause on a UNION query results in a table scan without using the indexes.</p>
<p>Example: There is a table like this:</p>
<pre><code>CREATE TABLE tblFigures
(
    EntryDate DATETIME,
    AmountType NUMBER,
    Amount DOUBLE
);

CREATE INDEX idxAmountType ON tblFigures (AmountType)
</code></pre>
<p>If you run the following query and view the query plan in JetShowPlan's showplan.out file, you will see that the index is being used:</p>
<pre><code>SELECT * FROM tblFigures
WHERE AmountType = 1;
</code></pre>
<p>If you now run the following UNION query, the query plan shows that a table scan is being performed instead of using the index. I suspect this is because the result set of a UNION query exists only virtually in memory and not in the database. Therefore, there are no indexes on the result set. (Maybe I'm wrong!?)</p>
<p>qryFiguresUnion looks like this:</p>
<pre><code>SELECT * FROM tblFigures
UNION ALL
SELECT * FROM tblFigures
</code></pre>
<p>Run qryFiguresUnion with the WHERE clause:</p>
<pre><code>SELECT * FROM qryFiguresUnion
WHERE AmountType = 1;
</code></pre>
<p>This poses a major performance problem for my application. For example, there are three amount types: 1, 2, and 3. In the subsequent calculation, the user has the option of performing the calculations based on one of the types or based on the sum of all three types.
Therefore, I initially tried writing a UNION query based on tblFigures and a SELECT of the sums of the three types for each EntryDate, and using the UNION query as the record source for the subsequent calculations. This works, but is very, very slow due to the table scanning.</p>
<p>Another idea would be to calculate the sums and insert them into tblFigures, for example, with the amount type 4, and then use tblFigures as the data source for further calculations. I think this would be fast, since the JET engine uses the index. However, I'm not very satisfied with this solution because the data -&gt; sum of the amount types is redundant.</p>
<p>Does anyone have a better idea?</p>
<p>Thanks in advance!</p>
",0,1,1,2025-08-25T20:40:19+00:00,1,122,True
79747062,3149869,,sql,How to cast a timestamp to date in format YYYY-MM-DD in griddb,"<p>I created a collection container name ‘orders’ with the below DDL using NewSQL Interface:</p>
<pre><code>CREATE TABLE orders 
(
    orderId STRING PRIMARY KEY,
    customerId STRING,
    orderDate TIMESTAMP,
    totalAmount DOUBLE,
    status STRING
); 
</code></pre>
<p>The values in the column <code>orderDate</code> are in timestamp format. How do I extract date from a <code>TIMESTAMP</code> as when I use <code>CAST(ord.orderDate AS date) as orderDate</code>, I am getting an error</p>
<blockquote>
<p>Invalid Query</p>
</blockquote>
<p>However when I am using the below query extracting the year, month and date and concatenating them it worked but its returning a string but how to return a date?</p>
<pre><code>SELECT
    EXTRACT(YEAR, ord.orderDate)||'-'||EXTRACT(MONTH, ord.orderDate)||'-'||EXTRACT(DAY, ord.orderDate) AS order_date,
    COUNT(ord.orderID) AS order_count
FROM
    orders ord
GROUP BY
    order_date;
</code></pre>
",3,3,0,2025-08-26T16:09:24+00:00,2,113,True
79747517,17700043,,sql,How to perform hourly aggregation in time-series table?,"<p>I am using GridDB Cloud for time-series analysis and need to calculate hourly average temperature readings from a table containing sensor data. I want to group my readings into 1-hour intervals and compute the average temperature for each interval, but I am not sure which SQL function or method is supported for this in GridDB Cloud.</p>
<p>Table schema:</p>
<pre><code>CREATE TABLE SensorData (
    timestamp TIMESTAMP,
    temp DOUBLE
);
</code></pre>
<p>Sample data:</p>
<pre><code>INSERT INTO SensorData (timestamp, temp) VALUES
(TIMESTAMP('2025-08-22T01:05:00Z'), 20.5),
(TIMESTAMP('2025-08-22T01:25:00Z'), 21.0),
(TIMESTAMP('2025-08-22T01:45:00Z'), 22.0),
(TIMESTAMP('2025-08-22T03:10:00Z'), 21.2),
(TIMESTAMP('2025-08-22T03:55:00Z'), 22.4),
(TIMESTAMP('2025-08-22T05:15:00Z'), 22.1);
</code></pre>
<p>Expected result:</p>
<pre><code>2025-08-22 01:00:00 → 21.17
2025-08-22 03:00:00 → 21.8
2025-08-22 05:00:00 → 22.1
</code></pre>
<p>I want to group this data by <strong>1-hour intervals</strong> and calculate the average temperature for each.</p>
<p>I looked for time-based aggregation functions in the GridDB documentation and online resources, but I couldn’t find a clear example of how to perform hourly grouping and averaging.</p>
<p><strong>Question:</strong><br />
How can I group this data into 1-hour intervals and calculate the average temperature for each in GridDB Cloud?</p>
",2,2,0,2025-08-27T04:09:19+00:00,2,139,True
79747841,13158157,,sql,How to efficiently count related rows in joined tables using window functions in SQL?,"<p>I have three tables:</p>
<ol>
<li>unique_headers (contains unique header rows)</li>
<li>documents_of_headers_a (documents related to header_a)</li>
<li>documents_of_headers_b (documents related to header_b)
I want to output a result like this:</li>
</ol>
<pre><code>SELECT 
  h.header_a_id, 
  h.header_b_id, 
  a_count,
  b_count
FROM unique_headers h
LEFT JOIN (
    SELECT 
      header_a_id, 
      COUNT(*) AS a_count
    FROM documents_of_headers_a
    GROUP BY header_a_id
) a ON h.header_a_id = a.header_a_id
LEFT JOIN (
    SELECT 
      header_b_id, 
      COUNT(*) AS b_count
    FROM documents_of_headers_b
    GROUP BY header_b_id
) b ON h.header_b_id = b.header_b_id
WHERE h.status = 'ACTIVE' 
  AND h.CreatedDate = '2025-01-01' 
  AND h.isDeleted = False
</code></pre>
<p><em>Problem:</em>
This query calculates the counts for all rows in documents_of_headers_a and documents_of_headers_b before joining, which is inefficient if those tables are large. I only want to count the related rows for the headers that match the WHERE conditions.</p>
<p>How can I optimize this query, perhaps using window functions, so that the row counts are only calculated for the relevant joined rows after filtering by the WHERE clause?</p>
",2,2,0,2025-08-27T10:25:40+00:00,2,145,True
79748214,3448637,,sql,Calculate quantity based on column from a different table,"<p>I currently am working with 2 tables <code>tbRecords</code> and <code>tbInspectedParts</code>.  <code>tbRecord</code> has an auto increment column that is mapped back to the <code>tbInspectionParts</code> through column <code>eRecordId</code>.</p>
<p>Example - <code>tbRecord</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Status</th>
<th>QTY</th>
<th>Part</th>
</tr>
</thead>
<tbody>
<tr>
<td>7998</td>
<td>Closed</td>
<td>10</td>
<td>7845</td>
</tr>
</tbody>
</table></div>
<p><code>tbInspectedParts</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>eRecordId</th>
<th>Disposition</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>7998</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>7998</td>
<td>5</td>
</tr>
<tr>
<td>3</td>
<td>7998</td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>7998</td>
<td>2</td>
</tr>
<tr>
<td>5</td>
<td>7998</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>7998</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>7998</td>
<td>4</td>
</tr>
<tr>
<td>8</td>
<td>7998</td>
<td>3</td>
</tr>
<tr>
<td>9</td>
<td>7998</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>7998</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>I am current running the query</p>
<pre><code>SELECT 
    tbInspectedParts.eMRBRecord,
    tbInspectedParts.Disposition,
    QTY, Part
FROM 
    [mrbDB].[dbo].[tbRecords] 
INNER JOIN 
    tbInspectedParts ON tbRecords.id = tbInspectedParts.eMRBRecord
WHERE 
    tbRecords.id = 7998
</code></pre>
<p>Which is returning this result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>eRecordId</th>
<th>Disposition</th>
<th>QTY</th>
<th>Part</th>
</tr>
</thead>
<tbody>
<tr>
<td>7998</td>
<td>4</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>5</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>2</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>2</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>1</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>3</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>4</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>3</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>3</td>
<td>10</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>3</td>
<td>10</td>
<td>7845</td>
</tr>
</tbody>
</table></div>
<p>This is taking the value of <code>tbRecord.QTY</code> which is correct because 10 parts were inspected from record 7998 but there are different disposition which I would like to quantify in a column.</p>
<p>Example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>eRecordId</th>
<th>Disposition</th>
<th>QTY</th>
<th>Part</th>
</tr>
</thead>
<tbody>
<tr>
<td>7998</td>
<td>4</td>
<td>2</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>5</td>
<td>1</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>2</td>
<td>2</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>1</td>
<td>1</td>
<td>7845</td>
</tr>
<tr>
<td>7998</td>
<td>3</td>
<td>4</td>
<td>7845</td>
</tr>
</tbody>
</table></div>
",1,1,0,2025-08-27T16:08:48+00:00,1,81,True
79748541,2819962,,sql,Entering Unicode codepoints in Informix,"<p>How would you enter a Unicode codepoint in an Informix char field?  I know you can use the octal like &quot;\011&quot; for a tab character.  How would I enter a Left Double Quotation Mark (U+201c)?  I would like to especially use it with the regex functions.</p>
<p>Here is a sample example.  I am hoping to use it as a constant in the where clause and possibly the select clause.  Something like the following.</p>
<pre><code>select 'U+2713', * from mytable where regex_match(myText, '[U+201c]');
</code></pre>
<hr />
<p>Edit 08/30/2025.</p>
<p>I am using Informix 11.10 or 12.X on Windows systems.  We have both versions install on different servers.  It depends on the location.</p>
<p>The real problem is what appears to be Unicode characters are starting to appear in our char and clob fields.  When it was just the clod fields, I figured the people were cutting and pasting from MS Word.  I have now found it outside of the CLOB fields in a regular text field like char(25).</p>
<p>I can see that there are characters there if I turn on show white spaces with DBeaver.  Without turning that on, it doesn't look like there is anything there.  On one of the Windows servers, it will actually show the characters.  Most of them are what you think they would be considering it probably came from word.  The majority are the fancy curly apostrophe and quote characters.  I can use the following SQL to find and even replace, but it is not fine grained enough to replace the fancy curly quotes with regular ASCII quotes.  I didn't think it was possible to store a UTF-8 in a char field.</p>
<pre><code>select counter, dtentered, notes, regex_replace(notes, '[^ -~\\012\\015\\011]', ' ')
from 
    mynotes
where
    date(dtentered) &gt; &quot;2025-08-01&quot;
    and regex_match(notes, '[^ -~\\012\\015\\011]')
order by dtentered desc;
</code></pre>
<p>the octal values are for the TAB, CR, and LF characters.</p>
",0,0,0,2025-08-27T23:43:56+00:00,0,121,False
79748742,30849230,,sql,Why UDF returns NULL for multi-row query but not for single row?,"<p>I ran the following query to retrieve data for a single investment ID:</p>
<pre class=""lang-sql prettyprint-override""><code>select *, getProductClass(InstrumentID, etf_option_mapper) as ProductClass 
from investor_pos 
where InvestorID = '7399000270'
</code></pre>
<p>It returned this result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>BrokerID</th>
<th>InvestorID</th>
<th>InstrumentID</th>
<th>YdPosition</th>
<th>Position</th>
<th>ProductClass</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>6001</td>
<td>7399000270</td>
<td>10008862</td>
<td>15</td>
<td>15</td>
<td>588000</td>
</tr>
</tbody>
</table></div>
<p>I then expanded the query to include multiple investment IDs:</p>
<pre class=""lang-sql prettyprint-override""><code>select *, getProductClass(InstrumentID, etf_option_mapper) as ProductClass 
from investor_pos 
where InvestorID in ('7399000270', '00130239')
</code></pre>
<p>Now I see this result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>BrokerID</th>
<th>InvestorID</th>
<th>InstrumentID</th>
<th>YdPosition</th>
<th>Position</th>
<th>ProductClass</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0089</td>
<td>00130239</td>
<td>IF2509</td>
<td>3</td>
<td>3</td>
<td>IF</td>
</tr>
<tr>
<td>1</td>
<td>0089</td>
<td>00130239</td>
<td>IM2509</td>
<td>-2</td>
<td>-2</td>
<td>IM</td>
</tr>
<tr>
<td>2</td>
<td>0089</td>
<td>00130239</td>
<td>IM2509</td>
<td>2</td>
<td>2</td>
<td>IM</td>
</tr>
<tr>
<td>3</td>
<td>0089</td>
<td>00130239</td>
<td>IF2509</td>
<td>-3</td>
<td>-3</td>
<td>IF</td>
</tr>
<tr>
<td>4</td>
<td>6001</td>
<td>7399000270</td>
<td>10008862</td>
<td>15</td>
<td>15</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Note the <code>ProductClass</code> value is now <code>NULL</code> on the last line. However, when I query for the single investment code, this specific data row returns a non-NULL result (corresponding to the above 588000 value).</p>
<p>Why would this produce different results? Could it be due to limitations of <code>iif</code> function when handling vectors versus scalars?</p>
<p>The complete reproducible script is provided below:</p>
<p>investor_pos.csv</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>BrokerID</th>
<th>InvestorID</th>
<th>InstrumentID</th>
<th>YdPosition</th>
<th>Position</th>
</tr>
</thead>
<tbody>
<tr>
<td>0089</td>
<td>00130239</td>
<td>IF2509</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>0089</td>
<td>00130239</td>
<td>IM2509</td>
<td>-2</td>
<td>-2</td>
</tr>
<tr>
<td>0089</td>
<td>00130239</td>
<td>IM2509</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>0089</td>
<td>00130239</td>
<td>IF2509</td>
<td>-3</td>
<td>-3</td>
</tr>
<tr>
<td>6001</td>
<td>7399000270</td>
<td>10008862</td>
<td>15</td>
<td>15</td>
</tr>
</tbody>
</table></div>
<p>etf_option_mapper</p>
<pre class=""lang-py prettyprint-override""><code>etf_option_mapper = dict(&quot;STRING&quot;, ANY)
etf_option_mapper[&quot;10008862&quot;] = [588000]
</code></pre>
<p>scripts</p>
<pre class=""lang-py prettyprint-override""><code>investor_pos = loadText(&quot;&lt;YourPath&gt;/nvestor_pos.csv&quot;)
def getProductClass(InstrumentID,etf_option_mapper){
    productClass = left(InstrumentID,4).regexReplace(&quot;[0-9]+&quot;,&quot;&quot;).regexReplace('HO','IH').regexReplace('IO','IF').regexReplace('MO','IM')
    return iif(isDigit(InstrumentID),etf_option_mapper[InstrumentID][0],productClass)
}
</code></pre>
<pre class=""lang-sql prettyprint-override""><code>select *,getProductClass(InstrumentID,etf_option_mapper) as ProductClass from investor_pos where InvestorID='7399000270'
select *,getProductClass(InstrumentID,etf_option_mapper) as ProductClass from investor_pos where InvestorID in ('7399000270','00130239')
</code></pre>
",2,2,0,2025-08-28T07:03:06+00:00,1,109,True
79749918,31259988,,sql,Calculate SUM over a primary key and between dates,"<p>My query:</p>
<pre><code>SELECT
    c.CustID,
    o.OrderID,
    SUM(ol.Qty * ol.Price) AS SUMOrder,
    AVG(SUM(ol.Qty * ol.Price)) OVER (PARTITION BY c.CustID) AS AVGAllOrders,
    COUNT(*) AS Countorders,
    SUM(SUM(ol.Qty * ol.Price)) OVER (PARTITION BY c.CustID) AS SumAllOrders
FROM
    Customer c
LEFT JOIN
    [Order] o ON o.CustID  c.CustID
LEFT JOIN
    OrderLine ol ON ol.OrderID = o.OrderID
GROUP BY
    c.CustID, o.OrderID
</code></pre>
<p><code>AVG</code> and <code>SUM</code> calculations need more depth. Besides calculating it over <code>CustID</code> I need to calculate it over 2 periods. So first half year and second half year. Is that possible with <code>PARTITION BY</code>?</p>
<p><code>Customer</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>CustID</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>10073</td>
<td>Test</td>
</tr>
<tr>
<td>10074</td>
<td>Test2</td>
</tr>
</tbody>
</table></div>
<p><code>Order</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>OrderID</th>
<th>CustID</th>
<th>OrderDate&amp;Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>VOR0060751</td>
<td>10073</td>
<td>2025-05-07 00:00:00.000</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>10073</td>
<td>2025-06-04 00:00:00.000</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>10073</td>
<td>2025-06-18 00:00:00.000</td>
</tr>
<tr>
<td>VOR0052554</td>
<td>10073</td>
<td>2024-07-17 00:00:00.000</td>
</tr>
</tbody>
</table></div>
<p><code>OrderLine</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>orderid</th>
<th>Orderline</th>
<th>OrderSubline</th>
<th>Qty</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>VOR0060751</td>
<td>010</td>
<td>000</td>
<td>1.000</td>
<td>15.60</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>010</td>
<td>005</td>
<td>1.000</td>
<td>12.27</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>020</td>
<td>000</td>
<td>1.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>020</td>
<td>005</td>
<td>1.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>030</td>
<td>000</td>
<td>1.000</td>
<td>16.19</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>030</td>
<td>005</td>
<td>1.000</td>
<td>8.87</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>040</td>
<td>000</td>
<td>1.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>040</td>
<td>005</td>
<td>1.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>050</td>
<td>000</td>
<td>1.000</td>
<td>23.49</td>
</tr>
<tr>
<td>VOR0060751</td>
<td>050</td>
<td>005</td>
<td>1.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>010</td>
<td>000</td>
<td>1.000</td>
<td>8.88</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>020</td>
<td>000</td>
<td>1.000</td>
<td>12.12</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>020</td>
<td>005</td>
<td>1.000</td>
<td>21.27</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>030</td>
<td>000</td>
<td>4.000</td>
<td>16.68</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>040</td>
<td>000</td>
<td>1.000</td>
<td>16.68</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>050</td>
<td>000</td>
<td>1.000</td>
<td>10.35</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>050</td>
<td>005</td>
<td>1.000</td>
<td>6.29</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>060</td>
<td>000</td>
<td>1.000</td>
<td>10.35</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>060</td>
<td>005</td>
<td>1.000</td>
<td>8.36</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>070</td>
<td>000</td>
<td>2.000</td>
<td>14.29</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>070</td>
<td>005</td>
<td>2.000</td>
<td>8.87</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>080</td>
<td>000</td>
<td>2.000</td>
<td>15.60</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>080</td>
<td>005</td>
<td>2.000</td>
<td>10.23</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>090</td>
<td>000</td>
<td>2.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>090</td>
<td>005</td>
<td>2.000</td>
<td>17.10</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>100</td>
<td>000</td>
<td>1.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>100</td>
<td>005</td>
<td>1.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>110</td>
<td>000</td>
<td>1.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>120</td>
<td>000</td>
<td>1.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>120</td>
<td>005</td>
<td>1.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>130</td>
<td>000</td>
<td>1.000</td>
<td>16.19</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>130</td>
<td>005</td>
<td>1.000</td>
<td>10.23</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>140</td>
<td>000</td>
<td>4.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>140</td>
<td>005</td>
<td>4.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>150</td>
<td>000</td>
<td>2.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>160</td>
<td>000</td>
<td>2.000</td>
<td>28.00</td>
</tr>
<tr>
<td>VOR0061499</td>
<td>160</td>
<td>005</td>
<td>2.000</td>
<td>38.45</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>010</td>
<td>000</td>
<td>1.000</td>
<td>6.58</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>010</td>
<td>005</td>
<td>1.000</td>
<td>6.07</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>020</td>
<td>000</td>
<td>1.000</td>
<td>6.58</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>020</td>
<td>005</td>
<td>1.000</td>
<td>6.07</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>030</td>
<td>000</td>
<td>1.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>040</td>
<td>000</td>
<td>2.000</td>
<td>10.35</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>040</td>
<td>005</td>
<td>2.000</td>
<td>8.36</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>050</td>
<td>000</td>
<td>1.000</td>
<td>14.29</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>050</td>
<td>005</td>
<td>1.000</td>
<td>8.87</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>060</td>
<td>000</td>
<td>1.000</td>
<td>15.60</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>060</td>
<td>005</td>
<td>1.000</td>
<td>10.23</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>070</td>
<td>000</td>
<td>1.000</td>
<td>18.19</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>070</td>
<td>005</td>
<td>1.000</td>
<td>17.10</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>080</td>
<td>000</td>
<td>2.000</td>
<td>16.19</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>080</td>
<td>005</td>
<td>2.000</td>
<td>8.36</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>090</td>
<td>000</td>
<td>2.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>090</td>
<td>005</td>
<td>2.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>100</td>
<td>000</td>
<td>2.000</td>
<td>20.53</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>100</td>
<td>005</td>
<td>2.000</td>
<td>18.01</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>105</td>
<td>000</td>
<td>1.000</td>
<td>0.00</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>110</td>
<td>000</td>
<td>1.000</td>
<td>23.49</td>
</tr>
<tr>
<td>VOR0061918</td>
<td>110</td>
<td>005</td>
<td>1.000</td>
<td>14.07</td>
</tr>
<tr>
<td>VOR0052554</td>
<td>010</td>
<td>000</td>
<td>2.000</td>
<td>14.23</td>
</tr>
</tbody>
</table></div>
<p>Desired result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Custid</th>
<th>OrderID</th>
<th>SUMOrder</th>
<th>AVGAllOrders LHY</th>
<th>Countorders LHY</th>
<th>SumAllOrders LHY</th>
<th>AVGAllOrders LHY2</th>
<th>Countorders LHY2</th>
<th>SumAllOrders LHY2</th>
</tr>
</thead>
<tbody>
<tr>
<td>10073</td>
<td>VOR0060751</td>
<td>161.29000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
<td>0.00000</td>
<td>0</td>
<td>0.00</td>
</tr>
<tr>
<td>10073</td>
<td>VOR0061499</td>
<td>702.04000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
<td>0.00000</td>
<td>0</td>
<td>0.00</td>
</tr>
<tr>
<td>10073</td>
<td>VOR0061918</td>
<td>318.19000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
<td>0.00000</td>
<td>0</td>
<td>0.00</td>
</tr>
<tr>
<td>10073</td>
<td>VOR0052554</td>
<td>28.46000</td>
<td>414.42333</td>
<td>3</td>
<td>1243.27000</td>
<td>28.46000</td>
<td>1</td>
<td>28.46000</td>
</tr>
</tbody>
</table></div>
",-1,2,3,2025-08-29T06:55:58+00:00,2,193,True
79750201,15136864,"Katowice, Polska",sql,How to create dynamic GORM query with Join avoiding SQL injection?,"<p>I'm trying to create query helpers for my repository pattern. Actually I'm dealing how to add join clauses. The only idea is to inject <code>Sprintf</code> statement but this behavior might cause SQL injection.</p>
<p>Here's my query builder</p>
<pre class=""lang-golang prettyprint-override""><code>func getQueryOptions(q *gorm.DB, opts ...query.QueryOption) *gorm.DB {
    options := query.NewQueryOptions(opts...)

    if joins := options.Joins; joins != nil {
        for _, join := range joins {
            q = q.Clauses(clause.Join{
                Type: clause.JoinType(join.Type),
                Table: clause.Table{
                    Name:  join.Table.Name,
                    Alias: join.Table.Alias,
                },
                ON: clause.Where{Exprs: []clause.Expression{
                    clause.Expr{SQL: join.On, Vars: join.Args},
                }},
            })
        }
    }
if filters := options.Filters; filters != nil {
        for _, filter := range filters {
            q = q.Where(filter.Condition, filter.Args...)
        }
    }
 [...]
}
</code></pre>
<p>here's my test</p>
<pre class=""lang-golang prettyprint-override""><code>dbGames, err := repo.FindAll(
                    query.WithJoin(query.JoinType.InnerJoin, &quot;platforms_games&quot;, &quot;pg&quot;, &quot;pg.game_id = games.id&quot;),
                    query.WithJoin(query.JoinType.InnerJoin, &quot;platforms&quot;, &quot;&quot;, &quot;pg.platform_id = platforms.id&quot;),
                    query.WithFilter(&quot;platforms.name = ?&quot;, &quot;Platform 1&quot;),
                )
                assert.NoError(t, err)
                assert.Len(t, dbGames, 1)
</code></pre>
<p>but it returns incorrect SQL query</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM &quot;games&quot; WHERE INNER JOIN &quot;platforms_games&quot; &quot;pg&quot; ON pg.game_id = games.id AND INNER JOIN &quot;platforms&quot; ON pg.platform_id = platforms.id AND platforms.name = 'Platform 1'
</code></pre>
",1,1,0,2025-08-29T11:10:23+00:00,1,152,True
79750659,320881,"Washington, United States",sql,Convert Month/Day/Year columns to date to check date in a SQL Query,"<p>I have the following SQL query that works correctly.</p>
<pre><code>// date is a .Net DateTime parameter.
    dataTable = Database.GetDataTable(&quot;SELECT Distinct ElementID &quot; + 
    &quot;FROM [DatabaseName].[dbo].[DataTable] dbd &quot; +
    &quot;JOIN [DatabaseName].[dbo].[DateaAndTimeTable] tsr &quot; +
    &quot;ON tsr.AccessionID = dbd.AccessionID &quot; +
    &quot;where (tsr.TSMonth = &quot; +
    date.Month.ToString() +
    &quot; AND tsr.TSDay = &quot; +
    date.Day.ToString() +
    &quot; AND tsr.TSYear = &quot; +
    date.Year.ToString() +
    &quot; AND tsr.TSHour = &quot; +
    date.Hour.ToString() +
    &quot; AND ISNUMERIC(LEFT(ElementID,2)) = 1&quot; +
    &quot;) &quot; +
    &quot;ORDER BY dbd.ElementID&quot;,
    DataConnectionString);
</code></pre>
<p>I need to update this to get a range of dates between a given start date and end date.</p>
<p>The 'DateAndTimeTable' has values for year, month, day, hour, minute and seconds.</p>
<p>I'm thinking I can cast the TSMonth, TSDay, TSYear, TSHour and TSMinute from DateAndTimeTable to a SMALLDATETIME, but I have never done this. These columns are all tinyint except TSYear is a small int.</p>
<p>I have only ever casted one string column to a SMALLDATETIME.</p>
<p>Is there a way to cast multiple columns into one value, to convert it to a SMALLDATETIME within one SQL query?</p>
",1,3,2,2025-08-29T18:27:13+00:00,3,212,True
79750698,3149869,,sql,Window function issue when more than one window function consecutively used in query,"<p>I created a collection named ‘orders’ with the below DDL using NewSQL Interface:</p>
<pre><code>CREATE TABLE orders 
(
    orderId STRING PRIMARY KEY,
    customerId STRING,
    orderDate TIMESTAMP,
    totalAmount DOUBLE,
    status STRING
); 
</code></pre>
<p>I am trying to get the previous and the following order date for a customer using the windows function, using the below query but I am getting an error 'Invalid Query'.</p>
<pre><code>SELECT
    ord.*,
    LEAD(orderDate, 1) OVER (PARTITION BY customerId ORDER BY orderDate) AS nextOrderDate,
    LAG(orderDate, 1) OVER (PARTITION BY customerId ORDER BY orderDate) AS prevOrderDate
FROM
    orders ord
ORDER BY
    customerID, orderDate;
</code></pre>
<p>However when I use just one window function in the query it works fine. When there are more than one it errors out. I can create outer queries and use the window function to derive one field at a time, but I am looking to do it in one single query.</p>
<p>Using this way it works:</p>
<pre><code>SELECT
    a.*,
    LEAD(orderDate, 1) OVER (PARTITION BY customerId ORDER BY orderDate) AS nextOrderDate
FROM 
    (SELECT
         ord.*,
         -- LEAD(orderDate, 1) OVER (PARTITION BY customerId ORDER BY orderDate) AS nextOrderDate,
         LAG(orderDate, 1) OVER (PARTITION BY customerId ORDER BY orderDate) AS prevOrderDate
     FROM
         orders ord) a
ORDER BY
    customerID, orderDate;
</code></pre>
",-2,0,2,2025-08-29T19:06:33+00:00,1,66,True
79750849,1200713,,sql,Can&#39;t specify target table for update in FROM clause,"<pre><code>UPDATE
  `details_audit` a
SET
  a.count_changes_below_100 = (
    SELECT
      COUNT(*)
    FROM
      `details_audit`
    WHERE
      sort_id &lt; 100
  );
</code></pre>
<p>This worked on a previous version of MySQL but not with version 8.0.43:</p>
<blockquote>
<p>Error Code: 1093<br />
You can't specify target table 'a' for update in FROM clause</p>
</blockquote>
<p>How can I solve this?</p>
",2,2,0,2025-08-29T23:44:21+00:00,2,141,True
79751843,2853583,,sql,postgresql complex grouping within json_build_object + sum,"<p>Please help me with a request. I have two tables. The first table stores data on the user's use of the application for a certain period of time.<br />
And I have a table of granules, which I compiled using a query on the first table.<br />
Screen my tables:<br />
<a href=""https://i.sstatic.net/f5KRpdW6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f5KRpdW6.png"" alt=""enter image description here"" /></a><br />
<a href=""https://i.sstatic.net/85H0BWTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/85H0BWTK.png"" alt=""enter image description here"" /></a><br />
Example value for field app_info:<br />
<a href=""https://i.sstatic.net/xV0oF2Ri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xV0oF2Ri.png"" alt=""enter image description here"" /></a><br />
But I need to get the resulting query, which for each granule will create an array of json for the programs used during the granule of time. I also need to group the programs by name or domain_site field and create an array of headers. For example, a user used the browser for 10 minutes, opening different tabs. We then need to get the following json:</p>
<pre class=""lang-json prettyprint-override""><code>            [
          {
            &quot;app_name&quot;: &quot;googlechrome&quot;,
            &quot;path&quot;: &quot;C:\\ProgramFiles\\google\\googlechrome.exe&quot;,
            &quot;domain_site&quot;: &quot;stackoverfloww.com&quot;,
            &quot;count_seconds&quot;: 540,
            &quot;titles&quot;: [
              {
    &quot;count_seconds&quot;: 320,
                &quot;url&quot;: &quot;https://stackoverflow.com/questions/40978290/construct-json-object-from-query-with-group-by-sum&quot;,
                &quot;title&quot;: &quot;Construct json object from query with group by / sum&quot;
              },
              {
    &quot;count_seconds&quot;: 220,
                &quot;url&quot;: &quot;https://stackoverflow.com/questions/43117033/aggregate-function-calls-cannot-be-nested-postgresql&quot;,
                &quot;title&quot;: &quot;aggregate function calls cannot be nested postgresql&quot;
              }
            ]
          }
        ]
</code></pre>
<p>And if the user used several applications in 10 minutes, then for each application such statistics. For each application, you need to count the number of seconds of its use.<br />
I expect to get such a table:<br />
<a href=""https://i.sstatic.net/eu9JjyvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eu9JjyvI.png"" alt=""enter image description here"" /></a></p>
<p>I can't cope with such a request. Here is the request I wrote:</p>
<pre class=""lang-sql prettyprint-override""><code>        select 
    employee_id,
    date,
    granula_start,
    granula_end,
    
    (select 
        array_agg(json_build_object('seconds', SUM( case 
                                          when end_time &gt; granula_end 
                                          then ((EXTRACT(MINUTE FROM granula_end) - EXTRACT(MINUTE FROM start_time))*60) 
                                          else (case  when EXTRACT(MINUTE FROM start_time) = EXTRACT(MINUTE FROM end_time) 
                                                      then 
                                                       EXTRACT(SECOND FROM end_time) - EXTRACT(SECOND FROM start_time)
                                                      else
                                                       (EXTRACT(MINUTE FROM end_time) - EXTRACT(MINUTE FROM start_time)) * 60 
                                               end )
                                          end ),
                          'domain_site', m.app_info::jsonb-&gt;'domain_site',
                          'app_name', m.app_info::jsonb-&gt;'app_name',
                          'app_type', m.app_info::jsonb-&gt;'app_type' )) as app_info
     from pps.my_temp m
     where start_time &gt;= granula_start and start_time &lt;= granula_end
     group by m.app_info::jsonb-&gt;'domain_site', m.app_info::jsonb-&gt;'app_name'
     )
     
    from granules
</code></pre>
<p>But I get an error...<br />
<a href=""https://i.sstatic.net/JXPmiI2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JXPmiI2C.png"" alt=""enter image description here"" /></a></p>
",-3,0,3,2025-08-31T15:21:51+00:00,2,113,True
79752209,5272283,"New Delhi, Delhi, India",sql,node js + postgres service returning vastly different SQL row count,"<p>I've created a backend service using node 18 for a frontend dashboard which requires execution of various types of filtering, grouping, export data SQLs to be executed on the postgres DB. For the export functionality, my backend first gets the count of total rows to be exported (for sending the export progress update) and then executes the actual SQL whose data is streamed to the front end. I tried 2 consecutive exports with same variables. The count returned by the SQLs in both cases is almost similar but the actual data returned by the main SQLs is vastly different.</p>
<p>Queries are being executed to the same DB and using the same DB connection pool.</p>
<p>Count SQLs in both cases :</p>
<pre><code>SELECT COUNT(*) as count 
FROM table_a a JOIN table_b b ON a.id = b.id
WHERE a.log_date_time &gt;= NOW() - CAST('15 days' AS INTERVAL)
</code></pre>
<p>Row Count 1 : 679045</p>
<p>Row Count 2 : 679270 (2 minutes later)</p>
<p>Main Data SQLs in both cases :</p>
<pre><code>SELECT TO_CHAR(a.log_date_time, 'YYYY-MM-DD HH24:MI:SS.MS') as log_date_time, ...
FROM table_a a JOIN table_b b ON a.id = b.id
WHERE a.log_date_time &gt;= NOW() - CAST('15 days' AS INTERVAL) 
ORDER BY a.log_date_time DESC
</code></pre>
<p>Row Count 1 : 677414</p>
<p>Row Count 2 : 501708 (2 minutes later)</p>
",-3,0,3,2025-09-01T06:15:40+00:00,1,95,False
79753336,22151564,,sql,Removing duplicates from combined tables with specific criteria,"<p>I need to remove duplicates in two combined tables using a specific criterion. Here is an example:</p>
<p>Table 1</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>EmployeeId</th>
<th>Name</th>
<th>Item</th>
<th>Payment</th>
</tr>
</thead>
<tbody>
<tr>
<td>67488</td>
<td>Ted</td>
<td>Pencil</td>
<td>Cash</td>
</tr>
<tr>
<td>85596</td>
<td>Marshall</td>
<td>Pencil</td>
<td>Cash</td>
</tr>
<tr>
<td>20036</td>
<td>Lily</td>
<td>Ballpen</td>
<td>Cash</td>
</tr>
</tbody>
</table></div>
<p>Table 2</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>EmployeeId</th>
<th>Name</th>
<th>Item</th>
<th>Payment</th>
</tr>
</thead>
<tbody>
<tr>
<td>67488</td>
<td>Ted</td>
<td>Pencil</td>
<td>Card</td>
</tr>
<tr>
<td>85596</td>
<td>Marshall</td>
<td>Folder</td>
<td>Card</td>
</tr>
<tr>
<td>90266</td>
<td>Barney</td>
<td>Folder</td>
<td>Card</td>
</tr>
<tr>
<td>20036</td>
<td>Lily</td>
<td>Book</td>
<td>Card</td>
</tr>
</tbody>
</table></div>
<p>New Table</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>EmployeeId</th>
<th>Name</th>
<th>Item</th>
<th>Payment</th>
</tr>
</thead>
<tbody>
<tr>
<td>67488</td>
<td>Ted</td>
<td>Pencil</td>
<td>Cash</td>
</tr>
<tr>
<td>85596</td>
<td>Marshall</td>
<td>Pencil</td>
<td>Cash</td>
</tr>
<tr>
<td>20036</td>
<td>Lily</td>
<td>Ballpen</td>
<td>Cash</td>
</tr>
<tr>
<td>85596</td>
<td>Marshall</td>
<td>Folder</td>
<td>Card</td>
</tr>
<tr>
<td>90266</td>
<td>Barney</td>
<td>Folder</td>
<td>Card</td>
</tr>
<tr>
<td>20036</td>
<td>Lily</td>
<td>Book</td>
<td>Card</td>
</tr>
</tbody>
</table></div>
<p>Table 1 are Cash transactions, Table 2 are Card transactions. I need to combine the tables but keeping the cash transaction if there are duplicates from columns 1-3. Ted purchase pencil twice using two modes of payment and I only want to keep the cash transaction.</p>
<p>Can someone help?</p>
<p>I tried <code>Coalesce</code> but it only keeps the first table.</p>
",0,1,1,2025-09-02T10:04:15+00:00,3,131,True
79753404,11515453,,sql,SQL get all items that belong to the same user group of the requesting user,"<p>I wish figure out how to get all appointments that belong to the same user group of the requesting user in just one SQL query.
My only data is the user_id of the requesting user.</p>
<p>Minimal reproducable example:</p>
<pre><code>appointments
----------------
id    user_id
1    2
2    3
3    1
4    2
5    2
</code></pre>
<pre><code>users
----------------
id    group_id
1    5
2    8
3    8
</code></pre>
<p>Task:
I'm user 3 and want to get all appointments that are from users in my group.
user 3 is in group 8, so is user 2.
appointments that belong to user 2 and 3 are: 1,2,4,5</p>
<p>What I have done so far, is using two SQL statements:</p>
<pre><code>SELECT group_id
FROM users
WHERE id = 3
</code></pre>
<p>then knowing my group_id:</p>
<pre><code>SELECT a.*
FROM appointments a
JOIN users u ON a.user_id = u.id
WHERE u.group_id = my_group_id
</code></pre>
",4,4,0,2025-09-02T10:58:17+00:00,5,200,True
79753976,6282952,,sql,Most efficient identification and concise expression of a set of disjoint elements,"<p>I am attempting to determine how to most efficiently identify the sets consisting of the least number of terms for a group of disjoint sets.
I have a set of Boxes (B##) and Cartons (S##) which are related.
A search of either Boxes or Cartons or both can be performed to identify relationships.
Search elements can be either singular or multi-term.
When a search consists of both Boxes and Cartons then there is a potential for one or more disjoint sets.
I want to form a set of statements which declares any non-relationships (disjoints) for the search terms.
The set of statements should consist of the least number of terms.
Example ...</p>
<p>Relationships:</p>
<pre><code>B01 C01
B01 C02
B01 C03
B02 C01
B03 C01
B04 C04
B04 C05
B05 C01
B05 C02
B05 C06
B06 C04
B06 C05
B06 C06
B07 C07
B08 C02
B09 C08
B09 C09
</code></pre>
<p>Search:</p>
<pre><code>B01,B02,B03,B07
C01,C02,C03,C05
</code></pre>
<p>NON-Relationships as derived from Search:</p>
<pre><code>B01 C05
B02 C02
B02 C03
B02 C05
B03 C02
B03 C03
B03 C05
B07 C01
B07 C02
B07 C03
B07 C05
</code></pre>
<p>Result:</p>
<pre><code>Option01 (non-repetitive Box; 12 terms)
    B01             C05
    B02,B03         C02,C03,C05
    B07             C01,C02,C03,C05
Option02 (non-repetitive Carton; 12 terms)
    B01,B02,B03,B07 C05
    B02,B03,B07     C02,C03
    B07             C01
Option03 (hybrid; 10 terms)
    B01             C05
    B02,B03,B07     C02,C03,C05
    B07             C01
</code></pre>
<p>Option03 is the desired solution as it consists of a total of 10 non-unique terms.
The final statements would be:</p>
<pre><code>Never (B01)(C05).
Never (B02,B03,B07)(C02,C03,C05).
Never (B07)(C01).
</code></pre>
<p>I used stuff and xml-path to create a list for each set of disjoints for each Box and Carton which provides a clear-path via grouping to Option01 and Option02, respectively.
However, I cannot determine how to get from that to the desired Option03.</p>
<pre><code>Box     Disjoint-list
B01     C05
B02     C02,C03,C05
B03     C02,C03,C05
B07     C01,C02,C03,C05

Carton  Disjoint-list
C01     B07
C02     B02,B03,B07
C03     B02,B03,B07
C05     B01,B02,B03,B07
</code></pre>
<pre><code>
        with
            cteRelationship as (
                select
                    Box,
                    Carton
                from (
                    values
                        ('B01', 'C01'),
                        ('B01', 'C02'),
                        ('B01', 'C03'),
                        ('B02', 'C01'),
                        ('B03', 'C01'),
                        ('B04', 'C04'),
                        ('B04', 'C05'),
                        ('B05', 'C01'),
                        ('B05', 'C02'),
                        ('B05', 'C06'),
                        ('B06', 'C04'),
                        ('B06', 'C05'),
                        ('B06', 'C06'),
                        ('B07', 'C07'),
                        ('B08', 'C02'),
                        ('B09', 'C08'),
                        ('B09', 'C09')
                    ) as X (Box, Carton)
                ),
            cteSearchBox as (
                select
                    [Value]
                from (
                    values
                        ('B01'),
                        ('B02'),
                        ('B03'),
                        ('B07')
                    ) as X ([Value])
                ),
            cteSearchCarton as (
                select
                    [Value]
                from (
                    values
                        ('C01'),
                        ('C02'),
                        ('C03'),
                        ('C05')
                    ) as X ([Value])
                ),
            cteDisjointSet as (
                select
                    S.Box,
                    S.Carton
                from (
                    select
                        cSB.[Value] as Box,
                        cSC.[Value] as Carton
                    from cteSearchBox as cSB
                    cross join cteSearchCarton as cSC
                    ) as S
                cross apply (
                    select
                        1 as M
                    from cteRelationship as cR
                    where 1 = 1
                        and cR.Box = S.Box
                        and cR.Carton = S.Carton
                    having count(1) = 0
                    ) as caR
                )
            select
                *
            from cteDisjointSet
            order by
                Box,
                Carton;
</code></pre>
",1,1,0,2025-09-02T21:38:15+00:00,1,127,True
79754036,3183213,,sql,Pivot multiple columns,"<p>I have a vertical table that  end users want converted to a horizontal table with some groupings. I'm writing this in SQL Server 2019.</p>
<p>My table looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>PKID</th>
<th>cID</th>
<th>vID</th>
<th>vDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>81</td>
<td>1996-04-04</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>82</td>
<td>1999-05-20</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>81</td>
<td>2000-01-01</td>
</tr>
<tr>
<td>4</td>
<td>3</td>
<td>82</td>
<td>2005-03-17</td>
</tr>
<tr>
<td>5</td>
<td>3</td>
<td>18</td>
<td>2010-08-30</td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>19</td>
<td>2015-02-01</td>
</tr>
</tbody>
</table></div>
<p>The desired output would be:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>cID</th>
<th>vID_1</th>
<th>vDate_1</th>
<th>vID_2</th>
<th>v_Date_2</th>
<th>vID_3</th>
<th>vDate_3</th>
<th>vID_4</th>
<th>vDate_4</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>81</td>
<td>1996-04-04</td>
<td>82</td>
<td>1999-05-20</td>
<td>NULL</td>
<td>NULL</td>
<td>NULL</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>81</td>
<td>2000-01-001</td>
<td>82</td>
<td>2005-03-17</td>
<td>18</td>
<td>2010-08-30</td>
<td>19</td>
<td>2015-02-01</td>
</tr>
</tbody>
</table></div>
<p>I have code to get me part of the way there but I can't figure out how to get all of it. The below code will produce this table. The dates in the second row should also be sorted ascending. But the bigger issue is that when I try to add vID_1, vID_2, etc. it adds rows for each record. I know I'm missing something simple but cannot figure it out.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>cID</th>
<th>vDate_1</th>
<th>vDate_2</th>
<th>vDate_3</th>
<th>vDate_4</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>1996-04-04</td>
<td>1999-05-20</td>
<td>NULL</td>
<td>NULL</td>
</tr>
<tr>
<td>3</td>
<td>2010-08-30</td>
<td>2015-02-01</td>
<td>2000-01-01</td>
<td>2005-03-17</td>
</tr>
</tbody>
</table></div>
<pre><code>  CREATE TABLE #tmp (PKID int, cID int, vID int, vDate date)
  INSERT INTO #tmp (PKID, cID, vID, vDate)
  VALUES (1,2,81,'1996-04-04'),
        (2,2,81,'1999-05-20'),
        (3,3,81,'2000-01-01'),
        (4,3,82,'2005-03-17'),
        (5,3,18,'2010-08-30'),
        (6,3,19,'2015-02-01');

SELECT cID
        ,[1] AS vDate_1
        ,[2] AS vDate_2
        ,[3] AS vDate_3
        ,[4] AS vDate_4
FROM (
SELECT cID
        ,vDate
        ,ROW_NUMBER() OVER (PARTITION BY cID ORDER BY vID) AS RN
    FROM #tmp
    GROUP BY cID,vID, vDate
) vax
PIVOT
(   
    MAX(vDate)
    FOR RN IN ([1],[2],[3],[4])
) as pvt

DROP TABLE #tmp;
</code></pre>
",1,2,1,2025-09-03T00:01:55+00:00,2,130,True
79754325,8064427,,sql,Cumulative Stock Query without the participation of an Initial Stock Table,"<p>I'm designing a stock management program, here I have an <strong>Incoming Table</strong> like below:</p>
<ul>
<li><strong>id</strong> (<em>integer</em>)</li>
<li><strong>date</strong> (<em>date</em>)</li>
<li><strong>type</strong> (<em>varchar</em>)</li>
<li><strong>warehouse_id</strong> (<em>tinyint</em>)</li>
<li><strong>goods_id</strong> (<em>integer</em>)</li>
<li><strong>quantity</strong> (<em>decimal</em>)</li>
<li><strong>price</strong> (<em>decimal</em>)</li>
<li><strong>amount</strong> (<em>decimal</em>)</li>
</ul>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>id</strong></th>
<th style=""text-align: center;""><strong>date</strong></th>
<th style=""text-align: center;""><strong>type</strong></th>
<th style=""text-align: center;""><strong>warehouse_id</strong></th>
<th style=""text-align: center;""><strong>goods_id</strong></th>
<th style=""text-align: center;""><strong>quantity</strong></th>
<th style=""text-align: center;""><strong>price</strong></th>
<th style=""text-align: center;""><strong>amount</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">¥1.50</td>
<td style=""text-align: center;"">¥300</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">¥1.50</td>
<td style=""text-align: center;"">¥450</td>
</tr>
</tbody>
</table></div>
<p>And an <strong>Outcoming Table</strong> like below:</p>
<ul>
<li><strong>id</strong> (<em>integer</em>)</li>
<li><strong>incoming_id</strong> (<em>integer</em>)</li>
<li><strong>date</strong> (<em>date</em>)</li>
<li><strong>type</strong> (<em>varchar</em>)</li>
<li><strong>quantity</strong> (<em>decimal</em>)</li>
</ul>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>id</strong></th>
<th style=""text-align: center;""><strong>incoming_id</strong></th>
<th style=""text-align: center;""><strong>date</strong></th>
<th style=""text-align: center;""><strong>type</strong></th>
<th style=""text-align: center;""><strong>quantity</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-2</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">50</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-3</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-4</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">30</td>
</tr>
<tr>
<td style=""text-align: center;"">4</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-6</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">70</td>
</tr>
<tr>
<td style=""text-align: center;"">5</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-7</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">90</td>
</tr>
</tbody>
</table></div>
<p>And I can easily design a query to combine the Incoming &amp; Outcoming Tables as below:</p>
<pre><code>WITH cte AS (
    SELECT id, incoming_id, date, type, quantity, SUM(quantity) OVER w AS cum_qty
    FROM outcoming
    WINDOW w AS (PARTITION BY incoming_id ORDER BY date, id)
)
SELECT A.id, A.date AS in_date, A.type AS in_type, A.warehouse_id AS wh_id, A.goods_id AS g_id, A.quantity AS in_qty, B.date AS out_date, B.type AS out_type, B.quantity AS out_qty, B.cum_qty, A.quantity-IFNULL(B.cum_qty,0) AS left_qty
FROM incoming AS A
LEFT JOIN cte AS B ON A.id=B.incoming_id
ORDER BY A.date, A.id, B.date, B.id
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>id</strong></th>
<th style=""text-align: center;""><strong>in_date</strong></th>
<th style=""text-align: center;""><strong>in_type</strong></th>
<th style=""text-align: center;""><strong>wh_id</strong></th>
<th style=""text-align: center;""><strong>g_id</strong></th>
<th style=""text-align: center;""><strong>in_qty</strong></th>
<th style=""text-align: center;""><strong>out_date</strong></th>
<th style=""text-align: center;""><strong>out_type</strong></th>
<th style=""text-align: center;""><strong>out_qty</strong></th>
<th style=""text-align: center;""><strong>cum_qty</strong></th>
<th style=""text-align: center;""><strong>left_qty</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">25-8-2</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">150</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">25-8-3</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">60</td>
<td style=""text-align: center;"">110</td>
<td style=""text-align: center;"">90</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">25-8-4</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: center;"">140</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">25-8-6</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">230</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">25-8-7</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">90</td>
<td style=""text-align: center;"">160</td>
<td style=""text-align: center;"">140</td>
</tr>
</tbody>
</table></div>
<p>My question is how can I achieve a <strong>Cumulative Stock Query</strong> across the Incoming parents to have the following effect, and without the participation of an Initial Stock Table?</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>wh_id</strong></th>
<th style=""text-align: center;""><strong>g_id</strong></th>
<th style=""text-align: center;""><strong>date</strong></th>
<th style=""text-align: center;""><strong>type</strong></th>
<th style=""text-align: center;""><strong>in_qty</strong></th>
<th style=""text-align: center;""><strong>out_qty</strong></th>
<th style=""text-align: center;""><strong>cum_in_qty</strong></th>
<th style=""text-align: center;""><strong>cum_out_qty</strong></th>
<th style=""text-align: center;""><strong>left_qty</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purchase</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">200</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-2</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">150</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-3</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">60</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">110</td>
<td style=""text-align: center;"">90</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-4</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">140</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purchase</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">500</td>
<td style=""text-align: center;"">140</td>
<td style=""text-align: center;"">360</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-6</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">500</td>
<td style=""text-align: center;"">210</td>
<td style=""text-align: center;"">290</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-7</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">90</td>
<td style=""text-align: center;"">500</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">200</td>
</tr>
</tbody>
</table></div>
<p>I don't want an <strong>Initial Stock Table</strong>, because that'll demand a Settlement operation to generate Initial Stock records every month, and the <strong>Incoming Table</strong> is acting as a de facto <strong>Initial Stock Table</strong>.</p>
<p>Thanks a lot in advance.</p>
",3,3,0,2025-09-03T08:26:39+00:00,2,83,True
79754363,10520566,,sql,Querying tag values at specific event times efficiently,"<p>I have an event tag that logs events, and a temperature tags that records data every 0.5 seconds. I want to know the temperature at the start of each event.</p>
<p>I'm using Microsoft SQL Server Management Studio 20 to query a Historian database system via a linked server, and I'm using this query for getting the events:</p>
<pre><code>SET QUOTED_IDENTIFIER OFF
SELECT * FROM OPENQUERY(INSQL, &quot;SELECT DateTime, [MyTag]
 FROM History
 WHERE [EventValye] = 50
 AND wwRetrievalMode = 'Full'
 AND wwQualityRule = 'Extended'
 AND wwVersion = 'Latest'
 AND DateTime &gt;= '20240101 00:00:00.000'
 AND DateTime &lt;= '20250101 00:05:00.000'&quot;)
</code></pre>
<p>Currently I tried using <code>OUTER APPLY TOP 1</code> and match the DateTimes, but this will take very slow becaust with retrieval mode <code>&quot;FULL&quot;</code> it fetches all records first.</p>
<pre><code>SET QUOTED_IDENTIFIER OFF;

WITH EventTag AS (
    SELECT *
    FROM OPENQUERY(INSQL, '
        SELECT DateTime, [EventValue]
        FROM History
        WHERE wwRetrievalMode = ''Full''
          AND wwQualityRule = ''Extended''
          AND wwVersion = ''Latest''
          AND DateTime &gt;= ''2024-01-01 00:00:00.000''
          AND DateTime &lt;= ''2025-01-01 00:05:00.000''
    ')
),
TempTag AS (
    SELECT *
    FROM OPENQUERY(INSQL, '
        SELECT DateTime, [TemperatureValue]
        FROM History
        WHERE wwRetrievalMode = ''Full''
          AND wwQualityRule = ''Extended''
          AND wwVersion = ''Latest''
          AND DateTime &gt;= ''2024-01-01 00:00:00.000''
          AND DateTime &lt;= ''2025-01-01 00:05:00.000''
    ')
)
SELECT 
    e.DateTime,
    e.[EventValue] AS Event,
    temp.[TemperatureValue] AS Temperature
FROM EventTag AS e
OUTER APPLY (
    SELECT TOP 1 *
    FROM TempTag
    WHERE DateTime &lt;= e.DateTime
    ORDER BY DateTime DESC
) AS temp
WHERE e.[EventValue] = 50
ORDER BY e.DateTime;
</code></pre>
<p>Splitting it into a cyclic style query, spacing every 10 seconds will not be precise enough.</p>
<p>Is there a more efficient way to get the temperature at these event DateTimes <strong>without scanning the entire temperature dataset?</strong></p>
",0,0,0,2025-09-03T08:56:18+00:00,1,155,False
79754689,31410573,,sql,Splitting and aggregating over time with BigQuery,"<p>Trying to write a query which will convert the input, which is list of <em>events</em> with a start and end time as well as the  number of <em>apples</em> for that time slot. Example input as is shown below (timings are timestamps in UTC).</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>event_id</th>
<th>start_time</th>
<th>end_time</th>
<th>apples</th>
</tr>
</thead>
<tbody>
<tr>
<td>23</td>
<td>2025-08-25 12:00</td>
<td>2025-08-25 13:00</td>
<td>60</td>
</tr>
<tr>
<td>45</td>
<td>2025-08-25 12:30</td>
<td>2025-08-25 13:00</td>
<td>100</td>
</tr>
<tr>
<td>87</td>
<td>2025-08-25 12:30</td>
<td>2025-08-25 12:45</td>
<td>25</td>
</tr>
</tbody>
</table></div>
<p>The finest granularity for the start and end times are 15 minute slots. The goal is to have an overview with all individual 15 minute slots complemented with the total number of apples for that slot. For this the periods of the <em>events</em> will first need to be split into 15 minute intervals.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>start_time</th>
<th>end_time</th>
<th>apples</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-08-25 12:00</td>
<td>2025-08-25 12:15</td>
<td>60</td>
</tr>
<tr>
<td>2025-08-25 12:15</td>
<td>2025-08-25 12:30</td>
<td>60</td>
</tr>
<tr>
<td>2025-08-25 12:30</td>
<td>2025-08-25 12:45</td>
<td>185</td>
</tr>
<tr>
<td>2025-08-25 12:45</td>
<td>2025-08-25 13:00</td>
<td>160</td>
</tr>
</tbody>
</table></div>
<p>Find below the CTE with the data as above:</p>
<pre><code>WITH
  input_data AS (
  SELECT
    23 AS event_id,
    PARSE_TIMESTAMP('%Y-%m-%d %H:%M', '2025-08-25 12:00') AS start_time,
    PARSE_TIMESTAMP('%Y-%m-%d %H:%M', '2025-08-25 13:00') AS end_time,
    60 AS apples,
  UNION ALL
  SELECT
    45,
    PARSE_TIMESTAMP('%Y-%m-%d %H:%M', '2025-08-25 12:30'),
    PARSE_TIMESTAMP('%Y-%m-%d %H:%M', '2025-08-25 13:00'),
    100,
  UNION ALL
  SELECT
    87,
    PARSE_TIMESTAMP('%Y-%m-%d %H:%M', '2025-08-25 12:30'),
    PARSE_TIMESTAMP('%Y-%m-%d %H:%M', '2025-08-25 12:45'),
    25,
    )
SELECT
  inp.event_id,
  inp.start_time,
  inp.end_time,
  inp.apples
FROM
  input_data inp;
</code></pre>
<p>Managed to do the first step; splitting the event start and end times into 15 minute intervals using GENERATE_TIMESTAMP_ARRAY.</p>
<p>Open for potential suggestions to improve how the number of apples are included in the array next to the timestamps after splitting in the snippet below.. It currently feels a bit of overkill to (again) use GENERATE_TIMESTAMP_ARRAY to just create an array with a single value being repeated.</p>
<pre><code>SELECT
  inp.event_id,
  STRUCT( GENERATE_TIMESTAMP_ARRAY( inp.start_time, TIMESTAMP_SUB(inp.end_time, INTERVAL 15 MINUTE), INTERVAL 15 MINUTE ) AS start_time,
    (
    SELECT
      ARRAY_AGG(inp.apples)
    FROM
      UNNEST(GENERATE_TIMESTAMP_ARRAY( inp.start_time, TIMESTAMP_SUB(inp.end_time, INTERVAL 15 MINUTE), INTERVAL 15 MINUTE ))) AS apples ) AS time_interval
FROM
  input_data inp;
</code></pre>
<p>This will end up with what I would consider the desired intermediate step:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>event_id</th>
<th>time_interval.start_time</th>
<th>time_interval.apples</th>
</tr>
</thead>
<tbody>
<tr>
<td>23</td>
<td>2025-08-25 12:00</td>
<td>60</td>
</tr>
<tr>
<td></td>
<td>2025-08-25 12:15</td>
<td>60</td>
</tr>
<tr>
<td></td>
<td>2025-08-25 12:30</td>
<td>60</td>
</tr>
<tr>
<td></td>
<td>2025-08-25 12:45</td>
<td>60</td>
</tr>
<tr>
<td>45</td>
<td>2025-08-25 12:30</td>
<td>100</td>
</tr>
<tr>
<td></td>
<td>2025-08-25 12:45</td>
<td>100</td>
</tr>
<tr>
<td>87</td>
<td>2025-08-25 12:30</td>
<td>25</td>
</tr>
</tbody>
</table></div>
<p>But now stuck on how to do the aggregation over time_interval.start_time (event_id can be ignored) to end up with the overview of number of apples per 15 minute period.</p>
",0,0,0,2025-09-03T13:59:50+00:00,0,77,False
79754727,12149443,,sql,Using a dynamic query in an Oracle stored procedure,"<p>I am trying to run something similar to this within a stored procedure in Oracle.</p>
<p><code>pWhereCLAUSE</code> is generic which I set depending on the table I am working on. The following code runs just if I have the proper defined where clause, but If I try to use this variable, I keep getting errors. I have tried to use <code>||</code>, single quotes, but nothing has worked. I am not an expert at stored procedures. But any help would be appreciated. I am just trying to figure how I can keep this generic and have this code work with <code>pWhereCLAUSE</code> working</p>
<pre><code>INSERT INTO ABC
    SELECT * 
    FROM ABCD 
    pWhereCLAUSE to_char(dateOne, 'DD-MON-YYYY') and ROWNUM &lt;= variable1;
commit;
</code></pre>
<p>I have also tried:</p>
<pre><code>BEGIN    
    sql_stmt := 'INSERT INTO TABLS_AB SELECT * FROM TABLE_C' || pWhereClause || to_char(offsetDate, 'DD-MON-YYYY');  
END;
EXECUTE IMMEDIATE sql_stmt; 
</code></pre>
<p>The procedure compiles just fine, and this never executes. Please assist.</p>
",3,3,0,2025-09-03T14:23:24+00:00,1,114,True
79755445,2663853,,sql,Update where current of cursor in a PL/pgSQL procedure. ERROR: cursor does not exist,"<p>I'm using 17.0.6 version (and 13 yet, in another test) of ODBC driver against a PostGreSQL 17 database, I got an unexplicable error when, from a PowerBuilder 12 application using the ODBC, I call a stored procedure; but the same procedure, when I execute it on PGAdmin works fine!</p>
<p>The error is:</p>
<blockquote>
<p>SQLSTATE = 34000<br />
ERROR: cursor &quot;doppie_ricette&quot; does not exists; Error while executing the query</p>
</blockquote>
<p>The stored is:</p>
<pre class=""lang-none prettyprint-override""><code>LANGUAGE 'plpgsql'
AS $BODY$
    declare ultima_riga     integer;
            ultima_nr         char(16);
            ultima_id         char(8);
            nR100             integer;
            ultima_prov     char(1);
            doppie_ricette    record;
begin
   
    ultima_nr     := '' ;
    ultima_id     := '';
    ultima_riga := 0;
    ultima_prov    := '';
    
    for doppie_ricette in
        select     numero_ricetta as nR,
                id_univoco as id,
                numero_riga_2 as _riga,
                anno as _anno,
                ospedale as osp,
                id_riga as idR,
                provenienza as _prov,
                  controllo as _controllo
        from AMB_LOMB
        where numero_ricetta is not null
                and Length(Trim(numero_ricetta))&gt;0
                and pronto_soccorso&lt;&gt;'P'
                and regime&lt;&gt;'7'
                and provenienza&lt;&gt;'S'
        order by 1 asc,7 asc,5 asc,4 asc,2 asc,6 asc
        FOR UPDATE    
    loop
        if     doppie_ricette.nr = ultima_nr
            and doppie_ricette._prov = ultima_prov
            and doppie_ricette.id &lt;&gt; ultima_id then
           
            -- Marcare la seconda (E TUTTE LE COLLEGATE) con codice di errore
            update amb_lomb
            set errore='10C',
                errore_grave=1,
                riferimento_errore=Trim(to_char(ultima_riga, '99999'))
            where current of doppie_ricette ;
        else
            -- TEST SU NOS_100 !
            select Count(1) into nR100
            from nos_100
            where     ris7 = doppie_ricette.nr
                    and ris8 = doppie_ricette._prov
                    and(ris15&lt;&gt;doppie_ricette.id or ris18&lt;&gt;doppie_ricette._anno or ris1&lt;&gt;doppie_ricette.osp);
           
            if nR100&gt;0 then
                if doppie_ricette._controllo in('A','C') then
                    update amb_lomb
                    set    errore='10E',
                        errore_grave=1,
                        riferimento_errore=''
                    where current of doppie_ricette ;
                else
                    update amb_lomb
                    set errore='10C',
                        errore_grave=1,
                        riferimento_errore='archivio'
                    where current of doppie_ricette ;
                end if ;
            end if ;

        end if;

        ultima_nr     := doppie_ricette.nr;
        ultima_id    := doppie_ricette.id;
        ultima_riga    := doppie_ricette._riga;
        ultima_prov    := doppie_ricette._PROV ;
       
    end loop ;
    
    call LOMB_AMB45_ERR_GEN('10C',3);
    call LOMB_AMB45_ERR_GEN('10E',3);

end;
$BODY$;
</code></pre>
<p>The mentioned table (amb_lomb) are without blobs or similar; there are many others stored like this, working fine.</p>
<p>Additional:I'm using only one db, only one session, only one connection, so by PgAdmin and ODBC.</p>
<p>The function has the variable &quot;doppie_ricette&quot; declared locally, and this name is never used elsewhere.</p>
<p>The very problems are:</p>
<pre><code>In the test cases, the query used in the cursor &quot;doppie_ricette&quot; never returns any row.

If I execute - like all the processing data before this procedure - in the PgAdmin environment, everything goes well, without errors; but if I do this in a PowerBuilder (12.6) program, doing exactly the same steps, when I call this procedure I obtain the error. The sequence of operations (INSERTs, CALLs) is obviously the same, and it's on a unique connection on the same PG 17 DB.
</code></pre>
<p>I have done only one direct setting of the ODBC connection: &quot;SET CLIENT_ENCODING TO 'UTF8'&quot;, for the rest the configuration of the ODBC is the original one.</p>
<p>After all:</p>
<p>In the process, after this procedure, I call another control procedure very similar to this: lomb_amb3_verifica_4(), in the exactly same way ... but this one doesn't generate any error in the ODBC env. (also in the PB one, of course); the procedure:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE PROCEDURE public.lomb_amb3_verifica_4()
LANGUAGE 'plpgsql'
AS $BODY$
    declare ultima_riga     integer;
            ultima_nr       char(16);
            ultima_id       char(8);
            ultima_osp      char(6);
            ultima_idr      char(2);
            doppi_nr        record;
begin
    /*
        creata  15.3.01
        
        5.3: Altra condizione di errore: come in DETTAGLIO ma sul file importato,
        non può mai essere che a parità di id univoco ci siano 2 id_riga eguali     
    */
    
    ultima_nr   := '' ;
    ultima_id   := '';
    ultima_riga := 0;
    ultima_idr  := '';
    ultima_osp  := '';
    
    for doppi_nr in 
        select  ospedale as osp,
                ANNO as _anno,
                id_univoco as id,
                id_riga as idR,
                numero_riga as _riga 
        from AMB_LOMB 
        order by 1 asc,2 asc,3 asc,4 asc 
        FOR UPDATE  
    loop 
        if  doppi_nr.osp = ultima_osp
            and doppi_nr.id = ultima_id
            and doppi_nr.idr = ultima_idr then
            
            -- Marcare la seconda con codice di errore
            update amb_lomb 
            set errore='22B',
                riferimento_errore='AMB1: '||TRIM(to_char(ultima_riga, '999999')),
                errore_grave=2 
            where current of doppi_nr ;

        end if;

        ultima_osp  := doppi_nr.osp;
        ultima_id   := doppi_nr.id;
        ultima_riga := doppi_nr._riga;
        ultima_idr  := doppi_nr.idr ;
        
    end loop ;
    
    call LOMB_AMB45_ERR_GEN('22B',3);

end;
$BODY$;
</code></pre>
<p>The cursor is almost the same, but the name 'doppi_nr' is different</p>
",3,3,0,2025-09-04T08:20:55+00:00,1,120,True
79755969,11720193,United States,sql,Implementing CASE-WHEN or Aggregations in Materialized Views in BigQuery,"<p>I have the below mentioned BigQuery SQL for a View. However, both the tables used in the query are huge in volume and hence I am facing terrible performance issues.</p>
<p>If you'd glance at the query, I am trying to do incremental/delta extracts first from one table and joining that to the entire second table, and vice-versa, and finally, I am combining both these result-sets in the final output. I am doing this <code>delta_A-to-full_B</code> and then <code>delta_B-to-full_A</code> logic so as to not miss any updates in the data from either tables.
However, please note that this is just a simplified version of the View query, in reality there are multiple large tables being incorporated in a similar way as shown in the sample query below.</p>
<p>Anyway, I was advised by the team expert for considering a <strong>BQ Materialized View</strong> instead of a regular view, which would automatically take care of any changes/updates to the underlying data in the participating tables thereby not requiring me to perform any specific delta-to-full and vice-versa logic as discussed above.
However, I have very little to no knowledge on BigQuery MVs and hence struggling to find a solution for this. I searched a lot online and the documentations and found that MVs won't allow usage of <code>CASE-WHEN</code>s or that it only allows limited aggregation capabilities, which coincidentally is being heavily used in my existing query.</p>
<p>So, can anyone please help by suggesting a workaround solution for implementing this logic (using aggregations &amp; <code>CASE-WHEN</code>s) via Materialized Views.</p>
<p>BigQuery SQL:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE VIEW `mydataset.cust_dem_view` AS (
    WITH 
        cust_delta AS (
            SELECT 
                cust_id,
                loc_id,
                SUM(
                    CASE custcode
                        WHEN 'A' THEN purch_amt_1
                        WHEN 'B' THEN purch_amt_2
                        WHEN 'D' THEN purch_amt_3
                        ELSE 0
                    END 
                ) AS tot_amt
            FROM
                `mydataset.cust_dtls`
            WHERE
                precs_cd IS NULL
                AND
                    (
                        load_dt &gt;= CURRENT_DATE() - 1
                        OR updt_dt &gt;= CURRENT_DATE() - 1
                    )
            GROUP BY ALL
        ),

        dem_delta AS (
            SELECT 
                cust_id,
                fiscal_period,
                num_scheme
            FROM
                `mydataset.cust_demg`
            WHERE
                load_dt &gt;= CURRENT_DATE() - 1
            OR  updt_dt &gt;= CURRENT_DATE() - 1
        ),

        cust_delta_to_dem_full AS (
            SELECT 
                a.cust_id,
                a.loc_id,
                a.tot_amt,
                b.fiscal_period,
                b.num_scheme
            FROM
                cust_delta AS a -- using delta from the first table as driving
                LEFT JOIN `mydataset.cust_demg` AS b -- join on the full table
                    ON a.cust_id = b.cust_id
        ),

        dem_delta_to_cust_full AS (
            SELECT 
                a.cust_id,
                b.loc_id,
                SUM(
                    CASE b.custcode
                        WHEN 'A' THEN b.purch_amt_1
                        WHEN 'B' THEN b.purch_amt_2
                        WHEN 'D' THEN b.purch_amt_3
                        ELSE 0
                    END
                ) AS tot_amt,
                a.fiscal_period,
                a.num_scheme
            FROM
                dem_delta AS a -- now delta from second table as driving 
                INNER JOIN `mydataset.cust_dtls` AS b -- join on the full table
                    ON a.cust_id = b.cust_id
            GROUP BY ALL
        )

    SELECT
        *
    FROM
        cust_delta_to_dem_full
    UNION ALL
    SELECT
        *
    FROM
        dem_delta_to_cust_full;
)
</code></pre>
<p>Thanks.</p>
",0,1,1,2025-09-04T16:19:19+00:00,1,102,False
79756891,3149869,,sql,Pattern matching in GridDB using NewSQL,"<p>I am looking for pattern matching in GridDB using NewSQL to return values true when matched and false when not matched.</p>
<p>The string can start with</p>
<ol>
<li><p><code>91</code> followed <code>33</code> followed by 8 digit number followed by upper case letters eg <code>913312345678JAYP</code> (should return true)</p>
</li>
<li><p><code>0</code> followed <code>33</code> followed by 8 digit number followed by upper case letters eg <code>03312345458MARYP</code> (should return true)</p>
</li>
<li><p>Starts with <code>33</code> followed by 8 digit number followed by upper case letters eg <code>3312345458TOMH</code> (should return true)</p>
</li>
</ol>
<p>If the string does not start with 91/0/none followed by 33 and 8 digit number and then letters it should return False. Below are some examples:</p>
<p><code>53312345458RYANH</code> --invalid</p>
<p><code>3312345458</code> --invalid</p>
<p><code>03312345458JAYP8</code> --invalid</p>
<p>Can anyone help with with solution. I used the function GLOB and it does not have the OR option so that I can choose between 0/91/none.</p>
",1,2,1,2025-09-05T14:27:02+00:00,1,150,True
79756982,21529621,,sql,How to get both max value and its corresponding time in a GROUP BY query in Apache IoTDB 2.0.4 table model?,"<p>I am using DBeaver to connect to IoTDB 2.0.4's table model and want to query the maximum powerand its corresponding time for a specific time period from the table uemll, grouped by metercode.</p>
<p>Table creation and data insertion SQL statements like this:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE uemll (
  time  TIMESTAMP TIME,
  metercode STRING TAG,
  power DOUBLE
);

INSERT INTO uemll(time, metercode, power) VALUES
  (2023-06-02 00:00:01, '111',  100.0),
  (2023-06-02 00:00:02, '111',  200.0),
  (2023-06-02 00:00:03, '222',  150.0),
  (2023-06-02 00:00:04, '222',  300.0);
</code></pre>
<p>I tried using the following SQL, but it reported an error.</p>
<pre class=""lang-sql prettyprint-override""><code>select time,
        max(power) as power,
        metercode 
from uemll 
where metercode in ('111','222') 
    and time &gt;= 2023-06-02 00:00:00 
    and time &lt;= 2023-06-04 23:59:59 
group by metercode
</code></pre>
<p>The error message is as follows:</p>
<pre><code>SQL Error[701]['time' must be an aggregate expression or appearin GROUP BY clause]
</code></pre>
<p>If I remove the time column, the query does not report an error, but it only returns the maximum power.</p>
<pre><code>+-----+---------+
|power|metercode|
+-----+---------+
|300.0|      222|
|200.0|      111|
+-----+---------+
</code></pre>
<p>What my expectation is</p>
<pre class=""lang-sql prettyprint-override""><code>+-----------------------------+-----+---------+
|                         time|power|metercode|
+-----------------------------+-----+---------+
|2023-06-02T00:00:04.000+08:00|300.0|      222|
|2023-06-02T00:00:02.000+08:00|200.0|      111|
+-----------------------------+-----+---------+
</code></pre>
<p>How should I write this SQL to retrieve both the maximum value and its corresponding time?</p>
",2,4,2,2025-09-05T16:02:33+00:00,1,49,True
79756998,23332429,,sql,Filling Winforms datagrid using Postgres stored procedure,"<p>I am trying to get data from Postgres table with a help of stored procedure and fill a Winforms DataGrid with it.</p>
<p>This is the stored procedure:</p>
<pre><code>CREATE OR REPLACE PROCEDURE public.get_list()
 LANGUAGE sql
BEGIN ATOMIC
 SELECT field1,
        field2,
        field3
    FROM some_table;
END;
</code></pre>
<p>Here's my C# code:</p>
<pre><code>DataTable dtt = new DataTable();
string ConString = &quot;Server=x.xxx.xx.xxx;Port=5432;User Id=xxx;Password=xxx;Database=xxx;&quot;;

NpgsqlConnection connection = new NpgsqlConnection(ConString);
connection.Open();
NpgsqlCommand cmd = new NpgsqlCommand(&quot;CALL public.get_list()&quot;);            

try
{
    cmd.Connection = connection;
    cmd.CommandType = System.Data.CommandType.StoredProcedure;

    using (var dataReader = cmd.ExecuteReader())
    {
        if (dataReader.HasRows)
        {
            GridView.Visible = true;                        
            dtt.Load(dataReader);                        

            GridView.DataSource = dtt;
            GridView.Update();    
        }
    }
}
catch (Exception ex)
{
    MessageBox.Show(ex.ToString());
}
</code></pre>
<p>After executing this code, the <code>DataReader</code> returns 0 rows.
The stored procedure returns nothing: <a href=""https://dbfiddle.uk/dT1i5M_N"" rel=""nofollow noreferrer"">fiddle</a></p>
<pre><code>CALL public.get_list();
</code></pre>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>CALL
</code></pre>
</blockquote>
<p>What am I doing wrong?</p>
<p>Maybe the analog of a stored procedure in other RDBMSs (for example in SQL Server) in Postgres is a function?</p>
",0,2,2,2025-09-05T16:21:02+00:00,1,136,True
79757554,5304058,San fransisco,sql,Multiple rows aggregated to single row,"<p>Business:<br />
I have one manager tied to multiple employees/single employees across different business and Manager and employees are in different rows. We have to map on ID and also on zip codes (both zip5 and zip3).If there is no match on ID, map it on zip5 and if there is no match then on zip3. Manager is main table.</p>
<p>Code:<br />
I tried to segregate manager and employee information into two different tables and then try to left join employee with manager. I have to join on ID Or zip5 or zip3 and business. I tried below logic but the join conditions are messing up the data because of nulls. I also tried max, but it wont work when there are multiple employees for same manager.</p>
<pre><code>DROP TABLE IF EXISTS #table;
create table #table
(ID int,
zip5 int,
zip3 int,
Manager_Region varchar(50),
manager_Name varchar(100),
Employee_name varchar(50),
Employee_region varchar(100),
business varchar(20),
source varchar(20)
)

Insert into #table
select 1234,null,null,'East','Mike',null,null,'Healthcare','Source1'
UNION ALL
select 1234,null,null,null,null,'John','NorthEast','Healthcare','Source1'
UNION ALL
select 1234,null,null,null,null,'Patty','EastCentral','Healthcare','Source1'
UNION ALL
select Null,34589,null,'West','Kat',Null,Null,'dental','Source2'
UNION ALL
select Null,34589,null,null,Null,'Kim','NorthWestern','dental','Source2'
UNION ALL
select Null,null,345,'South','rita',Null,Null,'Healthcare','Source3'
UNION ALL
select Null,null,345,null,Null,'Sam','southCentral','Healthcare','Source3'


DROP TABLE IF EXISTS #Manager_Is_Null;
select * into #Manager_Is_Null
from #table 
where  Manager_Region is null


DROP TABLE IF EXISTS #Employee_is_Null;
select * into #Employee_is_Null
from #table 
where  Employee_Region is null 


select s.ID,s.zip5,s.zip3,
s.business,
s.Manager_Region,
S.Manager_Name,
a.employee_Region,
a.employee_Name
from #Employee_is_Null s
left join #Manager_Is_Null a
on ((isnull(a.ID,'') = isnull(s.ID,'')) OR
        (isnull(a.ID,'') &lt;&gt; isnull(s.ID,'') AND isnull(a.zip5,'') = isnull(s.zip5,'')) OR
        (isnull(a.ID,'') &lt;&gt; isnull(s.ID,'') AND isnull(a.zip5,'') &lt;&gt; isnull(s.zip5,'') AND isnull(a.zip3,'') = isnull(a.zip3,'')))
        and a.business=s.business
</code></pre>
<p>Expected Output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>zip5</th>
<th>zip3</th>
<th>Manager_Region</th>
<th>Manager_Name</th>
<th>Employee_Name</th>
<th>Employee_Region</th>
<th>Business</th>
<th>source</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td><em>null</em></td>
<td><em>null</em></td>
<td>East</td>
<td>Mike</td>
<td>John</td>
<td>NorthEast</td>
<td>Healthcare</td>
<td>Source1</td>
</tr>
<tr>
<td>1234</td>
<td><em>null</em></td>
<td><em>null</em></td>
<td>East</td>
<td>Mike</td>
<td>Patty</td>
<td>EastCentral</td>
<td>Healthcare</td>
<td>Source1</td>
</tr>
<tr>
<td><em>null</em></td>
<td>34589</td>
<td><em>null</em></td>
<td>West</td>
<td>Kat</td>
<td>Kim</td>
<td>NorthWestern</td>
<td>dental</td>
<td>Source2</td>
</tr>
<tr>
<td><em>null</em></td>
<td><em>null</em></td>
<td>345</td>
<td>South</td>
<td>rita</td>
<td>Sam</td>
<td>southCentral</td>
<td>Healthcare</td>
<td>Source3</td>
</tr>
</tbody>
</table></div>
<p>How can I acheive this resultset?</p>
",2,3,1,2025-09-06T12:49:20+00:00,2,157,True
79757699,24888000,,sql,Exclude from the selection the values ​that are in the array,"<p>I have two tables:</p>
<p>my_temp:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employee_id</th>
<th>date</th>
<th>start_granula</th>
<th>end_granula</th>
<th>count_seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-22</td>
<td>07:40:00</td>
<td>07:50:00</td>
<td>625</td>
</tr>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-22</td>
<td>08:10:00</td>
<td>08:20:00</td>
<td>513</td>
</tr>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-22</td>
<td>12:35:00</td>
<td>12:41:00</td>
<td>128</td>
</tr>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-24</td>
<td>15:10:00</td>
<td>15:25:00</td>
<td>3206</td>
</tr>
</tbody>
</table></div>
<p>schedules:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employee_id</th>
<th>start_time</th>
<th>end_time</th>
<th>breaks</th>
<th>workdays</th>
</tr>
</thead>
<tbody>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>08:00:00</td>
<td>17:00:00</td>
<td><code>[{&quot;to&quot;: &quot;13:00&quot;, &quot;from&quot;: &quot;12:30&quot;, &quot;nextDay&quot;: false}, {&quot;to&quot;: &quot;15:00&quot;, &quot;from&quot;: &quot;15:30&quot;, &quot;nextDay&quot;: false}]</code></td>
<td><code>[1, 2, 3, 4, 5]</code></td>
</tr>
</tbody>
</table></div>
<p>I need to select only those time ranges from <code>my_temp</code> that correspond to the employee's schedule. Breaks must be excluded.</p>
<p>It is also necessary to check whether this day was a working day for the employee according to the schedule. If it is not, then leave only one entry in the table for this date and in column <code>day_type</code> write one of the following values: <code>['working_day', 'not_working_day']</code>. In the <code>start_time</code> and <code>end_time</code> columns, write the time according to the work schedule.
I'm having trouble excluding all entries that fall during the employee's break, entries that go beyond working hours are not excluded.</p>
<p>How to leave only one entry in the query for a date that is not a working day for the employee?</p>
<p>My query:</p>
<pre><code>    select 
                e.employee_id, 
                e.date,
                case when 
                    EXTRACT(DOW FROM e.date) = ANY(SELECT jsonb_array_elements(s.workDays::jsonb)) 
                then granula_start else s.start_time end as granula_start,
                case when 
                    EXTRACT(DOW FROM e.date) = ANY(SELECT jsonb_array_elements(s.workDays::jsonb)) 
                then granula_end else s.end_time end as granula_end,
                count_seconds,
                case when 
                    EXTRACT(DOW FROM e.date) = ANY(SELECT jsonb_array_elements(s.workDays::jsonb)) 
                then 'working_day' else 'not_working_day' end as day_type
            FROM my_temp e
            inner join schedules s on s.employee_id = e.employee_id
      where
     
            granula_start
            &gt;= 
            s.start_time
            and not exists (
                            select 1 from jsonb_array_elements(s.breaks::jsonb) br
                            where granula_start &gt; (br-&gt;&gt;'from')::time
                            and   granula_end &lt; (br-&gt;&gt;'to')::time
                          )
</code></pre>
<p>which returns:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employee_id</th>
<th>date</th>
<th>granula_start</th>
<th>granula_end</th>
<th>count_seconds</th>
<th>day_type</th>
</tr>
</thead>
<tbody>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>08:10:00</td>
<td>08:20:00</td>
<td>513</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>13:10:00</td>
<td>13:25:00</td>
<td>1205</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>15:10:00</td>
<td>15:25:00</td>
<td>3206</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>17:10:00</td>
<td>17:25:00</td>
<td>1205</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-24</td>
<td>08:00:00</td>
<td>17:00:00</td>
<td>6504</td>
<td>not_working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-24</td>
<td>08:00:00</td>
<td>17:00:00</td>
<td>7203</td>
<td>not_working_day</td>
</tr>
</tbody>
</table></div>
<p>Please help me. Here is my example: <a href=""https://dbfiddle.uk/G542L9Wh"" rel=""nofollow noreferrer"">https://dbfiddle.uk/G542L9Wh</a></p>
",0,0,0,2025-09-06T17:35:07+00:00,1,86,True
79758087,10635844,,sql,Get distinct items from one column with specific criteria,"<p>Please picture the following table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">book_id</th>
<th>title</th>
<th>lang</th>
<th>price</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td>Le tour du monde en 80 jours</td>
<td>fr</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td>Le petit prince</td>
<td>fr</td>
<td>25</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td>Asterix le Gaulois</td>
<td>fr</td>
<td>30</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td>Othello</td>
<td>en</td>
<td>15</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td>Of Mice and Men</td>
<td>en</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">6</td>
<td>The reluctant fundamentalist</td>
<td>en</td>
<td>18</td>
</tr>
<tr>
<td style=""text-align: right;"">7</td>
<td>Damals war es Friedrich</td>
<td>de</td>
<td>28</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td>Der kleine Raabe Socke</td>
<td>de</td>
<td>28</td>
</tr>
<tr>
<td style=""text-align: right;"">9</td>
<td>Ritter Rost</td>
<td>de</td>
<td>25</td>
</tr>
</tbody>
</table></div>
<p>From this table, I'd like to get the most expensive book for each language. Further languages may be added. If the top price for one book is the same, then the book with the higher book_id should be returned. Never more than one line per language.</p>
<p>The table I'm hoping to achieve looks something like this (I don't care about the order):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">book_id</th>
<th>title</th>
<th>lang</th>
<th>price</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">3</td>
<td>Asterix le Gaulois</td>
<td>fr</td>
<td>30</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td>Of Mice and Men</td>
<td>en</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td>Der kleine Raabe Socke</td>
<td>de</td>
<td>28</td>
</tr>
</tbody>
</table></div>
<p>I've already tried the following query, adapted from what I found <a href=""https://stackoverflow.com/questions/4662464/how-to-select-only-the-first-rows-for-each-unique-value-of-a-column"">here</a>:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT book_id, title, lang, MAX(price) FROM books GROUP BY lang;
</code></pre>
<p>That gave me a table where the book_id and the title did not match the price. It looked like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">book_id</th>
<th>title</th>
<th>lang</th>
<th>MAX(price)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">7</td>
<td>Damals war es Friedrich</td>
<td>de</td>
<td>28</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td>Othello</td>
<td>en</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td>Le tour du monde en 80 jours</td>
<td>fr</td>
<td>30</td>
</tr>
</tbody>
</table></div>
<p>I suppose I could find a way to request each language with a separate SQL-Prompt, but then I'd have to add a new prompt to the list whenever there is a book in a new language added.</p>
<p>Is there a better solution to this?</p>
<p>Thanks in advance!</p>
<p>Here is the query needed to enter the data from my sample table into an SQL database of your own:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE IF NOT EXISTS `books` (
  `book_id` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(50) NOT NULL DEFAULT '',
  `lang` varchar(2) NOT NULL DEFAULT '',
  `price` int(11) NOT NULL DEFAULT 0,
  PRIMARY KEY (`book_id`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;

INSERT INTO `books` (`book_id`, `title`, `lang`, `price`) VALUES
    (1, 'Le tour du monde en 80 jours', 'fr', 20),
    (2, 'Le petit prince', 'fr', 25),
    (3, 'Asterix le Gaulois', 'fr', 30),
    (4, 'Othello', 'en', 15),
    (5, 'Of Mice and Men', 'en', 20),
    (6, 'The reluctant fundamentalist', 'en', 18),
    (7, 'Damals war es Friedrich', 'de', 28),
    (8, 'Der kleine Raabe Socke', 'de', 28),
    (9, 'Ritter Rost', 'de', 25);
</code></pre>
",0,2,2,2025-09-07T11:31:55+00:00,1,144,True
79758791,1410583,,sql,ORACLEDB SQL MIN/MAX in LISTAGG,"<p>Let's say I have this kind of data.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SCHOOL_ID</th>
<th>STUDENT_ID</th>
<th>ROOM_ID</th>
<th>CHECKIN_TIME</th>
</tr>
</thead>
<tbody>
<tr>
<td>600</td>
<td>2222</td>
<td>10000</td>
<td>2025-07-01</td>
</tr>
<tr>
<td>600</td>
<td>2222</td>
<td>20000</td>
<td>2025-07-02</td>
</tr>
<tr>
<td>600</td>
<td>2222</td>
<td>10000</td>
<td>2025-08-03</td>
</tr>
<tr>
<td>600</td>
<td>1111</td>
<td>20000</td>
<td>2025-08-04</td>
</tr>
</tbody>
</table></div>
<p>I already have a query to group by school, and aggregate student_id and room_id, like this.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SCHOOL_ID</th>
<th>STUDENT_ID</th>
<th>ROOM_ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>600</td>
<td>1111|2222</td>
<td>10000|20000</td>
</tr>
</tbody>
</table></div>
<p>Now I want to add 2 other columns that <strong>contain list</strong> of MIN(checkin_time) and MAX(checkin_time) <strong>for each room in the list</strong>. The list order should follow rooms order as in ROOM_ID column.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SCHOOL_ID</th>
<th>STUDENT_ID</th>
<th>ROOM_ID</th>
<th>MIN_CHECKIN_TIME</th>
<th>MAX_CHECKIN_TIME</th>
</tr>
</thead>
<tbody>
<tr>
<td>600</td>
<td>1111|2222</td>
<td>10000|20000</td>
<td>2025-07-01|2025-07-02</td>
<td>2025-08-03|2025-08-04</td>
</tr>
</tbody>
</table></div>
<p>After many attempts, I still can't get correct query to get this data.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE TempDemo(
    school_id CHAR(3),
    student_id VARCHAR(10),
    room_id VARCHAR(10),
    checkin_date DATE
);

INSERT INTO TempDemo VALUES ('600', '2222', '10000', TO_DATE('2025-07-01', 'YYYY-MM-DD'));
INSERT INTO TempDemo VALUES ('600', '2222', '20000', TO_DATE('2025-07-02', 'YYYY-MM-DD'));
INSERT INTO TempDemo VALUES ('600', '2222', '10000', TO_DATE('2025-08-03', 'YYYY-MM-DD'));
INSERT INTO TempDemo VALUES ('600', '1111', '20000', TO_DATE('2025-08-04', 'YYYY-MM-DD'));

-- SELECT * FROM TempDemo ORDER BY room_id, student_id;

-- Existing Query
SELECT DISTINCT school_id,
    LISTAGG(DISTINCT student_id, '|') WITHIN GROUP (ORDER BY student_id DESC) student_id,
    LISTAGG(DISTINCT room_id, '|') WITHIN GROUP (ORDER BY room_id DESC) room_id
FROM TempDemo
GROUP BY school_id;

-- Attempt to add min_checkin_time and max_checkin_time
SELECT school_id,
    LISTAGG(DISTINCT student_id, '|') WITHIN GROUP (ORDER BY student_id DESC) student_id,
    LISTAGG(DISTINCT room_id, '|') WITHIN GROUP (ORDER BY room_id DESC) room_id,
    LISTAGG(DISTINCT min_checkin_date, '|') WITHIN GROUP (ORDER BY room_id DESC) min_checkin_date,
    LISTAGG(DISTINCT max_checkin_date, '|') WITHIN GROUP (ORDER BY room_id DESC) max_checkin_date
FROM (
    SELECT school_id, student_id, room_id, min(checkin_date) min_checkin_date, max(checkin_date) max_checkin_date FROM TempDemo
    GROUP BY school_id, student_id, room_id
) GROUP BY school_id;

DROP TABLE TempDemo;

</code></pre>
",-5,1,6,2025-09-08T10:54:14+00:00,2,99,True
79759241,1124682,,sql,Counting values using nested SQL query,"<p>I have the following tables with values:</p>
<pre><code>CREATE TABLE Produkty 
(
    Id NUMBER(10) NOT NULL,
    Nazwa VARCHAR2(50) NOT NULL,
    Status VARCHAR2(50) NOT NULL,
    CONSTRAINT CHK_Status CHECK (Status IN ('czynny', 'zakonczony')),
    CONSTRAINT PK_Id PRIMARY KEY (Id)
);

INSERT INTO Produkty VALUES (1, 'Koszula', 'czynny');
INSERT INTO Produkty VALUES (2, 'Buty', 'zakonczony');
INSERT INTO Produkty VALUES (3, 'Krawat', 'czynny');
INSERT INTO Produkty VALUES (4, 'Spodnie', 'zakonczony');
INSERT INTO Produkty VALUES (5, 'Sznurówki', 'czynny');
INSERT INTO Produkty VALUES (6, 'Kamizelka', 'zakonczony');
INSERT INTO Produkty VALUES (7, 'Marynarka', 'czynny');
INSERT INTO Produkty VALUES (8, 'Czapka', 'zakonczony');

CREATE TABLE Fundusze 
(
    Id NUMBER(10) NOT NULL,
    Nazwa VARCHAR2(50) NOT NULL,
    Waluta VARCHAR2(50) NOT NULL,
    Typ VARCHAR2(50) NOT NULL,
    CONSTRAINT CHK_StatusFundusze CHECK (Typ IN ('defensywny', 'akcyjny', 'oszczędnościowy')),
    CONSTRAINT PK_IdFundusze PRIMARY KEY (Id),
);

INSERT INTO Fundusze VALUES (1, 'Def1', 'PLN', 'defensywny');
INSERT INTO Fundusze VALUES (2, 'Def2', 'USD', 'defensywny');
INSERT INTO Fundusze VALUES (3, 'Akc1', 'PLN', 'akcyjny');
INSERT INTO Fundusze VALUES (4, 'Akc2', 'USD', 'akcyjny');
INSERT INTO Fundusze VALUES (5, 'Os1', 'PLN', 'oszczędnościowy');
INSERT INTO Fundusze VALUES (6, 'Os2', 'USD', 'oszczędnościowy');

CREATE TABLE Fundusze_produktow 
(
    Pr_Id NUMBER(10) NOT NULL,
    Fun_Id NUMBER(10) NOT NULL,
    Data_od DATE,
    Data_do DATE,
    Oplata NUMBER(20),
    CONSTRAINT fk_produkty FOREIGN KEY (Pr_Id) REFERENCES Produkty(Id),
    CONSTRAINT fk_fundusze FOREIGN KEY (Fun_Id) REFERENCES Fundusze(Id),
    CONSTRAINT PK_Fundusze_produktow PRIMARY KEY (Pr_Id, Fun_Id)
);

INSERT INTO Fundusze_produktow VALUES (1, 1, '12/17/1980', '12/20/1980', 11);
INSERT INTO Fundusze_produktow VALUES (1, 2, '12/17/1980', '12/20/1980', 12);
INSERT INTO Fundusze_produktow VALUES (1, 3, '12/17/1980', '12/20/1980', 9);
INSERT INTO Fundusze_produktow VALUES (1, 4, '12/17/1980', '12/20/1980', 13);
INSERT INTO Fundusze_produktow VALUES (1, 5, '12/17/1980', '12/20/1980', 15);
INSERT INTO Fundusze_produktow VALUES (1, 6, '12/17/1980', '12/20/1980', 17);
INSERT INTO Fundusze_produktow VALUES (2, 2, '12/17/1980', '12/20/1980', 14);
INSERT INTO Fundusze_produktow VALUES (2, 4, '12/17/1980', '12/20/1980', 18);
INSERT INTO Fundusze_produktow VALUES (2, 6, '12/17/1980', '12/20/1980', 19);
INSERT INTO Fundusze_produktow VALUES (3, 2, '12/17/1980', '12/20/1980', 12);
INSERT INTO Fundusze_produktow VALUES (4, 2, '12/17/1980', '12/20/1980', 12);
INSERT INTO Fundusze_produktow VALUES (5, 2, '12/17/1980', '12/20/1980', 16);
INSERT INTO Fundusze_produktow VALUES (6, 2, '12/17/1980', '12/20/1980', 8);
INSERT INTO Fundusze_produktow VALUES (6, 4, '12/17/1980', '12/20/1980', 9);
INSERT INTO Fundusze_produktow VALUES (7, 2, '12/17/1980', '12/20/1980', 11);
INSERT INTO Fundusze_produktow VALUES (7, 3, '12/17/1980', '12/20/1980', 13);
</code></pre>
<p>I'm now trying to fetch Produkty with status 'czynny' and a number of Fundusze with type 'akcyjny', which are associated with them. I use Oracle Apex and run the following query:</p>
<pre><code>SELECT 
    COUNT(*), z.fun_id, p.nazwa  
FROM 
    Produkty p, Fundusze_produktow z 
WHERE 
    p.Status = 'czynny' 
    AND z.fun_id IN (SELECT id FROM Fundusze 
                     WHERE Typ = 'akcyjny') 
    AND z.pr_id = p.id 
GROUP BY 
    z.fun_id, p.nazwa;
</code></pre>
<p>It returns correctly the nazwa from Produkty and associated z.fun_id (which in my case is 3 and 4), but it doesn't count them properly:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>COUNT(*)</th>
<th>FUN_ID</th>
<th>NAZWA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3</td>
<td>Koszula</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td>Koszula</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>Marynarka</td>
</tr>
</tbody>
</table></div>
<p>I tried everything and couldn't get the correct result.</p>
<p>Thanks for help.</p>
",1,2,1,2025-09-08T18:43:19+00:00,1,83,True
79759817,1124682,,sql,"how to trim, substring and make capital letters in a column","<p>I have a table in oracle apex:</p>
<pre><code>CREATE TABLE Fundusze (
    Id NUMBER(10) NOT NULL,
    Nazwa VARCHAR2(50) NOT NULL,
    Waluta VARCHAR2(50) NOT NULL,
    Typ VARCHAR2(50) NOT NULL,
    CONSTRAINT CHK_StatusFundusze CHECK (Typ IN ('defensywny', 'akcyjny', 'oszczędnościowy')),
    CONSTRAINT PK_IdFundusze PRIMARY KEY (Id)
);

INSERT INTO Fundusze VALUES (1, 'Def1', 'PLN', 'defensywny');
INSERT INTO Fundusze VALUES (2, 'Def2', 'USD', 'defensywny');
INSERT INTO Fundusze VALUES (3, 'Akc1', 'PLN', 'akcyjny');
INSERT INTO Fundusze VALUES (4, 'Akc2', 'USD', 'akcyjny');
INSERT INTO Fundusze VALUES (5, 'Os1', 'PLN', 'oszczędnościowy');
INSERT INTO Fundusze VALUES (6, 'Os2', 'USD', 'oszczędnościowy');
INSERT INTO Fundusze VALUES (7, 'Defensywny2', 'PLN', 'defensywny');
INSERT INTO Fundusze VALUES (8, 'Akcyjny2', 'PLN', 'akcyjny');
INSERT INTO Fundusze VALUES (9, ' Akc3yjny3', 'PLN', 'akcyjny');
INSERT INTO Fundusze VALUES (10, 'Akc4yjny4 ', 'PLN', 'akcyjny');
INSERT INTO Fundusze VALUES (11, ' Akc5yjny5 ', 'PLN', 'akcyjny');
INSERT INTO Fundusze VALUES (12, ' top20', 'PLN', 'defensywny');
INSERT INTO Fundusze VALUES (13, 'low30 ', 'PLN', 'defensywny');
INSERT INTO Fundusze VALUES (14, ' mid40 ', 'PLN', 'defensywny');
</code></pre>
<p>I want to make an update on this table values so that the nazwa is trimmed out of spaces, then add a string after the nazwa &quot;MBANK&quot; and then make everything into capital letters. I used the following query</p>
<pre><code>UPDATE Fundusze
SET nazwa = TRIM(UPPER(SUBSTR(nazwa, 0, 4))) || 'MBANK');
</code></pre>
<p>but it takes the spaces into the removal and doesn't substring properly, which is what I want. For example last three rows are returned into this:</p>
<pre><code>12  TOPMBANK    
13  LOW3MBANK   
14  MIDMBANK
</code></pre>
<p>and I want it to be:</p>
<pre><code>12  TOP2MBANK   
13  LOW3MBANK   
14  MID4MBANK
</code></pre>
<p>I tried everything, but failed to do it the way needed.</p>
",-4,0,4,2025-09-09T11:13:56+00:00,1,122,True
79760111,4978814,,sql,Query with aggregate functions is very slow,"<p>I have the following query:</p>
<pre><code>WITH RankedCallNotes AS (
    SELECT
        Id,
        CreatedOn,
        CandidateId,
        Text,
        ROW_NUMBER() OVER (PARTITION BY CandidateId ORDER BY CreatedOn DESC) AS rn
    FROM
        [dbo].[CandidateCallNotes]
)

SELECT 
    c.Id,
    c.FirstName,
    c.LastName,
    c.AvailableDate,
    c.Email,
    cm0.Modality AS PrimaryModality,
    cm1.Modality AS Modality2,
    c.Phone,
    s.State AS HomeState,
    STRING_AGG(cert.CertName, ', ') AS Certificates,
    c.ActiveStateLicenseId,
    r.Name AS Rep,
    n.CreatedOn AS MostRecentCallTsp,
    n.Text AS MostRecentCall
FROM [dbo].[Candidates] c
    LEFT JOIN 
        (
            SELECT cm.CandidateId,
                cm.ModalityId,
                m.Name AS Modality,
                cm.&quot;Order&quot; 
            FROM [dbo].[CandidateModalities] cm
            JOIN [dbo].[Modalities] m ON m.Id = cm.ModalityId
            WHERE &quot;Order&quot; = 0
        ) AS cm0 ON c.Id = cm0.CandidateId
    LEFT JOIN 
        (
            SELECT cm.CandidateId,
                cm.ModalityId,
                m.Name AS Modality,
                cm.&quot;Order&quot; 
            FROM [dbo].[CandidateModalities] cm
            JOIN [dbo].[Modalities] m ON m.Id = cm.ModalityId
            WHERE &quot;Order&quot; = 1
        ) AS cm1 ON c.Id = cm1.CandidateId
    LEFT JOIN
        [dbo].[States] s ON c.HomeStateId = s.Id
    LEFT JOIN
        [dbo].[CandidateModalities] cm ON cm.CandidateId = c.Id
    LEFT JOIN
        (
            SELECT CertName,
                CandidateModalityId 
            FROM [dbo].[CandidateCertifications]
            WHERE CertName IS NOT NULL
            AND CertName &lt;&gt; ''
        ) cert ON cm.Id = cert.CandidateModalityId
    LEFT JOIN
        [dbo].[Reps] r ON c.RepId = r.Id
    LEFT JOIN
        RankedCallNotes n ON n.CandidateId = c.Id
        AND n.rn = 1
GROUP BY
    c.Id,
    c.FirstName,
    c.LastName,
    c.AvailableDate,
    c.Email,
    cm0.Modality,
    cm1.Modality,
    c.Phone,
    s.&quot;State&quot;,
    c.ActiveStateLicenseId,
    r.Name,
    n.CreatedOn,
    n.Text
</code></pre>
<p>It takes around 40 seconds to run so I'm trying to figure a way to optimize it if I can. For context, the Candidates table is around 40,000 rows, the CandidateModalities table is around 40,000 rows, and the CandidateCallNotes table is around 200,000 rows.</p>
<p>Here's the data plan: <a href=""https://www.brentozar.com/pastetheplan/?id=0ctqpalAIr"" rel=""nofollow noreferrer"">https://www.brentozar.com/pastetheplan/?id=0ctqpalAIr</a></p>
<p>What I've noticed:</p>
<ol>
<li>There's a Filter operation that the plan says is taking 36983 ms to complete which seems really problematic.</li>
<li>There's a Sort operation that has a warning attached reading <code>Operator used tempdb to spill data during execution with spill level {0} and {1} spilled thread(s)</code> and its actual elapsed time is 36907 ms.</li>
<li>If I remove portions requiring aggregate functions (either for Certificates or for CandidateCallNotes), the execution time improves to &lt;1 second so I assume my issues primarily lie there.</li>
</ol>
<p>I'm extremely new to query optimization and am not sure exactly what I should be looking for within this plan, how to interpret that, and then what to do to address the issue. I assume indexes would help but I have Azure's automatic indexing in place and want to make sure I don't screw up what it's already doing.</p>
",2,3,1,2025-09-09T16:52:47+00:00,2,158,True
79760577,7860771,,sql,Expand a table by filling the holes between bounds - regular SQL (i.e. not PLSQL),"<p>I have a table with 1 line by stay in a room with arrival date and departure date, like this:</p>
<p>INPUT_TABLE:</p>
<pre><code>STAY_ID;CUSTOMER_NAME;ROOM_NO;STAY_START_DATE;STAY_END_DATE
00031;ROGER;08;20180316;20180324
00202;WILLIAM;12;20190518;20190522
00045;RICHARD;05;20181220;20181230
</code></pre>
<p>I want to expand the table to get 1 line by day of presence, so the final output result of the global select should return something like this:</p>
<pre><code>STAY_ID;CUSTOMER_NAME;ROOM_NO;STAY_START_DATE;STAY_END_DATE;ROOM_OCCUPATION_DATE
00031;ROGER;08;20180316;20180323;20180316
00031;ROGER;08;20180316;20180323;20180317
00031;ROGER;08;20180316;20180323;20180318
00031;ROGER;08;20180316;20180323;20180319
00031;ROGER;08;20180316;20180323;20180320
00031;ROGER;08;20180316;20180323;20180321
00031;ROGER;08;20180316;20180323;20180322
00031;ROGER;08;20180316;20180323;20180323
00202;WILLIAM;12;20190518;20190522;20190518
00202;WILLIAM;12;20190518;20190522;20190519
00202;WILLIAM;12;20190518;20190522;20190520
00202;WILLIAM;12;20190518;20190522;20190521
00202;WILLIAM;12;20190518;20190522;20190522
00045;RICHARD;05;20181220;20181228;20181220
00045;RICHARD;05;20181220;20181228;20181221
00045;RICHARD;05;20181220;20181228;20181222
00045;RICHARD;05;20181220;20181228;20181223
00045;RICHARD;05;20181220;20181228;20181224
00045;RICHARD;05;20181220;20181228;20181225
00045;RICHARD;05;20181220;20181228;20181226
00045;RICHARD;05;20181220;20181228;20181227
00045;RICHARD;05;20181220;20181228;20181228
</code></pre>
<p>I didn't figure out how to generate only the desired date range (from min(STAY_START_DATE) and max(STAY_END_DATE)) and how to join with the INPUT_TABLE to get the desired result.</p>
<p>I started this base of SQL code:</p>
<pre><code>WITH INPUT_TABLE as (
SELECT STAY_ID, CUSTOMER_NAME, ROOM_NO, STAY_START_DATE, STAY_END_DATE
FROM ROOM_STAYS
WHERE
  STAY_START_DATE &gt;= '20180301'
  AND STAY_END_DATE &lt;= '20190531'
  AND CUSTOMER_NAME IN ('ROGER','WILLIAM','RICHARD')
)

SELECT STAY_ID, CUSTOMER_NAME, ROOM_NO, STAY_START_DATE, STAY_END_DATE, DATES_LIST AS ROOM_OCCUPATION_DATE
FROM INPUT_TABLE
XXXX JOIN (
SELECT MIN(TO_DATE(INPUT_TABLE.STAY_START_DATE,'YYYYMMDD')) + LEVEL - 1 AS DATES_LIST
FROM DUAL
CONNECT BY LEVEL &lt;= MAX(TO_DATE(INPUT_TABLE.STAY_END_DATE,'YYYYMMDD')) - MIN(TO_DATE(INPUT_TABLE.STAY_START_DATE,'YYYYMMDD')) + 1
) DATES_TAB
ON YYYY
</code></pre>
<p>Could someone help me please?</p>
",1,1,0,2025-09-10T07:34:32+00:00,4,151,True
79760970,3059082,"Wilmington, Delaware",sql,DB2 (i Series) how to write SELECT query on bit data using hex string value,"<p>I'm trying to troubleshoot some C# .NET code that interacts with a DB2 database on iSeries/AS400.  These tables have several fields that are type <code>CHAR() FOR BIT DATA</code> which are populated as a hex byte array.</p>
<p>When I do a <code>SELECT * FROM [schema].[tablename]</code> on the table using the IBM ACS SQL tool, these fields display as a hex string.</p>
<p>I want to select a specific row based on a value in that field but it's unclear how to compose the WHERE clause.  What syntax do I need to leverage the hex string value?</p>
",0,1,1,2025-09-10T14:27:57+00:00,1,93,True
79760997,28397583,,sql,Populate a final table with NULL if that column does not exist in a global temp table,"<p>In our database, Orders have order lines. Order lines can have one or more misc charge. The misc charges are stored in a separate table from the rest of the order details but each <code>MiscChg</code> is in its own line. So what I'm trying to do is pull all the misc charges, store them in a temp table in a single row, then join that temp table to the order detail table to get a final result set that I can insert into my final table. I use these tables to create data sources that are used in Tableau.</p>
<p>I wrote a dynamic SQL query that pivots the <code>MiscChgs</code> so all the charges return in a single row. I use a global temp table so I can access it outside the dynamic SQL. It returns the data I want. My problem is when I try to insert these into my final result set, if a field is not in the global temp table, I get an error telling me the field doesn't exists. For example, very few records have a Labor misc charge but my final table has a column for this for those that do.</p>
<p>My question: how do I populate the final table with NULL if that column doesn't exist in the global temp table?</p>
<p>Table examples:</p>
<pre><code>CREATE TABLE MiscChg
(
    InvoiceNum  INT,
    InvoiceLine INT,
    MiscChgDesc VARCHAR(20),
    MiscAmt DECIMAL(10, 2),
    ChangeDate DATE
)
</code></pre>
<pre><code>INSERT INTO MiscChg(InvoiceNum, InvoiceLine, MiscChgDesc, MiscAmt)
VALUES (1000, 1, 'Tariff', 19.8, '2025-09-08'),
       (1001, 1, 'Freight', 215, '2025-09-08'),
       (1002, 1, 'Tariff', 32.45, '2025-09-08'),
       (1003, 1, 'Tariff', 5.410, '2025-09-08'),
       (1004, 1, 'Tariff', 1.2, '2025-09-08'),
       (1005, 1, 'Tariff', 15.5, '2025-09-08'),
       (1005, 3, 'Freight', 20, '2025-09-08'),
       (1005, 3, 'Tariff', 21.13, '2025-09-08'),
       (1005, 9, 'Tariff', 22.69, '2025-09-08'),
       (1005, 11, 'Tariff', 10.75, '2025-09-08')
</code></pre>
<pre><code>CREATE TABLE InvoiceDetail
(
    InvoiceNum INT,
    InvoiceLine INT,
    ExtendedLineCost DECIMAL(10, 2),
    TotalLineCost DECIMAL(10, 2)
)

INSERT INTO InvoiceDetail(InvoiceNum, InvoiceLine, ExtendedLineCost, TotalLineCost)
VALUES (1000, 1, 77.1, 77.1)
       (1001, 1, 429.86, 429.86)
       (1002, 1, 7.25, 7.25)
       (1003, 1, 8.29, 8.29)
       (1004, 1, 70.5, 70.5)
       (1005, 1, 163, 163)
       (1005, 3, 70.5, 70.5)
       (1005, 9, 8.29, 8.29)
       (1005, 11, 81.56, 81.56)
</code></pre>
<p>Here is my code to create the pivot of misc charges:</p>
<pre><code>DECLARE @sql AS NVARCHAR(MAX);
DECLARE @miscChgDesc AS NVARCHAR(MAX);

/* Get the distinct descriptions for changed records to create column names */
SELECT @miscChgDesc = STRING_AGG(QUOTENAME(REPLACE(MiscChgDesc, ' ', '')), ', ')
FROM (
    SELECT DISTINCT MiscChgDesc
    FROM MiscChg mc
    WHERE ChangeDate = '2025-09-08'
) AS Descriptions


/* Construct the dynamic SQL query to pull misc charges in one row for each InvoiceNum &amp; Line */
SET @sql = N'
    DROP TABLE IF EXISTS ##gblMiscChgs;

    SELECT * INTO ##gblMiscChgs FROM (
    SELECT InvoiceNum, InvoiceLine, ' + @miscChgDesc + '
    FROM (SELECT   mc.InvoiceNum
                  ,mc.InvoiceLine
                  ,mc MiscChgDesc
                  ,mc.MiscAmt                   
          FROM MiscChg mc
          INNER JOIN InvoiceDetail id
            ON mc.InvoiceNum = id.InvoiceNum
            and mc.InvoiceLine = id.InvoiceLine
          WHERE mc.ChangeDate = ''2025-09-08''
        ) as SourceTable          
    PIVOT (
        MAX(MiscAmt)
        FOR Description IN (' + @miscChgDesc + ')
    ) AS PivotTable
    ) AS FinalTable
'

INSERT INTO MyFinalTable(
            InvoiceNum
            ,InvoiceLine
            ,ExtendedLineCost
            ,Freight
            ,Labor
            ,Restocking
            ,Surcharge
            ,Tariff
            ,TotalLinecost
    )
SELECT DISTINCT
         id.InvoiceNum
        ,id.InvoiceLine
        ,id.ExtendedLineCost
        ,g.Freight
        ,g.Labor
        ,g.Restocking
        ,g.Surcharge
        ,g.Tariff
        ,id.TotalLinecost
FROM InvoiceDetail id   
LEFT JOIN ##gblMiscChgs g
    ON id.InvoiceNum = g.InvoiceNum
    AND id.InvoiceLine = g.InvoiceLine
    
DROP TABLE  ##gblMiscChgs;
</code></pre>
<p>I have tried a number of techniques I found scouring through StackOverflow and other websites to include using <code>CASE</code> statements and <code>IF EXISTS</code> and I get the same error:</p>
<blockquote>
<p>field doesn't exist</p>
</blockquote>
<p>This is what the final result set should look like. If a line does not have a certain misc charge, that column should show <code>NULL</code>.</p>
<p><a href=""https://i.sstatic.net/oT1HTi7A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oT1HTi7A.png"" alt=""enter image description here"" /></a></p>
",1,1,0,2025-09-10T14:57:16+00:00,1,87,True
79761648,17916903,,sql,What is the optimal BigQuery SQL query to group the same data multiple times over different groups?,"<p>I have a very large BigQuery table with web events that I want to aggregate into daily, weekly and monthly numbers of visitors. And I want to join them into the same table.</p>
<p>My initial guess is to do something like the following query. Which works, but since this is a huge table, and the run time is many hours, I would like to know if this is optimal. The queries are a bit simplified. In practice I group by two more fields and I select a certain set of partitions of the table, because running the entire table at once runs into my project's limits in terms of workers.</p>
<pre><code>WITH base_data AS (
  SELECT
    DATE(event_time) AS date,
    DATE_TRUNC(DATE(event_time), WEEK(MONDAY)) AS weekly_date,
    DATE_TRUNC(DATE(event_time), MONTH) AS monthly_date,
    visitor_id
  FROM
    dataset.table
), daily_data AS (
  SELECT
    date,
    COUNT(DISTINCT visitor_id) AS daily_visitors
  FROM
    base_data
  GROUP BY
    date
), weekly_data AS (
  SELECT
    weekly_date AS date,
    COUNT(DISTINCT visitor_id) AS weekly_visitors
  FROM
    base_data
  GROUP BY
    date
), monthly_data AS (
  SELECT
    monthly_date AS date,
    COUNT(DISTINCT visitor_id) AS monthly_visitors
  FROM
    base_data
  GROUP BY
    date
)

SELECT
  daily_data.*,
  weekly_data.weekly_visitors,
  monthly_data.monthly_visitors
FROM
  daily_data
LEFT JOIN
  weekly_data
ON
  DATE_TRUNC(daily_data.date, WEEK(MONDAY)) = weekly_data.date
LEFT JOIN
  monthly_data
ON
  DATE_TRUNC(daily_data.date, MONTH) = monthly_data.date
</code></pre>
<p>Just from the query I would assume that this retrieves the data only once, stores it somehow, do the groupings and join. However, from the Execution graph it seems to scan the same data for every group by statement, which seems inefficient to me.</p>
<p><a href=""https://i.sstatic.net/Wi8P7Giw.png"" rel=""nofollow noreferrer"">screenshot of Execution graph</a></p>
<p>Alternatively, I thought to do the same thing using window functions, which I am always told BigQuery is optimized for. This makes the Execution graph look more linear, and it doesn't split up looking like it scans the dataset three times. But in practice, this takes longer.</p>
<pre><code>WITH base_data AS (
  SELECT
    DATE(event_time) AS date,
    DATE_TRUNC(DATE(event_time), WEEK(MONDAY)) AS weekly_date,
    DATE_TRUNC(DATE(event_time), MONTH) AS monthly_date,
    visitor_id
  FROM
    dataset.table
), step2 AS (
  SELECT
    *,
    COUNT(DISTINCT visitor_id) OVER (PARTITION BY weekly_date) AS weekly_visitors,
    COUNT(DISTINCT visitor_id) OVER (PARTITION BY monthly_date) AS monthly_visitors
  FROM
    base_data
)

SELECT
  date,
  COUNT(DISTINCT visitor_id) AS daily_visitors,
  ANY_VALUE(weekly_visitors) AS weekly_visitors,
  ANY_VALUE(monthly_visitors) AS monthly_visitors,
FROM
  step2
GROUP BY
  date
</code></pre>
<p>I want to know whether it is possible to have BigQuery perform this process significantly more optimally, or if this (the first query) is simply roughly the best you can do. Also, I would like to understand the way BigQuery handles these types of queries better.</p>
<p>Any help is greatly appreciated!</p>
",3,3,0,2025-09-11T08:14:55+00:00,1,120,True
79762091,19453035,,sql,SQL JSON_OBJECT in RPGLE Character conversion is not defined,"<p>I try to make a JSON_OBJECT in my SQLRPGLE Program. But even the simplest doesnt work:</p>
<pre><code>**FREE
ctl-opt dftactgrp(*no) actgrp(*caller);
dcl-s  PAYLOAD     varchar(1000);
 EXEC SQL
          SELECT CAST(
                   JSON_OBJECT(
                    'Test' value 'hello'
            ) AS CLOB(1000000))
    INTO :PAYLOAD
FROM SYSIBM.SYSDUMMY1;
*inlr = *on;
return;  
</code></pre>
<p>I always get SQLSTATE 57017 and SQLCODE -332. <a href=""https://www.ibm.com/docs/en/i/7.3.0?topic=codes-listing-sqlstate-values"" rel=""nofollow noreferrer""> Character conversion is not defined.</a>
The Machine is a 7.3 and my job ccsid is 1141.
I also tried to make the payload as an sqltype:</p>
<pre><code>dcl-s  PAYLOAD     SQLTYPE(CLOB:1000000) CCSID(1208) INZ;
</code></pre>
<p>but nothing works.
Does anyone have a solution?
Thanks.</p>
",1,1,0,2025-09-11T15:04:27+00:00,1,102,True
79762521,21098120,,sql,How to calculate 1-minute forward moving average,"<p>I'm working with stock market snapshot data (3-second intervals) in DolphinDB and need to calculate the 1-minute forward moving average for each data point.</p>
<p>For backward moving average (past 1 minute), I can simply use:
<code>tmavg(time, price, 60)</code></p>
<p>But I'm struggling with the forward version. I tried:</p>
<pre><code>select date, code, time, ti=move(time,60s), price=ask1 from t
</code></pre>
<p>This returns an error because move() doesn't accept DURATION type parameters.</p>
<p>Question:</p>
<ul>
<li>What's the correct way to calculate 1-minute forward moving average in DolphinDB?</li>
<li>Are there any built-in functions specifically designed for forward-looking windows?</li>
</ul>
<p>Additional Context:</p>
<ul>
<li>Database: DolphinDB V3.00.4</li>
<li>Data frequency: 3-second snapshots</li>
<li>Need: Average of prices in the 1-minute window following each data point</li>
</ul>
",0,0,0,2025-09-12T03:46:37+00:00,0,109,False
79762879,16997930,,sql,Measure prescription adherence (date picking up a prescription &lt;&gt; date starting the prescription),"<p>I have data that looks like this (in snowflake/SQL) (we will call the table <strong>RX</strong>):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Person ID</th>
<th style=""text-align: center;"">Rx Filled Date</th>
<th style=""text-align: center;"">Days Supply</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 1, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 8, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 15, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 30, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">February 5, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">March 1, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">March 9, 2025</td>
<td style=""text-align: center;"">10</td>
</tr>
</tbody>
</table></div>
<p><strong>Person ID</strong> = the identifier of each unique person</p>
<p><strong>RX Filled Date</strong> = when the person picked up each prescription</p>
<p><strong>Days Supply</strong> = how many days the prescription should last</p>
<p>I'm trying to create a column that indicates when they would actually need to start taking each prescription. For example, the person's first prescription was picked up on January 1 and will last 10 days. Even though their second was filled on January 8, they won't actually start taking it until January 11. Even though their third was filled on January 15, they won't actually start taking it until January 21....and so on.</p>
<p>There may also be instances (as see in the second to last row) where they have gaps in their drugs that exceed the number of days supply they have. The member picked up a prescription on February 5 but wouldn't start taking it until February 10 and it would last them until February 19. However, they didn't pick up another prescription until March 1 which means that prescription would actually start on March 1.</p>
<p>Essentially I'm trying to create a column that indicates a true &quot;when did this member start taking this fill&quot;. Based on this made up data, it would look like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Person ID</th>
<th style=""text-align: center;"">Rx Filled Date</th>
<th style=""text-align: center;"">Days Supply</th>
<th style=""text-align: center;"">Actual Start Date</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 1, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">January 1, 2025</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 8, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">January 11, 2025</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 15, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">January 21, 2025</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">January 30, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">January 31, 2025</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">February 5, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">February 10, 2025</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">March 1, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">March 1, 2025</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">March 9, 2025</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: center;"">March 11, 2025</td>
</tr>
</tbody>
</table></div>
<p>This would need to be done on a person by person basis.</p>
",5,5,0,2025-09-12T11:38:56+00:00,2,108,True
79762940,31476322,,sql,Merge 2 rows with same primary key into 1 by adding a new column,"<p>I have a table with 2 columns:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>EMPLOYER_ID</th>
<th>PLAN_NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>Aetna</td>
</tr>
<tr>
<td>1234</td>
<td>Cigna</td>
</tr>
</tbody>
</table></div>
<p>What would be my SQL query to change it to:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>EMPLOYER_ID</th>
<th>PLAN_NAME</th>
<th>PLAN_NAME2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>Aetna</td>
<td>Cigna</td>
</tr>
</tbody>
</table></div>
<p>Basically I want to create a new column and move the data from the second row to that new column as long as the primary key is the same.</p>
<p>I tried multiple queries.</p>
",0,0,0,2025-09-12T12:32:36+00:00,1,127,False
79763147,26968350,,sql,How to efficiently query nested JSON fields in GridDB using TQL?,"<p>I’m experimenting with GridDB CE for a project where sensor data is stored as JSON objects inside a BLOB column. For example, I have a container defined as:</p>
<pre><code>    ContainerInfo containerInfo = new ContainerInfo();
containerInfo.setName(&quot;SensorData&quot;);
containerInfo.setColumnInfoList(new ColumnInfo[] {
    new ColumnInfo(&quot;id&quot;, GSType.STRING),
    new ColumnInfo(&quot;timestamp&quot;, GSType.TIMESTAMP),
    new ColumnInfo(&quot;payload&quot;, GSType.BLOB) // contains JSON
});
</code></pre>
<p>A typical payload JSON looks like this:</p>
<pre><code>{
  &quot;temperature&quot;: 22.5,
  &quot;humidity&quot;: 60,
  &quot;location&quot;: {
    &quot;lat&quot;: 36.812,
    &quot;lon&quot;: 10.165
  }
}
</code></pre>
<p>I’d like to query all rows where temperature &gt; 25 AND location.lat &gt; 35 using TQL, without having to pull all the rows into memory and parse the JSON manually.</p>
<p><strong>I’ve tried something like:</strong></p>
<pre><code>SELECT * FROM SensorData WHERE payload.temperature &gt; 25
</code></pre>
<p>But GridDB throws a syntax error saying the column does not exist.</p>
<p><strong>Question:</strong></p>
<p>Is there a way in GridDB to <strong>query JSON fields inside a BLOB column directly using TQL?</strong> If not, what’s the recommended schema design to handle this type of semi-structured data so I can still query it efficiently?</p>
",1,1,0,2025-09-12T16:01:52+00:00,0,57,False
79763306,31478792,,sql,"How to group by day in GridDB Cloud without manually concatenating year, month, and day?","<p>Table schema:</p>
<pre><code>CREATE TABLE WeatherReadings 
(
    ts TIMESTAMP,
    temp DOUBLE
);
</code></pre>
<p>Sample data:</p>
<pre><code>INSERT INTO WeatherReadings (ts, temp) 
VALUES
(TIMESTAMP('2025-08-22T01:05:00Z'), 20.5),
(TIMESTAMP('2025-08-22T05:25:00Z'), 21.8),
(TIMESTAMP('2025-08-22T12:15:00Z'), 29.2),
(TIMESTAMP('2025-08-23T03:10:00Z'), 19.5);
</code></pre>
<p>What I tried: I attempted to use <code>EXTRACT</code> on the timestamp column for grouping:</p>
<pre><code>SELECT 
    EXTRACT(YEAR, ts), 
    EXTRACT(MONTH, ts), 
    EXTRACT(DAY, ts), 
    MIN(temp), 
    MAX(temp)
FROM 
    WeatherReadings
GROUP BY 
    EXTRACT(YEAR, ts), 
    EXTRACT(MONTH, ts), 
    EXTRACT(DAY, ts);
</code></pre>
<p>But this query fails with an error:</p>
<blockquote>
<p>The invalid query was specified</p>
</blockquote>
<p>I expected to get min/max temperatures grouped by each calendar day:</p>
<pre><code>2025-08-22 → Min=20.5, Max=29.2
2025-08-23 → Min=19.5, Max=19.5
</code></pre>
<p>Documentation reference:</p>
<p>The GridDB docs for <a href=""https://docs.griddb.net/sqlreference/sql-commands-supported.html#extract"" rel=""nofollow noreferrer""><code>EXTRACT</code></a> show examples with literal timestamps (e.g. <code>EXTRACT(YEAR, TIMESTAMP('2018-12-01T10:30:02.392Z'))</code>), but do not explain whether <code>EXTRACT</code> can be used with columns in <code>GROUP BY</code>.</p>
<p><strong>Question</strong></p>
<p>Is there a way in GridDB Cloud to group directly by date (day precision) without having to manually concatenate <code>EXTRACT(YEAR)</code>, <code>EXTRACT(MONTH)</code>, and <code>EXTRACT(DAY)</code> into a string? Or is this not supported?</p>
",3,3,0,2025-09-12T19:55:37+00:00,1,99,True
79763815,17236968,,sql,Complex SQL query not sorting correctly,"<p>I have a complex SQL query that is not sorting the results properly. It is supposed to sort the list of questions in order of the <code>ORDERS</code> column of the <code>Questions</code> table, but instead, it is always sorting by the <code>ID</code> of the <code>Questions</code>.</p>
<p>If I change the order of the questions via the <code>ORDERS</code> column, then the questions are still listed in the order of the <code>ID</code> column, but they're numbered in the order of the <code>ORDERS</code> column.</p>
<pre><code>            SELECT 
                QID,
                QUESTION,
                QORDER,
                ISREQUIRED,
                QUESTIONTYPE,
                AID,
                ANSWER, 
                (SELECT  
                     CASE WHEN IDANSWER = 0
                        THEN ANSWER
                        ELSE IDANSWER
                     END 
                 FROM &quot;.__DBC_SCHEMATA_SURVEY_RESULTS__.&quot;
                 WHERE IDSURVEY = &quot;.(int)$idsurvey.&quot;
                   AND IDUSER = &quot;.(int)$iduser.&quot;
                   AND IDQUESTION = QID
                   AND IDANSWER = CASE WHEN IDANSWER &gt; 0
                                           THEN AID
                                           ELSE 0
                                  END
                ) AS RAID
            FROM(SELECT                   
               Q.ID AS QID,
               Q.QUESTION,
               Q.ORDERS AS QORDER,
               Q.ISREQUIRED,
               Q.QUESTIONTYPE,                   
               A.ID AS AID,
               A.ANSWER,
               A.ORDERS AS AORDER
            FROM
                &quot;.__DBC_SCHEMATA_QUESTIONS__.&quot; Q 
            LEFT OUTER JOIN 
                &quot;.__DBC_SCHEMATA_ANSWERS__.&quot; A                
            ON
                Q.ID = A.IDQUESTION
            WHERE
                Q.IDSURVEY=&quot;.(int)$idsurvey.&quot;
            ORDER BY 
                Q.ORDERS, A.ORDERS) R
</code></pre>
",-2,1,3,2025-09-13T16:12:53+00:00,1,130,True
79764873,2353911,"Berlin, Germany",sql,Way to re-compute CURRENT_DATE or SYSDATE for each record in a batch write?,"<p>Consider the following test SQL:</p>
<pre><code>create table TEST
(
    ID VARCHAR(20) ,
    THE_DATE              TIMESTAMP(6)      default SYSTIMESTAMP not null
);

INSERT INTO TEST (ID,THE_DATE)
WITH foo AS (
    SELECT 'a', CURRENT_DATE FROM dual UNION ALL
    SELECT 'b', CURRENT_DATE FROM dual
)
SELECT * FROM foo
</code></pre>
<p>This creates a dummy test table and populates 2 records in a batch insert statement.</p>
<p>The <code>THE_DATE</code> field of both records will be populated with the exact same date/time, including nanos.</p>
<p>Similarly:</p>
<pre><code>UPDATE TEST SET UPD_DATE=SYSDATE
</code></pre>
<p>... will once again set the exact same date/time for all impacted records' fields.</p>
<hr />
<p>I am wondering whether there is a way to force the <code>CURRENT_DATE</code> or <code>SYSDATE</code> function to be re-computed for each record impacted in the batch statement, so that the records created/updated will have a slightly different date (at least in terms of nanos), representing the exact time each record is modified.</p>
<p>Using Oracle in a range between v. 19 to 23c if that's any relevant.</p>
",5,5,0,2025-09-15T08:02:15+00:00,1,166,True
79765477,5429268,United States,sql,Exception executing SQL update,"<p>I just restored a database using a different owner and database name. Upon starting the application, I'm getting the following error:</p>
<pre><code>    Exception executing SQL update &lt;ALTER TABLE \&quot;AO_9412A1_AOUSER\&quot; ADD CONSTRAINT       U_AO_9412A1_AOUSER_USERNAME UNIQUE (\&quot;USERNAME\&quot;)&gt;
org.postgresql.util.PSQLException: ERROR: could not create unique index &quot;u_ao_9412a1_aouser_username&quot;
  Detail: Key (&quot;USERNAME&quot;)=(8a48869174e61af50178d2469d490089) is duplicated
</code></pre>
<p>So, I assuming that '8a48869174e61af50178d2469d490089' is already in that table.</p>
<p>I do the following command</p>
<pre><code>select * from &quot;AO_9412A1_AOUSER&quot; where 'USERNAME' = '8a48869174e61af50178d2469d490089';
</code></pre>
<p>And I get</p>
<p>(0 rows)</p>
<p>Any tips?</p>
",0,0,0,2025-09-15T18:19:01+00:00,1,51,True
79765624,31496851,,sql,"Query that checks if an item name contains a substring, and if true, perform a second query that grabs an image with a specific name","<p>I have this simple query that makes an image appear in a report based on its name</p>
<pre><code>SELECT image_data 
FROM image 
WHERE ((image_name='logo'));
</code></pre>
<p>I was able to modify this query so that if the name of the image exactly matches the item number listed on the report, the image will appear. This way I'm not limited to one specific image.</p>
<pre><code>SELECT image_data
FROM image
WHERE (image_name = (SELECT item_number FROM wo 
         JOIN itemsite ON wo_itemsite_id=itemsite_id
         JOIN item ON itemsite_item_id=item_id
WHERE wo_id=&lt;? value(&quot;wo_id&quot;) ?&gt;));
</code></pre>
<p>Now I'm trying to figure out how to modify it further so that the image name doesn't have to be a one to one match.</p>
<p>I'm imagining a case expression like this</p>
<pre><code>SELECT image_data
FROM image
WHERE (image_name = (
    SELECT item_number, 
        CASE
            WHEN item_number LIKE 'ITEM_A%' THEN 'ITEM A IMAGE'
            WHEN item_number LIKE 'ITEM_B%' THEN 'ITEM B IMAGE' 
            ELSE ''
        END 
    FROM wo
         JOIN itemsite ON wo_itemsite_id=itemsite_id
         JOIN item ON itemsite_item_id=item_id
    WHERE wo_id=&lt;? value(&quot;wo_id&quot;) ?&gt;
));
</code></pre>
<p>Unfortunately this doesn't seem to work. I've tried putting the case expression inside &quot;WHERE&quot;, &quot;FROM&quot;, and &quot;SELECT&quot;, but no success. I even tried making two separate queries but that didn't work either. What am I doing wrong?</p>
<p>9/16/25 EDIT:
I think Zegarek's solution is what I'm looking for but I still can't get it to work. It might just be a syntax error on my end though.</p>
<p>my original modified query had some other JOINs that I initially cut out of my question for readability. I'm VERY new to SQL so I thought showing those JOINs might make things more confusing. I added them back in up above.</p>
<p>Here's how I tried to implement Zegarek's solution</p>
<pre><code>SELECT a.image_data,
      CASE WHEN b.item_number LIKE 'PRODUCT-ANLG%' THEN 'PRODUCT ANALOG DATASHEET PAGE 1'
            WHEN b.item_number LIKE 'PRODUCT 21%' THEN 'PRODUCT 21 DATASHEET' 
            ELSE ''
       END AS image_label
FROM image AS a JOIN wo AS b ON a.image_name=b.item_number
          JOIN itemsite ON wo_itemsite_id=itemsite_id
                  JOIN item ON itemsite_item_id=item_id 
WHERE (b.wo_id = &lt;? value(&quot;wo_id&quot;) ?&gt;);
</code></pre>
<p>I then named the query &quot;DataSheetImage&quot; and then called the image_data column on the report.
<a href=""https://i.sstatic.net/TvQKhDJj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TvQKhDJj.png"" alt=""enter image description here"" /></a></p>
<p>Unfortunately this still didn't work. I tried calling image_data, image_label,  a.image_data, and a.image_label, but the image won't appear.</p>
<p>I'm thinking the problem is either
A. a syntax issue with the JOINs.
or
B. me not understanding how the &quot;image_label&quot; alias works and how to properly call it.</p>
<p>Where am I going wrong? What am I misunderstanding here?</p>
<p>SOLUTION EDIT: I finally was able to get the query working with a lot of help from Zegarek. Here is the solution that ended up working for me in Postgresql 9.6</p>
<pre><code>SELECT image_data
FROM image
WHERE (image_name = (SELECT image_name
         FROM wo
         JOIN itemsite ON wo_itemsite_id=itemsite_id
         JOIN item ON itemsite_item_id=item_id
INNER JOIN image ON (REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')=REGEXP_REPLACE(item_number,'^PRODUCT.(\w+).*','\1'))
         WHERE wo_id= &lt;? value(&quot;wo_id&quot;) ?&gt; ));
</code></pre>
<p>The solution was to JOIN the wo and image tables together and then compare image_name and item_number using REGEXP_REPLACE. I don't fully understand how the REGEXP_REPLACE works yet, but I think I can figure out the rest from here.</p>
<p>Here is the db fiddle showing how the tables look <a href=""https://dbfiddle.uk/-VaSugr5"" rel=""nofollow noreferrer"">https://dbfiddle.uk/-VaSugr5</a></p>
<p>Thank you so much to Zegarek for helping me figure this out.</p>
",1,1,0,2025-09-15T21:18:27+00:00,1,170,True
79765875,1633272,,sql,CASE-WHEN impacted by where clause,"<p>I have table saa_prices (id, symbol, price, date). And I create view to query quarterly prices.</p>
<pre><code>CREATE OR REPLACE VIEW saa_quarterly_prices_final1 AS
WITH last_dates AS (
    SELECT 
        symbol,
        MAX(date) AS last_date,
        YEAR(MAX(date)) AS report_year,
        QUARTER(MAX(date)) AS report_quarter
    FROM 
        saa.saa_prices
    GROUP BY 
        symbol, YEAR(date), QUARTER(date)
)
SELECT 
    p.symbol,
    ld.last_date AS date,
    p.price,
    ld.report_year AS year,
    ld.report_quarter AS quarter,
    CASE ld.report_quarter
        WHEN 1 THEN CAST(CONCAT(ld.report_year, '-03-31') AS DATE)
        WHEN 2 THEN CAST(CONCAT(ld.report_year, '-06-30') AS DATE)
        WHEN 3 THEN CAST(CONCAT(ld.report_year, '-09-30') AS DATE)
        WHEN 4 THEN CAST(CONCAT(ld.report_year, '-12-31') AS DATE)
    END AS report_date
FROM 
    last_dates ld
JOIN 
    saa.saa_prices p ON ld.symbol = p.symbol AND ld.last_date = p.date;
</code></pre>
<p>Query：</p>
<pre><code>SELECT p.* FROM saa.saa_quarterly_prices_final1 p
WHERE p.symbol IN ('600519') AND p.report_date &gt; '2025-01-01';
</code></pre>
<p>Query result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>symbol</th>
<th>date</th>
<th>price</th>
<th>year</th>
<th>quarter</th>
<th>report_date</th>
</tr>
</thead>
<tbody>
<tr>
<td>600519</td>
<td>2025-03-31</td>
<td>1561</td>
<td>2025</td>
<td>1</td>
<td>2020-03-31</td>
</tr>
<tr>
<td>600519</td>
<td>2025-06-30</td>
<td>1409.52</td>
<td>2025</td>
<td>2</td>
<td>2020-03-31</td>
</tr>
<tr>
<td>600519</td>
<td>2025-07-31</td>
<td>1421.67</td>
<td>2025</td>
<td>3</td>
<td>2020-03-31</td>
</tr>
</tbody>
</table></div>
<p>Why <code>year</code>, <code>quarter</code> is calculated correctly, but report_date is always '2020-03-31' and conflict with <code>where</code> statement?</p>
<p>If I query using this sql it returns correctly:</p>
<pre><code>SELECT p.*,
CASE p.quarter
        WHEN 1 THEN CAST(CONCAT(p.year, '-03-31') AS DATE)
        WHEN 2 THEN CAST(CONCAT(p.year, '-06-30') AS DATE)
        WHEN 3 THEN CAST(CONCAT(p.year, '-09-30') AS DATE)
        WHEN 4 THEN CAST(CONCAT(p.year, '-12-31') AS DATE)
    END AS report_date1
FROM saa.saa_quarterly_prices_final1 p
WHERE p.symbol IN ('600519') AND p.report_date &gt; '2025-01-01';
</code></pre>
<p>Result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>symbol</th>
<th>date</th>
<th>price</th>
<th>year</th>
<th>quarter</th>
<th>report_date</th>
<th>report_date1</th>
</tr>
</thead>
<tbody>
<tr>
<td>600519</td>
<td>2025-03-31</td>
<td>1561</td>
<td>2025</td>
<td>1</td>
<td>2020-03-31</td>
<td>2025-03-31</td>
</tr>
<tr>
<td>600519</td>
<td>2025-06-30</td>
<td>1409.52</td>
<td>2025</td>
<td>2</td>
<td>2020-03-31</td>
<td>2025-06-30</td>
</tr>
<tr>
<td>600519</td>
<td>2025-07-31</td>
<td>1421.67</td>
<td>2025</td>
<td>3</td>
<td>2020-03-31</td>
<td>2025-09-30</td>
</tr>
</tbody>
</table></div>
<p>MySQL version and saa_prices table:</p>
<pre><code>SELECT VERSION();
# VERSION()
'8.0.25'

show create table saa.saa_prices;
# Table, Create Table
'saa_prices', 'CREATE TABLE `saa_prices` (\n  `id` int NOT NULL AUTO_INCREMENT,\n  `symbol` varchar(8) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,\n  `price` double NOT NULL,\n  `date` date NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `symbol_date_UNIQUE` (`symbol`,`date`)\n) ENGINE=InnoDB AUTO_INCREMENT=5414885 DEFAULT CHARSET=utf8mb3'
</code></pre>
",0,0,0,2025-09-16T07:08:30+00:00,1,148,True
79765969,11065741,"Colombo, Sri Lanka",sql,Get total connection sold by same sale person to same customer,"<p>I want to add a new column to the sales table that shows, for each row, how many connections the same salesperson sold to the same customer (by NIC).</p>
<pre><code>SALE TABLE

Sale_Date   Sale_Person Connection
13/08/2025   7689       74856921 
15/09/2025   7689       77895412 
17/09/2025   1234       77453956
18/09/2025   4535       77455921
20/09/2025   1234       77345962
20/09/2025   7689       76567890
21/09/2025   3245       77535053
</code></pre>
<pre><code>Customer Table
Connection NIC
77453956   563563204V
77455921   345060425V
76567890   456723457V
77345962   563563204V
77895412   456723457V
74856921   456723457V
</code></pre>
<p>Expecting Output table as Sale table</p>
<pre><code>Sale_Date  Sale_Person Connection no_of_connection
13/08/2025 7689        74856921    3
15/09/2025 7689        77895412    3
17/09/2025 1234        77453956    2 
18/09/2025 4535        77455921    1 
20/09/2025 1234        77345962    2
20/09/2025 7689        76567890    3
21/09/2025 3245        77535053    0
</code></pre>
<p>This is the code I have tried, but it gives the wrong answer while using to actual table.</p>
<pre><code>First method:

SELECT 
    s.sale_date,
    s.salesperson_id,
    s.connection_number,
    s.sale_district,
        COUNT(c.connection_number) 
            OVER (PARTITION BY s.salesperson_id, c.nic_number) AS no_of_connections
FROM sales s
LEFT JOIN customer c
    ON s.connection_number = c.connection_number;

Second method:
WITH CLM_TABLE AS (
SELECT BI_ACCOUNT_ID, IDENTIFICATION_NUMBER
FROM CUSTOMER
WHERE ACCOUNT_TYPE = 'PREPAID'
AND IDENTIFICATION_NUMBER IS NOT NULL
    QUALIFY ROW_NUMBER() OVER (PARTITION BY BI_ACCOUNT_ID ORDER BY IDENTIFICATION_NUMBER) = 1
)

SELECT S.*,    
COUNT(c2.BI_ACCOUNT_ID) OVER (PARTITION BY S.PROFILE_ID, c.IDENTIFICATION_NUMBER) AS no_of_connection 
FROM ACTIVATION_DAILY S 
LEFT JOIN CLM_TABLE c  ON S.BI_ACCOUNT_ID = c.BI_ACCOUNT_ID 
LEFT JOIN CLM_TABLE c2 ON c.IDENTIFICATION_NUMBER = c2.IDENTIFICATION_NUMBER     
AND EXISTS ( SELECT 1          
             FROM ACTIVATION_DAILY S2          
             WHERE S2.BI_ACCOUNT_ID = c2.BI_ACCOUNT_ID            
             AND S2.PROFILE_ID = S.PROFILE_ID ) ```
</code></pre>
",1,2,1,2025-09-16T08:30:06+00:00,0,90,False
79766399,29480124,,sql,How to create pedegree for animals,"<p>I want to know how I can dynamically display from the database the Sire(paternal) and Dam(maternal) of all animals. For instance, Bull A and Cow A are the parents of Bull B and they are also the grandparent of Bull C. My problem is that I don't know how to display each animal parents (father &amp; mother), grandparents, and great grandparents. See codes below that I had tried.</p>
<pre><code>SELECT 
  a.Tagno,
  a.Animal AS Animal,
  a.Dam,
  a.Sire,
  p.Animal AS Parent,
  gp.Animal AS GrandParent,
  ggp.Animal AS GreatGrandParent,
  gggp.Animal AS GreatGreatGrandParent,
  ggggp.Animal AS GreatGreatGreatGrandParent
FROM Animals a
LEFT JOIN Animals p ON a.Offspringtag = p.Tagno
LEFT JOIN Animals gp ON p.Offspringtag = gp.Tagno
LEFT JOIN Animals ggp ON gp.Offspringtag = ggp.Tagno
LEFT JOIN Animals gggp ON ggp.Offspringtag = gggp.Tagno
LEFT JOIN Animals ggggp ON gggp.Offspringtag = ggggp.Tagno
</code></pre>
<p>Expected output</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>Tagno</th>
<th>Animal</th>
<th>Gender</th>
<th>DOB</th>
<th>Parent</th>
<th>Grandparents</th>
<th>Great Grandparents</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td>101</td>
<td>Bull A</td>
<td>Male</td>
<td>11/02/2020</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>02</td>
<td>102</td>
<td>Cow A</td>
<td>Female</td>
<td>11/02/2021</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>03</td>
<td>103</td>
<td>Bull B</td>
<td>Male</td>
<td>2/10/2022</td>
<td>Bull A</td>
<td></td>
<td></td>
</tr>
<tr>
<td>04</td>
<td>103</td>
<td>Bull B</td>
<td>Male</td>
<td>2/10/2022</td>
<td>Cow A</td>
<td></td>
<td></td>
</tr>
<tr>
<td>05</td>
<td>104</td>
<td>Cow B</td>
<td>Female</td>
<td>22/11/2022</td>
<td>Bull A</td>
<td></td>
<td></td>
</tr>
<tr>
<td>06</td>
<td>104</td>
<td>Cow B</td>
<td>Female</td>
<td>22/11/2022</td>
<td>Cow A</td>
<td></td>
<td></td>
</tr>
<tr>
<td>07</td>
<td>106</td>
<td>Bull C</td>
<td>Male</td>
<td>22/07/2025</td>
<td>Bull B</td>
<td>Bull A</td>
<td></td>
</tr>
<tr>
<td>08</td>
<td>106</td>
<td>Bull C</td>
<td>Male</td>
<td>22/07/2025</td>
<td>Cow B</td>
<td>Cow A</td>
<td></td>
</tr>
</tbody>
</table></div>
",0,1,1,2025-09-16T15:20:40+00:00,1,106,True
79766716,13483388,,sql,"Return a row where a match exists, otherwise return row where value is NULL","<p>Let's use the following set of sample data and variables:</p>
<pre><code>-- Create temporary table of sample data
SELECT *
INTO #t
FROM (VALUES (1, 'United States', 'CA', NULL, 'abc')
            ,(2, 'United States', 'CA', NULL, 'def')
            ,(3, 'United States', 'CA', NULL, 'ghi')
            ,(4, 'United States', 'CA', 'Los Angeles', 'abc')
            ,(5, 'United States', 'CA', 'Los Angeles', 'def')
            ,(6, 'United States', 'CA', 'Los Angeles', 'ghi')
            ,(7, 'United States', 'NY', NULL, 'jkl')
            ,(8, 'United States', 'WA', NULL, 'mno')
            ,(9, 'United States', NULL, NULL, 'pqr')
            ,(10, 'Canada', 'AB', NULL, 'stu')
            ,(11, 'Canada', 'BC', 'Vancouver', 'vwx')
     ) t([Id], [Country], [Region], [City], [Value])

-- Create variables to filter table
DECLARE @Country varchar(15)
       ,@Region char(2)
       ,@City varchar(100)

-- Select filtered table of data
SELECT *
FROM #t t

-- Drop temporary table
DROP TABLE #t
</code></pre>
<p>I want to know how to write a query that filters the data first on a match with the country, then the region, then the city. The country will always have a match; however, if the region does not find a match, I want it to return where the other filters match but the region is <code>NULL</code>. Similarly, if the city does not find a match, I want it to return where the other filters match but the city is <code>NULL</code>.</p>
<p>Hopefully the following examples help demonstrate the behavior I want to see:</p>
<p><strong>Example #1</strong></p>
<p>If <code>@Country = 'United States'</code>, <code>@Region = 'TX'</code>, and <code>@City = NULL</code>, then I'd want it to return the line where Id = 9</p>
<p><strong>Example #2</strong></p>
<p>If <code>@Country = 'United States', @Region = 'CA', and @City = 'San Francisco'</code>, then I'd want it to return the lines where Id = 1, 2, and 3</p>
<p><strong>Example #3</strong></p>
<p>If <code>@Country = 'United States', @Region = 'CA', and @City = 'Los Angeles'</code>, then I'd want it to return the lines where Id = 4, 5, and 6.</p>
",0,1,1,2025-09-16T21:48:48+00:00,3,132,True
79766822,20064390,,sql,Issue with Cast in SQL Case expression - not working properly,"<p>I am using SQL Server 2017 and I am writing an <code>UPDATE</code> statement with a <code>CASE</code> expression. In the update statement I am having an issue with the calculation. It is a simple calculation  when I cast the value I should get the following value: 4303558.530604</p>
<p>However, I get this value: 4303549.923500000000000</p>
<p>Here is the case expression</p>
<pre><code>--Compute FineArtsReq
UPDATE iac_admin.statisticsSFAFacilitySDGeneration
SET mtr_fineArtsReq  =  0

UPDATE iac_admin.statisticsSFAFacilitySDGeneration
SET mtr_fineArtsReq  =  COALESCE( CASE 
                WHEN mtr_schoolTypeNumber = 1 THEN 0 * mtr_sdMultiplier
                WHEN mtr_schoolTypeNumber = 2 THEN try_cast(mtr_futureEnrollment * 4.0 * mtr_sdMultiplier as decimal(25,13))
                WHEN mtr_schoolTypeNumber = 3 THEN try_cast(mtr_futureEnrollment * 5.0 * mtr_sdMultiplier as decimal(25,13))
                WHEN mtr_schoolTypeNumber = 4 THEN mtr_futureEnrollment * cast(3.0/10  as decimal(20,15))*4.0* mtr_sdMultiplier 
                WHEN mtr_schoolTypeNumber = 5 THEN mtr_futureEnrollment * cast(3.0/10  as decimal(20,15))*4.0 * mtr_sdMultiplier  
                WHEN mtr_schoolTypeNumber = 6 THEN (mtr_futureEnrollment*cast(3.0/14  as decimal(20,15))*4.0) *  (mtr_futureEnrollment*cast(4.0/14  as decimal(20,15))*4.0) * mtr_sdMultiplier 
                WHEN mtr_schoolTypeNumber = 7 THEN mtr_futureEnrollment*cast(3.0/7 as decimal(20,15))*4.0*mtr_futureEnrollment*cast(4.0/7 as decimal(20,15))*4.0 * mtr_sdMultiplier
                WHEN mtr_schoolTypeNumber = 8 THEN 0 * mtr_sdMultiplier
                WHEN mtr_schoolTypeNumber = 9 THEN 0 * mtr_sdMultiplier
                WHEN mtr_schoolTypeNumber = 10 THEN 0 * mtr_sdMultiplier
                WHEN mtr_schoolTypeNumber = 11 THEN 0 * mtr_sdMultiplier
                WHEN mtr_schoolTypeNumber = 12 THEN 0 * mtr_sdMultiplier
                ELSE 0
                end,0)
</code></pre>
<p>The value for <code>mtr_schoolTypeNumber</code> is 7.<br />
The value for <code>mtr_sdMultiplier</code> is 1.<br />
The value for <code>mtr_futureEnrollment</code> is 1048.00</p>
<p>Expected value: 4303558.530604</p>
<p>This is the value I am getting instead: 4303549.923500000000000</p>
<p>I am not sure how to fix this issue or why the issue is occurring.  When I write the case expression in a select statement it works perfectly however, not in the code as provided.</p>
<pre><code>pk_Id   
1167
    
mtr_futureEnrollment    
1048.000    

mtr_sdMultiplier    
1   
                
CurrentComputedValue
4303549.923500  

ExpectedValue
4303558.530604
</code></pre>
<p>Any assistance and insight would be greatly appreciated</p>
",1,2,1,2025-09-17T02:15:33+00:00,1,157,False
79767126,395720,,sql,Prioritize strings with more uppercase characters,"<p>I have the following query in SQL Server</p>
<pre><code>select distinct [data]
from (
    select 'A' as [data]
    union all select 'a'
    union all select 'b'
    union all select 'B'
    union all select 'C'
    union all select 'c'
    union all select 'Random_Word_1'
    union all select 'Random_WORD_1'
    union all select 'RAndom_WORD_1'
    union all select 'RaNdom_WORD_1'
    union all select 'RaNdoM_WORD_1'
    union all select 'RanDoM_WORD_1'
    union all select 'Random_WORD_2'
    union all select 'Random_Word_2'
) as tbl


data
-----------------
a
B
C
Random_WORD_1
Random_Word_2
</code></pre>
<p>I'm looking for a way to select distinct string values that have more upper case characters. Between <code>a</code> and <code>A</code> I need <code>A</code>, and between <code>Random_Word_1</code> and <code>Random_WORD_1</code> I need <code>Random_WORD_1</code>. If the 2 (or more) strings have the same number of uppercase values, the value with the most leading uppercase letter should be prioritised.. How can I make the <code>DISTINCT</code> operator prioritise the values I need, rather than what appear to be an arbitrary value?</p>
<p>So my expected results are:</p>
<pre><code>data
-----------------
A
B
C
RaNdoM_WORD_1
Random_WORD_2
</code></pre>
",-5,0,5,2025-09-17T09:50:14+00:00,1,126,True
79767306,8411980,,sql,DuckDB query that works with time intervals produces incorrect values,"<p>Running through python - no tables needed. See below query and result:</p>
<pre class=""lang-py prettyprint-override""><code>import duckdb

sampling_period_sec = 13
date_range = ('2023-01-01', '2023-01-02')

db_conn = duckdb.connect()

db_conn.query(
    f&quot;&quot;&quot;
    DROP TABLE IF EXISTS date_range_query;
    CREATE TEMPORARY TABLE date_range_query AS
    SELECT
        DATE_ADD(
            CAST('1970-01-01 00:00:00' AS TIMESTAMP), 
            TO_SECONDS(CAST(
                CEIL(
                    EXTRACT(EPOCH FROM RANGE)
                    / {sampling_period_sec}) AS INT
                ) * {sampling_period_sec}
            )
        ) AS ts_event
    FROM RANGE(
        TIMESTAMP '{date_range[0]}',
        TIMESTAMP '{date_range[1]}',
        INTERVAL {sampling_period_sec} SECOND);

    SELECT
        t1.ts_event
        ,t2.ts_event as ts_window
        ,EXP((MILLISECOND(t2.ts_event - t1.ts_event) / 10**3) / {sampling_period_sec} / {9}) as inter_decay
    FROM date_range_query t1
        JOIN date_range_query t2
            ON t2.ts_event BETWEEN t1.ts_event - INTERVAL
            {int(10 * sampling_period_sec)} SECOND AND t1.ts_event
            AND t1.ts_event in ('2023-01-01 00:01:03', '2023-01-01 00:01:16')
    ORDER BY 1, 2
    &quot;&quot;&quot;
).df()
</code></pre>
<p>Result:</p>
<pre>
              ts_event           ts_window  inter_decay
0  2023-01-01 00:01:03 2023-01-01 00:00:11     0.641180
1  2023-01-01 00:01:03 2023-01-01 00:00:24     0.716531
2  2023-01-01 00:01:03 2023-01-01 00:00:37     0.800737
3  2023-01-01 00:01:03 2023-01-01 00:00:50     0.894839
4  2023-01-01 00:01:03 2023-01-01 00:01:03     1.000000
<b>5  2023-01-01 00:01:16 2023-01-01 00:00:11     0.958165</b>
6  2023-01-01 00:01:16 2023-01-01 00:00:24     0.641180
7  2023-01-01 00:01:16 2023-01-01 00:00:37     0.716531
8  2023-01-01 00:01:16 2023-01-01 00:00:50     0.800737
9  2023-01-01 00:01:16 2023-01-01 00:01:03     0.894839
10 2023-01-01 00:01:16 2023-01-01 00:01:16     1.000000
</pre>
<p>The <b>bold-face</b> row, 5, is the problem. The <code>inter_decay</code> should be lower than in row 6, but it is not. Should be <code>0.624949</code> but it is <code>0.958165</code>.</p>
<p>I was able to show that:</p>
<pre><code>SELECT
   MICROSECOND(TIMESTAMP '2023-01-01 00:00:24' - TIMESTAMP '2023-01-01 00:01:06'),
   MICROSECOND(TIMESTAMP '2023-01-01 00:00:11' - TIMESTAMP '2023-01-01 00:01:06')
</code></pre>
<p>Produces: -52000000, -5000000</p>
<p>This explains the problem. But then after playing a little with the calculations (using MICROSECONDS/MILLISECONDS and POSITIVE/NEGATIVE intervals) the values for the interval came out correct. But the main query above still produces a wrong decay. This looks like some problem with DuckDB's engine, because I can't figure out what I am doing incorrectly, if anything.</p>
<p><b> ADDITION:</b></p>
<p>Adding another column:</p>
<pre><code>MILLISECOND(t2.ts_event - t1.ts_event) / 10**3 as delta_time
</code></pre>
<p>Results - shows my claim:</p>
<pre>
              ts_event           ts_window  delta_time  inter_decay
0  2023-01-01 00:01:03 2023-01-01 00:00:11       -52.0     0.641180
1  2023-01-01 00:01:03 2023-01-01 00:00:24       -39.0     0.716531
2  2023-01-01 00:01:03 2023-01-01 00:00:37       -26.0     0.800737
3  2023-01-01 00:01:03 2023-01-01 00:00:50       -13.0     0.894839
4  2023-01-01 00:01:03 2023-01-01 00:01:03         0.0     1.000000
5  2023-01-01 00:01:16 2023-01-01 00:00:11        -5.0     0.958165
6  2023-01-01 00:01:16 2023-01-01 00:00:24       -52.0     0.641180
7  2023-01-01 00:01:16 2023-01-01 00:00:37       -39.0     0.716531
8  2023-01-01 00:01:16 2023-01-01 00:00:50       -26.0     0.800737
9  2023-01-01 00:01:16 2023-01-01 00:01:03       -13.0     0.894839
10 2023-01-01 00:01:16 2023-01-01 00:01:16         0.0     1.000000
</pre>
",2,2,0,2025-09-17T12:16:51+00:00,1,143,True
79767989,25886068,,sql,Ensure macaddr value is unique across columns,"<p>I have a table that represents the <a href=""https://www.postgresql.org/docs/current/datatype-net-types.html#DATATYPE-NET-TYPES-TABLE"" rel=""nofollow noreferrer"">MAC addresses</a> of some devices. The table has a column for Ethernet, bluetooth, &amp; WiFi, all nullable:</p>
<pre class=""lang-sql prettyprint-override""><code>create table device (
   id bigint generated by default as identity primary key
  ,wifi_mac macaddr8 unique
  ,ethernet_mac macaddr8 unique
  ,bluetooth_mac macaddr8 unique);
</code></pre>
<p>I have unique constraints in place that ensures each MAC address is unique, but that only works for each category of address. This does not enforce that an ethernet address cannot also exist in the WiFi column.</p>
<p>Using a multi-column unique constraint doesn't work either as it only ensure that the <em>combination</em> of MAC addresses are unique.</p>
<p>How can I ensure uniqueness of individual entries across these three columns?</p>
",-3,2,5,2025-09-18T04:45:32+00:00,2,149,True
79769100,31496851,,sql,Query result overriding in db&lt;&gt;fiddle and xtuple,"<p>This a follow-up to a question from 9/15/25: <a href=""https://stackoverflow.com/questions/79765624/query-that-checks-if-an-item-name-contains-a-substring-and-if-true-perform-a-s"">Query that checks if an item name contains a substring, and if true, perform a second query that grabs an image with a specific name</a></p>
<p>I have a query that grabs image data from tables joined on substrings. The item substring I need to match changes depending on the item so I need to run multiple queries. Example:
<a href=""https://dbfiddle.uk/ZrUkfuab"" rel=""nofollow noreferrer"">https://dbfiddle.uk/ZrUkfuab</a></p>
<pre class=""lang-sql prettyprint-override""><code>create table image(
    image_name text,
    image_data bytea
);

INSERT INTO image
VALUES     ( 'PRODUCT ANALOG DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT ANALOG DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT MAIN DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT FRAM DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT AAAA DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT BD DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')); 

create table wo(wo_id int,item_number text);
insert into wo values(1,'PRODUCT - MAIN BD - FRAM-MSTRW/O FBR')
                ,(2,'PRODUCT-ANLG-GIC2B-2AIN')
                ,(3,'PRODUCT - ANALOG BD - 4CT/4AIN')
                ,(4,'PRODUCT-AAAA-BBBBB-4XX');

create table Patterns(Pattern_id int, PatternString text);
insert into Patterns values(1,'^PRODUCT - MAIN BD -.(\w+).*')
                ,(2,'^PRODUCT-ANLG.(\w+).*')
                ,(3,'^PRODUCT -.(\w+).*')
                ,(4,'^PRODUCT.(\w+).*');
</code></pre>
<p>This successfully returns the image data:</p>
<pre><code>SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 3),'\1')
WHERE wo_id=3;
</code></pre>
<p>A different pattern results in zero matches, no results:</p>
<pre><code>SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 4),'\1')
WHERE wo_id=3;
</code></pre>
<p>This third block combines both queries. The query that doesn’t return anything is first, followed by one that does. This block successfully returns the image data, from the second query in it.</p>
<pre><code>SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 4),'\1')
WHERE wo_id=3;

SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 3),'\1')
WHERE wo_id=3;
</code></pre>
<p>However, when I flip the order of queries in this block, no image data is returned. The (lack of) result of the second query seems to be overriding the result of the first query.</p>
<p><strong>How to get around it? For loops? Case Expressions? CTE’s?</strong></p>
<p>I really don’t want to give each query its own dedicated block because it would make organizing the report template a complete nightmare.</p>
",-1,1,2,2025-09-19T03:54:56+00:00,1,129,True
79769157,3617866,"Dhaka, Bangladesh",sql,Hourly true average between timestamps,"<p>I’m storing IoT readings in a GridDB container and need one row per hour with the true average of the points that actually fall inside each hour (not interpolated values):</p>
<pre><code>ts_bucket               avg_temp  min_temp  max_temp  n_rows
------------------------------------------------------------
2025-09-01T00:00:00Z     25.40     25.40     25.40       1
2025-09-01T01:00:00Z     26.10     26.10     26.10       1
2025-09-01T02:00:00Z     27.80     27.80     27.80       1
2025-09-01T03:00:00Z     28.20     28.20     28.20       1
...
2025-09-02T09:00:00Z     27.10     27.10     27.10       1
</code></pre>
<p>If an hour has no rows, either omit it or output NULL (or
an interpolated value).</p>
<p>Values come at irregular times and are assumed to hold until the next reading (time-weighted needed). For one hour:</p>
<pre><code>-- within 2025-09-01 01:00–02:00
(ts, temperature)
2025-09-01T01:00:00Z  20
2025-09-01T01:10:00Z  35
2025-09-01T01:50:00Z  20
</code></pre>
<p>Expected time-weighted average for 01:00–02:00:</p>
<pre><code>((10 min * 20) + (40 min * 35) + (10 min * 20)) / 60
= (200 + 1400 + 200) / 60
= 30.0
</code></pre>
<p>This is exactly what <code>TIME_AVG(...)</code> returns for a whole range; I need the same computation but per hour (1 row per hour across a larger range).</p>
<p>Schema <code>TimeSeries</code>:</p>
<pre><code>ts (TIMESTAMP, row key)
deviceid (STRING)
temperature (DOUBLE)
humidity (DOUBLE)
status (STRING)
</code></pre>
<p>Data:</p>
<pre><code>INSERT INTO TSDB (ts, deviceid, temperature, humidity, status) VALUES
  (TIMESTAMP('2025-09-01T00:00:00Z'), 'dev-001', 25.4, 35.0, 'OK'),
  (TIMESTAMP('2025-09-01T01:00:00Z'), 'dev-001', 26.1, 42.0, 'OK'),
  (TIMESTAMP('2025-09-01T02:00:00Z'), 'dev-001', 27.8, 48.0, 'WARN'),
  (TIMESTAMP('2025-09-01T03:00:00Z'), 'dev-001', 28.2, 38.0, 'OK'),
  (TIMESTAMP('2025-09-02T00:00:00Z'), 'dev-002', 23.5, 33.0, 'OK'),
  (TIMESTAMP('2025-09-02T01:00:00Z'), 'dev-002', 24.0, 46.0, 'WARN'),
  (TIMESTAMP('2025-09-02T02:00:00Z'), 'dev-002', 22.8, 41.0, 'OK'),
  (TIMESTAMP('2025-09-02T03:00:00Z'), 'dev-002', 21.9, 37.0, 'OK');
</code></pre>
<p>What I tried in TQL:</p>
<pre><code>-- Single weighted average for the whole range (works, but not per hour)
SELECT TIME_AVG(temperature)
WHERE ts &gt;= TIMESTAMP('2025-09-01T00:00:00Z')
  AND ts &lt;  TIMESTAMP('2025-09-03T00:00:00Z');

-- Hourly sampling returns interpolated values, not true bucket aggregates
SELECT TIME_SAMPLING(
  temperature,
  TIMESTAMP('2025-09-01T00:00:00Z'),
  TIMESTAMP('2025-09-03T00:00:00Z'),
  1, HOUR
);
</code></pre>
<p>TQL doesn’t support <code>GROUP BY</code> so I couldn’t express per-hour roll-ups as a single statement. But this produces the true per-hour aggregates in one SQL query:</p>
<pre><code>SELECT
  ts,                              -- bucket start time
  AVG(temperature) AS avg_temp,
  MIN(temperature) AS min_temp,
  MAX(temperature) AS max_temp,
  COUNT(*)        AS n_rows
FROM TSDB
WHERE ts &gt;= TIMESTAMP('2025-09-01T00:00:00Z')
  AND ts &lt;  TIMESTAMP('2025-09-03T00:00:00Z')
GROUP BY RANGE (ts) EVERY (1, HOUR)
ORDER BY ts;
</code></pre>
<p>To include hours with no data (or to interpolate/forward-fill), this variant works:</p>
<pre><code>SELECT ts, AVG(temperature) AS avg_temp
FROM TSDB
WHERE ts BETWEEN TIMESTAMP('2025-09-01T00:00:00Z') AND TIMESTAMP('2025-09-03T00:00:00Z')
GROUP BY RANGE (ts) EVERY (1, HOUR) FILL (NULL)   -- or FILL(LINEAR), PREVIOUS
ORDER BY ts;
</code></pre>
<ol>
<li>Is there a TQL-only way to return one row per hour with <code>AVG</code>/<code>MIN</code>/<code>MAX</code>/<code>COUNT</code> over a time range?</li>
<li>If not, is using SQL with <code>GROUP BY RANGE</code> possible, otherwise client-side bucketing when restricted to TQL?</li>
<li>Any performance tips for millions of rows?</li>
</ol>
",4,6,2,2025-09-19T05:47:34+00:00,0,138,False
79769329,31518488,,sql,Insert multiple values into columns from another table,"<p>I have three tables set up, simplified as:</p>
<p>TABLE 1</p>
<pre><code>OrderID / TeamID / Persons
</code></pre>
<p>TABLE 2</p>
<pre><code>TeamID / PersonID
</code></pre>
<p>TABLE 3</p>
<pre><code>PersonID / PersonName
</code></pre>
<p>I want to put names from table 3 into table 1. Table 1 has a <code>TeamID</code>, each <code>TeamID</code> can have up to five <code>PersonID</code>s in it (so could be five rows in table 2 all with same <code>TeamID</code>, but each with a different <code>PersonID</code>).</p>
<p>Not sure whether I need some sort of loop/while statement, to extract up to five names</p>
",-3,0,3,2025-09-19T09:11:23+00:00,1,79,True
79769401,12591556,,sql,Computing users based on months and year value using window function,"<p>I’ve written a query that gives me what I’m looking for in Redshift, but I want to rewrite the query without using UNION ALL, the idea is to have a cumulative count over a profile_type annual and all_time (condition field), all_time - profile_year will be null as it will be the cumulative of all the years broke down in cumulative months, annual - will have each year data broken down by months. So if there is a way to use window function to calculate the value for each profile_type, I don’t want to use pivot either</p>
<p>I know I can also use union all and have a query to calculate annul and all_time, I want to know if I can calculate annul and all_time at row level and only have the base query and the final output</p>
<p>I was hoping I could use CASE Partition OVER BY to calculate all_time and annual</p>
<pre><code>   WITH base AS (
    SELECT
        s.service AS service,
        r.user_id AS user_id,
        EXTRACT(YEAR FROM s.journey_date) AS year,
        EXTRACT(MONTH FROM s.journey_date) AS month,
        COUNT(DISTINCT s.user_sk) AS monthly_users,
        MIN(s.journey_date) OVER (PARTITION BY s.service, r.user_id) AS min_journey_date,
        MAX(s.journey_date) OVER (PARTITION BY s.service, r.user_id) AS max_journey_date
    FROM dummy s
    INNER JOIN dummy r
        ON s.service = r.agency_name
    GROUP BY s.service, r.user_id, s.journey_date, year, month
)
SELECT
    user_id,
    service,
    year AS profile_year,
    'annual' AS profile_type,
    1 AS has_one_year_flag,
    DATEDIFF(month, MIN(min_journey_date), MAX(max_journey_date)) AS months_data_capture,
    SUM(monthly_users) AS total_users_by_year,
    CAST(SUM(CASE WHEN month = 1  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS jan,
    CAST(SUM(CASE WHEN month = 2  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS feb,
    CAST(SUM(CASE WHEN month = 3  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS mar,
    CAST(SUM(CASE WHEN month = 4  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS apr,
    CAST(SUM(CASE WHEN month = 5  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS may,
    CAST(SUM(CASE WHEN month = 6  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS jun,
    CAST(SUM(CASE WHEN month = 7  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS jul,
    CAST(SUM(CASE WHEN month = 8  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS aug,
    CAST(SUM(CASE WHEN month = 9  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS sep,
    CAST(SUM(CASE WHEN month = 10 THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS oct,
    CAST(SUM(CASE WHEN month = 11 THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS nov,
    CAST(SUM(CASE WHEN month = 12 THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS dec
FROM base
GROUP BY user_id, service, year

UNION ALL

SELECT
    user_id,
    service,
    NULL AS profile_year,
    'all_time' AS profile_type,
    0 AS has_one_year_flag,
    DATEDIFF(month, MIN(min_journey_date), MAX(max_journey_date)) AS months_data_capture,
    SUM(monthly_users) AS total_users_by_year,
    CAST(SUM(CASE WHEN month = 1  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS jan,
    CAST(SUM(CASE WHEN month = 2  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS feb,
    CAST(SUM(CASE WHEN month = 3  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS mar,
    CAST(SUM(CASE WHEN month = 4  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS apr,
    CAST(SUM(CASE WHEN month = 5  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS may,
    CAST(SUM(CASE WHEN month = 6  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS jun,
    CAST(SUM(CASE WHEN month = 7  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS jul,
    CAST(SUM(CASE WHEN month = 8  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS aug,
    CAST(SUM(CASE WHEN month = 9  THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS sep,
    CAST(SUM(CASE WHEN month = 10 THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS oct,
    CAST(SUM(CASE WHEN month = 11 THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS nov,
    CAST(SUM(CASE WHEN month = 12 THEN monthly_users ELSE 0 END) AS FLOAT) / NULLIF(SUM(monthly_users),0) AS dec
FROM base
GROUP BY user_id, service
ORDER BY service ASC, profile_year DESC;
</code></pre>
<p>Outcome</p>
<p><a href=""https://i.sstatic.net/8Py7b0TK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8Py7b0TK.png"" alt=""enter image description here"" /></a></p>
",1,2,1,2025-09-19T10:37:34+00:00,1,79,True
79769546,3617866,"Dhaka, Bangladesh",sql,Time-Series Pagination: LIMIT/OFFSET vs keyset (seek by last timestamp) to avoid duplicates/skips while new rows arrive?,"<p>I’m paging through a large time range in a GridDB TimeSeries container and need a pattern that won’t duplicate or skip rows if new data arrives while I’m paging.</p>
<p><strong>Schema (TimeSeries):</strong></p>
<pre><code>ts (TIMESTAMP, row key)
deviceid (STRING)
temperature (DOUBLE)
</code></pre>
<p><strong>Minimal reproducible data (SQL inserts)</strong></p>
<pre><code>INSERT INTO TSDB (ts, deviceid, temperature) VALUES
  (TIMESTAMP('2025-09-01T00:00:00Z'),'dev-001',25.4),
  (TIMESTAMP('2025-09-01T00:10:00Z'),'dev-001',26.1),
  (TIMESTAMP('2025-09-01T00:20:00Z'),'dev-001',27.8),
  (TIMESTAMP('2025-09-01T00:30:00Z'),'dev-001',28.2),
  (TIMESTAMP('2025-09-01T00:40:00Z'),'dev-002',23.5),
  (TIMESTAMP('2025-09-01T00:50:00Z'),'dev-002',24.0),
  (TIMESTAMP('2025-09-01T01:00:00Z'),'dev-002',22.8),
  (TIMESTAMP('2025-09-01T01:10:00Z'),'dev-002',21.9),
  (TIMESTAMP('2025-09-01T01:20:00Z'),'dev-002',22.3),
  (TIMESTAMP('2025-09-01T01:30:00Z'),'dev-002',23.1);
</code></pre>
<p><strong>Goal:</strong> fetch results ascending by <code>ts</code> in pages of N rows (e.g., 1000) from <code>[start, end]</code>.</p>
<p><strong>Approach A</strong> — <code>LIMIT/OFFSET</code></p>
<p>Works, but I’m worried about duplication/omission if new rows are inserted between page calls:</p>
<pre><code>-- Page 1
SELECT *
FROM TSDB
WHERE ts &gt;= TIMESTAMP('2025-09-01T00:00:00Z')
  AND ts &lt;  TIMESTAMP('2025-09-05T00:00:00Z')
ORDER BY ts ASC
LIMIT 1000 OFFSET 0;

-- Page 2
... ORDER BY ts ASC
LIMIT 1000 OFFSET 1000;
</code></pre>
<p>If a new row with <code>ts</code> inside the range is inserted after Page 1, the next <code>OFFSET</code> may shift.</p>
<p><strong>Approach B</strong> — Keyset/seek (remember last <code>ts</code>)</p>
<pre><code>-- Page 1
SELECT *
FROM TSDB
WHERE ts &gt;= TIMESTAMP('2025-09-01T00:00:00Z')
  AND ts &lt;  TIMESTAMP('2025-09-05T00:00:00Z')
ORDER BY ts ASC
LIMIT 1000;

-- capture last_ts from Page 1, then:
-- Page 2
SELECT *
FROM TSDB
WHERE ts &gt; :last_ts
  AND ts &lt; TIMESTAMP('2025-09-05T00:00:00Z')
ORDER BY ts ASC
LIMIT 1000;
</code></pre>
<p>If there could be ties on <code>ts</code>, I’d add a tiebreaker:</p>
<pre><code>-- ORDER BY ts, deviceid
-- WHERE (ts &gt; :last_ts) OR (ts = :last_ts AND deviceid &gt; :last_deviceid)
</code></pre>
<p><strong>Questions:</strong></p>
<ol>
<li>For GridDB time-series, which pagination pattern is recommended: <code>LIMIT/OFFSET</code> or keyset/seek by last key(s)?</li>
<li>Does GridDB provide a consistent snapshot across multiple page queries (so <code>OFFSET</code> is safe), or should I assume results can shift under concurrent inserts?</li>
<li>Any server-side cursor / scrollable result guidance in SQL/TQL clients, or is the page-by-query approach the way to go?</li>
<li>If <code>ts</code> is the row key (unique), is <code>WHERE ts &gt; :last_ts</code> sufficient? If same-timestamp rows are possible, is the <code>(ts, deviceid)</code> tiebreaker pattern the right approach?</li>
<li>Any performance tips (indexes, ordering on row key, query hints) for paging millions of rows?</li>
</ol>
<p><strong>Environment:</strong> GridDB Cloud (Free plan), Console (TQL + SQL modes)</p>
",3,3,0,2025-09-19T13:07:35+00:00,0,102,False
79769552,31525243,,sql,REGEX_EXTRACT() with multiple capturing groups not returning what&#39;s expected,"<p>The following code shows my problem, I need to extract the JOBD value (with in reality variable and matches \w{1, 10}</p>
<pre><code>    with A (N, STR) as (
           values
          (1, 'SBMJOB MYJOB'),
          (2, 'SBMJOB JOB '),
          (3, 'SBMJOB JOB JOBD'),
          (4, 'SBMJOB JOB4 JOBD '),
          (5, 'SBMJOB JOB CMD('),
          (6, 'SBMJOB JOB JOBD CMD('),
          (7, 'SBMJOB JOB JOBD JOBQ('),
          (8, 'SBMJOB JOB CMD('),
          (9, 'SBMJOB JOB JOBQ(')
         ),
         R (REGEX) as (
           values
          'sbmjob(?: +((?!\w+ *\()\w+))?(?: +((?!\w+\()\w+))?(?: +(\w+)\()?'
         )
      select N,
             STR,
             REGEXP_EXTRACT(STR, REGEX, 1, 1, 'i', 1) JOB,
             REGEXP_EXTRACT(STR, REGEX, 1, 1, 'i', 2) JOBD,
             length(REGEXP_EXTRACT(STR, REGEX, 1, 1, 'i', 2)) LENGTH_JOBD,
             REGEXP_EXTRACT(STR, REGEX, 1, 1, 'i', 3) FIRST_KEYWORD
        from A
             cross join R
    ;   
</code></pre>
<p>I expect it to returns this</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">N</th>
<th style=""text-align: right;"">STR</th>
<th style=""text-align: right;"">JOB</th>
<th style=""text-align: right;"">JOBD</th>
<th style=""text-align: right;"">LENGTH_JOBD</th>
<th style=""text-align: right;"">FIRST_KEYWORD</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">SBMJOB MYJOB</td>
<td style=""text-align: right;"">MYJOB</td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">SBMJOB JOB</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">SBMJOB JOB JOBD</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;"">JOBD</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;""><em>null</em></td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">SBMJOB JOB4 JOBD</td>
<td style=""text-align: right;"">JOB4</td>
<td style=""text-align: right;"">JOBD</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;""><em>null</em></td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: right;"">SBMJOB JOB CMD(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;"">CMD</td>
</tr>
<tr>
<td style=""text-align: right;"">6</td>
<td style=""text-align: right;"">SBMJOB JOB JOBD CMD(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;"">JOBD</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">CMD</td>
</tr>
<tr>
<td style=""text-align: right;"">7</td>
<td style=""text-align: right;"">SBMJOB JOB JOBD JOBQ(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;"">JOBD</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">JOBQ</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td style=""text-align: right;"">SBMJOB JOB CMD(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;"">CMD</td>
</tr>
<tr>
<td style=""text-align: right;"">9</td>
<td style=""text-align: right;"">SBMJOB JOB JOBQ(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;""><em>null</em></td>
<td style=""text-align: right;"">JOBQ</td>
</tr>
</tbody>
</table></div>
<p>And it does with DB2 <strong>LUW</strong> (see <a href=""https://dbfiddle.uk/MSZBDpBo"" rel=""nofollow noreferrer"">fiddle</a>)</p>
<p>But it returns this when using DB2 for <strong>IBMi</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">N</th>
<th style=""text-align: right;"">STR</th>
<th style=""text-align: right;"">JOB</th>
<th style=""text-align: right;"">JOBD</th>
<th style=""text-align: right;"">LENGTH_JOBD</th>
<th style=""text-align: right;"">FIRST_KEYWORD</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">SBMJOB MYJOB</td>
<td style=""text-align: right;"">MYJOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">SBMJOB JOB</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">SBMJOB JOB JOBD</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">SBMJOB JOB4 JOBD</td>
<td style=""text-align: right;"">JOB4</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: right;"">SBMJOB JOB CMD(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">CMD</td>
</tr>
<tr>
<td style=""text-align: right;"">6</td>
<td style=""text-align: right;"">SBMJOB JOB JOBD CMD(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">7</td>
<td style=""text-align: right;"">SBMJOB JOB JOBD JOBQ(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td style=""text-align: right;"">SBMJOB JOB CMD(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">CMD</td>
</tr>
<tr>
<td style=""text-align: right;"">9</td>
<td style=""text-align: right;"">SBMJOB JOB JOBQ(</td>
<td style=""text-align: right;"">JOB</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">JOBQ</td>
</tr>
</tbody>
</table></div>
<p>I think I have to open a ticket but do you have any idea ?</p>
",1,2,1,2025-09-19T13:12:54+00:00,1,114,False
79769689,22092377,,sql,Creating custom dbplyr compatible function in SQL,"<p>I'm working with dbplyr and DuckDB to process very large Parquet files using limited system resources. To make my workflow more efficient, I want to create a custom function that can be seamlessly integrated into a dbplyr pipeline.</p>
<p>Because I couldn’t achieve the desired logic using only built-in dbplyr verbs, I implemented the core of this function using raw SQL. Now, I want to wrap this SQL logic into a function that can be used as a regular dbplyr step — meaning I can chain it between other dbplyr operations (before and after it) without breaking the pipeline.</p>
<p>What I’m struggling with is how to integrate this custom SQL-based function properly into the dbplyr workflow, so it behaves like any other dplyr-style function.</p>
<pre><code>library(tidyverse)
library(duckdb)
library(dplyr)
library(dbplyr)
library(glue)
library(purrr)


con &lt;- dbConnect(duckdb(), dbdir = &quot;:memory:&quot;)
my_data &lt;- tibble(
  id = 1:100,
  group1 = rep(c(&quot;A&quot;, &quot;B&quot;), 50),
  group2 = rep(c(&quot;C&quot;, &quot;D&quot;), each = 50),
  value = c(rnorm(90, 10), runif(10, 50, 100))
)
duckdb_register(con, &quot;df_tmp&quot;, my_data)
tbl &lt;- tbl(con, &quot;df_tmp&quot;)

remove_outliers &lt;- function(.data, var, by, threshold, ratio, remove = TRUE) {
  require(dbplyr)
  con &lt;- dbplyr::remote_con(.data)
  
  sql_query &lt;- glue(&quot;WITH winsor_limits AS (
        SELECT {paste0(.by, collapse = ', ')},
                PERCENTILE_DISC({threshold}) WITHIN GROUP (ORDER BY {var}) * {ratio}  AS threshold
        FROM {sql_render(.data)} as tbl
        GROUP BY {paste0(.by, collapse = ', ')}
     ) 
     SELECT t.* REPLACE (
                  CASE
                    WHEN {var} &lt; threshold then {var}
                    ELSE {ifelse(remove, 'NULL', 'threshold')}
                  END AS {var} 
                  )
     FROM {sql_render(.data)} as t
     JOIN winsor_limits as l
     ON t.group1 = l.group1 AND t.group2 = l.group2&quot;)
  
  # This is here where I don't know what to do to add this SQL request to the dbplyr   pipeline
  return(out)
}

dt &lt;- tbl |&gt; 
  remove_outliers(var =&quot;value&quot;,
                  by = c(&quot;group1&quot;, &quot;group2&quot;),
                  threshold = 0.8,
                  ratio = 1, remove = TRUE) 

</code></pre>
<p>If there is a better way to implement what I'm trying to do, please tell me!</p>
",3,3,0,2025-09-19T15:32:37+00:00,1,112,True
79770153,7419457,"Delhi, India",sql,"Clickhouse quantilemerge p99, p90, p50 all return same value","<p>I am trying to build a an aggregation structure such that.
Raw_events_table &gt; one_minute_aggregate_table &gt; 5_minute_aggregate_table.</p>
<p>My tables look like</p>
<pre><code>CREATE TABLE raw_events_dev
(
    `event` Nullable(String),
    `status` Nullable(String),
    `upstream_partner_id` Nullable(String),
    `upstream_latency` Nullable(Int64),
    `product` Nullable(String),
    `timestamp` Nullable(DateTime64(3))
)
ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/application_metrics/raw_events', '{replica}')
ORDER BY tuple()
SETTINGS index_granularity = 8192

CREATE TABLE agg_minute_dev
(
    `time_bucket` DateTime64(3, 'UTC'),
    `event_name` String,
    `event_count_success` AggregateFunction(sum, UInt64),
    `event_count_failure` AggregateFunction(sum, UInt64),
    `upstream_partner_id` String,
    `upstream_latency_avg` AggregateFunction(avg, Int64),
    `upstream_latency_p99` AggregateFunction(quantileExact(0.99), Int64),
    `upstream_latency_p90` AggregateFunction(quantileExact(0.9), Int64),
    `upstream_latency_p50` AggregateFunction(quantileExact(0.5), Int64),
    `product` String
)
ENGINE = AggregatingMergeTree
PARTITION BY toYYYYMM(time_bucket)
ORDER BY (time_bucket, upstream_partner_id, product)
SETTINGS index_granularity = 8192;

CREATE MATERIALIZED VIEW mv_agg_minute_dev
TO agg_minute_dev AS
SELECT
    toStartOfMinute(timestamp) AS time_bucket,
    ifNull(event, '') AS event_name,
    sumState(toUInt64(ifNull(status, '') = 'SUCCESS')) AS event_count_success,
    sumState(toUInt64(ifNull(status, '') != 'SUCCESS')) AS event_count_failure,
    ifNull(upstream_partner_id, '') AS upstream_partner_id,
    avgState(toInt64(upstream_latency)) AS upstream_latency_avg,
    quantileExactState(0.99)(toInt64(ifNull(upstream_latency, 0))) AS upstream_latency_p99,
    quantileExactState(0.90)(toInt64(ifNull(upstream_latency, 0))) AS upstream_latency_p90,
    quantileExactState(0.50)(toInt64(ifNull(upstream_latency, 0))) AS upstream_latency_p50,
    ifNull(product, '') AS product
FROM raw_events_dev
WHERE timestamp IS NOT NULL 
    AND upstream_latency IS NOT NULL  -- Only include records with actual latency values
GROUP BY
    time_bucket,
    event_name,
    upstream_partner_id,
    product;
</code></pre>
<p>In this the p99, p90, p50 values are the same for one row. This happens irrespective of the number of entries in the raw events table. I have tried using quantileState, but that also results in the same thing. The p50 and p90 value is equal to p99.</p>
<p>The value of avg is correct.</p>
<p>But when I do a select on raw events table with the same select as a materialized view.</p>
<pre><code>SELECT
    quantileExact(0.99)(toInt64(ifNull(upstream_latency, 0))) AS p99_exact,
    quantileExact(0.9)(toInt64(ifNull(upstream_latency, 0))) AS p90_exact,
    quantileExact(0.5)(toInt64(ifNull(upstream_latency, 0))) AS p50_exact
FROM application_metrics.setu_metrics_dg_switchboard_dev
WHERE timestamp &gt;= '2025-09-19 00:00:00'
</code></pre>
<p>The value is correct and as expected.
How can I get the correct value in my aggregate table?</p>
<p>Replicate: <a href=""https://fiddle.clickhouse.com/70a3ea1a-870d-4704-beae-3fd383990550"" rel=""nofollow noreferrer"">Clickhouse fiddle</a></p>
",0,0,0,2025-09-20T07:00:40+00:00,1,146,False
79770347,3617866,"Dhaka, Bangladesh",sql,&quot;Top-1 Per Group”: Get the latest row per deviceid in a time range,"<p>I'm using GridDB TimeSeries and need, for a given time range, the latest row per <code>deviceid</code> (i.e., <em>top-1 by timestamp within each device</em>), returned in one query.</p>
<p><strong>Expected output (example for two devices over a day):</strong></p>
<pre><code>deviceid  ts                        temperature
--------  ------------------------  -----------
dev-001   2025-09-01T09:00:00Z      24.9
dev-002   2025-09-01T09:00:00Z      27.1
</code></pre>
<hr />
<p><strong>My Schema (TimeSeries):</strong></p>
<pre><code>ts (TIMESTAMP, row key)
deviceid (STRING)
temperature (DOUBLE)
</code></pre>
<p><strong>Minimal reproducible data (SQL mode):</strong></p>
<pre><code>INSERT INTO TSDB (ts, deviceid, temperature) VALUES
  (TIMESTAMP('2025-09-01T00:00:00Z'),'dev-001',25.4),
  (TIMESTAMP('2025-09-01T01:00:00Z'),'dev-001',26.1),
  (TIMESTAMP('2025-09-01T02:00:00Z'),'dev-001',27.8),
  (TIMESTAMP('2025-09-01T09:00:00Z'),'dev-001',24.9),    
  (TIMESTAMP('2025-09-01T00:00:00Z'),'dev-002',23.5),
  (TIMESTAMP('2025-09-01T01:00:00Z'),'dev-002',24.0),
  (TIMESTAMP('2025-09-01T02:00:00Z'),'dev-002',22.8),
  (TIMESTAMP('2025-09-01T09:00:00Z'),'dev-002',27.1);
</code></pre>
<hr />
<p><strong>What I tried:</strong></p>
<p><strong>A) TQL</strong> (works for one device at a time)</p>
<pre><code> -- latest row for a single device
        SELECT * 
        WHERE deviceid = 'dev-001'
          AND ts &gt;= TIMESTAMP('2025-09-01T00:00:00Z')
          AND ts &lt;  TIMESTAMP('2025-09-02T00:00:00Z')
        ORDER BY ts DESC
        LIMIT 1;
</code></pre>
<p>But I’m looking for <strong>one result per device</strong> in a <strong>single query</strong>.</p>
<p><strong>B) SQL</strong> with subquery join (top-1 per group pattern)
Trying to compute <code>MAX(ts)</code> per <code>deviceid</code>, then join back to get the full row:</p>
<pre><code>SELECT t.*
FROM TSDB t
JOIN (
  SELECT deviceid, MAX(ts) AS max_ts
  FROM TSDB
  WHERE ts &gt;= TIMESTAMP('2025-09-01T00:00:00Z')
    AND ts &lt;  TIMESTAMP('2025-09-02T00:00:00Z')
  GROUP BY deviceid
) m
  ON t.deviceid = m.deviceid
 AND t.ts       = m.max_ts
ORDER BY t.deviceid;
</code></pre>
<p>If joins/subqueries aren’t supported in GridDB SQL, what’s the recommended approach?</p>
<hr />
<p><strong>Questions:</strong></p>
<ul>
<li>Is there a single SQL or TQL query to return the latest row per <code>deviceid</code> in a time range?</li>
<li>If not, what’s the recommended pattern in GridDB?</li>
<li>Any performance tips or indexing suggestions for large numbers of devices?</li>
</ul>
<hr />
<p><strong>Environment</strong></p>
<ul>
<li>GridDB Cloud (Free plan)</li>
<li>Console (SQL + TQL)</li>
</ul>
<hr />
<p><strong>SQL Output:</strong></p>
<p><a href=""https://i.sstatic.net/5QaaAMHO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5QaaAMHO.png"" alt=""enter image description here"" /></a></p>
<p><strong>TQL Output:</strong>
<a href=""https://i.sstatic.net/mVbBrFDs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mVbBrFDs.png"" alt=""enter image description here"" /></a></p>
",4,4,0,2025-09-20T14:03:18+00:00,1,156,True
79771875,1103606,,sql,"Oracle function to_char(character varying, unknown) does not exist","<p>I need to migrate this Oracle SQL to postgresql:</p>
<pre class=""lang-sql prettyprint-override""><code>create table tokens(id, date_of_birth)
  as values(1, '2025-09-22'::varchar(20));

    SELECT 
        s.id,
        to_char(s.date_of_birth, 'fmMM') dobMonth,
        to_char(s.date_of_birth, 'fmYYYY') dobYear,
        to_char(s.date_of_birth, 'fmDD') dobDay,
        s.type
    FROM 
        tokens s
    WHERE 
        s.token_id = :profileId
</code></pre>
<p>But I get an error:</p>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>ERROR:  function to_char(character varying, unknown) does not exist
LINE 1: select to_char(date_of_birth, 'fmMM') from tokens;
              ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
</code></pre>
</blockquote>
<p>Do you know what is the proper way to solve this issue?</p>
",0,2,2,2025-09-22T17:04:06+00:00,2,267,True
79772638,5513917,,sql,Oracle SQL DBLink with many tables and joins,"<p>I'm trying to execute an Oracle SQL query that can hit two different database servers at once. Both servers have the same table definitions however the SQL query is hitting many tables.</p>
<p>Simplified Example:</p>
<pre><code>with Stage2 as (select member_nbr, service_nbr from process p left outer join services s on s.service_nbr = p.service_nbr where s.date = sysdate)
select 
member_nbr, sub_nbr, service_nbr, ymdbirth
from
( select stage1.member_nbr, stage1.sub_nbr, stage1.service_nbr, stage1.ymdbirth from (select s.member_nbr, s.sub_nbr, s.service_nbr, mbr.ymdbirth 
from Stage2 
left outer join process p on stage2.service_nbr
left outer join service s on p.service_nbr = s.service_nbr
left outer join member mbr on mbr.member_nbr = s.member_nbr
where p.paid = 'N'
and s.status not in (91,92,93)) Stage1
order by stage1.member_nbr, stage1.service_nbr);
</code></pre>
<p>Is there a way to execute this between 2 DB instances were the query result is combined between the two?</p>
<pre><code>Instance 1:
member_nbr 1
sub_nbr 1
service_nbr 1
ymdbirth 19871001
</code></pre>
<pre><code>Instance 2:
member_nbr 2
sub_nbr 2
service_nbr 2
ymdbirth 20001118
</code></pre>
<pre><code>Desired results:
member_nbr      sub_nbr      service_nbr      ymdbirth
1                1                1             19871001
2                2                2             20001118
</code></pre>
",0,0,0,2025-09-23T13:03:33+00:00,1,135,True
79772829,31553892,,sql,Grouping repeated XML elements into JSON arrays,"<p>I am writing a T-SQL function to convert XML data into a JSON string. My goal is to automatically group repeated child elements with the same tag name into a JSON array, rather than creating separate key-value pairs.</p>
<p>The built-in <code>FOR JSON PATH</code> in SQL Server handles this automatically for repeated table rows, but my recursive function is based on <code>FOR XML PATH</code>, which doesn't handle the grouping as needed.</p>
<p>Example 1: multiple simple elements</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;root&gt;
    &lt;Book&gt;Book1&lt;/Book&gt;
    &lt;TransactionId&gt;abc123&lt;/TransactionId&gt;
    &lt;TransactionId&gt;abc456&lt;/TransactionId&gt;
    &lt;Publisher&gt;Amazon&lt;/Publisher&gt;
    &lt;PublisherId&gt;1&lt;/PublisherId&gt;
    &lt;UserId&gt;9457&lt;/UserId&gt;
&lt;/root&gt;
</code></pre>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;OrderRef&quot;: &quot;Book1&quot;,
        &quot;TransactionId&quot;: [ &quot;abc123&quot;, &quot;abc456&quot; ],
        &quot;Publisher&quot;: &quot;Amazon&quot;,
        &quot;PublisherId&quot;: 1,
        &quot;UserId&quot;: 9457
    }
]
</code></pre>
<p>Example 2: multiple complex elements</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;root&gt;
    &lt;Book&gt;Book1&lt;/Book&gt;
    &lt;TransactionId&gt;abc123&lt;/TransactionId&gt;
    &lt;TransactionId&gt;abc456&lt;/TransactionId&gt;
    &lt;Publisher&gt;Amazon&lt;/Publisher&gt;
    &lt;Edition&gt;
        &lt;Name&gt;Ed1&lt;/Name&gt;
        &lt;Color&gt;Red&lt;/Color&gt;
        &lt;Price&gt;100&lt;/Price&gt;
    &lt;/Edition&gt;
    &lt;Edition&gt;
        &lt;Name&gt;Ed2&lt;/Name&gt;
        &lt;Color&gt;Blue&lt;/Color&gt;
        &lt;Price&gt;200&lt;/Price&gt;
    &lt;/Edition&gt;
    &lt;PublisherId&gt;1&lt;/PublisherId&gt;
    &lt;UserId&gt;1234&lt;/UserId&gt;
&lt;/root&gt;
</code></pre>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;OrderRef&quot;: &quot;Book1&quot;,
        &quot;TransactionId&quot;: [ &quot;abc123&quot;, &quot;abc456&quot; ],
        &quot;Publisher&quot;: &quot;Amazon&quot;,
        &quot;Edition&quot;: [
          {
            &quot;Name&quot;: &quot;Ed1&quot;,
            &quot;Color&quot;: &quot;Red&quot;,
            &quot;Price&quot;: 100
          },
          {
            &quot;Name&quot;: &quot;Ed2&quot;,
            &quot;Color&quot;: &quot;Blue&quot;,
            &quot;Price&quot;: 200
          }
        ],
        &quot;PublisherId&quot;: 1,
        &quot;UserId&quot;: 1234
    }
]
</code></pre>
<p>My current function uses recursion with <code>FOR XML PATH</code>, but it incorrectly handles repeated elements by outputting them as separate key-value pairs.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE FUNCTION [dbo].[Func_XmlToJson](@XmlData xml)  
RETURNS nvarchar(max)  
AS  
BEGIN  
    DECLARE @m nvarchar(max)  

    SELECT @m= '[' + STUFF((SELECT theline 
                            FROM
                                (SELECT ','+' {' + STUFF((SELECT ',&quot;'+coalesce(b.c.value('local-name(.)', 'NVARCHAR(255)'),'')+'&quot;:'+  
                      case when b.c.value('count(*)','int')=0   
                      then  '&quot;'+isnull(replace(b.c.value('text()[1]','NVARCHAR(MAX)'),'\','\\'),'')+'&quot;'  
                      else dbo.Func_XmlToJson(b.c.query('.'))  
                      end  
                 FROM x.a.nodes('*') b(c)  
                 FOR XML PATH(''),TYPE).value('(./text())[1]','NVARCHAR(MAX)')  
               ,1,1,'')+'}'  
          FROM @XmlData.nodes('/*') x(a)  
       ) JSON(theLine)  
       FOR XML PATH(''),TYPE).value('.','NVARCHAR(MAX)')  
      ,1,1,'')+']'  
   RETURN @m  
END
</code></pre>
<p>How can I modify this T-SQL function to correctly identify and group repeated elements into JSON arrays, instead of creating duplicate keys?</p>
",3,3,0,2025-09-23T16:01:33+00:00,2,210,True
79773259,4730308,,sql,SQL Query to get the Min &amp; Max Date,"<p>I have 3 tables:</p>
<ol>
<li><p>Schedule (scheduleId, scheduleDate, requestBranch, assignBranch, customerName, customerAddress, invoiceNo, refkey)</p>
</li>
<li><p>Branches (branchId, branchName)</p>
</li>
<li><p>ActivityLog (activityLogId, invoiceNo, refkey, activityDate, status)</p>
<p>*<em>requestBranch</em> &amp; <em>assignBranch</em> are <strong>branchId of branchName</strong></p>
</li>
</ol>
<p>Result I want to achieve:</p>
<pre><code>REQUESTBRANCH | ASSIGNBRANCH |     NAME     |   ADDRESS  |    MINDATE    |    MAXDATE 
Bulacan       |    Manila    |  CUSTOMER A  |  Bulacan   |  2025-09-12   |  2025-09-15
Cavite        |    Manila    |  CUSTOMER B  |  Cavite    |  2025-09-18   |  2025-09-22
Cebu          |    Manila    |  CUSTOMER C  |  Cebu      |  2025-09-20   |  2025-09-24
</code></pre>
<p>And here is my Query:</p>
<pre><code>SELECT scheduleDate, requestBranch, assignBranch, 
    invoiceNo, customerName, customerAddress,
    (SELECT MIN(activityDate) FROM schedule INNER JOIN activitylog USING (refkey)) minDate,
    (SELECT MAX(activityDate) FROM schedule INNER JOIN activitylog USING (refkey)) maxDate,
FROM schedule a INNER JOIN activitylog USING (refkey)
LEFT JOIN branches b ON a.requestBranch=b.branchId
LEFT JOIN branches c ON a.assignBranch=c.branchId
WHERE a.assignBranch='012';
</code></pre>
<p>Result of above query:</p>
<pre><code>REQUESTBRANCH | ASSIGNBRANCH |    NAME      |   ADDRESS  |    MINDATE    |    MAXDATE 
Bulacan       |    Manila    |  CUSTOMER A  |  Bulacan   |  2025-09-12   |  2025-09-15
Cavite        |    Manila    |  CUSTOMER B  |  Cavite    |  2025-09-12   |  2025-09-15
Cebu          |    Manila    |  CUSTOMER C  |  Cebu      |  2025-09-12   |  2025-09-15
</code></pre>
<p>Problem is:
The minDate &amp; maxDate are the same in each rows which should be the minDate where it first came in the database and the maxDate is the date where it's completed.</p>
",0,1,1,2025-09-24T04:21:26+00:00,2,115,True
79774536,10944834,,sql,Does PostgreSQL use a multi column unique index when foreign key column order differs?,"<p>I am having EDB data base(Postgress with oracle compatibility). In data base it have two table called <code>institution</code> and <code>customer</code>. Each customer need to belong to an institution. <code>Intitution</code> has unique constraint with columns <code>(inst_id, inst_code)</code> in that order created by,</p>
<pre><code>ALTER TABLE IF EXISTS institution
ADD CONSTRAINT &quot;UK_institution_inst_code&quot; UNIQUE (inst_id, inst_code);
</code></pre>
<p>And <code>customer</code> table also has two columns with names <code>institution_id</code>, <code>institution_code</code>. I am setting foreign key from <code>customer</code> to <code>institution</code> by executing,</p>
<pre class=""lang-none prettyprint-override""><code>    ADD CONSTRAINT fk_customer_institution_id_institution_code FOREIGN KEY (institution_code, institution_id)
    REFERENCES institution (inst_code,inst_id) MATCH SIMPLE
    ON UPDATE NO ACTION
       ON DELETE NO ACTION;
</code></pre>
<p>Since there is a unique key in institution with <code>(inst_id, inst_code)</code> postgress allow to create foreign key and It is validating when I <code>Insert</code>,<code>Update</code>, or <code>Delete</code>. When enforcing the foreign key, does PostgreSQL actually use that unique index even though the column order is different?
Or do I need to create a matching unique key (or swap column order) to get efficient index usage?</p>
",2,2,0,2025-09-25T08:07:43+00:00,2,75,True
79774712,3058288,,sql,Cleanup MySQL tables after duplicate records where created and are referenced by second table,"<p>After a long period of this going wrong we found out that there is a error in our data set.</p>
<p>In the manufacturers table, manufacturers are often added multiple times and the product in the product table is referencing the duplicate ID's for the manufacturer.</p>
<p>This question is only about fixing these tables, we already prevent this from happening again.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>manufacturers_id</th>
<th>manufacturers_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Manufacturer #1</td>
</tr>
<tr>
<td>2</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>3</td>
<td>Manufacturer #3</td>
</tr>
<tr>
<td>4</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>5</td>
<td>Manufacturer #3</td>
</tr>
<tr>
<td>6</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>7</td>
<td>Manufacturer #1</td>
</tr>
<tr>
<td>8</td>
<td>Manufacturer #3</td>
</tr>
<tr>
<td>9</td>
<td>Manufacturer #2</td>
</tr>
</tbody>
</table></div>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>products_id</th>
<th>manufacturers_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>7</td>
<td>7</td>
</tr>
<tr>
<td>8</td>
<td>8</td>
</tr>
<tr>
<td>9</td>
<td>9</td>
</tr>
</tbody>
</table></div>
<p>We need to achieve two things:</p>
<ol>
<li>Remove the duplicate entries from the table manufacturers and keep the first entry</li>
<li>Update the product table where the duplicate manufacturers ID's are being replaced by the remaining first Id for that manufacturer</li>
</ol>
<p>Each step can be done manually but the quantity of different manufacturers and products makes this not suitable for a manual task.
And I am lacking the needed query knowledge, so help would be welcome.</p>
<p>This would be the desired result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>manufacturers_id</th>
<th>manufacturers_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Manufacturer #1</td>
</tr>
<tr>
<td>2</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>3</td>
<td>Manufacturer #3</td>
</tr>
</tbody>
</table></div>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>products_id</th>
<th>manufacturers_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
</tr>
<tr>
<td>8</td>
<td>3</td>
</tr>
<tr>
<td>9</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>Table structure:</p>
<pre><code>CREATE TABLE manufacturers 
(
    `manufacturers_id` int(11) NOT NULL,
    `manufacturers_name` varchar(32) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci;

ALTER TABLE manufacturers
    ADD PRIMARY KEY (`manufacturers_id`),
    ADD KEY `IDX_MANUFACTURERS_NAME` (`manufacturers_name`);
  
INSERT INTO manufacturers (`manufacturers_id`, `manufacturers_name`) 
VALUES (1, 'Manufacturer #1'),
       (2, 'Manufacturer #2'),
       (3, 'Manufacturer #3'),
       (4, 'Manufacturer #2'),
       (5, 'Manufacturer #3'),
       (6, 'Manufacturer #2'),
       (7, 'Manufacturer #1'),
       (8, 'Manufacturer #3'),
       (9, 'Manufacturer #2');

CREATE TABLE `products` 
(
    `products_id` int(11) NOT NULL,
    `manufacturers_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci;

ALTER TABLE `products`
    ADD PRIMARY KEY (`products_id`);
 
INSERT INTO `products` (`products_id`, `manufacturers_id`) 
VALUES (1, 1),
       (2, 2),
       (3, 3),
       (4, 4),
       (5, 5),
       (6, 6),
       (7, 7),
       (8, 8),
       (9, 9);
</code></pre>
",4,5,1,2025-09-25T10:55:01+00:00,2,129,True
79774762,31569631,,sql,"With SQLite, where is the as keyword optional, and what difference does adding it make, if any?","<p>Currently I am learning Python programming for manipulating SQLite database, and I am a bit confused with the following SQL statement which works.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * 
FROM student s, result r
WHERE s.studentid = r.studentid
</code></pre>
<p>When we give an alias to a table, we're taught to use the '<strong>as</strong>' keyword before an alias, and the above SQL statement should be like this one:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * 
FROM student as s, result as r
WHERE s.studentid = r.studentid
</code></pre>
<ul>
<li>Is the above SQL statement also correct?</li>
<li>What situation don't I need to add the keyboard '<strong>as</strong>'?</li>
<li>Adding the keyword 'as' depends on our preferences? We can choose to add it or omit it randomly?</li>
</ul>
",0,4,4,2025-09-25T11:36:26+00:00,2,148,True
79775059,3149869,,sql,Create a composite row key in GridDB using NewSQL,"<p>How to create a composite row key in GridDB using NewSQL?</p>
<p>I am creating a collection container for storing thermostat data for multiple devices. I want to have the composite key on device_id and device_ts.
I wanted to use a timeseries but it does not allow multiple keys. But even when using it with a collection container I am unable to add a composite key.
Please let me know the syntax for adding a composite row key in GridDB using SQL. I found a link but its using Java programming: <a href=""https://www.toshiba-sol.co.jp/en/pro/griddb/docs-en/v5/GridDB_ProgrammingGuide.html#handling-composite-row-keys-1"" rel=""nofollow noreferrer"">https://www.toshiba-sol.co.jp/en/pro/griddb/docs-en/v5/GridDB_ProgrammingGuide.html#handling-composite-row-keys-1</a></p>
<p>Current syntax used but gives error:</p>
<pre><code>CREATE TABLE thermostat_data (
   device_ts TIMESTAMP NOT NULL,
   device_id STRING NOT NULL,
   temperature DOUBLE,
   humidity DOUBLE,
   target_temperature DOUBLE,
   status STRING,
   PRIMARY KEY (device_id, device_ts)
) WITH (
   expiration_type='PARTITION',
   expiration_time=30,
   expiration_time_unit='DAY'
) PARTITION BY RANGE (device_ts) EVERY (1, DAY);
</code></pre>
<p>The error is</p>
<blockquote>
<p>Execute SQL failed (reason=CREATE TABLE failed (reason=Invalid primary key column definition)</p>
</blockquote>
<p>I referred to the doc link <a href=""https://griddb.org/docs-en/manuals/GridDB_SQL_Reference.html#create-table"" rel=""nofollow noreferrer"">https://griddb.org/docs-en/manuals/GridDB_SQL_Reference.html#create-table</a> and used the same syntax for primary key but still it failed:</p>
<blockquote>
<p>Syntax provided in the link:
CREATE TABLE [IF NOT EXISTS] table_name ( column definition [, column definition …] [, PRIMARY KEY(column name [, …])] ) [WITH (property_key=property_value)] PARTITION BY HASH (column_name_of_partitioning_key) PARTITIONS division_count;</p>
</blockquote>
",2,2,0,2025-09-25T15:51:46+00:00,1,89,True
79775188,9289419,,sql,Multiply a Row by a Multiplier of a Column,"<p>I have a query that output a dataset that looks like this in DB2:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Amount</th>
<th>Multiplier</th>
</tr>
</thead>
<tbody>
<tr>
<td>001</td>
<td>1000</td>
<td>0</td>
</tr>
<tr>
<td>002</td>
<td>2000</td>
<td>0</td>
</tr>
<tr>
<td>003</td>
<td>3000</td>
<td>3</td>
</tr>
<tr>
<td>004</td>
<td>4000</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>I'd like to add a nested query with this condition: When Multiplier value is greater than 0, multiply each row of record by the numbers in Multiplier column then add a unique suffix number to the ID column while keeping everything else the same. The output should be like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Amount</th>
<th>Multiplier</th>
</tr>
</thead>
<tbody>
<tr>
<td>001</td>
<td>1000</td>
<td>0</td>
</tr>
<tr>
<td>002</td>
<td>2000</td>
<td>0</td>
</tr>
<tr>
<td>0031</td>
<td>3000</td>
<td>3</td>
</tr>
<tr>
<td>0032</td>
<td>3000</td>
<td>3</td>
</tr>
<tr>
<td>0033</td>
<td>3000</td>
<td>3</td>
</tr>
<tr>
<td>0041</td>
<td>4000</td>
<td>2</td>
</tr>
<tr>
<td>0042</td>
<td>4000</td>
<td>2</td>
</tr>
</tbody>
</table></div>
",2,3,1,2025-09-25T18:20:27+00:00,4,134,True
79775198,31571439,,sql,Efficiently query the latest row per device in a time-series container,"<p>I’m working with <code>GridDB Cloud</code> (Free Tier) using the Python client (latest version on Ubuntu 22.04).</p>
<p>I have a container called sensor_data with this schema:</p>
<pre><code>device_id STRING,
created_at TIMESTAMP,
temperature DOUBLE
</code></pre>
<p>Each device sends a reading every few seconds. The table grows quickly with millions of rows.</p>
<p>What I need is: for every <code>device_id</code>, get only the latest row (highest <code>created_at</code>).</p>
<p>What I tried:</p>
<p>A simple query with <code>MAX(created_at)</code> but it doesn’t return the full row, just the <code>timestamp</code>:</p>
<pre><code>SELECT device_id, MAX(created_at) 
FROM sensor_data 
GROUP BY device_id
</code></pre>
<p>Tried a join back on <code>created_at</code> to get the other columns, but <code>GridDB</code> returned errors about unsupported subqueries.</p>
<p>Attempted to sort and limit in Python after fetching all rows, but that’s inefficient for millions of rows.</p>
<p>something like that</p>
<pre><code>device_id  created_at            temperature

A          2025-09-25T10:15:00Z  22.1
B          2025-09-25T10:16:30Z  19.8
</code></pre>
<p>Question:</p>
<p>What’s the most efficient way in GridDB Cloud to fetch only the latest row per device_id in a large time-series container? Is there a built-in query pattern or best practice for this?</p>
",1,2,1,2025-09-25T18:34:55+00:00,1,103,True
79775369,2859614,,sql,How to EXECUTE a Triggered Task,"<p>I have not been able to <a href=""https://docs.snowflake.com/en/sql-reference/sql/execute-task"" rel=""nofollow noreferrer""><code>EXECUTE</code></a> a <a href=""https://docs.snowflake.com/en/user-guide/tasks-triggered"" rel=""nofollow noreferrer"">Snowflake Triggered Task</a> and I would like to better understand why, and where the behavior is documented.  To demonstrate my problem:</p>
<pre><code>-- SETUP
USE SCHEMA EXAMPLE_DB.EXAMPLE_SCHEMA;
CREATE OR REPLACE TABLE TRIGGER_TABLE (TRIGGER_COL TIMESTAMP);
CREATE OR REPLACE STREAM TRIGGER_STREAM ON TABLE TRIGGER_TABLE;
CREATE OR REPLACE TABLE DESTINATION_TABLE (EXAMPLE_COL VARCHAR, INSERT_TIME TIMESTAMP); 

CREATE OR REPLACE TASK TRIGGERED_TASK
  WAREHOUSE = EXAMPLE_WAREHOUSE
  WHEN SYSTEM$STREAM_HAS_DATA('TRIGGER_STREAM')
  AS
    INSERT INTO DESTINATION_TABLE (EXAMPLE_COL, INSERT_TIME)
        VALUES ('TRIGGERED_TASK has run', CURRENT_TIMESTAMP());

ALTER TASK TRIGGERED_TASK RESUME;

-- Trigger the task
INSERT INTO TRIGGER_TABLE (TRIGGER_COL) VALUES (CURRENT_TIMESTAMP());
SELECT * FROM DESTINATION_TABLE;
-- Works great!
-- EXAMPLE_COL           |INSERT_TIME            |
-- ----------------------+-----------------------+
-- TRIGGERED_TASK has run|2025-09-25 15:25:33.638|

-- Execute the task
EXECUTE TRIGGERED_TASK;
-- Output: SQL Error [603] [XX000]: SQL execution internal error: Processing aborted due to error 370001:2832376018; incident XXXXXXX.

-- What does TASK_HISTORY have to say about this?
SELECT NAME, STATE, ERROR_CODE, ERROR_MESSAGE FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(TASK_NAME=&gt;'TRIGGERED_TASK'));
-- STATE    |ERROR_CODE|ERROR_MESSAGE                                           |
-- ---------+----------+--------------------------------------------------------+
-- SKIPPED  |0040003   |Conditional expression for task evaluated to false.     |    This is the `EXECUTE` attempt
-- SUCCEEDED|          |                                                        |    This is the `INSERT` attempt
</code></pre>
<p>Here I am told the conditional expression for the task (<code>WHEN SYSTEM$STREAM_HAS_DATA('TRIGGER_STREAM')</code>) evaluated to <code>False</code>.  I might have expected <code>EXECUTE</code> to override the condition, but OK.  The page on <a href=""https://docs.snowflake.com/en/user-guide/tasks-ts"" rel=""nofollow noreferrer"">Troubleshooting Tasks</a> has this for me:</p>
<blockquote>
<p>Step 4: Verify the condition</p>
<p>If the task includes a WHEN clause with a
SYSTEM$STREAM_HAS_DATA condition, verify that the specified stream
contained change data capture (CDC) records when the task was last
scheduled to run. Historical data for a stream can be queried using an
AT | BEFORE clause.</p>
</blockquote>
<p>I get the idea, there were no <em>new</em> records in the stream, so the condition was not met.  But then, why does <code>SYSTEM$STREAM_HAS_DATA</code> return <code>True</code> when I call it directly?</p>
<pre><code>-- Is the stream hydrated?
SELECT COUNT(*) FROM TRIGGER_STREAM;
-- COUNT(*)|
-- --------+
--        3|
-- OK but records already triggered the Task and are 'old'(?).
-- Does the Task's condition evaluate to True?
SELECT SYSTEM$STREAM_HAS_DATA('TRIGGER_STREAM') AS TASK_CONDITION;
-- TASK_CONDITION|
-- --------------+
-- true          |
</code></pre>
<p>Checking the documentation for <a href=""https://docs.snowflake.com/en/sql-reference/functions/system_stream_has_data"" rel=""nofollow noreferrer""><code>SYSTEM$STREAM_HAS_DATA(...)</code></a></p>
<blockquote>
<p>When the input is a view stream, the returned value is TRUE when
change data capture (CDC) records for the underlying tables change.
The function performs a diff on the version metadata for the
underlying tables rather than for the view itself. The result is a
false positive when the query in the source view definition does not
reference the rows in the underlying tables that have changed. The
rate of false positives increases as a view becomes more selective.</p>
<p>When this function is referenced in the optional WHEN parameter in a
task definition, the higher false positive rate means that tasks may
run when a view stream is empty more often than when a table stream is
the input for the function. However, this check still avoids task runs
when there is no change in the underlying table data.</p>
</blockquote>
<p>From that I would have expected either to see <code>FALSE</code> returned when I called the function myself, or to  for this to be a 'false positive' that would have allowed the task above to execute.  What am I missing here?</p>
",1,1,0,2025-09-25T23:09:01+00:00,1,132,True
79776085,282601,,sql,Using TYPO3 site settings in TCA leads to error,"<p>It's possible to use <a href=""https://docs.typo3.org/m/typo3/reference-coreapi/11.5/en-us/ApiOverview/SiteHandling/UseSiteInTCA.html"" rel=""nofollow noreferrer"">TYPO3 site settings in TCA SQL <code>foreign_table_where</code> clauses</a>, e.g.</p>
<pre><code>'config' =&gt; [
    'type' = 'select',
    'renderType' =&gt; 'selectMultipleSideBySide',
    'multiple' =&gt; true,
    'foreign_table' =&gt; 'tt_content',
    'foreign_table_where' =&gt; ' AND tt_content.deleted = 0'
        . ' AND tt_content.pid = ###SITE:settings.pages.autoelements###',
]
</code></pre>
<p>This works fine, but fails when a page outside a site is opened - e.g. when I have the following page tree structure:</p>
<pre><code>[Root]
+ company 1 (folder)
| + site 1  (site root)
| + site 2  (site root)
+ company 2 (folder)
  + site 1  (site root)
  + site 2  (site root)  
</code></pre>
<p>When editing the sysfolder &quot;company 1&quot;, the backend shows an error:</p>
<blockquote>
<p>Database Error</p>
<p>You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '' at line 1. A SQL error occurred. This may indicate a schema mismatch between TCA and the database. Try running database compare in the Install Tool.</p>
</blockquote>
<p>The generated SQL is</p>
<pre><code>SELECT
    `tt_content`.`uid`,
    [...]
FROM
    `tt_content`,
    `pages`
WHERE
    (
        tt_content.deleted = 0 AND tt_content.pid = ###SITE:settings.pages.autoelements###
    ) AND(1 = 1) AND(`pages`.`uid` = `tt_content`.`pid`) AND(
        (
            (`tt_content`.`deleted` = 0) AND(`pages`.`deleted` = 0)
        ) AND(
            (
                (`tt_content`.`t3ver_wsid` = 0) AND(
                    (`tt_content`.`t3ver_oid` = 0) OR(`tt_content`.`t3ver_state` = 4)
                )
            ) AND(
                (`pages`.`t3ver_wsid` = 0) AND(
                    (`pages`.`t3ver_oid` = 0) OR(`pages`.`t3ver_state` = 4)
                )
            )
        )
    )
</code></pre>
<p>so the site settings place holder does not get replaced - there is not site configuration.</p>
<hr />
<p>How can I prevent such SQL errors?</p>
",1,1,0,2025-09-26T14:48:17+00:00,1,86,False
79776168,1051933,"Milan, Italy",sql,Multi-table check in Check Constraint or in stored procedure,"<p>I am working on a data model for recording daily construction work reports in PostgreSQL.</p>
<p>A work report is composed by many rows, of three exclusive subtypes: People, Materials and Equipment.</p>
<p>Each work report is daily and details work performed on one job and has one Foreman.</p>
<p>Currently the data model has the following relevant tables (I am omitting details of the WorkReportRow and subtypes):</p>
<pre><code>Job
+---------------------+
| JobCode          PK |
+---------------------+
| JobName             |
| Address             |
+---------------------+

WorkReport
+---------------------+
| JobCode        PK.1 |
| Date           PK.2 |
+---------------------+
| Foreman          FK | -&gt; To PersonCode in Person table
+---------------------+

Person
+---------------------+
| PersonCode       PK |
+---------------------+
| InsuranceNumber     |
+---------------------+

Role
+---------------------+
| RoleCode         PK |
+---------------------+

PersonRole
+---------------------+
| RoleCode    PK.1 FK | -&gt; to RoleCode in Role table
| PersonCode  PK.2 FK | -&gt; to PersonCode in Person table
+---------------------+
</code></pre>
<p>Please note that a person can have more than one role.</p>
<p>I have to enforce a rule where the foreman for each <code>WorkReport</code> is a <code>Person</code> that in the <code>Role</code> has at least <code>Foreman</code>.</p>
<p>Due to the nature of the constraint I'd like for it to be enforced in the database. Ideally I'd add a view called <code>Foreman</code>:</p>
<pre><code>CREATE OR REPLACE VIEW Foremen AS
  SELECT p.PersonCode FROM Person p
  JOIN PersonRole pr on p.PersonCode = pr.PersonCode
  WHERE pr.RoleCode = 'Foreman'
</code></pre>
<p>And then in the <code>WorkReport</code> table the <code>Foreman</code> FK would point to the <code>Foremen</code> view, the problem is that PostgreSQL apparently doesn't support FK to views.</p>
<p>The two options I see now are:</p>
<p><strong>1.</strong> a <code>CHECK</code> constraint in the <code>WorkReport</code> table like this:</p>
<pre><code>ADD CONSTRAINT chk_foreman
CHECK (
  EXISTS (
    SELECT 1
    FROM &quot;PersonRole&quot; pr
    WHERE pr.&quot;PersonCode&quot; = &quot;WorkReport&quot;.&quot;Foreman&quot;
    AND pr.&quot;RoleCode&quot; = 'Foreman'
  )
);
</code></pre>
<p>I like this solution as it is in the &quot;schema&quot; of the database, therefore impossible to violate, but I don't like to have to per form a query in a check as it looks wasteful.</p>
<p><strong>EDIT</strong>: it was correctly pointed out in the comments that CHECK doesn't support queries to other tables in the database.<br />
The correct statement defined in the SQL standard would be ASSERTION, which is unfortunately not implemented in any RBDMS.<br />
This leaves, without duplicating data with a <code>Foremen</code> table or other data model changes (Subtyped for Person) only the following solution.</p>
<p><strong>2.</strong> A check in the stored procedure (current solution)</p>
<p>Due to the need to keep integrity (between WorkReport, WorkReportRow and RowPerson/RowMaterial/RowEquipment) WorkReport insertion is already handled by a stored procedure.</p>
<p>Currently I added this check in the stored procedure:</p>
<pre><code>  SELECT EXISTS (
    SELECT 1
    FROM &quot;PersonRole&quot;
    WHERE &quot;PersonCode&quot; = P_Foreman
    AND &quot;RoleCode&quot; = 'Foreman'
  ) INTO is_foreman;

  IF NOT is_foreman THEN
    RAISE EXCEPTION 'Error: Person % is not a Foreman.', P_Foreman;
  END IF;
</code></pre>
<p>The issue I have with the current implementation is that the check is performed at the Procedure level, and not as a schema constraint, which I think logically would be the best solution.</p>
<p>Do you have any input on this based on the relational model rules and standard?</p>
",2,2,0,2025-09-26T16:13:38+00:00,3,139,True
79776457,8006721,"Seattle, WA, United States",sql,XML Set the Element Name from Attribute Value,"<p>I have a table of answers from a submission form that I need to convert to an XML file and upload to a website. Each row is one answer from one submissions and the answers need to be stored as the element name.</p>
<p>Table example: <strong>Note</strong>: Answer is a string value. The form has multiple questions, I'm only concerned with the Answers.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Submission ID</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>foo</td>
</tr>
<tr>
<td>1</td>
<td>bar</td>
</tr>
<tr>
<td>2</td>
<td>baz</td>
</tr>
</tbody>
</table></div>
<p>Then the XML should like this:</p>
<pre><code>&lt;allSubmissions&gt;
    &lt;submission&gt;
        &lt;foo /&gt;
        &lt;bar /&gt;
    &lt;/submission&gt;
    &lt;submission&gt;
        &lt;baz /&gt;
    &lt;/submission&gt;
&lt;/allSubmissions&gt;
</code></pre>
<p>How can I accomplish this? I've been successful in using the <code>FOR XML EXPLICIT</code> to format tables where the column name is equal to the XML tag name, but not where the value from the column is supposed to be the name of the tag.</p>
<p>I would prefer NOT use some sort of concatenation work around if possible since I'd like to save the results to an XML file.</p>
<p>Answering the questions in the comment.</p>
<ol>
<li>XML is the required file type I need, so no CSV, text file, etc.</li>
<li>Correction; I mispoke when I said &quot;any string value&quot;. The answer will always be a string and it comes from a pre-defined list of about ~10 values. So I shouldn't have to worry about an illegal element name like something that starts with a number.</li>
<li>The example format provided is the required schema that the validator tests against. So unfortunately, I can't do something like <code>&lt;answer&gt;foo&lt;/answer&gt;</code></li>
<li>A text file that looks like XML would work provided that I can convert to an XML file.</li>
</ol>
",1,1,0,2025-09-26T23:52:16+00:00,3,178,True
79777512,22765093,,sql,Query on SSRS Content table for reports with hyperlink action returns all reports,"<p>I am trying to identify only reports with a hyperlink to another report.  I am using SQL Server 2019 (150).</p>
<p>My SQL:</p>
<pre><code>SELECT
    C.ItemID,
    C.Name AS ReportName,
    C.Path AS ReportPath,
    CONVERT(XML, CONVERT(VARBINARY(MAX), C.Content)) AS ReportDefinitionXML
FROM
    dbo.Catalog AS C
WHERE
    C.Type = 2 -- Type 2 indicates a Report
    AND CAST(C.Content AS XML).exist('//Action/ActionInfo/Actions/Action/ActionType/text() = &quot;URL&quot; or //Action/ActionInfo/Actions/Action/ActionType/text() = &quot;ReportLink&quot; or //Action/ActionInfo/Actions/Action/ActionType/text() = &quot;BookmarkLink&quot;') = 1
    AND C.Path Like '/MyCompany_Reports/%'
</code></pre>
<p>Report with a hyperlink:</p>
<pre><code>                              &lt;ActionInfo&gt;
                                &lt;Actions&gt;
                                  &lt;Action&gt;
                                    &lt;Hyperlink&gt;=&quot;JavaScript:void window.open('&quot; &amp;amp; Globals!ReportServerUrl &amp;amp; &quot;/Pages/Folder.aspx?&quot;
&amp;amp; &quot;/Timberlyne_Reports/DrillSubs/&quot;
&amp;amp; Switch(Fields!sort_document.Value=1,&quot;dtBacklogInvoice&quot;,Fields!sort_document.Value=2,&quot;dtBacklogOrders&quot;,True,&quot;dtBacklogCompletion&quot;)
&amp;amp; &quot;&amp;amp;SITE_ID=&quot; &amp;amp; CStr(Fields!SITE_ID.Value)
&amp;amp; &quot;&amp;amp;YEAR=&quot; &amp;amp; CStr(Fields!RPT_YEAR.Value)
&amp;amp; &quot;&amp;amp;START_PERIOD=&quot; &amp;amp; CStr(Fields!RPT_PERIOD.Value)
&amp;amp; &quot;&amp;amp;END_PERIOD=&quot; &amp;amp; CStr(Fields!RPT_PERIOD.Value)
&amp;amp; &quot;','_blank')&quot;&lt;/Hyperlink&gt;
</code></pre>
<p>I have tested reports without an action and none of the above XML code is in the report XML.
My SQL code always returns a value of 1 - true for all reports.</p>
<p>How can I query only reports with a hyperlink?</p>
",2,3,1,2025-09-28T18:14:19+00:00,2,85,True
79777567,14451585,,sql,How to insert SM (service mark) symbol in SQL,"<p>How can I select a service mark (SM) symbol in SQL?</p>
<pre><code>select CHAR(0153) --&gt; ™

select CHAR(0174) --&gt; ®

select CHAR(0169) --&gt; ©
</code></pre>
<p>What would the equivalent be for a SM symbol?</p>
",3,7,4,2025-09-28T20:01:29+00:00,1,237,True
79778041,29545498,,sql,Comments on Indexes in Oracle SQL Dialect,"<p>I know that in Oracle SQL we can add comments on tables.
But is it possible to add comments on indexes as well?
If yes, how can it be done?</p>
<p>-- What I tried:</p>
<p>I attempted something like:</p>
<pre><code>COMMENT ON INDEX my_index IS 'This is my test index';
</code></pre>
<p>But I wasn’t sure if this is supported, since I couldn’t find much in the documentation.</p>
<p>-- What I expected:
To be able to add descriptive comments on indexes in the same way we do for tables and columns.</p>
",1,1,0,2025-09-29T11:14:51+00:00,2,104,True
79778179,31510249,,sql,Get hourly timestamped data using a string identifier,"<p>In the scenario where sensor data is attached to a specific string identifier in GridDb/GridDb Cloud, the SQL query should return hourly sensor data only matching with a certain combination of letters.</p>
<p>Here is the standard of the string identifier (&quot;st_id&quot; column) <code>([A-Z0-9]-[A-Z0-9]-[A-Z0-9])</code>. Given a string identifier &quot;X1V2-BW9I&quot;. It should match &quot;X1V2-BW9I&quot;, &quot;X1V2&quot; and &quot;X1&quot;.</p>
<pre class=""lang-none prettyprint-override""><code>XXXX-XXXX-XXXX

X X X X   X X X X    X  X  X  X
        -          -
1 2 3 4 5 6 7 8 9 10 11 12 13 14

1-2: MID
1-4: VID
1-9: SID
11-14: CUID
1-14: STID
</code></pre>
<p><strong>Minimal reproducible example</strong></p>
<p>To create the table:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE sensor_data (row_key TIMESTAMP NOT NULL, st_id STRING, temperature DOUBLE, pressure DOUBLE, PRIMARY KEY (row_key));
</code></pre>
<p>To insert data:</p>
<pre class=""lang-none prettyprint-override""><code>INSERT INTO sensor_data VALUES
(TIMESTAMP('2025-09-29T16:00:00Z'), 'X1V2-BW9I-6K52', 60.7, 30.5),
.
.
.
(TIMESTAMP('2025-09-29T17:00:00Z'), 'X1V2-BW9I-6N52', 80.7, 50.5),
.
.
.
(TIMESTAMP('2025-09-29T18:00:00Z'), 'X1V3-BW9I-6N52', 60.2, 40.5);
</code></pre>
<p>SELECT query with CASE:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT TIMESTAMP_TRUNC(HOUR, row_key) AS row_key_hr, glob('X1V2-BW9I', st_id) as g,
CASE
    WHEN (SUBSTR(st_id, 1, 9) == SUBSTR('X1V2-BW9I', 1, 9)) THEN 'SID'
    WHEN (SUBSTR(st_id, 1, 4) == SUBSTR('X1V2-BW9I', 1, 4)) THEN 'VID'
    WHEN (SUBSTR(st_id, 1, 2) == SUBSTR('X1V2-BW9I', 1, 2)) THEN 'MID'
    ELSE ''
END AS STID
FROM sensor_data GROUP BY row_key_hr ORDER BY row_key_hr;
</code></pre>
<p>Instead of using <code>CASE</code>, <code>glob</code> should match either <code>SID</code>, <code>VID</code> or <code>MID</code> to a given user provided <code>ID</code>, and if it matches, then it should <strong>also</strong> match <strong>one extra condition</strong> described below:</p>
<p>If the given <code>ID</code> matches either <code>SID</code>, <code>VID</code>, <code>MID</code> <strong>then</strong> <code>CUID</code> should not contain a static <code>two-letter pattern(NONCUID)</code>.</p>
<p>Such as, if <code>ID = (XXXX-XXXX | XXXX | XX)</code> and <code>NONCUID = II</code> and <code>st_id = XXXX-XXXX-XXII</code> then it should return <code>false</code>.</p>
<p>Whereas, if <code>ID = (XXXX-XXXX | XXXX | XX)</code> and <code>NONCUID = II</code> and <code>st_id = XXXX-XXXX-XXXX</code> then it should return <code>true</code>.</p>
<p><strong>Expected SQL query:</strong></p>
<pre><code>SELECT glob(&quot;&lt;PATTERN&gt;&quot;, st_id) FROM sensor_data;
</code></pre>
<p>I haven't had any experience with &quot;glob&quot;, but it seems like a perfect option in this scenario, but I am not sure how to construct the query using &quot;glob&quot;.</p>
",1,3,2,2025-09-29T13:40:15+00:00,0,172,False
79778308,233286,"Sofia, Bulgaria",sql,Hibernate specification with parentheses,"<p>I am trying to create a Hibernate specification with parentheses but was not able to do it.</p>
<p>I have many specifications. My problem is with the following specification:</p>
<pre class=""lang-java prettyprint-override""><code>public static Specification&lt;ListingEntity&gt; locations(List&lt;String&gt; locations) {
    if (locations == null || locations.isEmpty()) {
        return null;
    } else {
        return (root, query, builder) -&gt; {
            Join&lt;ListingEntity, GeoLevelTwoEntity&gt; joinGeoLevelTwo = root.join(&quot;geoLevelTwoEntity&quot;);
            Join&lt;GeoLevelTwoEntity, GeoLevelOneEntity&gt; joinGeoLevelOne = joinGeoLevelTwo.join(&quot;geoLevelOneEntity&quot;);

            Predicate predicate = builder.conjunction();
            for (String location : locations) {

                if (location.contains(&quot;_&quot;)) {
                    String[] locationParts = location.split(&quot;_&quot;);
                    Predicate predicateLevelOne = builder.equal(joinGeoLevelOne.get(&quot;geographyLevelOne&quot;), locationParts[1]);

                    Predicate predicateLevelTwo = joinGeoLevelTwo.get(&quot;geographyLevelTwo&quot;).in(locationParts[0]);
                    Predicate locationPredicate = builder.and(predicateLevelOne, predicateLevelTwo);

                    predicate = builder.or(predicate, locationPredicate);
                }

            }
            return predicate;

        };
    }
}
</code></pre>
<p>It creates the following SQL script:</p>
<pre class=""lang-sql prettyprint-override""><code>from 
  listings le1_0 
  join geo_level_two glte1_0 on glte1_0.id = le1_0.geography_level_two_id 
  join geo_level_one gloe1_0 on gloe1_0.id = glte1_0.geography_level_one_id 
where 
  1 = 1 
  or gloe1_0.geography_level_one = ? 
  and glte1_0.geography_level_two in (?) 
  or gloe1_0.geography_level_one = ? 
  and glte1_0.geography_level_two in (?) 
order by 
  (
    select 
      0
  ) offset ? rows fetch first ? rows only
</code></pre>
<p>I want it to create the following way:</p>
<pre class=""lang-sql prettyprint-override""><code>where 
  1 = 1 
  and ( gloe1_0.geography_level_one = ? 
  and glte1_0.geography_level_two in (?) 
  or gloe1_0.geography_level_one = ? 
  and glte1_0.geography_level_two in (?) )
order by 
</code></pre>
<p>I have other specifications that create <code>and</code> conditions like <code>and floor = 2</code> etc. Thus I cannot think it without using parentheses. I think it needs to look like the following:</p>
<pre class=""lang-sql prettyprint-override""><code>(geographyL1 = 'New York' and geographyL2 = 'USA' or geographyL1 = 'London' and geographyL2 = 'UK') and floor = 2 and price &gt; 100000
</code></pre>
<p>I have no issue with other specifications that generate the following SQL. Each condition has its own separate specification:</p>
<pre class=""lang-sql prettyprint-override""><code>floor = 2 and price &gt; 100000 and numberOfRooms = 2
</code></pre>
<p>What should I do so that my specification for location lies inside parentheses?</p>
",0,0,0,2025-09-29T15:32:28+00:00,2,104,True
79779419,9289419,,sql,Count 3 different set of dates from two columns,"<p>I have a query with a result like this example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column_1</th>
<th>Column_2</th>
<th>Column_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td>9/1/2025</td>
<td>9/15/2025</td>
</tr>
<tr>
<td>Text</td>
<td>9/1/2025</td>
<td>9/15/2025</td>
</tr>
<tr>
<td>Text</td>
<td>9/1/2025</td>
<td>9/30/2025</td>
</tr>
<tr>
<td>Text</td>
<td>9/1/2025</td>
<td>9/30/2025</td>
</tr>
</tbody>
</table></div>
<p>I want to create a nested query to count each group of dates in Column_2 and Column_3. End result should have one single row with a sum of <strong>3</strong> (1 for 9/1/2025, 1 for 9/15/2025, and 1 for 9/30/2025). Prefer syntax that works with the DB2 database.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column_1</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td>3</td>
</tr>
</tbody>
</table></div>
",-1,0,1,2025-09-30T18:28:44+00:00,3,137,True
79780050,12230870,,sql,Using a schema as a parameter in stored procedure,"<p>So my application has the following two functions. However I would like to execute the sql in the lld_comparison_graph_data function as a stored procedure.</p>
<pre><code>def lld_comparison_graph_data(self, firm, request_type, drop, field):
    &quot;&quot;&quot;a function to return KRI data&quot;&quot;&quot;
    logger.info(&quot;{} lld_comparison_graph_data &quot;.format(current_user.username))
    engine_lld = get_db_lld()

    if len(drop) &gt; 0 and drop != &quot;All&quot; and drop[0].isdigit() is False:
        drop = datetime.strptime(drop, &quot;%a, %d %b %Y %H:%M:%S %Z&quot;)

    schema = self.get_schema_name(request_type)

    sql = &quot;&quot;&quot; SELECT Loan.LoanId, Loan.{} FROM [DataParsingDestination].[{}].[Loan] Loan 
                INNER JOIN [DataParsingDestination].[{}].[HeaderCurrent] Header ON Loan.FileId = Header.FileId
                WHERE Header.InstitutionNumber = '{}' AND YEAR(ReportingDate) = {} AND MONTH(ReportingDate) = {}&quot;&quot;&quot;
    data_returned = engine_lld.execute(sql.format(field, schema, schema, firm, drop.year, drop.month)).fetchall()

    return data_returned
    def get_schema_name(self, return_type):

        if return_type == &quot;Mortgages&quot;:
            schema = 'l02'
        elif return_type == &quot;Commercial&quot;:
            schema = 'l01'
        elif return_type == &quot;Consumer&quot;:
            schema = 'l03'
        elif return_type == &quot;HistoricCommercial&quot;:
            schema = 'l05'
        else: #return_type == &quot;HistoricMortgages&quot;:
            schema = 'l06'

        return schema
</code></pre>
<p>I have so far the following draft for the stored procedure:</p>
<pre><code>USE [DataParsingDestination]
GO

SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO

CREATE PROCEDURE [LLDComparisonGraphData] 
    @InstitutionNumber INT = '',
    @ReportingYear INT = '',
    @ReportingMonth INT = '',
    @LoanId VARCHAR(50) = '',
    @Loan VARCHAR(50) = '',
    @schema VARCHAR(50) = NULL
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE @Query VARCHAR(1000)

    SET @Query = 'SELECT Loan.LoanId, Loan = @Loan
                  FROM [DataParsingDestination].[@schema].[Loan] Loan
                  INNER JOIN [DataParsingDestination].[@schema].[HeaderCurrent] Header ON Loan.FileId = Header.FileId 
                  WHERE Header.InstitutionNumber = @InstitutionNumber  
                    AND YEAR(ReportingDate) = @ReportingYear 
                    AND MONTH(ReportingDate) = @ReportingMonth'

END
</code></pre>
<p>However I cannot seem to get it to work. Do I have to get rid of the get_schema_name function and replace it with another stored procedure or can I leave it as it is.</p>
",1,2,1,2025-10-01T12:38:16+00:00,1,85,True
79780374,31510249,,sql,Correct syntax for selecting data between NOW() and the previous hour,"<p>Considering that GridDB does not provide an <code>INTERVAL</code> keyword and will result in a syntax error, what is the correct syntax for selecting data between <code>NOW()</code> and the <code>previous hour</code> in GridDB/GridDB Cloud?</p>
<p>With below table structure:</p>
<pre><code>CREATE TABLE sensor_data 
(
    row_key TIMESTAMP NOT NULL, 
    temperature DOUBLE, 
    PRIMARY KEY (row_key)
);
</code></pre>
<p>select query given below won't work and end up in syntax error due to the absence of <code>INTERVAL</code> keyword:</p>
<pre><code>SELECT *  
FROM sensor_data 
WHERE row_key Between NOW() - INTERVAL '1 hour' AND NOW();
</code></pre>
",0,0,0,2025-10-01T18:40:42+00:00,2,92,True
79781575,3970831,,sql,Mark rows within comments,"<p>I have a table like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">name</th>
<th style=""text-align: center;"">line</th>
<th style=""text-align: left;"">text</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: left;"">some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: left;"">/* some text */</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: left;"">some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: left;"">/*</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">5</td>
<td style=""text-align: left;"">some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">6</td>
<td style=""text-align: left;"">some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">7</td>
<td style=""text-align: left;"">*/</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">8</td>
<td style=""text-align: left;"">some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">9</td>
<td style=""text-align: left;"">/*</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: left;"">* some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">11</td>
<td style=""text-align: left;"">* some text</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">12</td>
<td style=""text-align: left;"">*/</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">13</td>
<td style=""text-align: left;"">some text</td>
</tr>
</tbody>
</table></div>
<pre><code>with t as
(
    select 'aaa' name,  1 line, 'some text' text    from dual union all
    select 'aaa',       2,      '/* some text */'   from dual union all
    select 'aaa',       3,      'some text'         from dual union all
    select 'aaa',       4,      '/*'                from dual union all
    select 'aaa',       5,      'some text'         from dual union all
    select 'aaa',       6,      'some text'         from dual union all
    select 'aaa',       7,      '*/'                from dual union all
    select 'aaa',       8,      'some text'         from dual union all
    select 'aaa',       9,      '/*'                from dual union all
    select 'aaa',      10,      '* some text'       from dual union all
    select 'aaa',      11,      '* some text'       from dual union all
    select 'aaa',      12,      '*/'                from dual union all
    select 'aaa',      13,      'some text'         from dual
)
select t.*
from t
order by name, line
</code></pre>
<p>How can I calculate a value for a new column where lines within line or block comments would be marked as 1 and the rest as 0?</p>
<p>Like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">name</th>
<th style=""text-align: center;"">line</th>
<th style=""text-align: left;"">text</th>
<th style=""text-align: center;"">is_comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: left;"">some text</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: left;"">/* some text */</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: left;"">some text</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: left;"">/*</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">5</td>
<td style=""text-align: left;"">some text</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">6</td>
<td style=""text-align: left;"">some text</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">7</td>
<td style=""text-align: left;"">*/</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">8</td>
<td style=""text-align: left;"">some text</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">9</td>
<td style=""text-align: left;"">/*</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">10</td>
<td style=""text-align: left;"">* some text</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">11</td>
<td style=""text-align: left;"">* some text</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">12</td>
<td style=""text-align: left;"">*/</td>
<td style=""text-align: center;""><strong>1</strong></td>
</tr>
<tr>
<td style=""text-align: center;"">aaa</td>
<td style=""text-align: center;"">13</td>
<td style=""text-align: left;"">some text</td>
<td style=""text-align: center;"">0</td>
</tr>
</tbody>
</table></div>
",3,4,1,2025-10-03T08:36:31+00:00,2,117,True
79782524,3617866,"Dhaka, Bangladesh",sql,"Composite key (ts, deviceid) with timeseries container","<p>I’m on GridDB Cloud (Free). For a TimeSeries container, I want each device to have at most one row per timestamp, i.e., enforce uniqueness on <code>(ts, deviceid)</code>.</p>
<p><strong>Schema:</strong></p>
<pre><code>CREATE TABLE TSDB (
  ts TIMESTAMP PRIMARY KEY,   -- row key
  deviceid STRING,
  temperature DOUBLE
) USING TIMESERIES;
</code></pre>
<p><strong>Question:</strong>
Does GridDB TimeSeries support a composite primary key like <code>(ts, deviceid)</code> (or a <code>UNIQUE (ts, deviceid)</code> constraint), or is <code>ts</code> the only allowed row key? If composites aren’t supported, what’s the recommended pattern to enforce one row per device per timestamp (e.g., modeling or indexing approach)?</p>
<p><strong>Environment:</strong></p>
<ul>
<li>GridDB Cloud (Free)</li>
<li>SQL console.</li>
</ul>
",0,0,0,2025-10-04T14:25:58+00:00,0,74,False
79782544,3058288,,sql,How do I retrieve the primary key of the updated record?,"<p>I have this insert query for my MariaDB 10.6.19 database:</p>
<pre><code>CREATE TABLE manufacturers (
  manufacturers_id int(11) NOT NULL AUTO_INCREMENT key,
  manufacturers_name varchar(32) unique NOT NULL,
  manufacturers_image varchar(64) DEFAULT NULL
);

INSERT INTO manufacturers (manufacturers_id, manufacturers_name, manufacturers_image)
VALUES (1, 'Name', NULL);
</code></pre>
<p>Later I want to add the image to the data but I'm unaware if the manufacturer already exists in the database. So I use:</p>
<pre><code>INSERT INTO manufacturers (manufacturers_name, manufacturers_image)
  values ('Name', NULL)
ON DUPLICATE KEY
   UPDATE  manufacturers_name ='Name',
           manufacturers_image = 'MyImage.jpg';
</code></pre>
<p>And the record is updated, but how do I retrieve the primary key of the updated record?</p>
",-4,0,4,2025-10-04T15:27:55+00:00,1,100,True
79782665,2052584,,sql,Query upcoming birthday date,"<p>My table looks like</p>
<pre><code>ID | PersonName | Birthday   | IntDate
--------------------------------------
 1 | Joe        | 1977-08-20 |     820
 2 | Sandy      | 1985-02-27 |     227
 3 | Jane       | 1981-11-01 |    1101
</code></pre>
<p>The <code>INDEX</code>ed helper column <code>IntDate</code> is just an int = <code>month*100+days</code>, written at insert/update and used to speed up queries for all birthdays in a given date range (e.g. between [1001;1031] for October) independent of the year. (So I don't have to filter based on <code>EXTRACT</code> or <code>DATE_PART</code> functions for day and month in the WHERE clause, as the birthday DATE includes a year in the past.)</p>
<p>Given a certain date (e.g. <code>2025-05-01</code>), I want to query the next birthday date of the persons and the upcoming anniversary (what birthday is coming).
Then I want to sort the list by the next birthday date (this or next year).</p>
<p>For <code>2025-05-01</code>, the list might look like:</p>
<pre><code>ID | PersonName | Birthday   | NextBirthday ^ | Anniversary (at upcoming birthday)
-----------------------------------------------------------
 1 | Joe        | 1977-08-20 |   2025-08-20   |          48
 3 | Jane       | 1981-11-01 |   2025-11-01   |          44
 2 | Sandy      | 1985-02-27 |   2026-02-27   |          41
</code></pre>
<p>Is that possible using an SQL SELECT query in PostgreSQL?</p>
",1,3,2,2025-10-04T20:40:30+00:00,3,161,True
79782787,10462477,,sql,Merge &quot;WHEN MATCHED&quot; with &quot;AND&quot;,"<p>Does Oracle support merge <code>WHEN MATCHED</code> with <code>AND</code> condition ? I am trying to write a query like below and getting error <code>ORA-00905 : missing keyword</code></p>
<pre><code>MERGE INTO TEST T
USING (  SELECT  1    AS COL1
        ,'B'    AS COL2
        ,'C'    AS COL3
        ,'D'    AS COL4
        ,'X'    AS COL5
        FROM DUAL
      ) INP
ON (     T.COL1 = INP.COL1
     AND T.COL2 = INP.COL2
   ) 

WHEN MATCHED 
AND (T.COL3 &lt;&gt; INP.COL3)                --&gt; Errored Line
THEN
UPDATE SET   T.COL3     = INP.COL3
             ,T.COL4     = INP.COL4
             ,T.COL5     = INP.COL5
                            
WHEN NOT MATCHED THEN
INSERT  (COL1 ,COL2 ,COL3 ,COL4 ,COL5)
VALUES  (INP.COL1 ,INP.COL2 ,INP.COL3 ,INP.COL4 ,INP.COL5 );  

                 
</code></pre>
",2,2,0,2025-10-05T04:58:48+00:00,1,93,True
79783029,243872,,sql,Syntax error 1064 with LOAD DATA ... FIELDS OPTIONALLY ENCLOSED BY &#39;&quot;&#39;,"<p>My SQL statement in MySQL 5.7.44, Windows 10:</p>
<pre class=""lang-sql prettyprint-override""><code>drop table if exists test;
create table test (col1 VARCHAR(64), col2 VARCHAR(80) );
LOAD DATA LOCAL INFILE &quot;t.csv&quot; INTO TABLE test FIELDS OPTIONALLY ENCLOSED BY '&quot;' ESCAPED BY '&quot;' COLUMNS TERMINATED BY ';' LINES TERMINATED BY '\r\n' ;
</code></pre>
<p>The data:</p>
<pre class=""lang-none prettyprint-override""><code>E605;
E507;&quot;A string
spanning several lines &quot;some Umlauts in &quot;&quot;üöä&quot;&quot;  to show&quot;&quot;
escaping
last line&quot;
E600;&quot;once
again&quot;
</code></pre>
<p>As hexdump:</p>
<pre class=""lang-none prettyprint-override""><code>00000000  45 36 30 35 3b 0d 0a 45  35 30 37 3b 22 41 20 73  |E605;..E507;&quot;A s|
00000010  74 72 69 6e 67 0d 0a 73  70 61 6e 6e 69 6e 67 20  |tring..spanning |
00000020  73 65 76 65 72 61 6c 20  6c 69 6e 65 73 20 22 22  |several lines &quot;&quot;|
00000030  73 6f 6d 65 20 55 6d 6c  61 75 74 73 20 69 6e 20  |some Umlauts in |
00000040  22 22 c3 bc c3 b6 c3 a4  22 22 20 20 74 6f 20 73  |&quot;&quot;......&quot;&quot;  to s|
00000050  68 6f 77 22 22 0d 0a 65  73 63 61 70 69 6e 67 22  |how&quot;&quot;..escaping&quot;|
00000060  22 0d 0a 6c 61 73 74 20  6c 69 6e 65 22 0d 0a 45  |&quot;..last line&quot;..E|
00000070  36 30 30 3b 22 6f 6e 63  65 0d 0a 61 67 61 69 6e  |600;&quot;once..again|
00000080  22 0d 0a                                          |&quot;..|
00000083
</code></pre>
<p>The error occurs when I add the <code>FIELDS</code> related keywords <code>FIELDS OPTIONALLY ENCLOSED BY '&quot;' ESCAPED BY '&quot;'</code>. Intention is to have <code>VARCHAR</code>s either without <code>&quot;</code> and <code>VARCHAR</code>s framed in <code>&quot;</code>. But the outer framing <code>&quot;</code>s should not appear in the data.</p>
",0,1,1,2025-10-05T14:57:29+00:00,1,107,True
79784467,12892937,,sql,PostgreSQL correlated query ignore WHERE clause,"<p>For each <code>customer_id</code>, I want to find the first delivery order. For each row of <code>Delivery</code>, I compare <code>order_date</code> with the smallest <code>order_date</code> of that <code>customer_id</code>.</p>
<p>Why does the <code>SELECT</code> statement below only return the row that contains the smallest <code>order_date</code>?</p>
<p><a href=""https://onecompiler.com/postgresql/43ywzu7fb"" rel=""nofollow noreferrer"">Code.</a></p>
<pre><code>CREATE TABLE IF NOT EXISTS Delivery (
  delivery_id int,
  customer_id int,
  order_date date,
  customer_pref_delivery_date date
);

TRUNCATE TABLE Delivery;

INSERT INTO Delivery (delivery_id, customer_id, order_date, customer_pref_delivery_date)
VALUES (1, 1, '2019-08-01', '2019-08-02');
INSERT INTO Delivery VALUES (2, 2, '2019-08-02', '2019-08-02');
INSERT INTO Delivery VALUES (3, 1, '2019-08-11', '2019-08-12');
INSERT INTO Delivery VALUES (4, 3, '2019-08-24', '2019-08-24');
INSERT INTO Delivery VALUES (5, 3, '2019-08-21', '2019-08-22');
INSERT INTO Delivery VALUES (6, 2, '2019-08-11', '2019-08-13');
INSERT INTO Delivery VALUES (7, 4, '2019-08-09', '2019-08-09');

-- Write your PostgreSQL query statement below
WITH temp AS (
    SELECT
        d.delivery_id,
        d.customer_id,
        d.order_date,
        d.customer_pref_delivery_date
    FROM Delivery d
    WHERE order_date = (SELECT MIN(order_date) FROM Delivery D
                        WHERE d.customer_id = D.customer_id)
)
select * from temp;
</code></pre>
<p>Input:</p>
<pre><code>Delivery table:
+-------------+-------------+------------+-----------------------------+
| delivery_id | customer_id | order_date | customer_pref_delivery_date |
+-------------+-------------+------------+-----------------------------+
| 1           | 1           | 2019-08-01 | 2019-08-02                  |
| 2           | 2           | 2019-08-02 | 2019-08-02                  |
| 3           | 1           | 2019-08-11 | 2019-08-12                  |
| 4           | 3           | 2019-08-24 | 2019-08-24                  |
| 5           | 3           | 2019-08-21 | 2019-08-22                  |
| 6           | 2           | 2019-08-11 | 2019-08-13                  |
| 7           | 4           | 2019-08-09 | 2019-08-09                  |
+-------------+-------------+------------+-----------------------------+
</code></pre>
<p>Output:</p>
<pre><code> delivery_id | customer_id | order_date | customer_pref_delivery_date 
-------------+-------------+------------+-----------------------------
           1 |           1 | 2019-08-01 | 2019-08-02
(1 row)
</code></pre>
",2,2,0,2025-10-07T10:53:11+00:00,1,83,True
79784828,2510563,,sql,Passing an existing GUID as a variable value when creating NEW SQL record via Access,"<p>I have SQL database with the following tables. The table structure is condensed for this question.</p>
<p><code>tbl.Vendor</code></p>
<pre><code>VendorId (guid)  
AccountNumber
Note
</code></pre>
<p><code>tbl.VendorAlert</code></p>
<pre><code>AlertId (guid)
VendorId (guid)
AlertMsg
</code></pre>
<p><code>tbl.VendorAlert</code> has its uniqued Id (<code>AlertId</code>), but also contains unique id <code>VendorID</code> which links the record to <code>tbl.Vendor</code>.</p>
<p>The existing SQL tables <code>tbl.Vendor</code> and <code>tbl.VendorAlert</code> are linked tables within Access.</p>
<p>Access data entry will NOT be ADDING vendor records, only editing the value of the <code>Note</code> column for existing SQL <code>tbl.Vendor</code> rows. As such, I have been able to create a simple <code>EventProcedure</code> as follows to set the value for <code>tbl.Vendor.Note</code>:</p>
<pre><code>Private Sub btnCommand16_Click()

 Me.[Note] = &quot;ADBPT&quot;

 Me.Refresh
End Sub
</code></pre>
<p>The problem I have is when I also need to add a NEW record into the <code>tbl.VendorAlert</code>. I thought of creating a pass-through query as follows:</p>
<pre><code>INSERT INTO VendorAlert (AlertId, VendorId, AlertMsg)
VALUES (NewID(), 'guid ref problem', 'alert message text')
</code></pre>
<p>How do I specify the guid value for 'guid ref problem? The code needs to pull the existing VendorId guid from the Vendor table (ie the same &quot;Me&quot; value as referenced in the prior code.</p>
<p>Any help will be greatly appreciated.</p>
",0,0,0,2025-10-07T18:24:20+00:00,2,71,True
79785140,505306,,sql,Using `with` to find the instructors with the highest salary,"<p>I am learning SQL and was testing the <code>with</code> expression syntax when I ran into this error for my SQL query.  I want to get the ids and names of all instructors from a university databse with the maximum salary. This database is from the book Database System Concepts by Silberschatz et al.</p>
<p>The schema of the instructor relation is <code>instructor(ID, name, dept_name, salary)</code> where the last attribute salary is a numeric type, MySQL Workbench is complaining that</p>
<blockquote>
<p>Error Code: 1054. Unknown column 'max_salary.value' in 'where clause'</p>
</blockquote>
<p>Here is my code</p>
<pre><code>-- MySQL workbench complains with the above error!
with max_salary(value) as 
             (select max(salary) 
              from instructor)
select ID, name
from instructor
where max_salary.value = instructor.salary;
</code></pre>
<p>I know how to rewrite this without the <code>with</code> clause, as follows
but I was having trouble with the syntax of <code>with</code> for this particular problem.</p>
<pre><code>-- This works!
select ID, name
from instructor
where instructor.salary = (select max(salary) from 
                           instructor )

</code></pre>
",0,1,1,2025-10-08T06:38:56+00:00,1,109,True
79785552,2417205,,sql,C# NET Core LINQ To Entities doubling subquery for sorting,"<p>I am working with EF Core 6.0 Web APi.</p>
<p>Npgsql.EntityFrameworkCore.PostgreSQL 6.0.29</p>
<p>PostgreSQL 16.3</p>
<p>I have database (Postgres) with musical entities and lessons. Lessons related to musical entities by navigation property. I want to sort musical entities by latest lesson date. I have projection as domain model.</p>
<pre><code>new DomainMusicalEntity
{
    Id = entity.Id,
    OwnerId = entity.OwnerId,
    CreationDate = entity.CreationDate,
    Description = entity.Description,
    Title = entity.Title,
    IsPublic = entity.IsPublic,
    IsCompleted = entity.IsCompleted,
    YoutubeLink = entity.YoutubeLink,
    Author = entity.Author,
    UniqueId = entity.UniqueId,
    OriginId = entity.OriginId,
    DefaultFile = entity.MusicalEntityFiles.FirstOrDefault(x =&gt; x.IsDefault == true),
    LastLessonDate = entity.MusicalEntitiesToLessons.Select(x=&gt;x.Lesson.Date).FirstOrDefault(),
}
</code></pre>
<p>Have sort in SQL by LINQ to Entities.</p>
<pre><code>MusicalEntitySort.LastLessonDate =&gt; filter.SortDirection == SortDirection.Desc
     ? query.OrderByDescending(x =&gt; x.LastLessonDate)
     : query.OrderBy(x =&gt; x.LastLessonDate),
</code></pre>
<p>In SQL I have this query(postgresql):</p>
<pre><code>SELECT t.id, t.owner_id, t.creation_date, t.description, t.title, t.is_public, t.is_completed, t.youtube_link, t.author, t.unique_id, t.origin_id, (
          SELECT l0.date
          FROM musical_entities_to_lessons AS m2
          INNER JOIN lessons AS l0 ON m2.lesson_id = l0.id
          WHERE t.id = m2.musical_entity_id
          LIMIT 1), t0.id, t0.file_path, t0.is_default, t0.musical_entity_id, t0.original_name
      FROM (
          SELECT m.id, m.author, m.creation_date, m.description, m.is_completed, m.is_public, m.origin_id, m.owner_id, m.title, m.unique_id, m.youtube_link
          FROM musical_entities AS m
          WHERE m.owner_id = @__filter_OwnerId_0
          LIMIT @__p_2 OFFSET @__p_1
      ) AS t
      LEFT JOIN (
          SELECT t1.id, t1.file_path, t1.is_default, t1.musical_entity_id, t1.original_name
          FROM (
              SELECT m1.id, m1.file_path, m1.is_default, m1.musical_entity_id, m1.original_name, ROW_NUMBER() OVER(PARTITION BY m1.musical_entity_id ORDER BY m1.id) AS row
              FROM musical_entity_files AS m1
              WHERE m1.is_default = TRUE
          ) AS t1
          WHERE t1.row &lt;= 1
      ) AS t0 ON t.id = t0.musical_entity_id
      ORDER BY (
          SELECT l.date
          FROM musical_entities_to_lessons AS m0
          INNER JOIN lessons AS l ON m0.lesson_id = l.id
          WHERE t.id = m0.musical_entity_id
          LIMIT 1) DESC
</code></pre>
<p>I see that I have a doubling in SELECT and in ORDER BY.</p>
<p>How can I get rid of that? I tried a lot of ways, but didn't succeed.</p>
<p>It example is simple. In my business logic that I want to implement, I want to take the maximum of these dates and filter by concrete lesson statuses. But this simple example gives the same result as sql doubling problem. In my use case I tried different prodections, but I always have double and triple queries like in the example above.</p>
<p>I know about views and subquery. But I have this architecture with a generic repository like this.</p>
<pre><code>public Task&lt;TDomainModel[]&gt; FindDomainAsync(TFilter? filter, CancellationToken cancellationToken = default)
{
    IQueryable&lt;TEntity&gt; query = DbSet;

    query = ApplyFilterInternal(query, filter);
    query = GetPagedQuery(query, filter);
    query = query.AsNoTracking();

    var domainQuery = SelectFieldsProjection(query, filter!);

    return ApplySort(domainQuery, filter).ToArrayAsync(cancellationToken);
}
</code></pre>
",1,1,0,2025-10-08T14:13:40+00:00,1,111,True
79785709,24286848,,sql,Copy parts of different PostgreSQL tables to a single file and restore them,"<p>Let’s say I have two different schemas with 2 tables each:</p>
<ul>
<li><code>schema_one</code> with tables <code>this</code>, <code>that</code></li>
<li><code>schema_two</code> with tables <code>here</code>, <code>there</code></li>
</ul>
<p>The tables have the following structure:</p>
<ul>
<li><p><code>schema_one.this</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Apple</td>
</tr>
<tr>
<td>2</td>
<td>Orange</td>
</tr>
</tbody>
</table></div>
</li>
<li><p><code>schema_one.that</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>location</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>new_jersey</td>
<td>Apple</td>
</tr>
<tr>
<td>alabama</td>
<td>Orange</td>
</tr>
</tbody>
</table></div>
</li>
<li><p><code>schema_two.here</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Banana</td>
</tr>
<tr>
<td>2</td>
<td>Pear</td>
</tr>
</tbody>
</table></div>
</li>
<li><p><code>schema_two.there</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>favourite_colour</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Red</td>
<td>Grape</td>
</tr>
<tr>
<td>2</td>
<td>Blue</td>
<td>Watermelon</td>
</tr>
</tbody>
</table></div>
</li>
</ul>
<p>I want to copy data from all 4 tables, but to save on number of files I’m trying to do something like this:</p>
<pre class=""lang-bash prettyprint-override""><code>psql -f dump_all_schemas.sql &gt; output.sql
</code></pre>
<p><code>dump_all_schemas.sql</code> looks like this:</p>
<pre class=""lang-sql prettyprint-override""><code>COPY (SELECT * FROM schema_one.this WHERE favourite_fruit = ‘Apple’) TO STDOUT;
COPY (SELECT * FROM schema_one.that WHERE id=‘1’) TO STDOUT;
COPY (SELECT * FROM schema_two.here WHERE favourite_fruit = ‘Banana’) TO STDOUT;
COPY (SELECT * FROM schema_two.there WHERE favourite_colour = ‘Red’) TO STDOUT;
</code></pre>
<p>Is it possible to do the reverse of this restore the data to the correct tables? If not, what might be a good solution to this?</p>
<p>I have tried searching around but I can’t seem to discern one way or the other if it’s even possible to do what I’m doing. I know I can get the data from into the file, but the reverse is causing a headache.</p>
",1,1,0,2025-10-08T17:16:19+00:00,1,113,True
79785853,15762880,,sql,SELECT statement on Join example,"<p>I'm working through an inner join example on <a href=""https://www.geeksforgeeks.org/sql/sql-join-set-1-inner-left-right-and-full-joins#example-of-inner-join/"" rel=""nofollow noreferrer"">Geeks For Geeks</a> I came across a SELECT statement I don't understand.</p>
<p>We have two tables, <code>Student</code> and <code>StudentCourse</code> we are joining on a common column ROLL_NO.</p>
<pre><code>SELECT StudentCourse.COURSE_ID, Student.NAME, Student.AGE FROM Student
INNER JOIN StudentCourse
ON Student.ROLL_NO = StudentCourse.ROLL_NO;
</code></pre>
<p>My question is why the SELECT statement specifies one column from <code>StudentCourse</code> and two from <code>Student</code>, but then only says <code>FROM Student</code>. Shouldn't it also need to say <code>FROM StudentCourse</code> to get access to those columns? Why is that in scope?</p>
",1,2,1,2025-10-08T20:45:07+00:00,2,85,True
79785960,1346705,Czech Republic,sql,ASP.NET Core Web API + EF: how to convert the list of entities to queryable?,"<p>I need to rewrite the code for getting and filtering the list of products. The older version used directly the SQL table <code>Products</code>, and filtered the result by code or name of a product, category, etc, like this:</p>
<pre class=""lang-cs prettyprint-override""><code>public async Task&lt;List&lt;Product&gt;&gt; GetAllAsync(
    string? filterProd = null, string? filterKat = null,
    string? sortBy = null, bool isAscending = true,
    int pageNumber = 1, int pageSize = 10)
{
    var products = dbContext.Products.AsQueryable();

    // Filtering by part of the code or part of the name.
    if (!string.IsNullOrWhiteSpace(filterProd))
    {
        // filterProd can contain more words separated by spaces.
        var lst = filterProd.Split(' ');

        foreach (string s in lst)
        {
            products = products.Where(x =&gt; x.Nazev.Contains(s)
                                        || x.KodZbozi.Contains(s));
        }
    }

    // Similarly filtering by other parameters, ordering...
    // and finally getting the range of items for the page.
    var skipResults = (pageNumber - 1) * pageSize;

    return await products.Skip(skipResults).Take(pageSize).ToListAsync();
}
</code></pre>
<p>The new endpoint should return only the subset of products for currently logged customer, but the more user-dependent values are added -- like special prices for the customer. The API can call the <code>usp_products @login=@login</code> that does that. The functionality is more complex (not simple JOIN or the like that); so, the call to the SQL Server stored procedure cannot be replaced by working directly with EF and the tables.</p>
<pre class=""lang-cs prettyprint-override""><code>public async Task&lt;List&lt;Product2&gt;&gt; GetAll2Async(
    string? filterProd = null, string? filterKat = null,
    string? sortBy = null, bool isAscending = true,
    int pageNumber = 1, int pageSize = 10)
{
    string xlogin = ...get the user login here...;

    SqlParameter login = new SqlParameter(&quot;@login&quot;, xlogin);
    var productsLst = await dbContext.Products2
        .FromSql($&quot;EXEC usp_products @login={login}&quot;)
        .ToListAsync();

    var products = productsLst.AsQueryable();

    // Filtering by part of the code or part of the name.
    if (!string.IsNullOrWhiteSpace(filterProd))
    {
        // filterProd can contain more words separated by spaces.
        var lst = filterProd.Split(' ');

        foreach (string s in lst)
        {
            products = products.Where(x =&gt; x.Nazev.Contains(s)
                                        || x.KodZbozi.Contains(s));
        }
    }

    // Similarly filtering by other parameters, ordering...
    // and finally getting the range of items for the page.
    var skipResults = (pageNumber - 1) * pageSize;

    // Had to remove await and replace the .ToListAsync()
    return products.Skip(skipResults).Take(pageSize).ToList();
}
</code></pre>
<p>Is there a better or more usual way to implement it?</p>
",0,0,0,2025-10-09T01:02:24+00:00,2,83,True
79786226,9256914,,sql,Does source length of SQL functions matter,"<p>First, to define what I'm talking about, the &quot;length&quot; of a source is how many characters it has. Having more characters allows improving readability by using more descriptive function and variable names, as well as indentation.</p>
<p>In compiled languages, the length of the source's text does not matter, but in interpreted languages (which AFAIK SQL is) it can. Does that also extend to stored procedures/functions?</p>
<p>In other words, do I need to try to minimize something like this:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE FUNCTION dbo.Add_Highest_Subjects
    (@studentScoreInMath INT,
     @studentScoreInBiology INT,
     @studentScoreInLiterature INT)
RETURNS INT
AS
BEGIN
    DECLARE @bestOfTwo INT

    SET @bestOfTwo = 
        CASE
            WHEN @studentScoreInMath &gt; @studentScoreInBiology 
                AND @studentScoreInBiology &gt; @studentScoreInLiterature
                THEN @studentScoreInMath + @studentScoreInBiology

            WHEN @studentScoreInMath &gt; @studentScoreInBiology 
                AND @studentScoreInBiology &lt; @studentScoreInLiterature
                THEN @studentScoreInMath + @studentScoreInLiterature

            ELSE @studentScoreInBiology + @studentScoreInLiterature
        END

    RETURN @bestOfTwo
END
</code></pre>
<p>into that:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE FUNCTION dbo.ahs(@m INT, @b INT, @l INT)
RETURNS INT
AS
BEGIN
    DECLARE @r INT

    SET @r = CASE 
                 WHEN @m &gt; @b AND @b &gt; @l THEN @m + @b 
                 WHEN @m &gt; @b AND @b &lt; @l THEN @m + @l 
                 ELSE @b + @l 
             END
    RETURN @r
END
</code></pre>
<p>Or is there no need?</p>
<p>PS: this is quite hard to search for, as I keep finding explanations of how the LEN function works, instead of the answer to my question.</p>
<p>PPS: I can't believe I have to say this: the function above is an example stripped down to essentials (variable names and indentations). I don't need advice on how to add two integers efficiently, but thank you.</p>
",0,1,1,2025-10-09T09:23:16+00:00,1,200,True
79786299,19640317,,sql,jOOQ removes SQL inside ignore comments during execution,"<p>I'm using jOOQ with PostgreSQL and want to execute a query with an EXPLAIN (ANALYZE, FORMAT JSON) prefix.</p>
<p>I set the parser settings to ignore comments:</p>
<pre><code>Settings settings = new Settings()
                .withParseDialect(SQLDialect.ORACLE)
                .withParseUnknownFunctions(ParseUnknownFunctions.IGNORE)
                .withTransformTableListsToAnsiJoin(true)
                .withTransformUnneededArithmeticExpressions(TransformUnneededArithmeticExpressions.ALWAYS)
                .withTransformRownum(Transformation.ALWAYS)
                .withParamCastMode(ParamCastMode.DEFAULT)
                .withRenderOptionalAsKeywordForFieldAliases(RenderOptionalKeyword.ON)
                .withRenderOptionalAsKeywordForTableAliases(RenderOptionalKeyword.ON)
                .withRenderQuotedNames(RenderQuotedNames.EXPLICIT_DEFAULT_UNQUOTED)
                .withRenderNameCase(RenderNameCase.UPPER)
                .withRenderCoalesceToEmptyStringInConcat(true)
                .withBatchSize(DbConnectionHelper.MAX_BATCH_SIZE)
                .withParseIgnoreComments(true)
                ;
        return settings;
</code></pre>
<p>Then I delimit the EXPLAIN clause like this:</p>
<pre><code>String explainSql = &quot;&quot;&quot;
    /* [jooq ignore start] */
    EXPLAIN (ANALYZE, FORMAT JSON)
    /* [jooq ignore stop] */
&quot;&quot;&quot; + sql;

try (Statement stmt = conn.createStatement();
     ResultSet rs = stmt.executeQuery(explainSql)) {
    // process result
}
</code></pre>
<p><strong>Issue:</strong></p>
<p>jOOQ now ignores the EXPLAIN clause during parsing as expected, but it also removes it from the executed SQL.
So, the executed query no longer contains EXPLAIN (ANALYZE, FORMAT JSON) and fails to produce the plan.</p>
<p><strong>I'm using:</strong></p>
<ul>
<li>jOOQ version: 3.19.6</li>
<li>PostgreSQL 14</li>
<li>JDBC (not via DSL fetch)</li>
<li>Java 11</li>
<li>Database source dialect: Oracle</li>
<li>Destination dialect: PostgreSQL</li>
</ul>
<p><strong>Question:</strong></p>
<p>Is there a way to make jOOQ <strong>ignore a part of SQL during parsing</strong> but <strong>keep it in the actual execution</strong>?</p>
",2,2,0,2025-10-09T10:34:14+00:00,1,109,True
79786921,6410470,,sql,Get list of files from external snowflake stage based on where condition on timestamp column,"<p>I am trying to get the files that arrived on the snowflake external stage after that have <code>MODIFIED_DATE &gt; MAX(BATCH_END_TIME)</code>.
I came up with this query:</p>
<pre><code>LIST @my_internal_stage;

SELECT count(DISTINCT $1) AS count_files
, SUM($2) AS size 
FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) 
WHERE to_timestamp_ntz(left(&quot;last_modified&quot;, len(&quot;last_modified&quot;) - 4) || ' ' || '00:00', 'DY, DD MON YYYY HH:MI:SS TZH:TZM') &gt; (
    select CONVERT_TIMEZONE('UTC',max(batch_end_time)) 
    from log_table)

GROUP  BY file_name ;
</code></pre>
<p>In the above query, my_internal_stage is the snowflake stage name.
last_modified is one of the metadata columns returned by the LIST query.
But this query returns files that are lesser than the <code>max(batch_end_time)</code> as well. The <code>&gt;</code> filter is not working as expected.
If I give a literal like this :
<code>'2025-10-23:48:15.453000000'</code> after the <code>&gt;</code> sign, it works as expected.
Can someone please help me to fix this query?</p>
",0,1,1,2025-10-10T00:13:26+00:00,1,99,False
79787003,31510249,,sql,Get hierarchical structure from a single table,"<p>Expected result is somewhat hierarchical structure from a single table in <code>griddb cloud</code> as shown in below table, where every row contains hourly data in a way that there is only one device entry per timestamp along with the sensors aggregated information:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>hour_start</th>
<th>device_id</th>
<th>total_samples</th>
<th>temperature_samples</th>
<th>humidity_samples</th>
<th>avg_temperature</th>
<th>avg_humidity</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-10-10T10:00:00</td>
<td>DEV_001</td>
<td>100</td>
<td>70</td>
<td>30</td>
<td>40.56</td>
<td>20.46</td>
</tr>
<tr>
<td>2025-10-10T10:00:00</td>
<td>DEV_002</td>
<td>100</td>
<td>70</td>
<td>30</td>
<td>40.56</td>
<td>20.46</td>
</tr>
</tbody>
</table></div>
<p>There should not be multiple entries for same device in hourly range.</p>
<p>2025-10-10T10:00:00 - DEV_001</p>
<p>2025-10-10T10:00:00 - DEV_001</p>
<blockquote>
<p>hour_start: TIMESTAMP start (multiple entries per device)</p>
<p>device_id: STRING id (single entries)</p>
<p>total_samples: number of entries of the device_id in the time range</p>
<p>temperature_samples: number of entries of the temperature sensor in the time range</p>
<p>humidity_samples: number of entries of the humidity sensor in the time range</p>
<p>temperature_samples: avg value of the temperature sensor in the time range</p>
<p>humidity_samples: avg value of the humidity sensor in the time range</p>
</blockquote>
<p>sensor_id: can be <code>sensor_001</code>, <code>sensor_002</code>,<code>sensor_003</code></p>
<p>device_id: can be <code>device_001</code>, <code>device_002</code>,<code>device_003</code></p>
<pre><code>CREATE TABLE sensor_data (
    ts TIMESTAMP PRIMARY KEY,
    sensor_id STRING NOT NULL,
    device_id STRING NOT NULL,
    value     DOUBLE NOT NULL
) USING TIMESERIES WITH (
    expiration_type='PARTITION',
    expiration_time=90,
    expiration_time_unit='DAY',
) PARTITION BY RANGE (ts) EVERY (30, DAY);
</code></pre>
<pre><code>SELECT 
    TIMESTAMP_TRUNC(HOUR, ts) AS hour_start, device_id, COUNT(*) as total_samples,
    (SELECT COUNT(*) FROM sensor_data where sensor_id = 'temp_001') as temperature_samples,
    (SELECT COUNT(*) FROM sensor_data where sensor_id = 'hum_001') as humidity_samples,
    (SELECT AVG(value)   FROM sensor_data where sensor_id = 'temp_001') as avg_temperature,
    (SELECT AVG(value)   FROM sensor_data where sensor_id = 'hum_001') as avg_humidity,
FROM sensor_data WHERE ts &gt;= TIMESTAMP_ADD(DAY, NOW(), -1) AND ts &lt;= NOW()
GROUP BY RANGE (ts) EVERY (1, HOUR) FILL (NONE)
ORDER BY hour_start
</code></pre>
<p>Select query lacks &quot;one device entry per timestamp&quot; and throws error when using sub queries with range.</p>
<blockquote>
<p><strong>Subqueries in the SELECT clause with GROUP BY RANGE clause are not supported on executing query.</strong></p>
</blockquote>
<p>Question:</p>
<ul>
<li>How to get aggregated data per device/timestamp in hourly timestamp range without using subqueries as hierarchical structure?</li>
</ul>
",0,0,0,2025-10-10T05:00:40+00:00,1,78,False
79787584,12367620,,sql,Preserve a foreign key using a uniqueness constraint while dropping a primary constraint,"<p>I have a SQL Server database I mostly interact with using EF Core in a .NET project: I'll analogize my domain logic for simplicity.</p>
<p>I have a <code>Fruits</code> table holding general fruit data in columns <code>Weight</code>, <code>Volume</code>, <code>Tastiness</code>.  But, due to inheritance, I also have a variety of other tables for specific fruit data (<code>Grapes</code> with <code>Size</code>, <code>Squishiness</code>; <code>Bananas</code> with <code>Length</code>, <code>Girth</code>, <code>Curvature</code>; <code>Oranges</code> with <code>Acidity</code>, <code>Color</code>).</p>
<p>At the beginning of my project, I decided to use a natural PK for <code>Fruits</code> called <code>Name</code>. To connect fruit type tables to the <code>Fruits</code> table, I made the PK of those tables an FK to <code>Fruits.Name</code>.</p>
<p>However, it's not unusual for users to enter the wrong fruit name and not realize until after they've already entered all the fruit data, so I decided I should finally give them the ability to update <code>Fruits.Name</code>.  EF doesn't like this because once you change the PK it doesn't know what row it should be updating. This seems perfectly reasonable, so I decided I should change my PK to a surrogate.</p>
<p>My plan was to:</p>
<ol>
<li>Add an identity column</li>
<li>Add a uniqueness constraint to my <code>Name</code> column</li>
<li>Remove the primary constraint from <code>Name</code></li>
<li>Add the primary constraint to the identity column</li>
</ol>
<p>My hope was adding an explicit uniqueness constraint before removing the primary constraint would allow the foreign keys to be cool with the situation. Instead, I get an error</p>
<blockquote>
<p>The constraint 'PK_Fruit' is being referenced by table 'Grapes', foreign key constraint 'FK_Grapes_Fruits'</p>
</blockquote>
<p>My hypothesis is what I want to do can't be achieved without dropping the foreign keys and adding them back afterwards. The problem with this is there are a LOT of fruit types (way more than just <code>Grapes</code>, <code>Bananas</code>, and <code>Oranges</code>).</p>
<p>It makes sense to me the uniqueness constraint should be able to bypass the error.</p>
<p>How can I use a uniqueness constraint to bypass this error?</p>
",6,6,0,2025-10-10T17:24:27+00:00,3,223,True
79787796,31234800,,sql,How to perform asof join (aj) with strict inequality (&lt;) instead of default (≤),"<p>I am using DolphinDB version 3.00.0.6 and have two in-memory tables A and B.</p>
<p>I need the equivalent of a Non-Equi Join with strict inequality (A.date &gt; B.date) to find the latest record in B that is strictly before each record in A.</p>
<p>My Sample Data:</p>
<pre><code>A = table(
    2023.01.01 2023.01.02 2023.01.03 as date, 
    `AAPL`AAPL`AAPL as stock_id, 
    10.0 20.0 30.0 as valA
)
B = table(
    2023.01.01 2023.01.02 2023.01.04 as date,
    `AAPL`AAPL`AAPL as stock_id,
    100.0 200.0 400.0 as valB
)
</code></pre>
<p>I first tried asof join:</p>
<pre><code>select * from aj(A, B, `stock_id`date)

date       stock_id valA B_date     valB
---------- -------- ---- ---------- ----
2023.01.01 AAPL     10   2023.01.01 100 
2023.01.02 AAPL     20   2023.01.02 200 
2023.01.03 AAPL     30   2023.01.02 200 
</code></pre>
<p>But what I expect is:</p>
<pre><code>date       stock_id valA B_date     valB
---------- -------- ---- ---------- ----
2023.01.01 AAPL     10   
2023.01.02 AAPL     20   2023.01.01 100 
2023.01.03 AAPL     30   2023.01.02 200 
</code></pre>
<p>I also tried to use non-equi join syntax：</p>
<pre><code>select * from A join B on A.stock_id=B.stock_id and A.date &gt; B.date
</code></pre>
<p>But with an syntax error:</p>
<blockquote>
<p>Syntax Error: [line #14] A join filter can't involves columns in two or more tables.</p>
</blockquote>
<p>DolphinDB seems not support such syntax.</p>
<p>Is there a built-in way to modify aj for strict inequality? How can I achieve the equivalent of A.date &gt; B.date join condition in DolphinDB?</p>
",0,0,0,2025-10-11T04:01:28+00:00,1,86,True
79788189,1346705,Czech Republic,sql,How to wrap Azure SQL sp_execute_remote as a table function?,"<p>Having the SQL stored procedure <code>remote_usp_test</code> in the remote database, I can successfully call it using the <code>sp_execute_remote</code> from the local database like this:</p>
<pre><code>EXEC sp_execute_remote N'xxxDataSrc', N'EXEC remote_usp_test'
</code></pre>
<p>It returns the expected result of the SELECT command executed inside the <code>remote_usp_test</code> at the remote Azure SQL database.</p>
<p>I can store the resulting data in a table variable, like this:</p>
<pre class=""lang-sql prettyprint-override""><code>DECLARE @t TABLE (
    [Currency Code] [nvarchar](10) NOT NULL,
    [Starting Date] [datetime] NOT NULL,
    [Exchange Rate Amount] [decimal](38, 20) NOT NULL,
    [Relational Exch_ Rate Amount] [decimal](38, 20) NOT NULL
)

    INSERT INTO @t (
        [Currency Code], [Starting Date],
        [Exchange Rate Amount], [Relational Exch_ Rate Amount] 
    )
    EXEC sp_execute_remote N'xxxDataSrc', N'EXEC remote_usp_test'
</code></pre>
<p>I would like to wrap the <code>sp_execute_remote</code> call into a table function; so, it could be used as a table in other local SQL commands later.</p>
<p>However, the definition like below fails to be executed because of the error shown in the comment inside:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE FUNCTION dbo.remote_ufn_test (
    @cur nvarchar(10)
)
RETURNS @t TABLE (
    [Currency Code] [nvarchar](10) NOT NULL,
    [Starting Date] [datetime] NOT NULL,
    [Exchange Rate Amount] [decimal](38, 20) NOT NULL,
    [Relational Exch_ Rate Amount] [decimal](38, 20) NOT NULL
)
AS
BEGIN
    INSERT INTO @t (
        [Currency Code], [Starting Date],
        [Exchange Rate Amount], [Relational Exch_ Rate Amount] 
    )
    EXEC sp_execute_remote N'xxxDataSrc', N'EXEC remote_usp_test'

-- Msg 443, Level 16, State 14, Procedure remote_ufn_test, Line 21 [Batch Start Line 3]
-- Invalid use of a side-effecting operator 'INSERT EXEC' within a function.

    RETURN
END
</code></pre>
<p>Actually, the <em>Invalid use of a side-effecting operator 'INSERT EXEC' within a function</em> is not related specifically to the <code>sp_execute_remote</code>. It fails also when calling any local SQL stored procedure.</p>
<p>The question is: Is there <strong>any</strong> way to wrap the result of <code>sp_execute_remote</code> call by a table fuction?</p>
",-1,0,1,2025-10-11T19:19:53+00:00,0,53,False
79789105,31678548,,sql,How to reverse a DolphinDB table aggregated by group by + toArray back to its original form?,"<p>I have an in - memory DolphinDB table created as follows:</p>
<pre><code>ticker = `AAPL`IBM`IBM`AAPL`AMZN`AAPL`AMZN`IBM`AMZN
volume = 106 115 121 90 130 150 145 123 155;
t = table(ticker, volume);
t;
</code></pre>
<p>The output of the table t is:</p>
<pre><code>ticker  volume
-----   ----
AAPL    106
IBM     115
IBM     121
AAPL    90
AMZN    130
AAPL    150
AMZN    145
IBM     123
AMZN    155
</code></pre>
<p>I used the toArray function along with group by to aggregate the grouped data into array vectors in a single row, which makes it easier to view all data under each group:</p>
<pre><code>t1 = select toArray(volume) as volume_all from t group by ticker;
t1;
</code></pre>
<p>The output of the table t1 is:</p>
<pre><code>ticker   volume_all
------   ----------
AAPL     [106,90,150]
AMZN     [130,145,155]
IBM      [115,121,123]
</code></pre>
<p>Now, I want to reverse t1 back to the original table t. The reason for this requirement is that sometimes when I group by price, the time columns may become non - unique. To restore a unique time column, I first use the toArray approach.</p>
<p>I attempted the following method:</p>
<pre><code>select flatten(volume_all) as volume from t1
</code></pre>
<p>The output is:</p>
<pre><code>volume
-----
106
90
150
130
145
155
115
121
123
</code></pre>
<p>However, this only gives me the volumes without the corresponding tickers.</p>
<p>My questions are:</p>
<ol>
<li>Is there a built - in function or syntax in DolphinDB that can expand both the grouped keys (in this case, ticker) and their associated array elements (volume_all) simultaneously?</li>
<li>Does DolphinDB provide any documented patterns for reversing aggregation operations like this?
Any help or guidance would be greatly appreciated.</li>
</ol>
",1,1,0,2025-10-13T09:21:05+00:00,1,63,True
79789268,134713,"Hyderabad, India",sql,Allocate value to a constant column with a group of multiple rows,"<p>I have data in a table like this:</p>
<p><a href=""https://i.sstatic.net/5hhSFVHO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5hhSFVHO.png"" alt=""enter image description here"" /></a></p>
<p><code>columnA</code> here is constant value within the group of rows with <code>columnB</code> consisting of id's <code>1,2,3,4</code></p>
<p>I am trying to write a SQL query to allocate the value of the <code>columnC</code> until complete <code>columnA</code> is allocated in the order of rownum of <code>columnB</code>. For example <code>columnD</code> is the value I am allocating from <code>ColumnC</code>.</p>
<p>So the output should look like this:</p>
<p><a href=""https://i.sstatic.net/jtTO9OPF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jtTO9OPF.png"" alt=""enter image description here"" /></a></p>
<p>I could hardly get any ideas about how to do it.</p>
",0,2,2,2025-10-13T12:24:19+00:00,3,108,True
79789316,15755512,,sql,Normalize JSON array in Clickhouse,"<p>I have a Clickhouse table with following structure:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>json</th>
</tr>
</thead>
<tbody>
<tr>
<td>111</td>
<td>[{&quot;productId&quot;: &quot;718f4d00-210d-43f1-9c9c-e97733d38972&quot;, &quot;cost&quot;: &quot;170.00000&quot;}, {&quot;productId&quot;: &quot;df145ba0-ff68-4370-88e1-1c0ad246b1b7&quot;, &quot;cost&quot;: &quot;230.00000&quot;}]</td>
</tr>
<tr>
<td>112</td>
<td>[{&quot;productId&quot;: &quot;718f4d00-210d-43f1-9c9c-e97733d38972&quot;, &quot;cost&quot;: &quot;170.00000&quot;}]</td>
</tr>
<tr>
<td>113</td>
<td>[{&quot;productId&quot;: &quot;718f4d00-210d-43f1-9c9c-e97733d38972&quot;, &quot;cost&quot;: 170}, {&quot;productId&quot;: &quot;df145ba0-ff68-4370-88e1-1c0ad246b1b7&quot;, &quot;cost&quot;: 230}]</td>
</tr>
</tbody>
</table></div>
<p>I need to normalize JSON column keeping its row id.</p>
<p>Expected result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>productId</th>
<th>cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>111</td>
<td>718f4d00-210d-43f1-9c9c-e97733d38972</td>
<td>170</td>
</tr>
<tr>
<td>111</td>
<td>df145ba0-ff68-4370-88e1-1c0ad246b1b7</td>
<td>230</td>
</tr>
<tr>
<td>112</td>
<td>718f4d00-210d-43f1-9c9c-e97733d38972</td>
<td>170</td>
</tr>
<tr>
<td>113</td>
<td>718f4d00-210d-43f1-9c9c-e97733d38972</td>
<td>170</td>
</tr>
<tr>
<td>113</td>
<td>df145ba0-ff68-4370-88e1-1c0ad246b1b7</td>
<td>230</td>
</tr>
</tbody>
</table></div>
<p>I've tried basic JSON functions but they don't work with arrays like mine.</p>
<p>Is there a way to explode JSON row wise?</p>
",0,0,0,2025-10-13T13:27:29+00:00,1,79,True
79789474,4978814,,sql,SQL query produced by Entity Framework very slow,"<p>I have the following query being generated by EF:</p>
<pre><code>DECLARE @__term_0 VARCHAR(7) = 'woodrow'
DECLARE @__p_1 INT = 0
DECLARE @__p_2 INT = 15

SELECT [t].[Id], [t].[City], [t].[Contact], [t].[CreatedBy], [t].[CreatedOn], [t].[Deposit], [t].[Email], [t].[Fax], [t].[ModifiedBy], [t].[ModifiedOn], [t].[Name], [t].[NearbyCities], [t].[Notes], [t].[PayMethod], [t].[Pets], [t].[Phone], [t].[StateId], [t].[Street], [t].[Taxes], [t].[Utilities], [t].[VacateNotice], [t].[Website], [t].[Zip], [h0].[Id], [h0].[Cost], [h0].[CreatedBy], [h0].[CreatedOn], [h0].[HousingId], [h0].[ModifiedBy], [h0].[ModifiedOn], [h0].[Notes], [t0].[Id], [t0].[AdminOverhead], [t0].[BillCallBackRate], [t0].[BillHolidayRate], [t0].[BillOnCallRate], [t0].[BillOvertimeRate], [t0].[BillRegularRate], [t0].[BlsNeeded], [t0].[CallBackRule], [t0].[CanceledCandidateDetail], [t0].[CanceledCompanyDetail], [t0].[CanceledDate], [t0].[CanceledReason], [t0].[CandidateId], [t0].[ComplianceInfo], [t0].[ComplianceUserId], [t0].[ContractNotes], [t0].[CreatedBy], [t0].[CreatedOn], [t0].[DrugscreenZip], [t0].[EmailInvoice], [t0].[EndDate], [t0].[Eval1], [t0].[Eval2], [t0].[Eval3], [t0].[Eval4], [t0].[Eval5], [t0].[EvalComments], [t0].[EvalDate], [t0].[EvalRecommend], [t0].[FormType], [t0].[HolidayRule], [t0].[HourlyGuarantee], [t0].[HoursPerWeek], [t0].[HousingCost], [t0].[HousingId], [t0].[HousingPaid], [t0].[HousingPayOutDescription], [t0].[IsCancellationComplete], [t0].[IsExtension], [t0].[IsHousingAllowance], [t0].[IsOnboardComplete], [t0].[IsPaySplitPercentage], [t0].[IsPending], [t0].[JobId], [t0].[ModifiedBy], [t0].[ModifiedOn], [t0].[NeverWorked], [t0].[NumberOfWeeks], [t0].[OverHeadOverride], [t0].[OvertimeRule], [t0].[PONumber], [t0].[RtoRequestForContract], [t0].[SecondaryPaySplit], [t0].[ServiceNotes], [t0].[StartDate], [t0].[Status], [t0].[TaxOverride], [t0].[ThirdPaySplit], [t0].[TimesheetNotes], [t0].[VoidOverHeadFee], [t0].[VoidTaxOverride], [t0].[WageCallBackRate], [t0].[WageOnCallRate], [t0].[WageOvertimeRate], [t0].[WageRegularRate], [t0].[Id0], [t0].[ActiveStateLicenseId], [t0].[AvailableDate], [t0].[City], [t0].[CreatedBy0], [t0].[CreatedOn0], [t0].[DateOfBirth], [t0].[Email], [t0].[EmployeeId], [t0].[FirstName], [t0].[HasWorked], [t0].[HomeStateId], [t0].[HoursPreference], [t0].[HousingPreferences], [t0].[LastName], [t0].[Latitude], [t0].[Longitude], [t0].[MiddleName], [t0].[ModifiedBy0], [t0].[ModifiedOn0], [t0].[Pets], [t0].[Phone], [t0].[ReferredBy], [t0].[RepId], [t0].[ReqTimeOffDesc], [t0].[ReqTimeOffEndDate], [t0].[ReqTimeOffStartDate], [t0].[SecondaryPhone], [t0].[ShiftPreference], [t0].[Smoker], [t0].[Source], [t0].[Ssn], [t0].[StateId], [t0].[StatePreference], [t0].[Status0], [t0].[StatusId], [t0].[Street], [t0].[SubmissionBio], [t0].[TechBio], [t0].[TravelType], [t0].[TravelsWithFamily], [t0].[UnsubscribeEmail], [t0].[YearsExperience], [t0].[Zip], [t0].[Id1], [t0].[AppUserId], [t0].[CandidateCommission], [t0].[CompanyCommission], [t0].[CreatedBy1], [t0].[CreatedOn1], [t0].[ModifiedBy1], [t0].[ModifiedOn1], [t0].[Name], [t0].[Id2], [t0].[AgreementContactId], [t0].[BillCallBackRate0], [t0].[BillHolidayRate0], [t0].[BillOnCallRate0], [t0].[BillOvertimeRate0], [t0].[BillRegularRate0], [t0].[CompanyId], [t0].[CreatedBy2], [t0].[CreatedOn2], [t0].[EmailSenderId], [t0].[EmailSentDate], [t0].[EndDate0], [t0].[Equipment], [t0].[HoursPerWeek0], [t0].[IsCertRequired], [t0].[LocalDistance], [t0].[LocalOnly], [t0].[ModalityId], [t0].[ModifiedBy2], [t0].[ModifiedOn2], [t0].[Notes], [t0].[NumberOfPositions], [t0].[NumberOfWeeks0], [t0].[PositionType], [t0].[RepAssignedId], [t0].[RepId0], [t0].[Requirements], [t0].[Shift], [t0].[SignerContactId], [t0].[StartDate0], [t0].[Status1], [t0].[UnitOrDepartment], [t0].[VMSCustomDiscountId], [t0].[VendorManagerId], [t0].[Id3], [t0].[AcuteCare], [t0].[BedSize], [t0].[BillingNotes], [t0].[City0], [t0].[CompanySystemId], [t0].[CompanyType], [t0].[CreatedBy3], [t0].[CreatedOn3], [t0].[CredentialNotes], [t0].[DailyTimesheetInterval], [t0].[FinanceChargeNotes], [t0].[FinanceChargeRate], [t0].[JobNotes], [t0].[Latitude0], [t0].[LocalDistance0], [t0].[LocalOnly0], [t0].[Longitude0], [t0].[ModifiedBy3], [t0].[ModifiedOn3], [t0].[Name0], [t0].[PaymentTerms], [t0].[Phone0], [t0].[RepId1], [t0].[StateId0], [t0].[Street0], [t0].[VendorManagerId0], [t0].[Website], [t0].[Zip0]
FROM (
    SELECT [h].[Id], [h].[City], [h].[Contact], [h].[CreatedBy], [h].[CreatedOn], [h].[Deposit], [h].[Email], [h].[Fax], [h].[ModifiedBy], [h].[ModifiedOn], [h].[Name], [h].[NearbyCities], [h].[Notes], [h].[PayMethod], [h].[Pets], [h].[Phone], [h].[StateId], [h].[Street], [h].[Taxes], [h].[Utilities], [h].[VacateNotice], [h].[Website], [h].[Zip]
    FROM [Housings] AS [h]
    WHERE (((((@__term_0 = N'') OR (CHARINDEX(@__term_0, [h].[Name]) &gt; 0)) OR ((@__term_0 = N'') OR (CHARINDEX(@__term_0, [h].[City]) &gt; 0))) OR ((@__term_0 = N'') OR (CHARINDEX(@__term_0, [h].[NearbyCities]) &gt; 0))) OR EXISTS (
        SELECT 1
        FROM [Placements] AS [p]
        INNER JOIN [Candidates] AS [c] ON [p].[CandidateId] = [c].[Id]
        INNER JOIN [Jobs] AS [j] ON [p].[JobId] = [j].[Id]
        INNER JOIN [Companies] AS [c0] ON [j].[CompanyId] = [c0].[Id]
        INNER JOIN [Reps] AS [r] ON [c].[RepId] = [r].[Id]
        WHERE ([h].[Id] = [p].[HousingId]) AND (((((@__term_0 = N'') OR (CHARINDEX(@__term_0, [c].[FirstName]) &gt; 0)) OR ((@__term_0 = N'') OR (CHARINDEX(@__term_0, [c].[LastName]) &gt; 0))) OR ((@__term_0 = N'') OR (CHARINDEX(@__term_0, [c0].[Name]) &gt; 0))) OR ((@__term_0 = N'') OR (CHARINDEX(@__term_0, [r].[Name]) &gt; 0))))) OR ((@__term_0 = N'') OR (CHARINDEX(@__term_0, REPLACE(REPLACE(REPLACE(REPLACE([h].[Phone], N' ', N''), N'(', N''), N')', N''), N'-', N'')) &gt; 0))
    ORDER BY [h].[CreatedBy] DESC, [h].[StateId], [h].[City], [h].[Name]
    OFFSET @__p_1 ROWS FETCH NEXT @__p_2 ROWS ONLY
) AS [t]
LEFT JOIN [HousingPricingTiers] AS [h0] ON [t].[Id] = [h0].[HousingId]
LEFT JOIN (
    SELECT [p0].[Id], [p0].[AdminOverhead], [p0].[BillCallBackRate], [p0].[BillHolidayRate], [p0].[BillOnCallRate], [p0].[BillOvertimeRate], [p0].[BillRegularRate], [p0].[BlsNeeded], [p0].[CallBackRule], [p0].[CanceledCandidateDetail], [p0].[CanceledCompanyDetail], [p0].[CanceledDate], [p0].[CanceledReason], [p0].[CandidateId], [p0].[ComplianceInfo], [p0].[ComplianceUserId], [p0].[ContractNotes], [p0].[CreatedBy], [p0].[CreatedOn], [p0].[DrugscreenZip], [p0].[EmailInvoice], [p0].[EndDate], [p0].[Eval1], [p0].[Eval2], [p0].[Eval3], [p0].[Eval4], [p0].[Eval5], [p0].[EvalComments], [p0].[EvalDate], [p0].[EvalRecommend], [p0].[FormType], [p0].[HolidayRule], [p0].[HourlyGuarantee], [p0].[HoursPerWeek], [p0].[HousingCost], [p0].[HousingId], [p0].[HousingPaid], [p0].[HousingPayOutDescription], [p0].[IsCancellationComplete], [p0].[IsExtension], [p0].[IsHousingAllowance], [p0].[IsOnboardComplete], [p0].[IsPaySplitPercentage], [p0].[IsPending], [p0].[JobId], [p0].[ModifiedBy], [p0].[ModifiedOn], [p0].[NeverWorked], [p0].[NumberOfWeeks], [p0].[OverHeadOverride], [p0].[OvertimeRule], [p0].[PONumber], [p0].[RtoRequestForContract], [p0].[SecondaryPaySplit], [p0].[ServiceNotes], [p0].[StartDate], [p0].[Status], [p0].[TaxOverride], [p0].[ThirdPaySplit], [p0].[TimesheetNotes], [p0].[VoidOverHeadFee], [p0].[VoidTaxOverride], [p0].[WageCallBackRate], [p0].[WageOnCallRate], [p0].[WageOvertimeRate], [p0].[WageRegularRate], [c1].[Id] AS [Id0], [c1].[ActiveStateLicenseId], [c1].[AvailableDate], [c1].[City], [c1].[CreatedBy] AS [CreatedBy0], [c1].[CreatedOn] AS [CreatedOn0], [c1].[DateOfBirth], [c1].[Email], [c1].[EmployeeId], [c1].[FirstName], [c1].[HasWorked], [c1].[HomeStateId], [c1].[HoursPreference], [c1].[HousingPreferences], [c1].[LastName], [c1].[Latitude], [c1].[Longitude], [c1].[MiddleName], [c1].[ModifiedBy] AS [ModifiedBy0], [c1].[ModifiedOn] AS [ModifiedOn0], [c1].[Pets], [c1].[Phone], [c1].[ReferredBy], [c1].[RepId], [c1].[ReqTimeOffDesc], [c1].[ReqTimeOffEndDate], [c1].[ReqTimeOffStartDate], [c1].[SecondaryPhone], [c1].[ShiftPreference], [c1].[Smoker], [c1].[Source], [c1].[Ssn], [c1].[StateId], [c1].[StatePreference], [c1].[Status] AS [Status0], [c1].[StatusId], [c1].[Street], [c1].[SubmissionBio], [c1].[TechBio], [c1].[TravelType], [c1].[TravelsWithFamily], [c1].[UnsubscribeEmail], [c1].[YearsExperience], [c1].[Zip], [r0].[Id] AS [Id1], [r0].[AppUserId], [r0].[CandidateCommission], [r0].[CompanyCommission], [r0].[CreatedBy] AS [CreatedBy1], [r0].[CreatedOn] AS [CreatedOn1], [r0].[ModifiedBy] AS [ModifiedBy1], [r0].[ModifiedOn] AS [ModifiedOn1], [r0].[Name], [j0].[Id] AS [Id2], [j0].[AgreementContactId], [j0].[BillCallBackRate] AS [BillCallBackRate0], [j0].[BillHolidayRate] AS [BillHolidayRate0], [j0].[BillOnCallRate] AS [BillOnCallRate0], [j0].[BillOvertimeRate] AS [BillOvertimeRate0], [j0].[BillRegularRate] AS [BillRegularRate0], [j0].[CompanyId], [j0].[CreatedBy] AS [CreatedBy2], [j0].[CreatedOn] AS [CreatedOn2], [j0].[EmailSenderId], [j0].[EmailSentDate], [j0].[EndDate] AS [EndDate0], [j0].[Equipment], [j0].[HoursPerWeek] AS [HoursPerWeek0], [j0].[IsCertRequired], [j0].[LocalDistance], [j0].[LocalOnly], [j0].[ModalityId], [j0].[ModifiedBy] AS [ModifiedBy2], [j0].[ModifiedOn] AS [ModifiedOn2], [j0].[Notes], [j0].[NumberOfPositions], [j0].[NumberOfWeeks] AS [NumberOfWeeks0], [j0].[PositionType], [j0].[RepAssignedId], [j0].[RepId] AS [RepId0], [j0].[Requirements], [j0].[Shift], [j0].[SignerContactId], [j0].[StartDate] AS [StartDate0], [j0].[Status] AS [Status1], [j0].[UnitOrDepartment], [j0].[VMSCustomDiscountId], [j0].[VendorManagerId], [c2].[Id] AS [Id3], [c2].[AcuteCare], [c2].[BedSize], [c2].[BillingNotes], [c2].[City] AS [City0], [c2].[CompanySystemId], [c2].[CompanyType], [c2].[CreatedBy] AS [CreatedBy3], [c2].[CreatedOn] AS [CreatedOn3], [c2].[CredentialNotes], [c2].[DailyTimesheetInterval], [c2].[FinanceChargeNotes], [c2].[FinanceChargeRate], [c2].[JobNotes], [c2].[Latitude] AS [Latitude0], [c2].[LocalDistance] AS [LocalDistance0], [c2].[LocalOnly] AS [LocalOnly0], [c2].[Longitude] AS [Longitude0], [c2].[ModifiedBy] AS [ModifiedBy3], [c2].[ModifiedOn] AS [ModifiedOn3], [c2].[Name] AS [Name0], [c2].[PaymentTerms], [c2].[Phone] AS [Phone0], [c2].[RepId] AS [RepId1], [c2].[StateId] AS [StateId0], [c2].[Street] AS [Street0], [c2].[VendorManagerId] AS [VendorManagerId0], [c2].[Website], [c2].[Zip] AS [Zip0]
    FROM [Placements] AS [p0]
    INNER JOIN [Candidates] AS [c1] ON [p0].[CandidateId] = [c1].[Id]
    INNER JOIN [Reps] AS [r0] ON [c1].[RepId] = [r0].[Id]
    INNER JOIN [Jobs] AS [j0] ON [p0].[JobId] = [j0].[Id]
    INNER JOIN [Companies] AS [c2] ON [j0].[CompanyId] = [c2].[Id]
) AS [t0] ON [t].[Id] = [t0].[HousingId]
ORDER BY [t].[CreatedBy] DESC, [t].[StateId], [t].[City], [t].[Name], [t].[Id], [h0].[Id], [t0].[Id], [t0].[Id0], [t0].[Id1], [t0].[Id2], [t0].[Id3]
</code></pre>
<p>It takes about 40 seconds to run which is a lot longer than I need it to. I suspect I just need better indexing but I'm not sure how to analyze my data plan to get the proper insights.</p>
<p>Here's the XML for the data plan: <a href=""https://jumpshare.com/s/zkTdyVn2aYMXM5A8vJuF"" rel=""nofollow noreferrer"">https://jumpshare.com/s/zkTdyVn2aYMXM5A8vJuF</a>
(The XML is copied directly from Azure Data Studio. For some reason the XML wouldn't validate with Paste the Plan)</p>
<p>How can I improve this query's performance?</p>
",0,0,0,2025-10-13T16:51:15+00:00,1,157,True
79790339,6386632,"Glasgow, UK",sql,Unpivot / explode of JSON data in QuestDB,"<p>I have data in a <a href=""https://questdb.com/docs/"" rel=""nofollow noreferrer"">QuestDB</a> database  which is stored in JSON arrays, with one row per timestamp. These contain forecast data, with a forecast for the next <code>n</code> timestamps being produced at a given timestamp <code>t</code>.</p>
<p>e.g. I have rows like this (timestamps as integers for simplicity of illustration):</p>
<pre class=""lang-none prettyprint-override""><code>timestamp: 1, forecast_timestamps: &quot;[2,3]&quot;, forecasts: &quot;[3,4]&quot;
timestamp: 2, forecast_timestamps: &quot;[3,4]&quot;, forecasts: &quot;[5,6]&quot;
</code></pre>
<p>I want to convert this data to a long format, which is easier for display and analysis:</p>
<pre class=""lang-none prettyprint-override""><code>timestamp: 1, forecast: NULL, position: NULL
timestamp: 2, forecast: 3, position: 1
timestamp: 3, forecast: 4, position: 2
timestamp: 3, forecast: 5, position: 1
timestamp: 4, forecast: 6, position: 2
</code></pre>
<p>Variations of row output are fine for NULL handling etc. The forecast arrays are of variable  length, but generally the same within a batch of forecasts: i.e. n is generally constant within a batch, but I want to be able to change it on context, so ideally it should be treated as variable per row. I've left out the potentially necessary <a href=""https://questdb.com/docs/reference/function/json/"" rel=""nofollow noreferrer"">JSON_EXTRACT</a> functions for clarity.</p>
<p>Is there any way to do this? <strong>Is there a way to UNPIVOT data in QuestDB</strong> (stored as JSON or otherwise)? Or, otherwise to transform it as illustrated above.</p>
<p>There is apparently a <a href=""https://github.com/questdb/questdb/issues/997"" rel=""nofollow noreferrer"">PIVOT feature in the pipeline</a>. But I'm not sure if there's a similar UNPIVOT function. I think other database systems can do this (e.g. <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-ver17"" rel=""nofollow noreferrer"">MSSQL</a>, though MySQL seems not to support it, and <a href=""https://stackoverflow.com/questions/15184381/mysql-how-to-unpivot-columns-to-rows"">UNION ALL is suggested</a>.</p>
<p>QuestDB support has said an UNPIVOT feature is in the pipeline: I'm looking for alternatives in the meantime. I could possibly change the process that writes to the table in question, but that is neither easy nor convenient, hence why I'm looking for alternatives.</p>
",-4,1,5,2025-10-14T15:21:47+00:00,2,229,True
79790643,742402,,sql,How to remove optional prefixes and suffixes from string,"<p>Product codes may contain 5 different prefixes CK0, CK, C, 0, K0
and two character suffixes from AA to AZ: AA,AB,...,AZ</p>
<p>Sample codes:</p>
<pre><code>CK04721310AE
CK04721310AD
CK04721310AC
CK04721310AB
4721310AE
4721310AC
K04721310AE
04721310AE
C4721310
</code></pre>
<p>How to remove those this prefix and or suffix from code? Result should be <strong>4721310</strong> for all those codes (here with prefix and suffix in italics):</p>
<p><em>CK0</em><strong>4721310</strong><em>AE</em><br />
<em>CK0</em><strong>4721310</strong><em>AD</em><br />
<em>CK0</em><strong>4721310</strong><em>AC</em><br />
<em>CK0</em><strong>4721310</strong><em>AB</em><br />
<strong>4721310</strong><em>AE</em><br />
<strong>4721310</strong><em>AC</em><br />
<em>K0</em><strong>4721310</strong><em>AE</em><br />
<em>0</em><strong>4721310</strong><em>AE</em><br />
<em>C</em><strong>4721310</strong></p>
<p>Tried</p>
<pre><code>select substring( 'CK04721310AE' , '(?:CK0|CK|C|0|K0)?(.+)(A[A-Z])?' )
</code></pre>
<p>But this does not remove suffix for unknown reason.
It returns suffix also:</p>
<blockquote>
<p>4721310AE</p>
</blockquote>
<p>How to get code without suffix and prefix ?</p>
<p>Using PostgreSQL 15.2</p>
",-7,0,7,2025-10-14T21:37:58+00:00,1,176,True
79791075,742402,,sql,How to split row to mutiple rows based on quantity,"<p>Stock status table contains product codes and quantities. Same product code may be in multiple rows, no primary key:</p>
<pre><code>Product Quantity
Code1   2
Code2   1
Code1   3
Code3   3
</code></pre>
<p>How to split it to multiple rows based on quantity? Result should be</p>
<pre><code>Code1
Code1
Code2
Code1
Code1
Code1
Code3
Code3
Code3
</code></pre>
<p>Result can be unordered.
Using Postgresql 17</p>
",-4,0,4,2025-10-15T10:46:37+00:00,1,92,True
79791379,1346705,Czech Republic,sql,How to update by the other GROUPed table values effectively?,"<p>I have the tables <code>Invoices</code> (with <code>No_</code> key) and the related <code>InvoiceLines</code> (with <code>[Document No_]</code> key that bounds to their invoices). The subset of invoice numbers is inserted into the table variable (no problem here).</p>
<p>The next step is to update the table variable with the count of lines for the invoice, and with the maximum timestamp of the lines of the invoice. So far, I am updating the values as shown below:</p>
<pre class=""lang-sql prettyprint-override""><code>    DECLARE @inv TABLE (
        No_ nvarchar(20) PRIMARY KEY NOT NULL,
        timestamp_line binary(8),
        line_count int
    )

    -- @inv is filled by subset of invoice numbers (No_) here.

    UPDATE @inv 
       SET timestamp_line = (SELECT MAX(lin.timestamp) 
                               FROM InvoiceLines AS lin WITH (NOLOCK)
                              WHERE lin.[Document No_] = No_),
           line_count =     (SELECT COUNT(*) 
                               FROM InvoiceLines AS lin WITH (NOLOCK)
                              WHERE lin.[Document No_] = No_) 

</code></pre>
<p>It works. Anyway, is there a better way to get the aggregate values from the <code>InvoiceLines</code>? Is the aggregation of <code>InvoiceLines</code> here done twice, or is it optimized by the SQL engine to be done only once? Is there a way to get the <code>COUNT(*)</code> and the <code>MAX(timestamp)</code> in one command, and to use the values for the UPDATE?</p>
",0,0,0,2025-10-15T15:58:21+00:00,3,135,True
79791439,5378187,,sql,How to perform a time-sequenced (temporal) LEFT OUTER JOIN,"<p>TL;DR In the book <a href=""https://www2.cs.arizona.edu/%7Erts/tdbbook.pdf"" rel=""nofollow noreferrer"">Developing Time-Oriented Database Applications in SQL</a> by Richard T. Snodgrass, section 6.3.1 he demonstrates how to perform a sequenced inner join on a transaction-time state table, but does not discuss how to perform a sequenced left outer join.</p>
<h2>Initial setup</h2>
<p>Suppose the following events occur:</p>
<ul>
<li><strong>2025-01-01</strong> - User 0 (John) is created. Order 0 (for User 0) is created. Order 1 (for User 1) is created.</li>
<li><strong>2025-01-02</strong> - User 1 (Mary) is created.</li>
<li><strong>2025-01-03</strong> - User 0 (Jon) is renamed.</li>
<li><strong>2025-01-04</strong> - Order 0 is moved from User 0 (Jon) to User 1 (Mary).</li>
</ul>
<p>The resulting transaction-time state tables are these:</p>
<blockquote>
<p><em>Note: I have deliberately combined the history and current table, unlike the official SQL server temporal tables feature which has them separated. This is similar to querying temporal tables with FOR SYSTEM_TIME ALL when using the official temporal table feature in SQL Server 2016</em></p>
</blockquote>
<p><strong>Orders</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>UserId</th>
<th>ValidFrom</th>
<th>ValidTo</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>2025-01-01</td>
<td>2025-01-04</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>2025-01-04</td>
<td>9999-12-31</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2025-01-01</td>
<td>9999-12-31</td>
</tr>
</tbody>
</table></div>
<p><strong>Users</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>Name</th>
<th>ValidFrom</th>
<th>ValidTo</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>John</td>
<td>2025-01-01</td>
<td>2025-01-03</td>
</tr>
<tr>
<td>0</td>
<td>Jon</td>
<td>2025-01-03</td>
<td>9999-12-31</td>
</tr>
<tr>
<td>1</td>
<td>Mary</td>
<td>2025-01-02</td>
<td>9999-12-31</td>
</tr>
</tbody>
</table></div>
<p><strong>DDL/DML for Copy Paste</strong></p>
<pre><code>CREATE TABLE Orders (
    Id INT,
    UserId INT ,
    ValidFrom DATE,
    ValidTo DATE,
);

INSERT INTO Orders
VALUES (0, 0, '2025-01-01', '2025-01-04'),
       (0, 1, '2025-01-04', '9999-12-31'),
       (1, 1, '2025-01-01', '9999-12-31');

CREATE TABLE Users (
    Id INT,
    Name VARCHAR(50),
    ValidFrom DATE,
    ValidTo DATE
);

INSERT INTO Users
VALUES (0, 'John', '2025-01-01', '2025-01-03'),
       (0, 'Jon', '2025-01-03', '9999-12-31'),
       (1, 'Mary', '2025-01-02', '9999-12-31');
</code></pre>
<p>Now, I can use the following query to get all the current orders, and the associated user name:</p>
<pre><code>SELECT O.Id, U.Name 
FROM Orders O
LEFT JOIN Users U ON U.Id = O.UserId
WHERE O.ValidTo = '9999-12-31' 
  AND U.ValidTo = '9999-12-31'
</code></pre>
<p><strong>Results</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>O.Id</th>
<th>U.Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Mary</td>
</tr>
<tr>
<td>1</td>
<td>Mary</td>
</tr>
</tbody>
</table></div>
<h2>Question</h2>
<p>I want to see how the results of the above query have changed over time. What query should I write to get the results shown below?</p>
<h3>Desired Results</h3>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>O.Id</th>
<th>U.Name</th>
<th>ValidFrom</th>
<th>ValidTo</th>
<th>Note (for reference, not actually part of the query)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>John</td>
<td>2025-01-01</td>
<td>2025-01-03</td>
<td>User 0 + Order 0 are created</td>
</tr>
<tr>
<td>1</td>
<td>NULL</td>
<td>2025-01-01</td>
<td>2025-01-02</td>
<td>Order 1 is created</td>
</tr>
<tr>
<td>1</td>
<td>Mary</td>
<td>2025-01-02</td>
<td>9999-12-31</td>
<td>User 1 is created</td>
</tr>
<tr>
<td>0</td>
<td>Jon</td>
<td>2025-01-03</td>
<td>2025-01-04</td>
<td>User 0 is renamed</td>
</tr>
<tr>
<td>0</td>
<td>Mary</td>
<td>2025-01-04</td>
<td>9999-12-31</td>
<td>Order 0 is moved from User 0 to User 1</td>
</tr>
</tbody>
</table></div>
",5,5,0,2025-10-15T17:09:05+00:00,2,221,True
79791669,3163203,,sql,How to use the type of a referenced column as a VARIADIC argument type in a (Postgres) SQL function or procedure definition?,"<p><a href=""https://www.postgresql.org/docs/17/sql-createfunction.html"" rel=""nofollow noreferrer"">The PostgreSQL documentation for <code>argtype</code></a> in function and procedure definitions states:</p>
<blockquote>
<p>The type of a column is referenced by writing <code>table_name.column_name%TYPE</code>.</p>
</blockquote>
<p>This works fine for <code>IN</code> mode arguments, but for <code>VARIADIC</code> it will say:</p>
<blockquote>
<p>ERROR: VARIADIC parameter must be an array</p>
</blockquote>
<p>If brackets are added to the end to make the array type ‘<code>table_name.column_name%TYPE[]</code>’, it will say:</p>
<blockquote>
<p>ERROR:  syntax error at or near &quot;[&quot;</p>
</blockquote>
<p>even though <code>INTEGER[]</code> would work.</p>
<p>If <a href=""https://www.postgresql.org/docs/current/plpgsql-declarations.html#PLPGSQL-DECLARATION-TYPE"" rel=""nofollow noreferrer"">the <code>ARRAY[]</code> syntax</a> is used, it will say:</p>
<blockquote>
<p>ERROR:  syntax error at or near &quot;ARRAY&quot;</p>
</blockquote>
<p>Is there a working syntax, or is this broken?</p>
<p>Complete SQL script for replication:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE SCHEMA s;

CREATE TABLE s.&quot;the table&quot; (
    a INTEGER,
    b INTEGER,
    c INTEGER
);

CREATE PROCEDURE s.&quot;the table operation&quot;(
          IN a s.&quot;the table&quot;.a%TYPE,
          IN b s.&quot;the table&quot;.b%TYPE,
    VARIADIC c s.&quot;the table&quot;.c%TYPE[]
)
LANGUAGE plpgsql AS $$
BEGIN
END;
$$;

DROP SCHEMA s CASCADE;
</code></pre>
",3,3,0,2025-10-15T23:13:50+00:00,2,91,True
79792215,8330450,,sql,SQL Query to select a string after an extension,"<p>Values:</p>
<pre><code>1.zzz- Pipo
test.zzz-Done
blabla2.zzz-zip3
</code></pre>
<p>My question is, how can I get everything after <code>.zzz-</code> so the select statement from the value above will return:</p>
<pre><code> Pipo
Done
zip3
</code></pre>
",0,2,2,2025-10-16T13:43:24+00:00,1,139,True
79792652,31703127,,sql,Get records where an entered value is in any two columns,"<p>Step by step I have made a simple Access file with several tables, forms and queries. All those and also all columns are in Portuguese, but I'll translate them.</p>
<p>The database is related to teabags, which I collect.</p>
<p>I want to filter records to show in a form that corresponds to a specific tea brand and variety that I'll input.</p>
<p>The main table has the columns for each teabag. Four of those columns are: &quot;Main Brand&quot;, &quot;Secondary Brand&quot;, &quot;Variety 1&quot; and &quot;Variety 2&quot;.</p>
<p>For the <code>Brands</code>, I have another table for them and both <code>Brand</code> columns of the main table are picked from there. However, there's no specific table for the varieties, they are stored only in the main table. That is because the text for the <code>Variety</code> column vary a lot (almost free text). It's not feasible to have a separate table for that.</p>
<p>My query asks for a brand, which are chosen from the <code>Brands</code> table. That means the records are selected if the brand I want is on either of those two brand columns (main and secondary). Then the query asks for a variety. The problem is to get records where that variety is on any of those two variety columns of the main table (Variety 1 and 2).</p>
<p>A note to say that the text I enter for both brand and variety may not be complete. That is, if I want to get the brand &quot;LIPTON&quot; I just need to input &quot;LIP&quot;, for example. The same for the variety... &quot;ROOIB&quot; will select &quot;ROOIBOS&quot; and &quot;ROOIBOS PURE&quot;, etc.</p>
<p>I think I made the query using the builder and then completed it in design mode. But here's the respective SQL. I use translator to get the English names for the tables and fields. I hope you understand.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT [Tea Brands].BrandName
    , [BD Teas].[Variety 1]
    , [BD Teas].*
    , [Tea Brands].*
    , Countries.*
FROM Countries 
    INNER JOIN (
        [Tea Brands] 
        INNER JOIN [BD Teas] ON [Tea Brands].BrandID = [BD Teas].[Main Brand]
    ) ON Countries.CountryID = [Tea Brands].BrandCountry
WHERE ((([Tea Brands].BrandName) Like &quot;*&quot; &amp; [Which Brand to search for?] &amp; &quot;*&quot;) 
    AND (([BD Teas].[Variety 1]) Like &quot;*&quot; &amp; [Which Variety to search for?] &amp; &quot;*&quot;));
</code></pre>
<p>I think what is missing in the query is some OR function for the variety but I don't want to ask it twice.</p>
<p><a href=""https://i.sstatic.net/ot8CvJA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ot8CvJA4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/rUIWQaOk.png"" rel=""nofollow noreferrer"">One of the records in its Form</a></p>
",0,1,1,2025-10-16T23:54:57+00:00,1,179,True
79793019,23059744,,sql,How would I use N.EntityFramework.Extensions to insert into another table,"<p>I am trying to use the <code>InsertFromQuery()</code> to add data to a SQL table:</p>
<pre><code>await cx.Users
        .Where(x =&gt; x.Country == &quot;Romania&quot;)
        .InsertFromQueryAsync(&quot;Mailbox&quot;, mb =&gt; new MailboxEntity { Port = 587});
</code></pre>
<p>I get an error on <code>GetObjectProperties()</code>:</p>
<blockquote>
<p>Encountered an unsupported expression type</p>
</blockquote>
<p>I have also tried</p>
<pre><code>cx.Users
  .Where(x =&gt; x.CountryIsoCode == &quot;HR&quot;)
  .InsertFromQuery(&quot;Mailboxes&quot;, ut =&gt; new { Port = 587 });
</code></pre>
<p>This generates the following SQL:</p>
<pre><code>INSERT INTO [Mailboxes] (Port)
    SELECT Port
    FROM [dbo].[AspNetUsers] AS [Extent1]
</code></pre>
<p>The problem is I don't have a <code>Port</code> column in <code>AspNetUsers</code>. I do have it in the <code>Mailboxes</code> table.</p>
<p>I am wondering how to use <code>InsertFromQuery</code> to insert certain properties from objects from one table into a different table that has different column names.</p>
",1,1,0,2025-10-17T11:34:19+00:00,1,110,False
79793067,11148823,,sql,Select data with self reference in where conditions,"<p>I work with a health insurance database.</p>
<p><code>tab1</code> contains data regarding drug dispensing. There is about one billion rows per months.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE tab1 
(
    id NUMBER, -- patient id
    dte DATE, -- date of dispensing
    code NUMBER -- drug packaging code
);

INSERT INTO tab1 VALUES (1, DATE '2020-01-01', 12);
INSERT INTO tab1 VALUES (1, DATE '2020-06-01', 34);
INSERT INTO tab1 VALUES (2, DATE '2021-03-15', 56);
INSERT INTO tab1 VALUES (3, DATE '2020-06-01', 78);
INSERT INTO tab1 VALUES (4, DATE '2020-06-01', 56);

SELECT * FROM tab1;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">ID</th>
<th style=""text-align: left;"">DTE</th>
<th style=""text-align: right;"">CODE</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2020-01-01</td>
<td style=""text-align: right;"">12</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2020-06-01</td>
<td style=""text-align: right;"">34</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">2021-03-15</td>
<td style=""text-align: right;"">56</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">2020-06-01</td>
<td style=""text-align: right;"">78</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">2020-06-01</td>
<td style=""text-align: right;"">56</td>
</tr>
</tbody>
</table></div>
<p><code>tab2</code> is a reference table for drugs:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE tab2 
(
    code NUMBER, -- drug packaging code (no duplicates)
    label VARCHAR2(10) -- drug international nonproprietary name
);

INSERT INTO tab2 VALUES (12, 'x');
INSERT INTO tab2 VALUES (34, 'y');
INSERT INTO tab2 VALUES (56, 'x');
INSERT INTO tab2 VALUES (78, 'y');

SELECT * FROM tab2;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">CODE</th>
<th style=""text-align: left;"">LABEL</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">12</td>
<td style=""text-align: left;"">x</td>
</tr>
<tr>
<td style=""text-align: right;"">34</td>
<td style=""text-align: left;"">y</td>
</tr>
<tr>
<td style=""text-align: right;"">56</td>
<td style=""text-align: left;"">x</td>
</tr>
<tr>
<td style=""text-align: right;"">78</td>
<td style=""text-align: left;"">y</td>
</tr>
</tbody>
</table></div>
<p>I need to select data from <code>tab1</code> regarding drugs labelled <code>x</code> dispensed in 2020, i.e.:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE
  mytab AS
SELECT
    *
FROM
    tab1
WHERE
    dte BETWEEN DATE '2020-01-01' AND DATE '2020-12-31'
    AND EXISTS (SELECT *
                FROM tab2
                WHERE tab1.code = tab2.code
                  AND tab2.label = 'x');

SELECT * FROM mytab;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">ID</th>
<th style=""text-align: left;"">DTE</th>
<th style=""text-align: right;"">CODE</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2020-01-01</td>
<td style=""text-align: right;"">12</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">2020-06-01</td>
<td style=""text-align: right;"">56</td>
</tr>
</tbody>
</table></div>
<p>Additionally, I need to select data from <code>tab1</code> regarding drugs labelled <code>y</code> dispensed in 2020, but only for patients who satisfy the previous condition, i.e.:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    *
FROM
    tab1
WHERE
    dte BETWEEN DATE '2020-01-01' AND DATE '2020-12-31'
    AND EXISTS (SELECT *
                FROM mytab
                WHERE tab1.id = mytab.id);
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">ID</th>
<th style=""text-align: left;"">DTE</th>
<th style=""text-align: right;"">CODE</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2020-01-01</td>
<td style=""text-align: right;"">12</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2020-06-01</td>
<td style=""text-align: right;"">34</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">2020-06-01</td>
<td style=""text-align: right;"">56</td>
</tr>
</tbody>
</table></div>
<p><strong>Question</strong></p>
<p>Is there a readable way to do this in a single query? I am primarly seeking to prevent code duplication. The example is heavely simplified. Selecting drug dispensing requires querying 3 tables, 12 keys, and several where clause. The query is often adjusted and the two parts easily become out of sync.</p>
<p>I tried to add another <code>EXISTS</code> clause with a self-reference to <code>tab1</code> - but without success.</p>
<p>The concrete database is Oracle, but the solution must be ANSI SQL.</p>
",1,1,0,2025-10-17T12:50:14+00:00,6,255,True
79793409,11585676,,sql,Check if values ​from a column exist in another table,"<p>I'm using SQL in MS Access. I have two tables (table_original and table_new) and I want to iterate through each row of table_new and check if the ID column exists in table_original. If the ID exists, the query should insert the value &quot;Yes&quot; into the <strong>Active</strong> column of table_new. If it doesn't exist, it should insert the value &quot;No&quot;.</p>
<p>This is the structure of my tables:</p>
<pre><code>table_original:
ID, Country, Level

table_new:
ID, Country, Level, Active
</code></pre>
<p>I created the code below, but for some reason it doesn't work and I don't know why. What can I do?</p>
<pre><code>UPDATE table_new
SET
    table_new.Active = IIF(
        table_new.[ID] IN (
            SELECT
                table_original.[ID] &amp; ''
            FROM
                table_original
        ),
        'Yes',
        'No'
    );
</code></pre>
<p>PS: I put a <em><strong>table_original.[ID] &amp; ''</strong></em> in the ID of the table_original because in this table, this column is of type number and in the new_table the ID column is of type text.</p>
",-1,1,2,2025-10-17T20:11:53+00:00,1,113,True
79795209,30076223,,sql,How to count the number of folders at each level?,"<p>I have an output from PowerShell that has directory, size, date information. I will be moving the files into a system that can only have 500 folders at each level. I need a script to count the number of folders at each level (just one level deep)  and if the number exceeds 500, add a level.</p>
<p>INPUT</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>level1</th>
<th>level2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Group1</td>
<td>data1</td>
</tr>
<tr>
<td>Group2</td>
<td>data1</td>
</tr>
<tr>
<td>Group2</td>
<td>data2</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>Group2</td>
<td>data500</td>
</tr>
<tr>
<td>Group2</td>
<td>data501</td>
</tr>
</tbody>
</table></div>
<p>OUTPUT</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>level1</th>
<th>new level2</th>
<th>new level3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Group1</td>
<td>data1</td>
<td></td>
</tr>
<tr>
<td>Group2</td>
<td>[0-499]</td>
<td>data1</td>
</tr>
<tr>
<td>Group2</td>
<td>[0-499]</td>
<td>data2</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>.....</td>
</tr>
<tr>
<td>Group2</td>
<td>[0-499]</td>
<td>data500</td>
</tr>
<tr>
<td>Group2</td>
<td>[500-999]</td>
<td>data501</td>
</tr>
</tbody>
</table></div>
<p>I know I can do this for each level to determine which ones are over the 500-folder threshold,</p>
<pre><code>select count(level2) over (partition by level1) as num_folders_level2_per_level1
from ( 
    select distinct level1, level2
    from table
) q
</code></pre>
<p>but I don't know how to add to the path if it is over 500 nor how to extend the code for unknown number of levels (dynamic SQL?)</p>
<p>I searched for CTE examples, but the examples all had child, parentid, not the full path , and I don't know what column to join, as I really need this for leve1, versus level2, then level2 vs level3 (<a href=""https://stackoverflow.com/questions/35025926/getting-counts-at-each-level-of-a-hierarchical-query"">Getting counts at each level of a hierarchical query</a>)</p>
<p>Thanks</p>
",1,1,0,2025-10-20T19:17:14+00:00,2,145,True
79795265,2661251,"Atlanta, GA",sql,Generic Date Add SQL that works in both MySQL and H2,"<pre><code>select DATE_ADD(CURRENT_DATE(), INTERVAL -30 DAY)
</code></pre>
<p>works in MySQL.</p>
<pre><code>SELECT DATEADD(‘DAY’, -30, CURRENT_DATE())
</code></pre>
<p>works in H2(2.2.224).</p>
<p>I am looking for a generic SQL solution that works in both the databases.</p>
<p>I tried</p>
<pre><code>SELECT CURRENT_DATE() - INTERVAL :days DAY
</code></pre>
<p>which worked in MySQL but didn't work in H2.</p>
",2,2,0,2025-10-20T20:36:00+00:00,2,101,True
79795528,1941545,,sql,How to get the number of rows from multiple related tables with one query,"<p>I have four tables, one primary and three dependents, let's call them A, B, C and D.<br>
All tables have the column <code>ID</code>, with <code>ID</code> being the primary key for A and the foreign key for B-D.<br>
B-D also have a column called <code>timestamp</code> and (non-unique) indices combining <code>ID</code> and <code>timestamp</code> (in this order).</p>
<p>For a given subset of ID's from 'A', how do I get the number of rows in the tables B-D, optionally filtered by a range of timestamps?<br>
Preferably without using dependent subqueries, because these are (supposedly) 'very inefficient'.</p>
<p>What I've tried is something like this:</p>
<pre><code>SELECT A.`ID`,
       (COUNT(`B`.`timestamp`) + COUNT(`C`.`timestamp`) + COUNT(`D`.`timestamp`)) as `eventCount`
FROM `A`
LEFT JOIN `B` ON A.`ID` = `B`.`ID`
LEFT JOIN `C` ON A.`ID` = `C`.`ID`
LEFT JOIN `D` ON A.`ID` = `C`.`ID`
GROUP BY A.`ID`
</code></pre>
<p>However this yields a ridiculously high number akin to a cartesian product (10296 instead of 26 + 12 + 11, as the actual data for a specific row would be).<br>
I have considered using <code>COUNT(DISTINCT...)</code>, but am worried that it would also filter out duplicate timestamps, which are perfectly possible.</p>
<p>I have added <a href=""https://www.db-fiddle.com/f/bYucazioSy4ffXiR9m7vch/0"" rel=""nofollow noreferrer"">a fiddle</a> for you to try your queries against.
Additionally I want to note, that I do not, in fact, have 'a small set of data', I am talking about multiple 10ks of rows, which is why I am worried about efficiency to begin with.</p>
<p>However the query itself should have little to do with 'how many rows' there are.</p>
",3,3,0,2025-10-21T07:02:30+00:00,5,177,True
79795573,7110049,,sql,How to not lock tables in an ODBC connection?,"<p>I'm working on a Node.js project that reads data from an HFSQL database. I'm using the <code>odbc</code> library to create a simple connection like this:</p>
<pre class=""lang-js prettyprint-override""><code>connection = await odbc.connect(&quot;DSN=xxxxx;UID=xxxxx;PWD=xxxxx;CHARSET=UTF8;&quot;);
</code></pre>
<p>However, if another process tries to write data to the HFSQL database while my connection is open, it receives an error saying that a table is locked — even though my project doesn't read from or access that table at all.</p>
<p>My project is completely read-only. I tried adding <code>ReadOnly=1</code> in the connection string, but it didn't make any difference.</p>
",0,0,0,2025-10-21T07:59:49+00:00,0,72,False
79795578,8060017,,sql,Why doesn&#39;t index creation work any more in Postgres v17 and v18?,"<p>Considering the following SQL code:</p>
<pre class=""lang-sql prettyprint-override""><code>create table test (a int, b int) ;
create type tst as (a int, b int) ;
create function tst (a int, b int) returns tst[] language sql immutable as 
  $$ select array( select row(a,b) :: tst) ; $$ ;
create index index_test on test using gin (tst(a,b) array_ops) ;
</code></pre>
<p>This SQL code works well with Postgres until v16, see <a href=""https://dbfiddle.uk/rbnfJHRs"" rel=""nofollow noreferrer"">dbfiddle</a>, but it doesn't work any more with Postgres v17 and v18, see <a href=""https://dbfiddle.uk/Ynk3HZIT"" rel=""nofollow noreferrer"">dbfiddle</a>.</p>
<blockquote>
<p>ERROR:  type &quot;tst&quot; does not exist<br />
LINE 1:  select array( select row(a,b) :: tst) ;<br />
QUERY:   select array( select row(a,b) :: tst) ;<br />
CONTEXT:  SQL function &quot;tst&quot; during inlining</p>
</blockquote>
<p>What has changed with Postgres v17 &amp; v18, and how to fix this issue?</p>
",0,4,4,2025-10-21T08:03:08+00:00,1,219,True
79796306,31731133,,sql,Count columns until the value changes,"<p>I have a table that has values for each week in each column. I want to count the number of consistent values starting from the first week until it changes. I've tried searching for answers but I couldn't find anything that is remotely close.</p>
<p>Sample initial table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>Col 1</th>
<th>Col 2</th>
<th>Col 3</th>
<th>Col 4</th>
<th>Col 5</th>
<th>Col 6</th>
<th>Col 7</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>A</td>
<td>X</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>X</td>
<td>A</td>
<td>A</td>
</tr>
<tr>
<td>3</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>A</td>
<td>X</td>
<td>A</td>
<td>A</td>
</tr>
<tr>
<td>4</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
</tr>
</tbody>
</table></div>
<p>Desired result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>Col 1</th>
<th>Col 2</th>
<th>Col 3</th>
<th>Col 4</th>
<th>Col 5</th>
<th>Col 6</th>
<th>Col 7</th>
<th>Consistent Columns</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>A</td>
<td>X</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>X</td>
<td>A</td>
<td>A</td>
<td>4</td>
</tr>
<tr>
<td>3</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>A</td>
<td>X</td>
<td>A</td>
<td>A</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>7</td>
</tr>
</tbody>
</table></div>
<p>Is it possible to have this calculation?</p>
",1,2,1,2025-10-21T23:40:05+00:00,4,243,True
79796876,10188661,"Minsk, Belarus",sql,Do you need index on is_archived if not many rows are archived?,"<p>Let's say I have book table:</p>
<pre><code>id: int
is_archived: bool
creator_id: int
created_at: Date
</code></pre>
<p>There are like 1,000,000+ books and only 10 have <code>is_archived=true</code>.</p>
<p>In queries you always filter <code>is_archived=false</code>, like:</p>
<pre><code>SELECT * 
FROM book 
WHERE creator_id = 1 
  AND is_archived = false 
LIMIT 10

SELECT * 
FROM book 
WHERE created_at &gt; NOW() 
  AND is_archived = false 
LIMIT 10
</code></pre>
<p>You have separate indexes on <code>creator_id</code> and <code>created_at</code>, does it make sense to delete them and create instead <code>creator_id+is_archived=false</code> and <code>created_at+is_archived=false</code>?</p>
<p>My understanding is that <code>creator_id</code> index is enough because it will be used by postgres first to let's say get 10,000 rows with <code>creator_id=1</code> and then just do sequential scan to pick first 10 with <code>is_archived=false</code></p>
",0,0,0,2025-10-22T14:16:22+00:00,1,88,True
79796877,31735217,,sql,“SELECT list expression references column X which is neither grouped nor aggregated” when using COUNT(*)?,"<p>I'm trying to run a simple SQL query in BigQuery like this:</p>
<pre><code>SELECT usertype, COUNT(*)
FROM `project.dataset.table`;
</code></pre>
<p>But I get an error:</p>
<blockquote>
<p>SELECT list expression references column usertype which is neither grouped nor aggregated at [2:2]</p>
</blockquote>
<p>I'm confused because I thought <code>COUNT(*)</code> can be used to count rows.
Do I always need to use <code>GROUP BY</code> with <code>COUNT(*)</code>?</p>
",-1,2,3,2025-10-22T14:16:57+00:00,3,102,True
79797012,5647659,"Qu&#233;bec City, QC, Canada",sql,ORA-00904 when querying a sys.anydata column,"<p>Why am I getting a <em>ORA-00904: invalid identifier</em> error? How can I select the value of a <code>sys.anydata</code> column?</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE temp (
  id NUMBER PRIMARY KEY,
  val sys.anydata
);

INSERT INTO temp (ID, val) VALUES (1, sys.anydata.convertnumber(123));

SELECT ID, temp.val.accessnumber() AS val FROM temp
</code></pre>
<p>I tried with <code>getnumber()</code> instead of <code>accessnumber()</code> but I get the same error.</p>
",2,3,1,2025-10-22T16:10:05+00:00,1,93,True
79798285,23512643,,sql,Is there a SQL equivalent of the R setdiff() function?,"<p>I have two tables:</p>
<pre><code>CREATE TABLE t1 (
    NAME1 VARCHAR(20),
    date DATE
);

INSERT INTO t1 (NAME1, date) VALUES
('RED', '2020-01-05'),
('BLUE', '2020-03-15'),
('GREEN', '2020-06-20'),
('YELLOW', '2020-09-10'),
('PURPLE', '2020-12-25'),
('BLUE', '2020-02-20'),
('RED', '2020-07-15'),
('GREEN', '2020-11-10'),
('BLUE', '2021-02-14'),
('RED', '2021-05-01'),
('ORANGE', '2021-07-04'),
('GREEN', '2021-08-15'),
('PINK', '2021-11-20'),
('BROWN', '2021-03-10'),
('BLUE', '2021-09-22'),
('YELLOW', '2021-12-05'),
('RED', '2022-01-10'),
('PURPLE', '2022-04-22'),
('ORANGE', '2022-07-30'),
('PINK', '2022-10-15');


CREATE TABLE t2 (
    NAME2 VARCHAR(20),
    date DATE
);

INSERT INTO t2 (NAME2, date) VALUES
('BLUE', '2020-02-10'),
('GREEN', '2020-04-18'),
('YELLOW', '2020-07-22'),
('CYAN', '2020-10-05'),
('MAGENTA', '2020-11-30'),
('BLUE', '2020-05-15'),
('GREEN', '2020-08-20'),
('BLUE', '2021-01-20'),
('SILVER', '2021-03-25'),
('YELLOW', '2021-06-12'),
('GOLD', '2021-09-08'),
('CORAL', '2021-12-15'),
('YELLOW', '2021-02-28'),
('GREEN', '2022-02-28'),
('CYAN', '2022-05-14'),
('BLUE', '2022-08-19'),
('MAGENTA', '2022-09-25'),
('SILVER', '2022-11-11'),
('YELLOW', '2022-12-05'),
('CORAL', '2022-12-20'),
('CYAN', '2022-06-30');
</code></pre>
<p><strong>Problem:</strong> (Using SQL) In each year, I want to find out: How many distinct names are only in t1, how many distinct names are only in t2, how many distinct names are both in table2. The intended result should look like this:</p>
<pre><code>  year only_in_t1 only_in_t2 common_to_both total_distinct_t1 total_distinct_t2 total_distinct_combined
 2020          2          2              3                 5                 5                       7
 2021          5          3              2                 7                 5                      10
 2022          4          7              0                 4                 7                      11
</code></pre>
<p>Using the R programming language, I could have used the <code>setdiff()</code> function to accomplish this.</p>
<p>I tried to write an SQL solution, but it got very long:</p>
<pre><code>WITH t1_names AS (
  SELECT DISTINCT 
    YEAR(date) as year_date,
    NAME1 as name
  FROM t1
),
t2_names AS (
  SELECT DISTINCT 
    YEAR(date) as year_date,
    NAME2 as name
  FROM t2
),
all_years AS (
  SELECT DISTINCT year_date FROM t1_names
  UNION
  SELECT DISTINCT year_date FROM t2_names
),
t1_by_year AS (
  SELECT year_date, COUNT(DISTINCT name) as total_distinct_t1
  FROM t1_names
  GROUP BY year_date
),
t2_by_year AS (
  SELECT year_date, COUNT(DISTINCT name) as total_distinct_t2
  FROM t2_names
  GROUP BY year_date
),
common_names AS (
  SELECT t1_names.year_date, COUNT(DISTINCT t1_names.name) as common_to_both
  FROM t1_names
  INNER JOIN t2_names ON t1_names.year_date = t2_names.year_date 
                      AND t1_names.name = t2_names.name
  GROUP BY t1_names.year_date
),
only_t1 AS (
  SELECT t1_names.year_date, COUNT(DISTINCT t1_names.name) as only_in_t1
  FROM t1_names
  LEFT JOIN t2_names ON t1_names.year_date = t2_names.year_date 
                     AND t1_names.name = t2_names.name
  WHERE t2_names.name IS NULL
  GROUP BY t1_names.year_date
),
only_t2 AS (
  SELECT t2_names.year_date, COUNT(DISTINCT t2_names.name) as only_in_t2
  FROM t2_names
  LEFT JOIN t1_names ON t2_names.year_date = t1_names.year_date 
                     AND t2_names.name = t1_names.name
  WHERE t1_names.name IS NULL
  GROUP BY t2_names.year_date
),
combined_names AS (
  SELECT year_date, name FROM t1_names
  UNION
  SELECT year_date, name FROM t2_names
),
combined_by_year AS (
  SELECT year_date, COUNT(DISTINCT name) as total_distinct_combined
  FROM combined_names
  GROUP BY year_date
)
SELECT 
  ay.year_date as year,
  COALESCE(ot1.only_in_t1, 0) as only_in_t1,
  COALESCE(ot2.only_in_t2, 0) as only_in_t2,
  COALESCE(cn.common_to_both, 0) as common_to_both,
  COALESCE(t1y.total_distinct_t1, 0) as total_distinct_t1,
  COALESCE(t2y.total_distinct_t2, 0) as total_distinct_t2,
  COALESCE(cby.total_distinct_combined, 0) as total_distinct_combined
FROM all_years ay
LEFT JOIN t1_by_year t1y ON ay.year_date = t1y.year_date
LEFT JOIN t2_by_year t2y ON ay.year_date = t2y.year_date
LEFT JOIN common_names cn ON ay.year_date = cn.year_date
LEFT JOIN only_t1 ot1 ON ay.year_date = ot1.year_date
LEFT JOIN only_t2 ot2 ON ay.year_date = ot2.year_date
LEFT JOIN combined_by_year cby ON ay.year_date = cby.year_date
ORDER BY ay.year_date;
</code></pre>
<p><strong>Question:</strong> Is there an equivalent to the <code>setdiff()</code> function in SQL which would allow me to reduce the length of this code?</p>
<p>Thanks!</p>
<hr />
<p>R solution:</p>
<pre><code>t1$year &lt;- year(t1$date)
t2$year &lt;- year(t2$date)


analyze_names_by_year &lt;- function(t1, t2) {
  
  all_years &lt;- sort(unique(c(t1$year, t2$year)))
  
  results &lt;- data.frame()
  
  for (yr in all_years) {
    names_t1 &lt;- unique(t1$NAME1[t1$year == yr])
    names_t2 &lt;- unique(t2$NAME2[t2$year == yr])
    
    only_t1 &lt;- length(setdiff(names_t1, names_t2))
    only_t2 &lt;- length(setdiff(names_t2, names_t1))
    common &lt;- length(intersect(names_t1, names_t2))
    total_t1 &lt;- length(names_t1)
    total_t2 &lt;- length(names_t2)
    total_combined &lt;- length(union(names_t1, names_t2))
    
    results &lt;- rbind(results, data.frame(
      year = yr,
      only_in_t1 = only_t1,
      only_in_t2 = only_t2,
      common_to_both = common,
      total_distinct_t1 = total_t1,
      total_distinct_t2 = total_t2,
      total_distinct_combined = total_combined
    ))
  }
  
  return(results)
}


summary_by_year &lt;- analyze_names_by_year(t1, t2)
</code></pre>
",2,3,1,2025-10-23T23:47:11+00:00,2,157,True
79799851,2840697,,sql,Finding Repeated patterns in SQL?,"<p>Let's assume that we have a table containing the following column</p>
<pre><code>     URL                             Repeated 
    www.b.com/aa/aa/aa                X
    www.b.com/aa/                     X
    www.xy.com                        X
    .
    .
    .
</code></pre>
<p>Repeated column here just takes the default value of 'X'. I want this column to check whether there's the repeated patterns at the end. For instance</p>
<pre><code>    www.b.com/aa/aa/aa   
    www.xyz.com/bc/bc
</code></pre>
<p>both contain repeated patterns, delimited by '/'.</p>
<p>I was wondering if there's a way to check it in SQL. Is there any built-in function in SQL that makes it easy? Also, can we extend this concept? (so that we can count how many repeated patterns we see)</p>
<p>Any reference or help would be greatly appreciated. Thanks.</p>
<ol>
<li>this is Trino SQL</li>
<li>to clarify repeated patterns, it means the exact matches consecutively if you split the URL '/'</li>
</ol>
<p><a href=""http://www.c.com/aa/bb/aa"" rel=""nofollow noreferrer"">www.c.com/aa/bb/aa</a>, <a href=""http://www.aaa.com/aaa"" rel=""nofollow noreferrer"">www.aaa.com/aaa</a> don't count as repeated patterns, but</p>
<p><a href=""http://www.c.com/aa/aa"" rel=""nofollow noreferrer"">www.c.com/aa/aa</a>  or <a href=""http://www.c.com/aa/bb/bb"" rel=""nofollow noreferrer"">www.c.com/aa/bb/bb</a>  or <a href=""http://www.aaa.com/aaa/aaa"" rel=""nofollow noreferrer"">www.aaa.com/aaa/aaa</a> would count as repeated patterns</p>
",1,2,1,2025-10-26T05:50:07+00:00,3,165,True
79799862,8167524,,sql,How to include a recursive query in the main query?,"<p>I have a database in SQLite with these tables and columns:</p>
<ul>
<li><code>Products</code>: <code>id</code>, <code>name</code>, <code>number</code>, <code>price</code>, <code>category_id</code>.</li>
<li><code>Orders</code>: <code>id</code>, <code>client_id</code>, <code>datetime</code>, <code>sum</code>.</li>
<li><code>OrderProducts</code>: <code>order_id</code>, <code>product_id</code>, <code>product_count</code>.</li>
<li><code>Categories</code>: <code>id</code>, <code>parent_id</code>, <code>name</code>.</li>
<li><code>Clients</code>: <code>id</code>, <code>name</code>, <code>address</code>.</li>
</ul>
<p>I need to get the &quot;Top 5 most purchased products over the past month&quot; (by number of units in orders).</p>
<p>The report should include <code>Product Name</code>, <code>Level 1 Category</code>, and <code>total number of units sold</code>.</p>
<p>I wrote this SQL code, it works fine:</p>
<pre><code>SELECT Products.Name, COUNT(*) * Product_count AS Result_count, Categories.Parent_id
FROM Orders
JOIN OrderProducts ON Orders.Id = OrderProducts.Order_id
JOIN Products ON OrderProducts.Product_id = Products.Id
JOIN Categories ON Products.Category_id = Categories.Id
WHERE Orders.Created_at &gt; DATETIME('now', '-30 day')
GROUP BY Product_id
ORDER BY Result_count DESC 
LIMIT 5;
</code></pre>
<p>There is a separate query for to get all parents:</p>
<pre><code>WITH RECURSIVE owners (Id, Parent_Id, Name, Number) AS
(
    SELECT Id, Parent_Id, Name, Number
    FROM Categories c
    WHERE c.Id  = 9

    UNION ALL 

    SELECT c1.Id, c1.Parent_Id, c1.Name, c1.Number
    FROM (owners o 
    JOIN Categories c1 ON (o.Parent_Id = c1.Id))
) 
SELECT * 
FROM owners;
</code></pre>
<p>How do I:</p>
<ol>
<li>Get the <strong>top level</strong> only parent from the recursive query?</li>
<li>Combine two blocks of code</li>
</ol>
",4,5,1,2025-10-26T06:26:32+00:00,1,145,True
79802861,31759654,,sql,Combine multiple selects and write to 1 row in a temp table,"<p>I have a stored procedure in Microsoft SQL Server. I want to combine multiple results into 1 row like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Period</th>
<th>Income</th>
</tr>
</thead>
<tbody>
<tr>
<td>202501_a</td>
<td>202501</td>
<td>50528.55</td>
</tr>
</tbody>
</table></div>
<p>NOT LIKE THIS</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Period</th>
<th>Income</th>
</tr>
</thead>
<tbody>
<tr>
<td>202501_a</td>
<td>NULL</td>
<td>NULL</td>
</tr>
<tr>
<td>Null</td>
<td>202501</td>
<td>NULL</td>
</tr>
<tr>
<td>NULL</td>
<td>NULL</td>
<td>50528.55</td>
</tr>
</tbody>
</table></div>
<p>The data and columns are for demonstration only, I understand you would not need a <code>ID</code> and a <code>Period</code>, you could use 1 column for both. It is just an example.</p>
<p>Selecting from table <code>accounting_balance</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>column</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>period</td>
<td>varchar(8)</td>
</tr>
<tr>
<td>credit</td>
<td>decimal(20,2)</td>
</tr>
<tr>
<td>debit</td>
<td>decimal(20,2)</td>
</tr>
<tr>
<td>account_no</td>
<td>varchar(20)</td>
</tr>
</tbody>
</table></div>
<pre><code>--create temp table
--create a temp table with 3 columns.
IF (OBJECT_ID('tempdb..#financials') IS NOT NULL)
BEGIN
    DROP TABLE #financials
END

CREATE TABLE #financials
(
     id     VARCHAR(8),
     period VARCHAR(7),
     income DECIMAL(20, 2)
)

INSERT INTO #financials (id, period, income)
    --create the ids
    SELECT 
        CONCAT(period, '_a'), NULL, NULL
    FROM   
        accounting_balance
    WHERE  
        period LIKE '2025%'
    UNION ALL
    --get 1st column
    SELECT 
        NULL, period, NULL
    FROM   
        accounting_balance
    WHERE  
        period LIKE '2025%'
    UNION ALL
    -- get 2nd column
    SELECT 
        NULL, NULL, ABS(SUM(credit - debit))
    FROM   
        accounting_balance
    WHERE  
        account_no = '4000-001-01'

-- display temp table data
SELECT *
FROM #financials 

-- drop the table
DROP TABLE #financials 
</code></pre>
<p><img src=""https://i.sstatic.net/bF2ADEUr.png"" alt=""A table showing sample results"" /></p>
",0,0,0,2025-10-28T13:25:39+00:00,1,113,True
79802903,20983123,,sql,Does sqlite ignore WHERE clause after ON CONFLICT of UPSERT?,"<p>This is a minimal example:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE t(x PRIMARY KEY,y);
INSERT INTO t VALUES (1,2);
INSERT INTO t VALUES (1,3)
    ON CONFLICT(x) WHERE 0 DO UPDATE SET y = 9
    ON CONFLICT DO UPDATE SET y = 10;
SELECT * FROM t;
</code></pre>
<p>The result is confusingly <code>1|9</code> where <code>1|10</code> is expected from my sense.</p>
<p><a href=""https://sqlite.org/lang_upsert.html"" rel=""nofollow noreferrer"">sqlite docs</a> seems did not mention this where clause at all, although it is in the diagram.</p>
",4,4,0,2025-10-28T14:06:38+00:00,2,113,True
79803445,23512643,,sql,Using SQL to keep track of changes occurring in a table?,"<p>I start with this table in SQL on day=0 (e.g. 2025-10-29):</p>
<pre><code>CREATE TABLE TABLE_0 (
    NAME VARCHAR(20),
    VAR1 INT,
    VAR2 INT
);


INSERT INTO TABLE_0 (NAME, VAR1, VAR2) VALUES
('RED', 1, 2),
('GREEN', 1, 2),
('BLUE', 1, 2);
</code></pre>
<p>On day=1, the same table looks like this (RED stayed the same, GREEN changed, YELLOW has now appeared and BLUE has disappeared):</p>
<pre><code>CREATE TABLE TABLE_0 (
    NAME VARCHAR(20),
    VAR1 INT,
    VAR2 INT
);

INSERT INTO TABLE_0 (NAME, VAR1, VAR2) VALUES
('RED', 1, 2),
('GREEN', 3, 2),
('YELLOW', 1, 2);
</code></pre>
<p>On day=2, the table looks like this (RED stayed the same, GREEN has disappeared, YELLOW has changed, PURPLE has appeared and BLUE has reappeared):</p>
<pre><code>  CREATE TABLE TABLE_0 (
    NAME VARCHAR(20),
    VAR1 INT,
    VAR2 INT
);

INSERT INTO TABLE_0 (NAME, VAR1, VAR2) VALUES
('RED', 1, 2),
('YELLOW', 2, 2),
('PURPLE', 1, 3),
('BLUE', 1, 2);
</code></pre>
<hr />
<p><strong>My Question:</strong> Starting on day=0, I want to make a table which tracks all changes that are occurring. I want to run this SQL query every day. There should be 2 new columns: VALID_ON, EXPIRED_ON.</p>
<p>The result at the end of day=1 should look like this:</p>
<pre><code>NAME    VAR1  VAR2  VALID_ON     EXPIRED_ON
------  ----  ----  -----------  -----------
RED     1     2     2025-10-29   9999-12-31
GREEN   1     2     2025-10-29   2025-10-30
GREEN   3     2     2025-10-30   9999-12-31
BLUE    1     2     2025-10-29   2025-10-30
YELLOW  1     2     2025-10-30   9999-12-31
</code></pre>
<p>The result at the end of day=2 should look like this:</p>
<pre><code>NAME    VAR1  VAR2  VALID_ON     EXPIRED_ON
------  ----  ----  -----------  -----------
RED     1     2     2025-10-29   9999-12-31
GREEN   1     2     2025-10-29   2025-10-30
GREEN   3     2     2025-10-30   2025-10-31
BLUE    1     2     2025-10-29   2025-10-30
BLUE    1     2     2025-10-31   9999-12-31
YELLOW  1     2     2025-10-30   2025-10-31
YELLOW  2     2     2025-10-31   9999-12-31
PURPLE  1     3     2025-10-31   9999-12-31
</code></pre>
<hr />
<p>I tried to do this in two parts. In the first part, I create the table that will be used on the very first day:</p>
<pre><code>CREATE TABLE TRACKING_DAY_0 AS
SELECT 
    NAME,
    VAR1,
    VAR2,
    CURRENT_TIMESTAMP() AS VALID_ON,
    CAST('9999-12-31 00:00:00' AS TIMESTAMP) AS EXPIRED_ON
FROM TABLE_0;
</code></pre>
<p>Then, I tried to create a table which will be run each day that references the table created on the previous day:</p>
<pre><code>CREATE TABLE TRACKING_DAY_1 AS
SELECT 
    t.NAME,
    t.VAR1,
    t.VAR2,
    t.VALID_ON,
    t.EXPIRED_ON
FROM TRACKING_DAY_0 t
INNER JOIN TABLE_0 c
    ON t.NAME = c.NAME 
    AND t.VAR1 = c.VAR1 
    AND t.VAR2 = c.VAR2
WHERE t.EXPIRED_ON = CAST('9999-12-31 00:00:00' AS TIMESTAMP)

UNION ALL

SELECT 
    t.NAME,
    t.VAR1,
    t.VAR2,
    t.VALID_ON,
    CURRENT_TIMESTAMP() AS EXPIRED_ON
FROM TRACKING_DAY_0 t
LEFT JOIN TABLE_0 c
    ON t.NAME = c.NAME 
    AND t.VAR1 = c.VAR1 
    AND t.VAR2 = c.VAR2
WHERE t.EXPIRED_ON = CAST('9999-12-31 00:00:00' AS TIMESTAMP)
    AND c.NAME IS NULL

UNION ALL

SELECT 
    NAME,
    VAR1,
    VAR2,
    VALID_ON,
    EXPIRED_ON
FROM TRACKING_DAY_0
WHERE EXPIRED_ON &lt; CAST('9999-12-31 00:00:00' AS TIMESTAMP)

UNION ALL

SELECT 
    c.NAME,
    c.VAR1,
    c.VAR2,
    CURRENT_TIMESTAMP() AS VALID_ON,
    CAST('9999-12-31 00:00:00' AS TIMESTAMP) AS EXPIRED_ON
FROM TABLE_0 c
LEFT JOIN TRACKING_DAY_0 t
    ON c.NAME = t.NAME 
    AND c.VAR1 = t.VAR1 
    AND c.VAR2 = t.VAR2
    AND t.EXPIRED_ON = CAST('9999-12-31 00:00:00' AS TIMESTAMP)
WHERE t.NAME IS NULL;
</code></pre>
<p>I am afraid I have made this too long - is there a simpler way to solve this problem?</p>
<hr />
<p><strong>Update based on Guillaume's comments:</strong></p>
<pre><code>CREATE TABLE TRACKING_DAY_1 AS
SELECT 
    t.NAME,
    t.VAR1,
    t.VAR2,
    t.VALID_ON,
    CURRENT_TIMESTAMP() AS EXPIRED_ON
FROM TRACKING_DAY_0 t
LEFT JOIN TABLE_0 c
    ON t.NAME = c.NAME 
    AND t.VAR1 = c.VAR1 
    AND t.VAR2 = c.VAR2
WHERE t.EXPIRED_ON = CAST('9999-12-31 00:00:00' AS TIMESTAMP)
    AND c.NAME IS NULL

UNION ALL

SELECT 
    NAME,
    VAR1,
    VAR2,
    VALID_ON,
    EXPIRED_ON
FROM TRACKING_DAY_0
WHERE EXPIRED_ON &lt; CAST('9999-12-31 00:00:00' AS TIMESTAMP)

UNION ALL

SELECT 
    c.NAME,
    c.VAR1,
    c.VAR2,
    COALESCE(t.VALID_ON, CURRENT_TIMESTAMP()) AS VALID_ON,
    CAST('9999-12-31 00:00:00' AS TIMESTAMP) AS EXPIRED_ON
FROM TABLE_0 c
LEFT JOIN TRACKING_DAY_0 t
    ON c.NAME = t.NAME 
    AND c.VAR1 = t.VAR1 
    AND c.VAR2 = t.VAR2
    AND t.EXPIRED_ON = CAST('9999-12-31 00:00:00' AS TIMESTAMP);
</code></pre>
",-3,1,4,2025-10-29T05:04:34+00:00,2,189,True
79803510,2471473,"Olympia, WA",sql,Recursive CTE looking for all combinations of values that equal one target amount,"<p>I found examples online using a recursive CTE to find all combinations of values that equal one target amount. The database column ledger_amount is <code>DECIMAL(26,6)</code>.</p>
<pre class=""lang-sql prettyprint-override""><code>DECLARE @TARGET_AMOUNT DECIMAL(26, 6) = 15.02;

IF (OBJECT_ID('tempdb..#Temp_Ledger_Amounts') IS NOT NULL)
    DROP TABLE #Temp_Ledger_Amounts;

CREATE TABLE #Temp_Ledger_Amounts (
    [transaction_id] [bigint] IDENTITY(1, 1) NOT NULL
    , [ledger_amount] [decimal](26, 6) NOT NULL
    );

INSERT INTO #Temp_Ledger_Amounts (ledger_amount)
VALUES (17.38)
    , (12.11)
    , (1.34)
    , (7.31)
    , (8.93)
    , (6.99)
    , (8.03);

WITH CombinationsCTE (
    CombinationString
    , CurrentSum
    , LastID
    )
AS (
    -- Anchor member: Start with each individual number
    SELECT CAST(ledger_amount AS NVARCHAR(MAX))
        , ledger_amount
        , transaction_id
    FROM #Temp_Ledger_Amounts
    WHERE ledger_amount &lt;= @TARGET_AMOUNT
    
    UNION ALL
    
    -- Recursive member: Add numbers to existing combinations
    SELECT c.CombinationString + ', ' + CAST(n.ledger_amount AS NVARCHAR(MAX))
        , c.CurrentSum + n.ledger_amount
        , n.transaction_id
    FROM CombinationsCTE c
    -- Ensure unique combinations
    JOIN #Temp_Ledger_Amounts n ON n.transaction_id &gt; c.LastID
    WHERE c.CurrentSum + n.ledger_amount &lt;= @TARGET_AMOUNT
    )
SELECT CombinationString
    , CurrentSum
FROM CombinationsCTE
WHERE CurrentSum = @TARGET_AMOUNT
ORDER BY CombinationString;
</code></pre>
<p>The error I get is:</p>
<blockquote>
<p>Types don't match between the anchor and the recursive part in column &quot;CurrentSum&quot; of recursive query &quot;CombinationsCTE&quot;.</p>
</blockquote>
",-4,0,4,2025-10-29T07:06:45+00:00,1,183,True
79803688,30676690,,sql,"LATERAL conversion issues when Calcite performs SQL rewrite, NullPointerException","<p>I'm trying to use Calcite+PostgreSQL to rewrite SQL queries, the following error occurs during runtime. How should I solve it?
The main warning is like this：</p>
<blockquote>
<p>[warn] call_rewriter failed at idx=1: Java rewriter failed (code=1). stderr=Exception in thread &quot;main&quot; java.lang.NullPointerException: rightResult.neededAlias is null</p>
</blockquote>
<p>The whole error of one SQL during rewriting as follows：</p>
<pre><code>[warn] call_rewriter failed
at idx=1: Java rewriter failed (code=1). stderr=Exception in thread &quot;main&quot; java.lang.NullPointerException: 
rightResult.neededAlias is null, 
node is SELECT * FROM (`db`.`CUSTOMER`AS`$cor5`, LATERAL (SELECT * FROM `db`.`CUSTOMER_ADDRESS`WHERE`$cor5`.`c_current_addr_sk`=`ca_address_sk`) AS `t15`) AS `$cor3`, LATERAL (SELECT * FROM (SELECT * FROM `db`.`CUSTOMER_DEMOGRAPHICS`WHERE`cd_marital_status` = 'U' AND CAST(`cd_education_status`AS CHAR(15)) = 'Advanced Degree') AS`t16`WHERE`$cor3`.`c_current_cdemo_sk`=`cd_demo_sk`) AS `t17\`\`
</code></pre>
<p>after this warning，there are some lines I suppose have something to do with calcite, I intercepted some of the content as follows：</p>
<pre><code>    at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:533)   at org.apache.calcite.rel.rel2sql.RelToSqlConverter.dispatch(RelToSqlConverter.java:155)
    at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visitInput(RelToSqlConverter.java:163)
    at org.apache.calcite.rel.rel2sql.SqlImplementor.visitInput(SqlImplementor.java:233)
    at org.apache.calcite.rel.rel2sql.SqlImplementor.visitInput(SqlImplementor.java:221)
    at org.apache.calcite.rel.rel2sql.SqlImplementor.visitRoot(SqlImplementor.java:197)
    at com.sure.calcite.RewriterCli.main(RewriterCli.java:101)         Suppressed: java.lang.Throwable: Error while converting RelNode to SqlNode: LogicalAggregate(group=[{}], EXPR$0=[MIN($0)], EXPR$1=[MIN($22)], EXPR$2=[MIN($53)], EXPR$3=[MIN($46)])   LogicalFilter(condition=[=($127, $153)])
</code></pre>
<p>The codes snippet of the error part are as follows，：</p>
<pre class=""lang-py prettyprint-override""><code># PG connection
    runner = PgRunner(
        host=args.pg_host, port=args.pg_port, user=args.pg_user,
        password=args.pg_password, db=args.dataset
    )

    logger = RewriteLogger(out_file)
    df = pd.read_csv(csv_path)
    n_total = len(df)
    if args.limit and args.limit &gt; 0:
        n_total = min(n_total, args.limit)
    print(f&quot;[info] loaded {n_total} queries from {csv_path}&quot;)

    successes = 0
    for idx in range(n_total):
        row = df.iloc[idx]
        original_sql = str(row.get(&quot;original_sql&quot;) or row.get(&quot;sql&quot;) or row.get(&quot;query&quot;) or &quot;&quot;).strip()
        if not original_sql:
            continue

        # === Calling the Java Rewriter (Note: Returns a tuple, not a dictionary) ===
        #⚠️error part⚠️
        try:
            rewritten_sql, rules_applied = call_rewriter(
                args.dataset, original_sql, jar_path=args.jar, schema_path=schema_path
            )
        except Exception as e:
            ⚠️print(f&quot;[warn] call_rewriter failed at idx={idx}: {e}&quot;)
            continue

        if not rewritten_sql or not rewritten_sql.strip():
            continue

        # === Actual Execution Time (EXPLAIN ANALYZE) ===
     
        try:
            t_before, plan_before = runner.explain_analyze_time(original_sql)
        except Exception as e:
            ⚠️print(f&quot;[warn] explain_analyze original failed idx={idx}: {e}&quot;)
            continue

        try:
            t_after, plan_after = runner.explain_analyze_time(rewritten_sql)
        except Exception as e:
            ⚠️print(f&quot;[warn] explain_analyze rewritten failed idx={idx}: {e}&quot;)
            continue
</code></pre>
<p>Defination of call_rewriter：</p>
<pre class=""lang-py prettyprint-override""><code>def call_rewriter(db_id: str,
                  sql: str,
                  jar_path: Optional[str] = None,
                  schema_path: Optional[str] = None) -&gt; Tuple[str, List[str]]:
    &quot;&quot;&quot;
    调用 Java 重写器，返回 (rewritten_sql, rules_applied)。
    Java CLI 约定输出 JSON：
      {&quot;rewritten_sql&quot;:&quot;...&quot;, &quot;rules_applied&quot;:[...], &quot;plan_before&quot;:&quot;...&quot;, &quot;plan_after&quot;:&quot;...&quot;}
    &quot;&quot;&quot;
    jar = _auto_find_jar(jar_path)
    if not schema_path or not os.path.exists(schema_path):
        raise FileNotFoundError(f&quot;schema_path 不存在：{schema_path}&quot;)

    cmd = [&quot;java&quot;, &quot;-jar&quot;, jar, &quot;--schema&quot;, schema_path, &quot;--sql&quot;, sql]
    proc = subprocess.run(cmd, text=True, capture_output=True)

    # 优先从 stdout 抓 JSON；不行就从 stderr 抓（过滤掉日志噪声）
    obj = _extract_json(proc.stdout) or _extract_json(proc.stderr)
    if obj is None:
        # 仍没有 JSON，抛错（上层会跳过这条）
        raise RuntimeError(
            f&quot;Java rewriter failed (code={proc.returncode}). &quot;
            f&quot;stderr={proc.stderr.strip()}&quot;
        )

    rewritten = obj.get(&quot;rewritten_sql&quot;) or &quot;&quot;
    rules = obj.get(&quot;rules_applied&quot;) or []
    if not isinstance(rewritten, str):
        rewritten = str(rewritten)
    if not isinstance(rules, list):
        rules = []
    return rewritten, rules
</code></pre>
<p>Class PgRunner :</p>
<pre class=""lang-py prettyprint-override""><code>class PgRunner:
    def __init__(self, host=&quot;&quot;, port=, user=&quot;&quot;, password=&quot;&quot;, db=&quot;dsb&quot;):
        self.conn = psycopg2.connect(host=host, port=port, user=user, password=password, dbname=db)
        self.conn.autocommit = True

    def close(self):
        try: self.conn.close()
        except Exception: pass

    def explain_analyze(self, sql: str) -&gt; Dict[str, Any]:
        with self.conn.cursor() as cur:
            cur.execute(&quot;EXPLAIN (ANALYZE, BUFFERS) &quot; + sql)
            rows = cur.fetchall()
        plan_text = &quot;\n&quot;.join([r[0] for r in rows])
        m = _EXEC_TIME_RE.search(plan_text)
        exec_ms = float(m.group(1)) if m else float(&quot;nan&quot;)
        return {&quot;execution_time_ms&quot;: exec_ms, &quot;plan_text&quot;: plan_text}

    def checksum_of_results(self, sql: str, max_rows: int = 100000) -&gt; Tuple[str, int]:
        text = f&quot;SELECT * FROM ({sql}) t&quot;
        md5 = hashlib.md5(); n = 0
        with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.itersize = 1000
            cur.execute(text)
            for row in cur:
                n += 1
                if n &gt; max_rows: break
                md5.update(json.dumps(row, sort_keys=True, default=str).encode(&quot;utf-8&quot;))
        return md5.hexdigest(), n
</code></pre>
<p>The code in the Java file is as follows：</p>
<pre class=""lang-java prettyprint-override""><code>// Configure Calcite settings
SchemaPlus root = Frameworks.createRootSchema(true);
SchemaPlus schemaPlus = root.add(spec.schema, new SimpleSchema(spec));

SqlParser.Config parserCfg = SqlParser.config()
        .withCaseSensitive(false)
        .withUnquotedCasing(Casing.TO_LOWER)
        .withQuotedCasing(Casing.UNCHANGED)
        .withQuoting(Quoting.DOUBLE_QUOTE)
        .withConformance(SqlConformanceEnum.BABEL);
        //.withLex(Lex.POSTGRESQL);  // The runtime error indicated that POSTGRESQL was not present, so I commented out this line.

Properties props = new Properties();
props.setProperty(&quot;caseSensitive&quot;, &quot;false&quot;);

FrameworkConfig cfg = Frameworks.newConfigBuilder()
        .defaultSchema(schemaPlus)
        .parserConfig(parserCfg)
        .context(Contexts.of(new CalciteConnectionConfigImpl(props)))
        .build();

//Rule set + Dialect
HepProgramBuilder hp = new HepProgramBuilder();
RuleSets.addDefaultRules(hp);  // Current set of rules: The rule set is one I defined myself, which I have included in the code snippet below.
SqlDialect dialect = pickDialect(spec.dialect);

// RelNode before and after rule rewriting
RelNode before = planner.rel(validated).project();
String planBefore = RelOptUtil.toString(before);

HepPlanner hep = new HepPlanner(hp.build());
RuleCaptureListener listener = new RuleCaptureListener();// Listener for recording hit rules
hep.addListener(listener);
hep.setRoot(before);
RelNode after = hep.findBestExp();            
String planAfter = RelOptUtil.toString(after);

// ★ Location where the NPE was triggered（RelNode→SQL）
RelToSqlConverter conv = new RelToSqlConverter(PostgresqlSqlDialect.DEFAULT);
String sqlAfter = conv
    .visitRoot(after)                        
    .asStatement()
    .toSqlString(PostgresqlSqlDialect.DEFAULT)
    .getSql();
</code></pre>
<p>Partial interception of the code defined by the rule set：</p>
<pre class=""lang-java prettyprint-override""><code>    public static void addDefaultRules(HepProgramBuilder hp) {
        hp.addMatchOrder(org.apache.calcite.plan.hep.HepMatchOrder.TOP_DOWN);

        // Filter相关
        hp.addRuleInstance(CoreRules.FILTER_MERGE);
        hp.addRuleInstance(CoreRules.FILTER_MULTI_JOIN_MERGE);
        hp.addRuleInstance(CoreRules.FILTER_REDUCE_EXPRESSIONS);
        hp.addRuleInstance(CoreRules.FILTER_INTO_JOIN);
        ...
</code></pre>
<p>It should be noted that what I want to achieve in my complete code is to read SQL, rewrite it using calcite, obtain the rule name used in the rewrite, and then use postgreSQL to actually execute the SQL before and after the rewrite to obtain the actual execution time, and record the SQL before and after the rewrite, execution time, rule name, logical plan and other information.
The SQLI use is in a csv file, which is from dsb.</p>
<pre class=""lang-sql prettyprint-override""><code>select min(item1.i_item_sk), min(item2.i_item_sk), min(s1.ss_ticket_number), min(s1.ss_item_sk) 
FROM item AS item1, item AS item2, store_sales AS s1, store_sales AS s2, date_dim, customer, customer_address, customer_demographics 
WHERE item1.i_item_sk &lt; item2.i_item_sk 
  AND s1.ss_ticket_number = s2.ss_ticket_number 
  AND s1.ss_item_sk = item1.i_item_sk and s2.ss_item_sk = item2.i_item_sk 
  AND s1.ss_customer_sk = c_customer_sk 
  and c_current_addr_sk = ca_address_sk 
  and c_current_cdemo_sk = cd_demo_sk 
  AND d_year between 2000 and 2000 + 1 
  and d_date_sk = s1.ss_sold_date_sk 
  and item1.i_category in ('Books', 'Children') 
  and item2.i_manager_id between 6 and 25 
  and cd_marital_status = 'M' 
  and cd_education_status = 'Advanced Degree' 
  and s1.ss_list_price between 87 and 101 
  and s2.ss_list_price between 87 and 101  ;
</code></pre>
",0,2,2,2025-10-29T10:49:34+00:00,0,70,False
79803727,9573694,,sql,Avoiding duckdb OutOfRangeException when multiplying Decimals,"<p>I'm working with DuckDB and have several client-provided SQL expressions that use <code>DECIMAL(38,10)</code> columns (fixed precision with 10 digits after the decimal point).</p>
<p>For example:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT S1__AMOUNT * S1__PRICE * S1__UNITS * 1000
</code></pre>
<p>All columns like <code>S1__AMOUNT</code>, <code>S1__PRICE</code>, etc. are <code>DECIMAL(38,10)</code>.</p>
<p>When multiplying several of these columns (especially three or more) and then multiplying by a constant (e.g. * 1000), I get:</p>
<pre class=""lang-none prettyprint-override""><code>duckdb.duckdb.OutOfRangeException: Out of Range Error: 
Overflow in multiplication of DECIMAL(38) (194586756000000000000000000000000000 * 1000).
You might want to add an explicit cast to a decimal with a smaller scale.
</code></pre>
<p><strong>Limitations:</strong></p>
<p>I can not cast to DOUBLE, but after each operation it is allowed to cast its result to <code>decimal(38,10)</code></p>
<p>I know that I can manually rewrite SQL clauses to cast after each step like:</p>
<pre class=""lang-sql prettyprint-override""><code>CAST(
  CAST(
    CAST(S1__AMOUNT * S1__PRICE AS DECIMAL(38,10))
    * S1__UNITS AS DECIMAL(38,10)
  )
  * 1000 AS DECIMAL(38,10)
)
</code></pre>
<p>But since SQL clauses are written by client and can be pretty random I do not want to use this clumsy and bug prone way.</p>
<p><strong>The question:</strong></p>
<p>Is there any way to configure DuckDB so that it:</p>
<ul>
<li>Automatically reduces intermediate decimal scale/precision if needed to fit inside <code>DECIMAL(38,10)</code>, without throwing an OutOfRangeException, or</li>
<li>Automatically casts intermediate arithmetic results back to a safe <code>DECIMAL(38,10)</code>, or</li>
<li>Provides an expression/function to safely multiply with overflow-safe decimal promotion?</li>
</ul>
<p>If not, is the only reliable approach to rewrite all expressions and insert explicit casts after every multiplication/division?</p>
<p><strong>Exact code to reproduce the problem:</strong></p>
<pre class=""lang-py prettyprint-override""><code>import duckdb
import polars as pl

df = pl.DataFrame({&quot;AMOUNT&quot;: 8760, &quot;PRICE&quot;: 22.2131, &quot;RATE&quot;: 1})
df = df.cast(pl.Decimal(scale=10))

result = duckdb.sql(&quot;&quot;&quot;
FROM df
SELECT AMOUNT * PRICE * RATE * 1000
&quot;&quot;&quot;).pl()

print(result)
</code></pre>
",1,1,0,2025-10-29T11:26:12+00:00,1,167,True
79803969,5304058,San fransisco,sql,Join on one column and if null join another column,"<p>I need some help handling null columns in joins. I have three tables (below are just examples) <code>Map</code>, <code>Employee</code> and <code>Region</code>. Primary table is <code>Map</code> where I join <code>Employee</code> table on the <code>MapID</code> column to get <code>employeeID</code> and <code>Employee</code> table is used to join with <code>Region</code> table on <code>employeeID</code>.</p>
<p>Requirement: for any given <code>mapID</code>, check if there are any region for <code>EmployeeID</code>. If there is a match, pick it. Only if <code>EmployeeID</code> is null, then check for zip5 and pick the region.</p>
<p>I tried the code shown here; for <code>mapID = 7890</code>, it returns region for both <code>EmployeeID</code> Null and 200. For this <code>mapid</code>, there exists <code>employeeID = 200</code>, and only this value should be returned, ignoring the other.</p>
<p>For <code>mapID = 4567</code>, <code>employeeID</code> is null and correct value is being picked.</p>
<p>Can anyone tell me what my mistake is?</p>
<p>Expected output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>mapid</th>
<th>Prodcode</th>
<th>proddesc</th>
<th>amount</th>
<th>zip5</th>
<th>region</th>
</tr>
</thead>
<tbody>
<tr>
<td>7890</td>
<td>23458</td>
<td>POT</td>
<td>1234789</td>
<td>45678</td>
<td>West S San Diago</td>
</tr>
</tbody>
</table></div>
<p>Actual output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>mapid</th>
<th>Prodcode</th>
<th>proddesc</th>
<th>amount</th>
<th>zip5</th>
<th>region</th>
</tr>
</thead>
<tbody>
<tr>
<td>7890</td>
<td>23458</td>
<td>POT</td>
<td>1234789</td>
<td>45678</td>
<td>West S San Diago</td>
</tr>
<tr>
<td>45678</td>
<td>7890</td>
<td>23458</td>
<td>POT</td>
<td>1234789</td>
<td>West So. CA</td>
</tr>
</tbody>
</table></div>
<pre><code>DROP TABLE IF EXISTS #map

CREATE TABLE #map
(
    zip5 varchar(10),
    mapid varchar(10),
    Prodcode int,
    proddesc varchar(10),
    amount float
)

DROP TABLE IF EXISTS #employee

CREATE TABLE #employee
(
    EmployeeID varchar(10),
    mapid varchar(10)
)

DROP TABLE IF EXISTS #Region

CREATE TABLE #Region
(
    mapid varchar(10),
    EmployeeID varchar(10),
    zip5 varchar(10),
    Region varchar(50)
)

INSERT INTO #map
    SELECT '04987', '9879', 24567, 'ISC', '17645.00'
    UNION
    SELECT '45678', '7890', 23458, 'POT', '1234789.00'
    UNION
    SELECT '56333', '5678', 24567, 'MHT', '23400.00'
    UNION
    SELECT '00899', '4567', 24567, 'PIT', '1234.00'
    UNION
    SELECT '00899', '3457', 24567, 'ISC', '17645.00'

INSERT INTO #employee
    SELECT '100', '9879'
    UNION
    SELECT '200', '7890'
    UNION
    SELECT '400', '5678'
    UNION
    SELECT NULL, '4567'
    UNION
    SELECT '500', '3457'
 

INSERT INTO #Region
    SELECT '9879', '100', '04987', 'South'
    UNION
    SELECT '7890', '200', '45678', 'West S San Diago'
    UNION
    SELECT '7890', NULL, '45678', 'West So. CA'
    UNION
    SELECT '5678', '400', '56333', 'EastCentral'
    UNION
    SELECT '4567', NULL, '00899', 'south'

SELECT * FROM #employee WHERE mapid = '7890'

SELECT * FROM #map WHERE mapid = '7890'

SELECT * FROM #Region WHERE mapid = '7890'

SELECT DISTINCT
    m.*, region 
FROM
    #map m
LEFT JOIN
    #Region r ON r.mapid = m.mapid
LEFT JOIN
    #employee e ON e.mapid = m.mapid
                AND r.employeeid = ISNULL(e.employeeid, '') 
                 OR (r.employeeid &lt;&gt; ISNULL(e.employeeid, '') 
                     AND r.zip5 = m.zip5)
WHERE
    m.mapid = '7890'
</code></pre>
<p>(<a href=""https://dbfiddle.uk/j-wRZXVl"" rel=""nofollow noreferrer""><code>db&lt;&gt;fiddle</code> with the whole case</a>)</p>
",2,3,1,2025-10-29T15:35:04+00:00,2,209,True
79804204,522663,"Salt Lake City, UT",sql,Compare the results of Boolean expressions,"<p>Is there any way to compare the results of Boolean expressions as in the following query? This expression gives the error <code>Incorrect syntax near '='.</code> at the first <code>'='</code>.</p>
<pre><code>select *
from Transfers
where (ToTruckId is null) = (ToTruckProductId is null)
    and (FromTruckId is null) = (FromTruckProductId is null)
</code></pre>
",0,1,1,2025-10-29T20:45:40+00:00,1,162,True
79804271,13667729,,sql,Performance issue with ST_CONTAINS,"<p>I have a performance issue with a query in MySQL. I need to compare location data and am trying to use ST_CONTAINS in a join. However, I am having a performance issue as it is quite slow, taking around 16 seconds to retrieve 1,000 records. I need to process several thousand records in the query. I am searching the internet for the best solution. I have already tried using other checking functions, ST_Within and MBRContains. I added a spatial index for the geo column, setting the same SRID, but unfortunately the index did not want to use itself. I forced its use with USE INDEX, but this made the query even slower. I created a separate point column to get rid of Point(address.lat, address.lng), but to no avail. Is there anything else I can do? Has anyone encountered this problem or has any ideas for a solution, or can point out what I am doing wrong? Additionally, I am including the explain queries.</p>
<p>Time execute: 12s</p>
<pre><code>SELECT * FROM table_localities AS address
LEFT JOIN table_province AS province ON (ST_CONTAINS(province.geo, Point(address.lat, address.lng)))
WHERE province.delivery_area_id = :id
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>partitions</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>filtered</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>address</td>
<td></td>
<td>ALL</td>
<td>fk_delivery_area_id</td>
<td></td>
<td></td>
<td></td>
<td>1850</td>
<td>50.0</td>
<td>Using where</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>region</td>
<td></td>
<td>ref</td>
<td>fk_delivery_area_id,idx_geo</td>
<td>fk_delivery_area_id</td>
<td>8</td>
<td>const</td>
<td>300</td>
<td>100.0</td>
<td>Using where</td>
</tr>
</tbody>
</table></div>
<p>Time execute: much more than 16s</p>
<pre><code>SELECT * FROM table_localities AS address
LEFT JOIN table_province AS province USE INDEX (idx_geo) ON (ST_CONTAINS(province.geo, Point(address.lat, address.lng)))
WHERE province.delivery_area_id = :id
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>partitions</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>filtered</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>address</td>
<td></td>
<td>ALL</td>
<td>fk_delivery_area_id</td>
<td></td>
<td></td>
<td></td>
<td>1850</td>
<td>50.0</td>
<td>Using where</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>region</td>
<td></td>
<td>ALL</td>
<td>idx_geo</td>
<td></td>
<td></td>
<td></td>
<td>21699</td>
<td>100.0</td>
<td>Range checked for each record (index map: 0x20)</td>
</tr>
</tbody>
</table></div>
",2,2,0,2025-10-29T22:29:22+00:00,0,152,False
79804286,7491096,,sql,How can I retrieve data between certain times?,"<p>My data decoded from a radio receiver is stored in tables <code>Flights</code> and <code>Aircraft</code> with a common field <code>AircraftID</code> to enable a <code>JOIN</code>. There are fields for when the radio signal starts and ends (<code>StartTime</code> and <code>Endtime</code>). To select data between two points in time and anything transmitted during that time, this query works:</p>
<pre><code>SELECT ModeS, Registration, Manufacturer, Type, SerialNo, RegisteredOwners, UserTag, StartTime, EndTime, Callsign, FirstSquawk, LastSquawk, FirstIsOnGround, LastIsOnGround, FirstLat, LastLat, FirstLon, LastLon, FirstGroundSpeed, LastGroundSpeed, FirstAltitude, LastAltitude
FROM Flights
INNER JOIN Aircraft ON Aircraft.AircraftID = Flights.AircraftID
WHERE StartTime &gt; '2025-10-24 23:00' AND EndTime &lt; '2025-10-26 01:00';
</code></pre>
<p>I need to replace the last line so that it becomes less specific and I don't have to manually change the date every day (it runs once a day in the morning, to report for 24 hours of yesterday's data + 1 hour before and 1 hour after).<br />
Pseudo-code:</p>
<pre><code>WHERE StartTime &gt; 23:00 the day before yesterday, until EndTime &lt; 01:00 today
</code></pre>
<p>Attempt based on an example I found:</p>
<pre><code>WHERE StartTime &gt; DATE('now','-25 hours') AND EndTime &lt; DATE('now','+1 hour);
</code></pre>
<p>It would be easier if there weren't two different time fields involved.</p>
",0,0,0,2025-10-29T23:03:41+00:00,1,128,True
79804329,14033292,,sql,Conditional Join in SQL query,"<p>I need a single SQL query that would satisfy the below criteria.</p>
<p>I have 2 tables, say <code>Table1</code> with these values:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ID</th>
<th>Code</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td>1000</td>
<td>abc</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td>1000</td>
<td>pqr</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td>1000</td>
<td>efg</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td>2300</td>
<td>rst</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td>2300</td>
<td>uvw</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td>2930</td>
<td>xyz</td>
</tr>
<tr>
<td style=""text-align: left;"">7</td>
<td>3500</td>
<td>aaa</td>
</tr>
</tbody>
</table></div>
<p>And I have another table <code>Table2</code> with these values:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>FromCode</th>
<th>ToCode</th>
<th>IsExcluded</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1000</td>
<td>1000</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>2000</td>
<td>2999</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>2930</td>
<td>2930</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>I need a single query that shall give me the results like:</p>
<pre><code>If exists (Select 1 From Table2 where IsExcluded = 0)
    -- Get the records that match the criteria
    Select * 
    From table1 t1
    Inner Join table2 t3 on (t1.Code between t3.FromCode and t3.ToCode) and t3.IsExcluded = 0
    Left Join table2 t2 on (t1.Code between t2.FromCode and t2.ToCode ) and t2.IsExcluded = 1
    Where t2.ID is Null
Else
    Select * 
    From table1 t1
    Left Join table2 t2 on (t1.Code between t2.FromCode and  t2.ToCode) and t2.IsExcluded = 1  
    Where t2.ID is Null
</code></pre>
<p>So, basically return rows from <code>Table1</code> that have:</p>
<ul>
<li>all the matching rows when <code>IsExcluded = 0</code>. This should be true only if rows exists in <code>Table2</code> with condition <code>isExcluded = 0</code>, otherwise return all the rows from <code>Table1</code></li>
<li>exclude those rows from <code>Table1</code> where <code>IsExcluded = 1</code>. Again, here also if rows exists with the condition <code>IsExcluded = 1</code>, then exclude the rows that meet the condition, otherwise return all the rows. This has been taken care of using left join with <code>IsExcluded = 1</code></li>
<li>please note that rows may or may not exists in <code>Table2</code>. If they do not exists, then return all the rows from <code>Table1</code></li>
</ul>
<p>In other words, return the rows from <code>Table 1</code> that satisfy the ranges defined in <code>Table 2</code> when <code>IsExcluded = 0</code>, and ignore rows with condition <code>IsExcluded = 1</code>. If there is no condition (i.e. no rows are present in <code>Table 2</code>), then display all the rows from <code>Table 1</code>.</p>
<p>Any help would be greatly appreciated.</p>
<p>Thanks.</p>
",0,1,1,2025-10-30T01:24:51+00:00,7,355,True
79805212,521586,,sql,Problem updating Postgres ENUM from DuckDB,"<p>I'm on DuckDB 1.4.1 experiencing difficulty updating a Postgres 17.6 ENUM field status:</p>
<pre><code>CREATE TYPE mystatus_enum AS ENUM (
    'IN_STOCK', 'OUT_OF_STOCK', 'NOT_FOUND', 'NOT_A_PRODUCT'
);

CREATE TABLE mytable
(
    id INTEGER primary key,
    status mystatus_enum
);
</code></pre>
<p>All of the following give me:</p>
<blockquote>
<p>Not implemented Error: Enums in Postgres must be named - unnamed enums are not supported. Use CREATE TYPE to create a named enum.</p>
</blockquote>
<p>Making a similar update from within Postgres without DuckDB is not a problem.</p>
<p>Both for DuckDB sources:</p>
<pre><code>update mypg.mytable set status=ddb.status where ddb.id=mypg.mytable.id;
update mypg.mytable set status=ddb.status::varchar where ddb.id=mypg.mytable.id;
update mypg.mytable set status=ddb.status::text where ddb.id=mypg.mytable.id;
update mypg.mytable set status=ddb.status::mypg.mystatus_enum where ddb.id=mypg.mytable.id;
</code></pre>
<p>and PG sources:</p>
<pre><code>update mypg.mytable set status=mypg.mytable2.status where mypg.mytable2.id=mypg.mytable.id;
</code></pre>
<p>and as above but with PG sources.</p>
<p>I've also tried using an equivalent ENUM declared in DuckDB.</p>
<p>Any suggestions for how I can avoid this error?
I'd like to avoid my last resort of switching the Postgres field from ENUM to VARCHAR as it is a very large and active table.</p>
",1,1,0,2025-10-30T20:57:38+00:00,0,113,False
79805913,10852841,,sql,Grouping by and making flag based on patterns,"<p>I have a table <code>p</code> that looks like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Col1</th>
</tr>
</thead>
<tbody>
<tr>
<td>AAA</td>
<td>kddd</td>
</tr>
<tr>
<td>AAA</td>
<td>13bd</td>
</tr>
<tr>
<td>AAA</td>
<td>14cd</td>
</tr>
<tr>
<td>AAA</td>
<td>15cd</td>
</tr>
<tr>
<td>BBB</td>
<td>15cd</td>
</tr>
<tr>
<td>BBB</td>
<td>23fd</td>
</tr>
<tr>
<td>BBB</td>
<td>4rre</td>
</tr>
<tr>
<td>BBB</td>
<td>tr3e</td>
</tr>
<tr>
<td>CCC</td>
<td>kddd</td>
</tr>
<tr>
<td>CCC</td>
<td>12ed</td>
</tr>
<tr>
<td>DDD</td>
<td>rrr4</td>
</tr>
<tr>
<td>DDD</td>
<td>rtt4</td>
</tr>
<tr>
<td>DDD</td>
<td>rrt4</td>
</tr>
</tbody>
</table></div>
<p>I have three lists of patterns that classify each group based on the values matching in Col1.</p>
<ol>
<li>If the codes are like ('_ddd', '_ccc', '_bbb', '_aaa') then return 'b'</li>
<li>If the codes are like ('_3c_', '_3b_', '_3a_') then return 'S'</li>
<li>If the codes are like ('_5c_', '_5b_', '_5a_') then return 'U'</li>
<li>If none of the codes match then return 'U'</li>
</ol>
<p>The patterns are much longer so I made temporary tables to store and call them</p>
<pre><code>CREATE OR REPLACE TEMPORARY TABLE b_codes (value VARCHAR(4));
INSERT INTO b_codes (value) VALUES ('_ddd'), ('_ccc'), ('_bbb'), ('_aaa');
</code></pre>
<p>I did the same for <code>s_codes</code> and <code>u_codes</code>.</p>
<p>From the codes, if an ID contains none of the codes then mark 'U'.
If an ID has any <code>u_codes</code> then mark 'U' if no <code>s_codes</code> or <code>b_codes</code> are present. If an ID has any <code>b_codes</code>, then mark as 'b'. If there are <code>u_codes</code> and <code>s_codes</code> mark 'S'.</p>
<p>The resulting table should look like</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Col1</th>
</tr>
</thead>
<tbody>
<tr>
<td>AAA</td>
<td>S</td>
</tr>
<tr>
<td>BBB</td>
<td>U</td>
</tr>
<tr>
<td>CCC</td>
<td>b</td>
</tr>
<tr>
<td>DDD</td>
<td>U</td>
</tr>
</tbody>
</table></div>
<p>My attempt</p>
<pre><code>SELECT ID, MAX(t.Flag) AS Flag
FROM (
   SELECT 
     ID,
     CASE
       WHEN (p.Col1 LIKE ANY (SELECT value FROM u_codes) AND
         NOT (
              p.Col1 LIKE ANY (SELECT value FROM s_codes) OR
              p.Col1 LIKE ANY (SELECT value FROM b_codes)
         ) THEN 'U'
       WHEN (p.Col1 LIKE ANY (SELECT value FROM s_codes) AND
         NOT (
              p.Col1 LIKE ANY (SELECT value FROM u_codes) OR
              p.Col1 LIKE ANY (SELECT value FROM b_codes)
         ) THEN 'S'
       WHEN (p.Col1 LIKE ANY (SELECT value FROM b_codes) THEN 'b'
       WHEN (
         NOT p.Col1 LIKE ANY (SELECT value FROM u_codes) AND
         NOT p.Col1 LIKE ANY (SELECT value FROM s_codes) AND
         NOT p.Col1 LIKE ANY (SELECT value FROM b_codes)
         ) THEN NULL
       ELSE NULL
     END AS Flag

) AS t
GROUP BY ID;
</code></pre>
<p>The sub-query should return</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Col1</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td>AAA</td>
<td>kddd</td>
<td>b</td>
</tr>
<tr>
<td>AAA</td>
<td>13bd</td>
<td>S</td>
</tr>
<tr>
<td>AAA</td>
<td>14cd</td>
<td>NULL</td>
</tr>
<tr>
<td>AAA</td>
<td>15cd</td>
<td>U</td>
</tr>
<tr>
<td>BBB</td>
<td>15cd</td>
<td>U</td>
</tr>
<tr>
<td>BBB</td>
<td>23fd</td>
<td>NULL</td>
</tr>
<tr>
<td>BBB</td>
<td>4rre</td>
<td>NULL</td>
</tr>
<tr>
<td>BBB</td>
<td>tr3e</td>
<td>NULL</td>
</tr>
<tr>
<td>CCC</td>
<td>kddd</td>
<td>b</td>
</tr>
<tr>
<td>CCC</td>
<td>12ed</td>
<td>NULL</td>
</tr>
<tr>
<td>DDD</td>
<td>rrr4</td>
<td>NULL</td>
</tr>
<tr>
<td>DDD</td>
<td>rtt4</td>
<td>NULL</td>
</tr>
<tr>
<td>DDD</td>
<td>rrt4</td>
<td>NULL</td>
</tr>
</tbody>
</table></div>
<p>I tried using Snowflake's lexicographical ordering in the MAX function, but I don't think that works. What would be a better way to get the correct labels in the MAX function?</p>
",2,3,1,2025-10-31T14:55:28+00:00,4,146,True
79806051,24472210,,sql,How do I write a query that pulls 0 minutes over time dynamically?,"<p>I have this data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Client_Id</th>
<th>VisitDate</th>
<th>Mins</th>
<th>period_Id</th>
<th>program_Id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2025-01-01</td>
<td>3</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>1</td>
<td>2025-01-02</td>
<td>0</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>1</td>
<td>2025-01-03</td>
<td>45</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>2</td>
<td>2025-01-01</td>
<td>0</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>2</td>
<td>2025-01-02</td>
<td>0</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>2</td>
<td>2025-01-03</td>
<td>15</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>3</td>
<td>2025-01-01</td>
<td>120</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>3</td>
<td>2025-01-02</td>
<td>90</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>3</td>
<td>2025-01-03</td>
<td>0</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>4</td>
<td>2025-01-01</td>
<td>20</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>4</td>
<td>2025-01-02</td>
<td>25</td>
<td>600</td>
<td>T</td>
</tr>
<tr>
<td>4</td>
<td>2025-01-03</td>
<td>30</td>
<td>600</td>
<td>T</td>
</tr>
</tbody>
</table></div>
<p>Query #1:</p>
<pre><code>select 
    client_Id,
    period_Id,
    program_Id,
    visitdate,
    sum(mins) as minutes 
from 
    ClientMinutes
where 
    visitdate between '2025-01-01' and '2025-01-03'
group by 
    client_Id, period_id, program_Id, visitdate
having 
    sum(mins) = 0
</code></pre>
<p>This query returns this data set:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>client_Id</th>
<th>period_Id</th>
<th>program_Id</th>
<th>visitdate</th>
<th>minutes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>600</td>
<td>T</td>
<td>2025-01-02</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>600</td>
<td>T</td>
<td>2025-01-01</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>600</td>
<td>T</td>
<td>2025-01-02</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>600</td>
<td>T</td>
<td>2025-01-03</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>Per the query 1#, <code>client_Id = 2</code> has zero minutes only during that dataframe, but that's not true because they had 15 minutes during that dateframe.</p>
<p>This query below does what I want which is none of the clients had 'true' zero minutes within that dateframe. but the problem I am facing is that this query will be used to create a report in a tool like power bi. How will  the end user utilize the date filter if it's only in the where clause. Is there a way to write this in such a way that the end user can utilize the date filter. If i include the visitdate as part of the columns I will have to do a group by which is what I want to avoid:</p>
<pre><code>select 
    client_Id,
    period_Id,
    program_Id,
    sum(mins) as minutes 
from 
    ClientMinutes
where 
    visitdate between '2025-01-01' and '2025-01-03'
group by 
    client_Id, period_id, program_Id
having 
    sum(mins) = 0
</code></pre>
",0,1,1,2025-10-31T17:14:12+00:00,2,172,False
79806384,4451315,Quox,sql,SQLGlot: how to raise on unsupported write features?,"<p>If I do</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; sqlglot.transpile('select mean(a), b from df group by all', read='duckdb', write='postgres', error_level=sqlglot.errors.ErrorLevel.WARN)
['SELECT MEAN(a), b FROM df GROUP BY ALL']
</code></pre>
<p>then I get no error or warning, even though group by all isn't valid in postgres SQL</p>
<p>How can I transpile from DuckDB SQL to Postgres SQL (or any other dialect), in such a way that it will raise or warn if it encounters something not valid in the target dialect?</p>
",0,1,1,2025-11-01T07:34:49+00:00,0,65,False
79806960,20135187,,sql,Pyspark SQL: How to do GROUP BY with specific WHERE condition,"<p>So I am doing some SQL aggregation transformations of a dataset and there is a certain condition that I would like to do, but not sure how.</p>
<p>Here is a basic code block:</p>
<pre><code>le_test = spark.sql(&quot;&quot;&quot;
SELECT Country,
ROUND(MIN(Life_Expectancy)) AS Min_LE,
ROUND(AVG(Life_Expectancy)) AS Avg_LE,
ROUND(MAX(Life_Expectancy)) AS Max_LE,
ROUND(MAX(Life_Expectancy) - MIN(Life_Expectancy)) AS LE_range
FROM le_cleaned
GROUP BY Country
&quot;&quot;&quot;)
</code></pre>
<p>This dataset has countries split up by years (over the course of about two decades), hence the aggregation.  What I want to do is an additional column based on the difference in values between the first year row and last year row for each country, which is not the same as min and max values.  So basically, the line should resemble something like this:</p>
<pre><code>ROUND((Life_Expectancy WHERE &quot;YEAR&quot; = 2019) - (Life_Expectancy WHERE &quot;YEAR&quot; = 2000)) AS LE_difference
</code></pre>
<p>There is a long way to do this, but I assume there must be a short way if you can already do the same calculation with a min and max values in a groupby.</p>
",0,0,1,2025-11-02T06:39:33+00:00,6,163,True
79807261,31801658,,sql,ERROR 1064 (42000): You have an error in your SQL syntax; Getting error for adding foreign key after creating table,"<p>I created 2 tables students and course, in course I want to add <code>student_id</code> as foreign key but I am adding the key after creating the table and it is showing error. Need help in solving this thing.</p>
<pre><code>mysql&gt; create table students
(student_id INT PRIMARY KEY, 
first_name VARCHAR(60) NOT NULL, 
last_name VARCHAR(60) NOT NULL, 
email VARCHAR(100));
Query OK, 0 rows affected (0.02 sec)
</code></pre>
<pre><code>mysql&gt; INSERT INTO students (student_id, first_name, last_name, email) 
`VALUES (1, 'ana', 'smith', 'anasmith@gmail.com'),
(2, 'ben', 'brown', 'benbrown@gmail.com'), 
(3, 'cirus', 'smith', 'cirussmith@gmail.com');
Query OK, 3 rows affected (0.01 sec)
Records: 3  Duplicates: 0  Warnings: 0`
</code></pre>
<p><strong>Method1</strong></p>
<pre><code>`mysql&gt; ALTER TABLE course FOREIGN KEY (student_id) REFERENCES students(student_id);`
</code></pre>
<blockquote>
<p>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FOREIGN KEY (student_id) REFERENCES students(student_id)' at line 1</p>
</blockquote>
<p><strong>Method2</strong>
(I added ` on columns )</p>
<pre><code>`mysql&gt; ALTER TABLE course FOREIGN KEY (`student_id`) REFERENCES students(`student_id`);`
</code></pre>
<blockquote>
<p>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FOREIGN KEY (<code>student_id</code>) REFERENCES students(<code>student_id</code>)' at line 1</p>
</blockquote>
",-8,0,8,2025-11-02T16:56:43+00:00,1,123,True
79807277,31571439,,sql,How to improve query performance on large time-series container in GridDB Cloud?,"<p>I’m using GridDB Cloud (Free Tier) with Python to store time-series IoT data.<br />
My container currently has around 10 million rows, and it continues to grow daily.</p>
<p><strong>Schema:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>device_id STRING,
created_at TIMESTAMP,
temperature DOUBLE,
humidity DOUBLE
</code></pre>
<p>I frequently need to run queries like this to retrieve one day of data for a specific device:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM sensor_data
WHERE device_id = 'A123'
AND created_at BETWEEN TIMESTAMP('2025-09-25T00:00:00Z')
                    AND TIMESTAMP('2025-09-26T00:00:00Z');
</code></pre>
<p>However, this query now takes around <strong>3–5 seconds per device</strong>, and I’m concerned it will become slower as the dataset grows.</p>
<p><strong>What I’ve tried:</strong></p>
<ul>
<li>Partitioning the data into multiple containers (one per day) → still slow when querying across multiple days</li>
<li>Using a time-series container with <code>ROWKEY</code> on <code>created_at</code></li>
<li>Fetching all rows in Python and filtering locally → too inefficient for millions of rows</li>
</ul>
",2,3,1,2025-11-02T17:40:13+00:00,1,118,False
79807577,1159764,"Melbourne, Australia",sql,Sort aggregated query results by two methods simultaneously,"<p>I need to sort a query's results by two methods at the same time.
I want the first 3 records (returned) to be based on their prevalence in another table
And then I want the rest of the results sorted alphabetically.</p>
<p>Assuming I have 6 records in a query result set....</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>empl_Type</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>10</td>
</tr>
<tr>
<td>B</td>
<td>5</td>
</tr>
<tr>
<td>C</td>
<td>2</td>
</tr>
<tr>
<td>D</td>
<td>1</td>
</tr>
<tr>
<td>E</td>
<td>1</td>
</tr>
<tr>
<td>F</td>
<td>20</td>
</tr>
</tbody>
</table></div>
<p>then results should be
<code>F</code>,<code>A</code>,<code>B</code>,<code>C</code>,<code>D</code>,<code>E</code></p>
<ul>
<li><code>F</code>,<code>A</code>,<code>B</code> sorted by their count in another table</li>
<li><code>C</code>,<code>D</code>,<code>E</code> (remainder of rows) sorted alphabetically</li>
</ul>
<p>I have the first part working in the following contrived example:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
    et.id,
    et.employeetype_description,
    count(e.employeetypeid) as thesortorder
FROM 
    employeetype et
LEFT JOIN
    employee e ON e.employeetypeid = et.id
GROUP BY
    et.id,
    et.employeetype_description
ORDER BY thesortorder DESC
</code></pre>
<p>And this is where (hopefully you come in...)
How do I meet the rest of the requirements?
Thanks</p>
",5,5,0,2025-11-03T06:12:30+00:00,4,249,True
79807775,8947333,,sql,Why does appending data with PySpark raise a &quot;SQLServerException: CREATE TABLE permission denied&quot; exception?,"<p>In my Databricks cluster I'm trying to write a DataFrame to my table with the following code:</p>
<p><code>df.write.jdbc(url=JDBCURL, table=table_name, mode=&quot;append&quot;)</code></p>
<p>And this line fails with</p>
<blockquote>
<p>Py4JJavaError: An error occurred while calling o53884.jdbc.
:</p>
<p>com.microsoft.sqlserver.jdbc.SQLServerException: CREATE TABLE permission denied in database '[REDACTED]'.
at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1676)</p>
</blockquote>
<p>What I don't understand is, I'm not trying to <strong>create</strong> a table, I'm trying to <strong>append</strong> data to a table that <strong>already exists</strong>.</p>
<p>(<strong>EDIT:</strong> The table did not, in fact, exist.)</p>
<p>In this example, the user I specified in <code>JDBCURL</code> has SELECT, INSERT, UPDATE and DELETE rights on the destination schema.</p>
",3,3,0,2025-11-03T10:53:58+00:00,1,149,True
79808347,920731,,sql,MySQL query to calculate based on 2 sets of data,"<p>I have a MySQL database that I have the following type of data records. It's a little more in depth than this but its good enough for the sample question.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Ticket Number</th>
<th>Event (Start or Stop)</th>
<th>Location</th>
<th>Date</th>
</tr>
</thead>
</table></div>
<p>For each Ticket Number there will be 2 records (which represent a start location and an end location) There might be many records in the database.</p>
<p>Here is what I'd like to accomplish:</p>
<ul>
<li>Search for all records on a specific date.</li>
<li>Loop through the records and for every Stop record, I would need to locate the associated Ticket Number record that has the event Start.</li>
<li>Then I will do some calculations or display some info based on the Start and Stop information.</li>
</ul>
<p>As I am thinking about this I might have set the database up incorrectly. Where I have separate rows for each ticket &amp; start - stop entry I might (or should have made it 1 entry and update the start record with the stop data when I get it.</p>
<p>I am doing this in PHP also.</p>
",0,0,0,2025-11-03T22:01:47+00:00,6,146,True
79808530,31802849,,sql,Calculate a running highest streak,"<p>I am working on a gaps &amp; islands problem in MS SQL Server. I'm doing a daily challenge, and the table is a list of days I accomplished the task and the challenge day (calendar days since the first day). The table does not include days I didn't do the task.</p>
<p>I have created a column with the current streak for each day. Next, I want to know on which days I've beaten the previous record streak, so I want to add a column with the highest streak to date. If the current streak is reset by missing a day, I want the <code>highest_streak</code> column to show the previous streak record until it's broken. If I am on a record streak, the <code>highest_streak</code> needs to count up with the <code>current_streak</code>.</p>
<p>I cannot figure out how to do that. The best I've gotten is showing the total of the current streak.</p>
<p>Desired output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>challenge_day</th>
<th>current_streak</th>
<th>highest_streak</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-10-14</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2025-10-15</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025-10-16</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>2025-10-17</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025-10-18</td>
<td>5</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-20</td>
<td>7</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-21</td>
<td>8</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-22</td>
<td>9</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-23</td>
<td>10</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-26</td>
<td>13</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-27</td>
<td>14</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-28</td>
<td>15</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-29</td>
<td>16</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-30</td>
<td>17</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-31</td>
<td>18</td>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>2025-11-01</td>
<td>19</td>
<td>7</td>
<td>7</td>
</tr>
</tbody>
</table></div>
<p>Here's my code:</p>
<pre class=""lang-sql prettyprint-override""><code>-- create table and insert values
DROP TABLE IF EXISTS tracker;

CREATE TABLE tracker
(
    date DATE NOT NULL,
    challenge_day INT NOT NULL,
);

INSERT INTO tracker (date, challenge_day)
VALUES (2025-10-14, 1),
       (2025-10-15, 2),
       (2025-10-16, 3),
       (2025-10-17, 4),
       (2025-10-18, 5),
       (2025-10-20, 7),
       (2025-10-21, 8),
       (2025-10-22, 9),
       (2025-10-23, 10),
       (2025-10-26, 13),
       (2025-10-27, 14),
       (2025-10-28, 15),
       (2025-10-29, 16),
       (2025-10-30, 17),
       (2025-10-31, 18),
       (2025-11-01, 19);
  
-- ranks dates
WITH dense_rank_cte AS 
(
    SELECT 
        *,
        DENSE_RANK () OVER(ORDER BY challenge_day) AS rank
    FROM
        tracker
), -- creates groups
group_cte AS 
(
    SELECT 
        *,
        challenge_day - rank AS groups
    FROM
        dense_rank_cte
), -- running total of each group, ie current streak
count_cte AS 
(
    SELECT 
        *,
        COUNT(*) OVER(PARTITION BY groups ORDER BY challenge_day) AS current_streak
    FROM
        group_cte
), -- total of each streak
streak_count AS 
(
    SELECT 
        groups, COUNT(*) AS streak_total
    FROM 
        group_cte
    GROUP BY 
        groups
), -- I want this to calculate the current highest streak, but I'm just getting the the total of the current streak
streak_total_cte AS 
(
    SELECT 
        c.date, c.challenge_day, c.current_streak, 
        GREATEST(current_streak, streak_total) AS highest_streak
    FROM
        count_cte c
    JOIN 
        streak_count s ON c.groups = s.groups
)
SELECT *
FROM streak_total_cte
ORDER BY date;
</code></pre>
<p>Current output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>challenge_day</th>
<th>current_streak</th>
<th>highest_streak</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-10-14</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-15</td>
<td>2</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-16</td>
<td>3</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-17</td>
<td>4</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-18</td>
<td>5</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>2025-10-20</td>
<td>7</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2025-10-21</td>
<td>8</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2025-10-22</td>
<td>9</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>2025-10-23</td>
<td>10</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>2025-10-26</td>
<td>13</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>2025-10-27</td>
<td>14</td>
<td>2</td>
<td>7</td>
</tr>
<tr>
<td>2025-10-28</td>
<td>15</td>
<td>3</td>
<td>7</td>
</tr>
<tr>
<td>2025-10-29</td>
<td>16</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>2025-10-30</td>
<td>17</td>
<td>5</td>
<td>7</td>
</tr>
<tr>
<td>2025-10-31</td>
<td>18</td>
<td>6</td>
<td>7</td>
</tr>
<tr>
<td>2025-11-01</td>
<td>19</td>
<td>7</td>
<td>7</td>
</tr>
</tbody>
</table></div>
<p>Thanks for any assistance.</p>
",6,6,0,2025-11-04T05:01:18+00:00,1,140,True
79808801,23370832,,sql,Create a trigger to update a column on a table after deleting an entry in another table,"<p>I'm making a very simple webpage for an animal shelter (school project) and I want to create a trigger where, if I delete a row in he 'adoptions' HTML table, it sets the situation of the adopted animal from 'adopted' to 'shelter'. Before adding the logic to the webpage, I'm doing tests in phpmyadmin and it says &quot;column id_animal in WHERE clause is unknwon&quot; and it doesn't work.</p>
<p>I want to match the id in &quot;id_animal&quot; to the id in the table &quot;animal&quot;</p>
<p>So far I've tried this:</p>
<pre><code>CREATE TRIGGER devolver_a_refugio2
AFTER DELETE ON adopciones
FOR EACH ROW
UPDATE animal SET situacion = 'refugio' WHERE id_animal = id;

--to delete just in cause

DROP TRIGGER IF EXISTS devolver_a_refugio2;

--my tables

Create table animal(
id INT AUTO_INCREMENT PRIMARY KEY,
foto VARCHAR (255),
nombre VARCHAR (50), 
especie ENUM ('perro', 'gato'),
raza VARCHAR (50),
sexo ENUM ('macho', 'hembra'),
cumple DATE,
ingreso DATE,
caso_especial ENUM ('ninguno','urgente', 'conjunta', 'tratamiento_medico', 'anciano') DEFAULT 'ninguno',
situacion ENUM ('refugio', 'acogida', 'adoptado') DEFAULT 'refugio',
descripcion TEXT);


CREATE TABLE adopciones (

id_adopcion INT AUTO_INCREMENT PRIMARY KEY,
fecha DATE DEFAULT CURRENT_DATE,
nombre_adoptante VARCHAR (255), 
primap_adoptante VARCHAR (255), 
segap_adoptante VARCHAR (255),
dni VARCHAR (9),
telefono VARCHAR(50),
direccion VARCHAR (255),
id_animal INT,
nombre_animal VARCHAR (50),
FOREIGN KEY (id_animal) REFERENCES animal(id));

</code></pre>
<p>What am I doing wrong? thank you in advance</p>
",4,5,1,2025-11-04T10:39:23+00:00,3,98,True
79809217,22000063,,sql,Equivalent to replace input date from User/API In PostgreSQL from Oracle,"<p>I am in the process of transitioning Oracle queries to PostgreSQL and I am trying to pass a date range to pull back the results.</p>
<p>The original Oracle query is below:</p>
<pre><code>select 
    iss_id
   ,date_created
from issues i 
where 
   trunc(i.date_created) 
   BETWEEN trunc(TO_DATE(:StartDt ,'mmddyyyy'))
    --                    ^^^^^^^
        AND trunc(TO_DATE(:EndDt ,'mmddyyyy'))  
    --                    ^^^^^^
</code></pre>
<p>My problem is with the <code>:StartDt</code> &amp; <code>:EndDt</code>, when this is run in Oravle Developer it will produce a popup box that will allow the user to input a date value for these two variables.  What I am looking for is an equivalent to this in PostgreSQL.</p>
<p>Any assistance to this would be greatly appreciated.</p>
",0,1,1,2025-11-04T17:35:52+00:00,1,93,True
79809632,31500636,,sql,How to find duplicate records in a SQL table based on multiple columns?,"<p>I'm performing a data quality check and need to find duplicates in a table using multiple columns (email, dob, and country). Here's my current query:</p>
<pre><code>SELECT email, dob, country, COUNT(*) AS cnt
FROM customers
GROUP BY email, dob, country
HAVING COUNT(*) &gt; 1;
</code></pre>
<p>It works, but I also want to return the id of each duplicate record. How can I modify this query to include all duplicate rows?</p>
",-3,3,6,2025-11-05T05:10:21+00:00,1,151,True
79809996,15232938,,sql,Go database/sql + godror: no error returned on Oracle ORA-56735 timeout from long-running query,"<p>Environment: Go 1.24.6, github.com/godror/godror v0.49.0, Oracle DB using database/sql DB.QueryContext (no explicit context timeout).</p>
<p>Problem:
I’m running a long-running query via database/sql with godror. The Oracle server enforces an elapsed time limit and logs:
<code>ORA-56735: elapsed time limit exceeded - call aborted</code></p>
<p>I expected QueryContext (or reading from rows) to return an error when the server aborts the call. Instead, I see neither rows nor an error from QueryContext.</p>
<p>Code snippet (will provide full working sample in comments):</p>
<pre><code>ctx := context.Background() // note: no explicit timeout here

rows, err := db.QueryContext(ctx, /* long-running query here */ SELECT /*+ some hints */ ..., namedParams...)
if err != nil {
    // I expected ORA-56735 here
    log.Fatalf(&quot;QueryContext error: %v&quot;, err)
}
defer rows.Close()
</code></pre>
<p>What I expected:
Either QueryContext returns an error, or rows.Next()/rows.Err() surfaces ORA-56735 when the server aborts the call.</p>
<p>What actually happens:
Oracle server logs show ORA-56735 (elapsed time limit exceeded – call aborted).</p>
<p>In the application: QueryContext returns rows without error, iteration returns no rows, and rows.Err() is nil.
Questions:</p>
<p>Is this a known behavior/bug with database/sql + godror where ORA-56735?</p>
",1,1,0,2025-11-05T11:51:03+00:00,1,133,False
79810483,13509293,,sql,For multiple records in first column with distinct value in second column,"<p>I have a SQL statement which works fine for a single value of <code>columnD</code>, but when I need data for multiple <code>columnD</code> values, it's not returning any data.</p>
<p>This is the working SQL statement:</p>
<pre><code>Select max(columnB), max(columnC), max(columnA)
From table
Where columnD = 123
Group by columnA
</code></pre>
<p>I need data for <code>columnD IN (123, 456, 789)</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ColA</th>
<th style=""text-align: center;"">ColumB</th>
<th style=""text-align: right;"">ColD</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">abcd</td>
<td style=""text-align: center;"">4675</td>
<td style=""text-align: right;"">123</td>
</tr>
<tr>
<td style=""text-align: left;"">pqrs</td>
<td style=""text-align: center;"">479</td>
<td style=""text-align: right;"">123</td>
</tr>
<tr>
<td style=""text-align: left;"">fghj</td>
<td style=""text-align: center;"">962</td>
<td style=""text-align: right;"">123</td>
</tr>
<tr>
<td style=""text-align: left;"">stu</td>
<td style=""text-align: center;"">279</td>
<td style=""text-align: right;"">123</td>
</tr>
<tr>
<td style=""text-align: left;"">abcd</td>
<td style=""text-align: center;"">7500</td>
<td style=""text-align: right;"">456</td>
</tr>
<tr>
<td style=""text-align: left;"">pqrs</td>
<td style=""text-align: center;"">443</td>
<td style=""text-align: right;"">456</td>
</tr>
<tr>
<td style=""text-align: left;"">fghj</td>
<td style=""text-align: center;"">925</td>
<td style=""text-align: right;"">456</td>
</tr>
<tr>
<td style=""text-align: left;"">stu</td>
<td style=""text-align: center;"">209</td>
<td style=""text-align: right;"">456</td>
</tr>
<tr>
<td style=""text-align: left;"">abcd</td>
<td style=""text-align: center;"">7900</td>
<td style=""text-align: right;"">789</td>
</tr>
<tr>
<td style=""text-align: left;"">pqrs</td>
<td style=""text-align: center;"">403</td>
<td style=""text-align: right;"">789</td>
</tr>
<tr>
<td style=""text-align: left;"">fghj</td>
<td style=""text-align: center;"">425</td>
<td style=""text-align: right;"">789</td>
</tr>
<tr>
<td style=""text-align: left;"">stu</td>
<td style=""text-align: center;"">900</td>
<td style=""text-align: right;"">789</td>
</tr>
</tbody>
</table></div>
",-1,0,1,2025-11-05T18:52:06+00:00,1,87,True
79810487,31821897,,sql,SQL Query Resulting in Multiple Rows,"<p>The database I'm working in is Snowflake. I also have to work through an interface and can't edit the SQL query directly. I'm trying to add a JOIN to a FROM clause that is connecting two schemas with 1 key that is shared between the two tables.</p>
<p>The goal is to get all of the information I am selecting to show in one row, the issue is that my query is causing the table to display up to four rows for the same personID.</p>
<p>DW.DTBL_PERSON</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Person_Key</th>
<th>Person_ID</th>
<th>Person_Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>4500</td>
<td>Person A</td>
</tr>
<tr>
<td>2</td>
<td>4501</td>
<td>Person B</td>
</tr>
<tr>
<td>3</td>
<td>4502</td>
<td>Person C</td>
</tr>
</tbody>
</table></div>
<p>REPORTING.MTBL_CONTACTS</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Person_Key</th>
<th>Contact_Priority_Order</th>
<th>Contact_First_Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>Anna</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>Steve</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Joseph</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>Mimi</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>Mitchell</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>Chris</td>
</tr>
</tbody>
</table></div>
<pre><code>SELECT DISTINCT
    dtbl_person.person_key as &quot;KEY&quot;,
        dtbl_person.person_name as &quot;Person Name&quot;,
    CASE 
            WHEN T2.CONTACT_PRIORITY_ORDER = 1 THEN T2.CONTACT_FIRST_NAME as &quot;First Contact First Name&quot;
    END,
    CASE 
            WHEN T2.CONTACT_PRIORITY_ORDER = 2 THEN T2.CONTACT_FIRST_NAME as &quot;Second Contact First Name&quot;
    END,
    CASE 
            WHEN T2.CONTACT_PRIORITY_ORDER = 3 THEN T2.CONTACT_FIRST_NAME as &quot;Third Contact First Name&quot;
    END

FROM
    DW.DTBL_PERSON
INNER JOIN 
        REPORTING.MTBL_CONTACTS AS T1 ON DTBL_PERSON.PERSON_KEY = T1.PERSON_KEY
INNER JOIN 
        REPORTING.MTBL_CONTACTS AS T2 ON CONCAT(T2.PERSON_KEY, T2.CONTACT_PRIORITY_ORDER) = CONCAT(T1.PERSON_KEY, T1.CONTACT_PRIORITY_ORDER)

ORDER BY
    dtbl_students.person_id
</code></pre>
<p>What it's doing</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>KEY</th>
<th>Person Name</th>
<th>First Contact First Name</th>
<th>Second Contact First Name</th>
<th>Third Contact First Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Person A</td>
<td></td>
<td>Steve</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>Person A</td>
<td>Anna</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Person B</td>
<td>Joseph</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Person C</td>
<td></td>
<td></td>
<td>Chris</td>
</tr>
<tr>
<td>3</td>
<td>Person C</td>
<td>Mimi</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Person C</td>
<td></td>
<td>Mitchell</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>What I want it to do</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>KEY</th>
<th>Person Name</th>
<th>First Contact First Name</th>
<th>Second Contact First Name</th>
<th>Third Contact First Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Person A</td>
<td>Anna</td>
<td>Steve</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Person B</td>
<td>Joseph</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Person C</td>
<td>Mimi</td>
<td>Mitchell</td>
<td>Chris</td>
</tr>
</tbody>
</table></div>
<p>Right now it's pulling all the information, just creating new rows to display the information. I want it to show all of the information in the same row for the same key. The second INNER JOIN is supposed to act as a self join which I was hoping to use to resolve the issue of displaying multiple rows instead of displaying the information for each key in one row.</p>
<p>Any help on this issue would be much appreciated.</p>
",1,1,0,2025-11-05T18:54:04+00:00,1,170,True
79810659,3607041,,sql,Make Postgres query fast without UNION,"<p>The ORDMBS is <em>PostgreSQL 17.5 on x86_64-suse-linux-gnu, compiled by gcc (SUSE Linux) 7.5.0, 64-bit</em>.</p>
<p>I have big table (about 150 GB), partitioned.</p>
<p>Full table description:</p>
<pre><code>CREATE TABLE table_partition 
(
    id int4,
    &quot;key&quot; text, 
    value_type int4, 
    value jsonb, 
    device_time timestamptz, 
    ts timestamptz
) PARTITION BY RANGE (ts);

CREATE INDEX table_partition_ts_id_index ON table_partition (id, ts);
CREATE INDEX table_partition_ts_id_index_2 ON table_partition (ts DESC, id);
CREATE INDEX table_partition_key_idx ON table_partition (key);
CREATE INDEX table_partition_key_ts_idx ON table_partition (key, ts);
CREATE INDEX table_partition_ts_key_idx ON table_partition (ts, key);

CREATE TABLE table_partition202506 PARTITION OF table_partition  FOR VALUES FROM ('2025-06-01') TO ('2025-07-01'); -- 30 GB
CREATE TABLE table_partition202507 PARTITION OF table_partition  FOR VALUES FROM ('2025-07-01') TO ('2025-08-01'); -- 31 GB
CREATE TABLE table_partition202508 PARTITION OF table_partition  FOR VALUES FROM ('2025-08-01') TO ('2025-09-01'); -- 66 GB
CREATE TABLE table_partition202509 PARTITION OF table_partition  FOR VALUES FROM ('2025-09-01') TO ('2025-10-01'); -- 1.3 GB
CREATE TABLE table_partition202510 PARTITION OF table_partition  FOR VALUES FROM ('2025-10-01') TO ('2025-11-01'); -- 22 GB
CREATE TABLE table_partition202511 PARTITION OF table_partition  FOR VALUES FROM ('2025-11-01') TO ('2025-12-01'); -- 160K
</code></pre>
<p>Two of the most interesting columns are:</p>
<pre><code>key text NOT NULL,
ts timestamptz NOT NULL
</code></pre>
<p>The first, slow query is:</p>
<pre><code>    SELECT csh.key, csh.ts
    FROM table_partition202508 csh
    WHERE csh.key IN ('string1', 'string2')
    ORDER BY csh.ts
    LIMIT 10;
</code></pre>
<p>The execution plan with SET track_io_timing = on is</p>
<pre><code>Limit  (cost=0.57..48.31 rows=10 width=109) (actual time=292447.149..292582.363 rows=10 loops=1)
  Output: key, ts
  Buffers: shared hit=8 read=1104471
  I/O Timings: shared read=275637.044
  -&gt;  Index Scan Backward using table_partition202508_ts_key_idx on table_partition202508 csh  (cost=0.57..11776409.95 rows=2466994 width=109) (actual time=292447.147..292582.353 rows=10 loops=1)
        Output: key, ts
        Index Cond: (csh.key = ANY ('{string1,string2}'::text[]))
        Buffers: shared hit=8 read=1104471
        I/O Timings: shared read=275637.044
Settings: effective_cache_size = '9592GB', work_mem = '32MB', search_path = 'public, public, &quot;$user&quot;'
Planning:
  Buffers: shared hit=232 read=6 dirtied=1
  I/O Timings: shared read=2.570
Planning Time: 3.980 ms
Execution Time: 292582.395 ms
</code></pre>
<p>The second, fast query is:</p>
<pre><code>SELECT csh.key, csh.ts
FROM table_partition202508 csh
WHERE csh.key IN ('string1', 'string2')

UNION ALL

SELECT csh.key, csh.ts
FROM table_partition202508 csh
WHERE csh.key = 'rubbish'  -- there is no such a key value in the dB!
ORDER BY csh.ts
LIMIT 10;
</code></pre>
<p>The execution plan (fast one) is</p>
<pre><code>Limit  (cost=7365718.00..7365719.17 rows=10 width=109) (actual time=78.605..82.522 rows=10 loops=1)
  Output: csh.key,  csh.ts
  Buffers: shared hit=12582
  -&gt;  Gather Merge  (cost=7365718.00..7725513.02 rows=3083742 width=109) (actual time=78.604..82.519 rows=10 loops=1)
        Output: csh.key,  csh.ts
        Workers Planned: 2
        Workers Launched: 2
        Buffers: shared hit=12582
        -&gt;  Sort  (cost=7364717.98..7368572.66 rows=1541871 width=109) (actual time=74.467..74.470 rows=8 loops=3)
              Output: csh.key,  csh.ts
              Sort Key: csh.ts DESC
              Sort Method: top-N heapsort  Memory: 27kB
              Buffers: shared hit=12582
              Worker 0:  actual time=73.081..73.084 rows=10 loops=1
                Sort Method: top-N heapsort  Memory: 27kB
                Buffers: shared hit=2488
              Worker 1:  actual time=73.079..73.082 rows=10 loops=1
                Sort Method: top-N heapsort  Memory: 27kB
                Buffers: shared hit=4877
              -&gt;  Parallel Append  (cost=13868.17..7331398.70 rows=1541871 width=109) (actual time=1.458..73.329 rows=4359 loops=3)
                    Buffers: shared hit=12566
                    Worker 0:  actual time=0.167..72.114 rows=2577 loops=1
                      Buffers: shared hit=2480
                    Worker 1:  actual time=0.180..71.840 rows=5076 loops=1
                      Buffers: shared hit=4869
                    -&gt;  Parallel Bitmap Heap Scan on table_partition202508 csh  (cost=27696.34..4072318.22 rows=1027914 width=109) (actual time=1.361..72.909 rows=4359 loops=3)
                          Output: csh.key,  csh.ts
                          Recheck Cond: (csh.key = ANY ('{string1,string2}'::text[]))
                          Heap Blocks: exact=5194
                          Buffers: shared hit=12562
                          Worker 0:  actual time=0.166..71.894 rows=2577 loops=1
                            Buffers: shared hit=2480
                          Worker 1:  actual time=0.179..71.465 rows=5076 loops=1
                            Buffers: shared hit=4869
                          -&gt;  Bitmap Index Scan on table_partition202508_key_idx  (cost=0.00..27079.59 rows=2466994 width=0) (actual time=2.156..2.156 rows=13076 loops=1)
                                Index Cond: (csh.key = ANY ('{string1,string2}'::text[]))
                                Buffers: shared hit=19
                    -&gt;  Parallel Bitmap Heap Scan on table_partition202508 csh_1  (cost=13868.17..3251371.12 rows=513957 width=109) (actual time=0.288..0.288 rows=0 loops=1)
                          Output: csh_1.key,  csh_1.ts
                          Recheck Cond: (csh_1.key = 'rubbish'::text)
                          Buffers: shared hit=4
                          -&gt;  Bitmap Index Scan on table_partition202508_key_idx  (cost=0.00..13559.80 rows=1233497 width=0) (actual time=0.038..0.039 rows=0 loops=1)
                                Index Cond: (csh_1.key = 'rubbish'::text)
                                Buffers: shared hit=4
Settings: effective_cache_size = '9592GB', work_mem = '32MB', search_path = 'public, public, &quot;$user&quot;'
Planning:
  Buffers: shared hit=101
Planning Time: 0.371 ms
Execution Time: 82.582 ms
</code></pre>
<p>You could see the time difference: 275759.240 ms v.s. 82.582 ms ms!</p>
<p>Is it possible to convince Postgres to use the second execution plan w/o &quot;magic&quot; unions?</p>
",0,2,2,2025-11-05T22:19:15+00:00,1,216,True
79810955,31553892,,sql,Correctly escape a trailing backslash in a string within XML to JSON function?,"<p>I have created a recursive SQL Server scalar-valued function that converts XML data to a JSON string. The function works well for most cases, including nested elements and handling of arrays (using a <code>json:Array</code> attribute).</p>
<pre><code>CREATE OR ALTER FUNCTION dbo.XmlToJson(@XmlData xml)  
RETURNS nvarchar(max)
WITH RETURNS NULL ON NULL INPUT
AS  
BEGIN  
    DECLARE @m nvarchar(max);

    WITH XMLNAMESPACES (N'http://james.newtonking.com/projects/json' AS json)
    SELECT @m = '{' + STRING_AGG(
  '&quot;' + STRING_ESCAPE(name, 'json') + '&quot;:' + value,
  ','
) + '}'
    FROM 
        (SELECT
             v.name,
             CONCAT(CASE WHEN COUNT(*) &gt; 1 OR MAX(isArray) = 1 THEN '[' END,
                    STRING_AGG(ISNULL('&quot;' + REPLACE(STRING_ESCAPE(x.a.value('text()[1]', 'nvarchar(max)'), 'json'), '\', '\\') + '&quot;', dbo.XmlToJson(x.a.query('./*'))), ','),
                    CASE WHEN COUNT(*) &gt; 1 OR MAX(isArray) = 1 THEN ']' END
                   ) AS value
         FROM @XmlData.nodes('./*') x(a)
         CROSS APPLY 
             (SELECT
                  x.a.value('local-name(.)', 'nvarchar(4000)') AS name,
                  x.a.value('xs:int(xs:boolean(@json:Array))', 'int') AS isArray) v
         GROUP BY
             v.name) grouped;

    SET @m = ISNULL(@m, 'null');
    SET @m = REPLACE(@m, '\/', '/');

    RETURN @m;
END;
</code></pre>
<p>However, I'm facing an issue with escaping backslashes in text content. Specifically, when an XML element's text content ends with a backslash (), my current logic results in an extra backslash escape in the final JSON output.</p>
<p>The desired output for a path like <code>C:\Books\Book1\Book1.pdf\</code> should be <code>&quot;C:\\Books\\Book1\\Book1.pdf\\&quot;</code>. My current output is producing <code>&quot;C:\\\\Books\\\\Book1\\\\Book1.pdf\\\\&quot;</code>.</p>
<p>For this input</p>
<pre><code>DECLARE @xml xml = N'&lt;root&gt;
    &lt;Book&gt;Book1&lt;/Book&gt;
    &lt;TransactionId  xmlns:json=&quot;http://james.newtonking.com/projects/json&quot; json:Array=&quot;true&quot;&gt;abc123&lt;/TransactionId&gt;
    &lt;Publisher&gt;Amazon&lt;/Publisher&gt;
    &lt;Edition  xmlns:json=&quot;http://james.newtonking.com/projects/json&quot; json:Array=&quot;true&quot;&gt;
        &lt;Name&gt;Ed1&lt;/Name&gt;
        &lt;Color&gt;Red&lt;/Color&gt;
        &lt;Price&gt;100&lt;/Price&gt;
        &lt;file&gt;C:\Books\Book1\Book1.pdf\&lt;/file&gt;
    &lt;/Edition&gt;
    &lt;PublisherId&gt;1&lt;/PublisherId&gt;
    &lt;UserId&gt;1234&lt;/UserId&gt;
    &lt;Release /&gt;
&lt;/root&gt;
';
</code></pre>
<p>I get this output:</p>
<pre><code>{&quot;Book&quot;:&quot;Book1&quot;,&quot;Edition&quot;:[{&quot;Color&quot;:&quot;Red&quot;,&quot;file&quot;:&quot;C:\\Books\\Book1\\Book1.pdf\\&quot;,&quot;Name&quot;:&quot;Ed1&quot;,&quot;Price&quot;:&quot;100&quot;}],&quot;Publisher&quot;:&quot;Amazon&quot;,&quot;PublisherId&quot;:&quot;1&quot;,&quot;Release&quot;:null,&quot;TransactionId&quot;:[&quot;abc123&quot;],&quot;UserId&quot;:&quot;1234&quot;}
</code></pre>
<p>The issue seems to stem from a conflict between <code>STRING_ESCAPE</code> and a manual <code>REPLACE</code> I'm using to handle general backslashes within the string, and how this interacts when the character is at the very end of the text.</p>
<p>Attached the DB fiddle for reference: <a href=""https://dbfiddle.uk/rUlklVK8"" rel=""nofollow noreferrer"">https://dbfiddle.uk/rUlklVK8</a></p>
<p>However I cannot replicate the same issue which I'm facing on my SQL Server.</p>
<p>Details:</p>
<pre><code>Microsoft SQL Server 2019 (RTM-CU22-GDR) (KB5029378) - 15.0.4326.1 (X64) 
Copyright (C) 2019 Microsoft Corporation 
Developer Edition (64-bit) on Windows Server 2019 Standard 10.0 &lt;X64&gt; (Build 17763: ) (Hypervisor) 
</code></pre>
",2,2,0,2025-11-06T07:39:40+00:00,3,163,True
79812598,14396396,,sql,ORA-12015: cannot create a fast refresh materialized view from a complex query with 2 joins,"<pre><code>CREATE MATERIALIZED VIEW anon_flag_mv
    REFRESH FAST ON DEMAND
AS
SELECT t2.id               AS t2_id,
       t3.cfg_id           AS cfg_id,
       t3.parent_ref_id    AS parent_ref_id,
       t1.category_ref_id  AS category_ref_id,
       t1.is_on_demand     AS is_on_demand,
       t3.id               AS t3_id,
       t1.id               AS t1_id
FROM table_one t1
         JOIN table_two t2
              ON t2.id = t1.category_ref_id
         JOIN table_three t3
              ON t1.id = t3.parent_ref_id;
</code></pre>
<p>I'm trying to create a materialized view in Oracle with FAST REFRESH, but I get the following error:</p>
<blockquote>
<p>ORA-12015: cannot create a fast refresh materialized view from a complex query</p>
</blockquote>
<p>I've already created logs</p>
<pre><code>CREATE MATERIALIZED VIEW LOG ON table_one
    WITH PRIMARY KEY, SEQUENCE
    (category_ref_id, is_on_demand)
    INCLUDING NEW VALUES;

CREATE MATERIALIZED VIEW LOG ON table_two
    WITH PRIMARY KEY, SEQUENCE
    (cfg_id, parent_ref_id)
    INCLUDING NEW VALUES;

CREATE MATERIALIZED VIEW LOG ON table_three
    WITH PRIMARY KEY, SEQUENCE
    INCLUDING NEW VALUES;
</code></pre>
<p>How can I create materialized view and use fast refresh?</p>
",1,1,0,2025-11-07T16:59:54+00:00,2,86,True
79812842,31836437,,sql,UPDATE with LEFT JOIN and condition IS NULL,"<p>I have the following table definitions:</p>
<p>Table <code>public.messages</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column</th>
<th>Type</th>
<th>Collation</th>
<th>Nullable</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>ip</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>msg</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ignore</td>
<td>boolean</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Table <code>public.host</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column</th>
<th>Type</th>
<th>Collation</th>
<th>Nullable</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>ip</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Yes, I could set type as IP for the ip, but for this example it doesn't matter.</p>
<p>I run this query:</p>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip;
</code></pre>
<p>And get this result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>Test1</td>
<td></td>
<td>1.1.1.1</td>
<td>host1</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Sample data can be found here: <a href=""https://dbfiddle.uk/3LTo9shO"" rel=""nofollow noreferrer"">https://dbfiddle.uk/3LTo9shO</a></p>
<p>I want to update all rows where there is no host name to set ignore to true.</p>
<p>So basically, just the ignore column for this result:</p>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip 
WHERE host.name IS NULL;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I can do an update for the row that <strong>does</strong> have a host name. When I set the value, everything works:</p>
<pre><code>UPDATE messages 
SET ignore = FALSE 
FROM host 
WHERE messages.ip = host.ip AND host.name IS NOT NULL;
</code></pre>
<blockquote>
<p>UPDATE 1</p>
</blockquote>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>Test1</td>
<td>f</td>
<td>1.1.1.1</td>
<td>host1</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>However, setting true for those that don't have a host name doesn't work.</p>
<pre><code>UPDATE messages 
SET ignore = TRUE 
FROM host 
WHERE messages.ip = host.ip AND host.name IS NULL;
</code></pre>
<blockquote>
<p>UPDATE 0</p>
</blockquote>
<p>I tried with a <code>LEFT JOIN</code>, but then <em>everything</em> was updated:</p>
<pre><code>UPDATE messages 
SET ignore = TRUE 
FROM messages m 
LEFT JOIN host h ON m.ip = h.ip 
WHERE h.name IS NULL;
</code></pre>
<blockquote>
<p>UPDATE 2</p>
</blockquote>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>Test1</td>
<td>t</td>
<td>1.1.1.1</td>
<td>host1</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td>t</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p><a href=""https://dbfiddle.uk/fOJ55JBM"" rel=""nofollow noreferrer"">https://dbfiddle.uk/fOJ55JBM</a></p>
<p>How can I update just rows without a host name?</p>
",5,5,0,2025-11-07T22:39:37+00:00,4,227,True
79812952,20195288,,sql,How to handle a “ghost” product in a SellDetails table without breaking normalization?,"<p>I'm developing a desktop application for gym management. One feature is a store for selling <code>memberships</code> and <code>products</code>.</p>
<p>My client sometimes wants to sell something that is not in the inventory or memberships—like a special discount, a fee, or a custom item.</p>
<p>It needs to appear in the sale, but it’s not a registered product or membership.</p>
<p>This ghost thing behaves like a product or membership, but isn't in either table.</p>
<pre><code>CREATE TABLE &quot;Membership&quot; 
(
    &quot;id&quot;    INTEGER NOT NULL,
    &quot;name&quot;  TEXT,
    &quot;price&quot; INTEGER,
    &quot;days&quot;  INTEGER NOT NULL,
    PRIMARY KEY(&quot;id&quot; AUTOINCREMENT)
)

CREATE TABLE &quot;Product&quot; 
(
    &quot;id&quot;    INTEGER NOT NULL,
    &quot;name&quot;  TEXT,
    &quot;price&quot; INTEGER,
    &quot;stock&quot; INTEGER NOT NULL,
    PRIMARY KEY(&quot;id&quot; AUTOINCREMENT)
)

CREATE TABLE &quot;Sell&quot; 
(
    &quot;id&quot;    INTEGER NOT NULL,
    &quot;client_code&quot;   INTEGER NOT NULL,
    &quot;operator&quot;  TEXT NOT NULL,
    &quot;date&quot;  TEXT NOT NULL,
    &quot;total&quot; INTEGER NOT NULL,
    PRIMARY KEY(&quot;id&quot; AUTOINCREMENT),
    FOREIGN KEY(&quot;client_code&quot;) REFERENCES &quot;Client&quot;(&quot;code&quot;)
)

CREATE TABLE &quot;SellDetails&quot; 
(
    &quot;id&quot;    INTEGER NOT NULL,
    &quot;id_sell&quot;   INTEGER NOT NULL,
    PRIMARY KEY(&quot;id&quot; AUTOINCREMENT),
    FOREIGN KEY(&quot;id_sell&quot;) REFERENCES &quot;Sell&quot;(&quot;id&quot;)
)
</code></pre>
<p>How should I implement the <code>SellDetails</code> table so that it can store</p>
<ul>
<li>a reference to a <code>Product</code>, or</li>
<li>a reference to a <code>Membership</code>, or</li>
<li>a custom ghost item not in either table</li>
</ul>
<p>without completely breaking normalization?</p>
<p>Maybe I could add the ghost item to either the <code>Product</code> or <code>Membership</code> table at the business logic level.</p>
<p>But that doesn't add value to those tables, and
it would eventually turn messy and confusing.</p>
",0,0,0,2025-11-08T03:38:19+00:00,5,105,True
79814262,3149869,,sql,GridDB SQL error while using GROUP BY RANGE,"<p>I am getting error using GROUP BY RANGE in GridDB sql. I am referring to the example mention in the doc <a href=""https://griddb.org/docs-en/manuals/GridDB_SQL_Reference.html#group-by-range"" rel=""nofollow noreferrer"">https://griddb.org/docs-en/manuals/GridDB_SQL_Reference.html#group-by-range</a></p>
<p>name: trend_data1</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ts</th>
<th style=""text-align: center;"">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">2023-01-01T00:00:00</td>
<td style=""text-align: center;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">2023-01-01T00:00:10</td>
<td style=""text-align: center;"">30</td>
</tr>
<tr>
<td style=""text-align: left;"">2023-01-01T00:00:20</td>
<td style=""text-align: center;"">30</td>
</tr>
<tr>
<td style=""text-align: left;"">2023-01-01T00:00:30</td>
<td style=""text-align: center;"">50</td>
</tr>
<tr>
<td style=""text-align: left;"">2023-01-01T00:00:40</td>
<td style=""text-align: center;"">50</td>
</tr>
<tr>
<td style=""text-align: left;"">2023-01-01T00:00:50</td>
<td style=""text-align: center;"">70</td>
</tr>
</tbody>
</table></div>
<pre><code>SELECT ts, avg(value)
FROM trend_data1
WHERE ts BETWEEN TIMESTAMP('2023-01-01T00:00:00Z')
             AND TIMESTAMP('2023-01-01T00:01:00Z')
GROUP BY RANGE ts EVERY (20,SECOND)
</code></pre>
<blockquote>
<p>error: The invalid query was specified</p>
</blockquote>
<p>Table definition:</p>
<pre><code>CREATE TABLE trend_data1 (
    ts TIMESTAMP,
    value DOUBLE
)
</code></pre>
<p>What is the wrong with the query as I am using the same query as provided in the document.</p>
",0,0,0,2025-11-08T15:07:25+00:00,0,111,False
79814599,31749517,,sql,How to efficiently calculate an exponential moving average in postgres?,"<p>I'm trying to calculate the <a href=""https://en.wikipedia.org/wiki/Average_true_range#:%7E:text=%5B4%5D-,Calculation,-%5Bedit%5D"" rel=""nofollow noreferrer"">average true range</a> on some time series dataset stored in postgres. Its calculation requires a 14 period exponential moving average of true range which based on the <a href=""https://stackoverflow.com/questions/60024643/sql-calculate-exponential-moving-average-with-ctes-or-aggregates"">answer</a> here is obtained using:</p>
<pre><code>with recursive p as (
    select *,
           greatest(
               high - low,
               abs(high - lag(close) over (order by timestamp)),
               abs(low - lag(close) over (order by timestamp))
           ) as true_range,
           row_number() over (order by timestamp) as seqnum
    from core_price
    where timestamp &gt; 1751329927
      and symbol_id = 1
    limit 20000
),
cte as (
    select seqnum,
           high,
           low,
           close,
           timestamp,
           true_range,
           true_range::float as average_true_range
    from p
    where seqnum = 1

    union all

    select p.seqnum,
           p.high,
           p.low,
           p.close,
           p.timestamp,
           p.true_range,
           (cte.average_true_range * 13.0 / 14 + p.true_range / 14) as average_true_range
    from cte
    join p on p.seqnum = cte.seqnum + 1
)
select *
from cte
order by seqnum;
</code></pre>
<p>It calculates the correct values however, it's very slow (takes 3s on m4 max mbp for 20,000 rows) and there's millions of rows so it's not by any means efficient or acceptable. The same calculation takes milliseconds in <code>pandas</code> using <code>df.ewm</code> for the entire period of 7M rows. How can it be optimized?</p>
<p>One possible way of optimization is to replace <code>ema</code> with <code>sma</code> which can be calculated quickly in postgres without recursion, using window functions but it doesn't calculate the standard ATR and is less sensitive to recent changes in price, so I'm not interested in that approach.</p>
",1,3,2,2025-11-09T04:53:32+00:00,2,182,True
79815133,43615,"Munich, Bavaria, Germany",sql,How to avoid &quot;ON CONFLICT DO UPDATE command cannot affect row a second time&quot; error in WITH statement,"<p>I have two tables: <a href=""https://dbfiddle.uk/q5pH246i"" rel=""nofollow noreferrer""><sub>demo at db&lt;&gt;fiddle</sub></a></p>
<ol>
<li>Table <code>keywords</code> has two columns <code>id</code> and <code>v</code> (which hold the keyword's value)
<pre class=""lang-sql prettyprint-override""><code>create table keywords(
  id int generated always as identity primary key
 ,v text unique);
</code></pre>
</li>
<li>Table <code>main</code> has, among others, two columns that are each a foreign key into the <code>keywords</code> table's <code>id</code>, named <code>key1_id</code> and <code>key2_id</code>
<pre class=""lang-sql prettyprint-override""><code>create table main(
  id int generated always as identity primary key
 ,key1_id int references keywords(id)
 ,key2_id int references keywords(id));
</code></pre>
</li>
</ol>
<p>Now, I want to insert pairs of keywords (key1 and key2 as <code>$1</code> and <code>$2</code>) into the main table, like this:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH key1 AS (INSERT INTO keywords (v) 
              VALUES ($1) 
              ON CONFLICT (v) DO UPDATE
              SET v = EXCLUDED.v
              RETURNING id),
     key2 AS (INSERT INTO keywords (v) 
              VALUES ($2) 
              ON CONFLICT (v) DO UPDATE
              SET v = EXCLUDED.v
              RETURNING id)
INSERT INTO main (key1_id, key2_id)
SELECT key1.id, key2.id
FROM key1, key2
RETURNING id
</code></pre>
<p>Basically, the two keys often have recurring values, so I use the keywords table to keep a unique set of them (mostly for storage optimization, as I have millions of rows with them).</p>
<p>But if <code>$1</code> and <code>$2</code> have identical values, I get this error, whereas there's no issue if they're different:</p>
<blockquote>
<p>pg_query_params(): Query failed: ERROR:  ON CONFLICT DO UPDATE command cannot affect row a second time<br />
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.</p>
</blockquote>
<p>The goal is that both <code>key1_id</code> and <code>key2_id</code> point to the correct row in <code>keywords</code> based on the values passed as <code>$1</code> and <code>$2</code>, even if they're both the same.</p>
<p>How do I modify the SQL statement (ideally, it should remain a single one) so that I can insert these keys without getting the error?</p>
<p>I am using Postgresql 16.9.</p>
",2,2,0,2025-11-10T02:02:26+00:00,2,229,True
79815901,21029891,,sql,Excel VBA SQL Insert or Update Data in closed workbook,"<p>I use SQL to read data from a closed workbook. Is it possible to write new records or updates to a record in a closed workbook using SQL?</p>
",0,0,0,2025-11-10T17:02:54+00:00,2,129,True
79817472,945871,,sql,Complete query in Oracle to find PK name,"<p>According to Google this should be the query to find the PK name:</p>
<pre><code>SELECT constraint_name
FROM user_constraints
WHERE table_name = 'YOUR_TABLE_NAME' AND constraint_type = 'P';
</code></pre>
<p>However, it does not use fully qualified table name of <code>catalog.schema.table</code>.</p>
<p>How do I modify that query to filter according to fully qualified name?</p>
<p>EDIT:</p>
<p>Imagine the following scenario:</p>
<p>1 Oracle server
2. Databases - data2024 and data2025
Each database have a table called accounting
Each table has its own PK which has its own name.</p>
<p>So when I login to the server how do I modify the query to differentiate the tables?</p>
<p>All this structure was created by the user called root.</p>
",0,0,0,2025-11-12T08:05:59+00:00,4,147,True
79817823,31865186,,sql,Scroll fails while using order by string column in manticore,"<p>I’m using Manticore Search 14.1.0 (Docker image manticoresearch/manticore:latest) on Ubuntu 20.04.6 LTS. I rely on cursor-style pagination with OPTION scroll, but when the sort clause includes a string column the scroll token is ignored and I just keep getting the first page.</p>
<p><strong>Minimal reproducible example:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE test (id BIGINT, brand_name STRING);
INSERT INTO test (id, brand_name) VALUES
  (1, 'A'),
  (2, 'A'),
  (3, 'B'),
  (4, 'B'),
  (5, 'C');
</code></pre>
<p><strong>Page 1 works as expected:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM test
ORDER BY brand_name DESC, id DESC
LIMIT 2;
/*
+------+------------+
| id   | brand_name |
+------+------------+
|    5 | C          |
|    4 | B          |
+------+------------+
*/
</code></pre>
<p><strong>The SHOW SCROLL; command returns a token (decoded form shown for clarity):</strong></p>
<pre class=""lang-js prettyprint-override""><code>{
  &quot;order_by_str&quot;: &quot;brand_name desc, id desc&quot;,
  &quot;order_by&quot;: [
    { &quot;attr&quot;: &quot;brand_name&quot;, &quot;desc&quot;: true, &quot;value&quot;: &quot;B&quot;, &quot;type&quot;: &quot;string&quot; },
    { &quot;attr&quot;: &quot;id&quot;, &quot;desc&quot;: true, &quot;value&quot;: 4, &quot;type&quot;: &quot;int&quot; }
  ]
}
</code></pre>
<p>But passing that token straight back still gives me the first page:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM test
ORDER BY brand_name DESC, id DESC
LIMIT 2
OPTION scroll='eyJvcmRlcl9ieV9zdHIiOiJicmFuZF9uYW1lIGRlc2MsIGlkIGRlc2MiLCJvcmRlcl9ieSI6W3siYXR0ciI6ImJyYW5kX25hbWUiLCJkZXNjIjp0cnVlLCJ2YWx1ZSI6IkIiLCJ0eXBlIjoic3RyaW5nIn0seyJhdHRyIjoiaWQiLCJkZXNjIjp0cnVlLCJ2YWx1ZSI6NCwidHlwZSI6ImludCJ9XX0=';
</code></pre>
<p>Expected result for page 2:</p>
<pre><code>+------+------------+
| id   | brand_name |
+------+------------+
|    3 | B          |
|    2 | A          |
+------+------------+
</code></pre>
<p>Actual result:</p>
<pre><code>+------+------------+
| id   | brand_name |
+------+------------+
|    5 | C          |
|    4 | B          |
+------+------------+
</code></pre>
<p>Has anyone found a way to get OPTION scroll to advance when sorting by a string column (with a secondary numeric tie-breaker), or is this simply unsupported right now?</p>
",0,0,0,2025-11-12T14:05:45+00:00,0,73,False
79818030,3821009,,sql,Sybase ASE avoid permission issues,"<p>I want to query multiple tables across databases, something like:</p>
<pre><code>declare @sql varchar(100)
select @sql = 'select * from mydb1..mytable'
select @sql = 'select * from mydb2..mytable'
exec(@sql)
</code></pre>
<p>However, if the user doesn't have permissions to log in to some of the databases, I get:</p>
<blockquote>
<p>User XXX not allowed in database 'mydb1' - only the owner of this database can access it.</p>
</blockquote>
<p>Is there a way to either:</p>
<ol>
<li>Ignore the errors for the databases that the user doesn't have permissions to or</li>
<li>Check the permissions and run only for the ones that the user has permissions for?</li>
</ol>
",0,0,0,2025-11-12T16:56:09+00:00,1,70,False
79818073,43615,"Munich, Bavaria, Germany",sql,"Grouping rows, and then deleting only a sub range (based on their dates) from each of those groups","<p>I use Postgres on my web server in order to record incoming queries into a table <code>calls2</code>, basically writing a single row each time with lots of repeating information, such as a date field (<code>&quot;when&quot;</code>) of when the recording was made plus lots of other statistical information (ip address, query parameters etc). One field also identifies each event's caller (<code>uid</code>).</p>
<p>Now the table has gotten way too large, and I like to thin it out, by removing all but the oldest and newest (based on the date field, which I called <code>&quot;when&quot;</code>) entry, because that's all I really need.</p>
<p>So, assuming I group the rows by their <code>uid</code> (integer), how do I either select only the oldest and newest row (which could be the same) of all my records? And how do select all the others so that I can delete them all at once, for all <code>uid</code>s?</p>
<p>I've created a <a href=""https://dbfiddle.uk/SZ5849jK"" rel=""nofollow noreferrer"">db fiddle</a> with an example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">id</th>
<th style=""text-align: right;"">uid</th>
<th style=""text-align: left;"">when</th>
<th style=""text-align: left;"">other</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">a1</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-02</td>
<td style=""text-align: left;"">a2</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-03</td>
<td style=""text-align: left;"">a3</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-04</td>
<td style=""text-align: left;"">a4</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: right;"">22</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">b1</td>
</tr>
<tr>
<td style=""text-align: right;"">6</td>
<td style=""text-align: right;"">33</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">c1</td>
</tr>
<tr>
<td style=""text-align: right;"">7</td>
<td style=""text-align: right;"">33</td>
<td style=""text-align: left;"">2010-01-02</td>
<td style=""text-align: left;"">c2</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">d1</td>
</tr>
<tr>
<td style=""text-align: right;"">9</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: left;"">2010-01-02</td>
<td style=""text-align: left;"">d2</td>
</tr>
<tr>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: left;"">2010-01-03</td>
<td style=""text-align: left;"">d3</td>
</tr>
</tbody>
</table></div>
<p>The goal would be to identify <code>id</code>s <code>2</code>, <code>3</code> and <code>9</code> for deletion.</p>
<p>It would also be helpful if I could be shown how I select the other IDs in a way that they list a single row for each <code>uid</code>, showing the <code>uid</code> plus the values of oldest and latest pairs for <code>&quot;when&quot;</code> and <code>other</code>. Or at least just list even row that has the lowest and highest entry for each <code>uid</code> (i.e. the inverse of what I want to delete).</p>
<p>I need help with this because anything involving grouping or clustering in Psql breaks my head.</p>
",1,1,0,2025-11-12T17:52:00+00:00,5,102,True
79818089,6447160,,sql,SQL attempting to group by multiple columns with &quot;chained&quot; column generation,"<p>I'm trying to get SQL to generate a count of rows based on 2 values, from a table like shown below</p>
<p>Table Data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>NAME</th>
<th>VALUE</th>
<th>device_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>name_1</td>
<td>enabled</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>name_2</td>
<td>enabled</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>name_3</td>
<td>enabled</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>name_1</td>
<td>disabled</td>
<td>1</td>
</tr>
<tr>
<td>5</td>
<td>name_2</td>
<td>disabled</td>
<td>4</td>
</tr>
<tr>
<td>6</td>
<td>name_3</td>
<td>disabled</td>
<td>6</td>
</tr>
<tr>
<td>1</td>
<td>name_1_ts</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>name_2_ts</td>
<td></td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>name_3_ts</td>
<td></td>
<td>5</td>
</tr>
<tr>
<td>.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>99999</td>
<td>name_1</td>
<td>enabled</td>
<td>8329</td>
</tr>
<tr>
<td>100000</td>
<td>name_2</td>
<td>disabled</td>
<td>5</td>
</tr>
</tbody>
</table></div>
<p>Return data that I want to generate</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>count(*)</th>
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>500</td>
<td>name_1</td>
<td>enabled</td>
</tr>
<tr>
<td>1000</td>
<td>name_2</td>
<td>enabled</td>
</tr>
<tr>
<td>3500</td>
<td>name_3</td>
<td>enabled</td>
</tr>
<tr>
<td>1500</td>
<td>name_1</td>
<td>disabled</td>
</tr>
<tr>
<td>2000</td>
<td>name_2</td>
<td>disabled</td>
</tr>
<tr>
<td>1500</td>
<td>name_3</td>
<td>disabled</td>
</tr>
</tbody>
</table></div>
<p>I'm trying</p>
<pre><code>select count(*), name, value 
from TABLE 
where value = 'enabled' or value = 'disabled' 
group by name, value;
</code></pre>
<p>But it's not generating the results I'm looking for. What am I doing wrong here, and what I can I do fix it?</p>
",0,1,1,2025-11-12T18:08:29+00:00,2,95,True
79818502,31868866,,sql,Should I generally use a LIMIT clause when testing my SQL queries to a database?,"<p>I am learning SQL from a class online.</p>
<p>I wanted to ask if it's generally best to use LIMIT in my queries to make them run faster, or at least until I have them tested and pulling the right data.</p>
<p>If yes, where should I put it in the order of things? Does it need to go at the very end, or does it depend on what kind of SQL software I'm using?</p>
<p>Thanks a lot for your help!</p>
",0,0,0,2025-11-13T06:33:18+00:00,8,143,True
79818692,31746931,,sql,Oracle - how to extract the date from string?,"<p>I am trying to find the right pattern which can extract the date from any string</p>
<p>&quot;&quot;&quot;</p>
<p>28/11/22 11-23333  to    28/11/22</p>
<p>11-23333 28/11/22  to    28/11/22</p>
<p>something 20.02.2022 end   to 20.02.2022</p>
<p>7-03-21 start   to 7-03-21</p>
<p>no date here  to null</p>
<p>date is 2023/11/12 something   to 2023/11/12</p>
<p>prefix2023-11-12suffix  to   2023-11-12</p>
<p>2023.11.12 at start  to    2023.11.12</p>
<p>&quot;&quot;&quot;</p>
<p>I tried many ways including below. However I want to make sure which pattern should works for extracting date from any string pattern. Because the text from where the date is to extract is un-predictable and dynamic. So I can't make changes in the code all the time if the given pattern couldn't extract the date.</p>
<p>Pattern I used:</p>
<pre><code>REGEXP_SUBSTR(
        note,
        '((([0-9]{4})([./-])([0-9]{1,2})\4([0-9]{1,2}))|(([0-9]{1,2})([./-])([0-9]{1,2})\9([0-9]{2,4})))'
    ) AS extracted_date
</code></pre>
<p>and also want to filter the column which contain date pattern, so that it ensure the validation for any dynamic test</p>
<p>I tried this as well</p>
<pre><code>REGEXP_LIKE(
    r.note,
    '((([0-9]{4})([./-])([0-9]{1,2})\4([0-9]{1,2}))|(([0-9]{1,2})([./-])([0-9]{1,2})\9([0-9]{2,4})))'
);
</code></pre>
<p>Is this the right option?</p>
",0,0,0,2025-11-13T09:55:29+00:00,7,168,True
79818993,8261566,France,sql,Kotlin Exposed SQL query data with children - N+1 issue,"<p>Currently, we have a lot of code like this one to query data inside our repositories:</p>
<pre class=""lang-kotlin prettyprint-override""><code>database.suspendedTransaction {
    Items
        .selectAll()
        .where { Items.userId eq userId }
        .map {
            Items.toItem(
                it,
                fees = Fees
                    .selectAll()
                    .where { Fees.itemId eq it[Items.id].value }
                    .map(Fees::toFee)
            )
        }
}
</code></pre>
<p>This is an issue because when having a lot of items, it makes the code do N+1 queries (which is really slow sometimes)</p>
<p>So we're thinking about including everything inside one single query. For now, we've achieved something like this:</p>
<pre class=""lang-kotlin prettyprint-override""><code>database.suspendedTransaction {
    val items = Items
        .selectAll()
        .where { Items.userId eq userId }
    val itemIds = items.map { it[Items.id].value }.toSet()
    val fees = Fees
        .selectAll()
        .where { Fees.itemId inList itemIds }
        .map(Fees::toFee)
    items.map {
        Items.toItem(
            it,
            fees = fees.filter { fee -&gt; fee.itemId == it[Items.id].value }
        )
    }
}
</code></pre>
<p>This is better since we have 2 queries instead of N+1.</p>
<p>But:</p>
<ul>
<li>I'm not sure about if it's the best way (simplest and most performant way) to do it</li>
<li>Maybe there is a way to do a simple query that returns everything at once</li>
</ul>
<p>I though about using a left join, but it means we will return the data of the items a lot of times and might not be optimal at all (for example, if I have 10 fees for one item I will get 10 times the data of the item), because it makes the result way heavier. And when you have tables with multiple joins, you multiply the volume of data each time you add a left join (so you can get the same data duplicated hundred of times!)</p>
<pre class=""lang-kotlin prettyprint-override""><code>database.suspendedTransaction {
    Items
        .leftJoin(Fees, Items.id, Fees.itemId)
        .selectAll()
        .where { Items.userId eq userId }
        .groupBy { it[Items.id].value }
        .map {
            Items.toItem(
                it.value.first(),
                fees = it.value.map(Fees::toFee) // This would need complex grouping if having multiple joins!
            )
        }
}
</code></pre>
<p>So what's the appropriate way to do this, reducing the number of queries and the volume of transmited data?</p>
<p>NOTE: <code>Items::toItem</code> and <code>Fees::toFee</code> are additional methods mapping a <code>ResultRow</code> to a simple data class, to later be used and serialized to JSON when responding to API requests.</p>
",1,0,0,2025-11-13T14:30:13+00:00,0,77,False
79820232,9242492,EU,sql,Correlated queries without the LATERAL keyword,"<p>I am using <code>MariaDB</code> <code>v10.11.15</code> (a MySQL fork) which does not support the <code>LATERAL</code> keyword, to lookup the countries of 30 IPv4 numbers.  I have the following query that retrieves the country when given an IPv4 number (in decimal form), for example:</p>
<pre><code>SELECT
    ips.country
FROM (
    SELECT *
    FROM ips 
    WHERE ip4_beg &lt;= 36843009
    ORDER BY ip4_beg DESC
    LIMIT 1
    ) AS ips
WHERE ips.ip4_end &gt;= 36843009;
</code></pre>
<p>The table <code>ips</code> contains millions of IPv4 ranges defined by the columns <code>ip4_beg</code> and <code>ip4_end</code> (inclusively).  The ranges are unique and non-overlapping but there are gaps between the ranges, so it can happen that a given IPv4 number is not found.</p>
<p>There is a UNIQUE index on the column <code>ips.ip4_beg</code>.</p>
<p>HOW IT WORKS: The inner query finds the first start of range (<code>ip4_beg</code>) that is =&lt; than the sought after IP number. The outer query only checks whether the end of this range (<code>ip4_end</code>) is &gt;= than the sought after IP number.  Due to the nature of the data <strong>it makes no sense</strong> for the inner query to return more than 1 row.  Also if the condition in the <code>WHERE</code> clause fails in the outer query, then it makes <strong>no sense</strong> for the inner query to continue searching due to the nature of the data.</p>
<p>The query performs very well as illustrated by the following output of the <code>ANALYZE</code> command:</p>
<pre><code>id  select_type table       type      p_keys   key      keylen  ref   rows     r_rows  filtered r_filtered  Extra
-------------------------------------------------------------------------------------------------------------------------
1   PRIMARY     &lt;derived2&gt;  ALL       NULL     NULL     NULL    NULL  2        1.00    100.00   100.00      Using where 
2   DERIVED     ips         range     idx_beg  idx_beg  4       NULL  1148647  1.00    100.00   100.00      Using where
</code></pre>
<p><code>r_rows</code> is the number of rows that were actually processed.</p>
<p>In practical terms, the query above looks up the country of an IP number in less than 1ms with an index only on <code>ips.ip4_beg</code>.</p>
<p>I have a second query that simply retrieves 30 IPv4 numbers as well as id_member and url.  It is as follows:</p>
<pre><code>SELECT 
     id_member, url, ip
FROM log_online
ORDER BY id_member
LIMIT 0, 30;
</code></pre>
<p>There is a <code>UNIQUE</code> index on the column <code>log_online.id_member</code>
The type of <code>log_online.ip</code> is the same as the type of <code>ips.ip4_beg</code> and <code>ips.ip4_end</code>.</p>
<p><strong>Q</strong>: How to execute the first query that retrieves the <code>ips.country</code> for every <code>log_online.ip</code> number returned by the second query (i.e.: for the 30 ip numbers that it returns).  The final output must contain:
log_online.id_member,
log_online.url,
log_online.ip,
ips.country</p>
<p>I see no reason for the combined queries to have the execution time longer than:
second_query + first_query * 30,
because the second_query returns 30 rows, so I will not accept answers with queries that have a longer execution time.</p>
<p>I tried using the first query as a correlated subquery in the SELECT list of the second query but I could not reference the <code>log_online.ip</code> or <code>lo.ip</code> inside it:</p>
<pre><code>SELECT 
     id_member, url, ip,
     (SELECT
        innerq.country
      FROM (
             SELECT *
             FROM ips 
             WHERE ip4_beg &lt;= lo.ip   -- it doesn't see lo.ip here
             ORDER BY ip4_beg DESC
             LIMIT 1
           ) AS innerq
      WHERE innerq.ip4_end &gt;= lo.ip
     )
FROM log_online AS lo
ORDER BY id_member
LIMIT 0, 30;
</code></pre>
<p>I got a question in the comments why don't I try the following:</p>
<pre><code>SELECT 
     id_member, url, ip, 
     (
      SELECT
         country
      FROM ips 
      WHERE lo.ip BETWEEN ip4_beg AND ip4_end
     )
FROM log_online AS lo
ORDER BY id_member
LIMIT 0, 30;
</code></pre>
<p>The output of the <code>ANALYZE</code> command speaks for itself:</p>
<pre><code>id  select_type   table  type   p_keys   key        keylen  ref   rows     r_rows      filtered  r_filtered  Extra
--------------------------------------------------------------------------------------------------------------------------
1   PRIMARY       lo     index  NULL     id_member  3       NULL  531      30.00       100.00    100.00         
2   DEP SUBQUERY  ips    ALL    idx_beg  NULL       NULL    NULL  2418126  2418278.00  100.00      0.00      Using where  
</code></pre>
<p>Adding an <code>ORDER BY</code> clause and <code>LIMIT 1</code> improves performance marginally:</p>
<pre><code>ANALYZE SELECT 
     id_member, url, ip, 
     (
      SELECT
         country
      FROM ips 
      WHERE lo.ip BETWEEN ip4_beg AND ip4_end
      ORDER BY ip4_beg DESC
      LIMIT 1
     )
FROM log_online AS lo
ORDER BY id_member
LIMIT 0, 30;
</code></pre>
<p>Again, the output of the <code>ANALYZE</code> command speaks for itself:</p>
<pre><code>id  select_type    table  type    p_keys   key        keylen  ref   rows     r_rows     filtered  r_filtered  Extra
---------------------------------------------------------------------------------------------------------------------------
1   PRIMARY        lo     index   NULL     id_member  3       NULL  410      30.00      100.00    100.00         
2   DEP SUBQUERY   ips    index   idx_beg  idx_beg    4       NULL  2418126  879673.03  100.00      0.00      Using where    
</code></pre>
<p>In practical terms, the query above takes ~4 minutes to look up the countries of only 30 IPs ...Ouch !!!</p>
<p>P.S.<br>
In the future, I am planning to return more columns from the <code>ips</code> table ( from the same row !!! ) - most likely using the <code>JSON_OBJECT()</code> so a separate subquery is not run for multiple columns in <strong>the same row</strong>.</p>
",1,1,0,2025-11-14T16:10:41+00:00,4,219,True
79820507,28884281,,sql,How to make it so a parent NEEDS its children to exist in MySQL?,"<p>I am designing a DB where basically parent can't exist without two children. E.g: Entity set Marriage, CAN'T exist without entity set Man and entity set Woman, but neither Man or Woman need Marriage Entity set to exist. (Lets imagine only Man and Woman marriages so its less complex). Also, lets imagine Marriage can have many men or many women, but men and women can only have ONE marriage. I need help on the query, how do I enforce this?</p>
<pre><code>CREATE TABLE Marriage(
    marriageId INT
)

CREATE TABLE Man(
    manId INT,
    marriageId INT,
    FOREIGN KEY (marriageId) REFERENCES Marriage(marriageId) 
        ON DELETE SET NULL
)
CREATE TABLE Woman(
    womanId INT,
    marriageId INT,
    FOREIGN KEY (marriageId) REFERENCES Marriage(marriageId) 
        ON DELETE SET NULL
)
</code></pre>
<p>This is basically what I have now. (other attributes don't matter right now).</p>
<p>In summary my question is, how can I enforce the TOTAL participation in both ends from marriage?</p>
",0,0,0,2025-11-14T23:42:35+00:00,9,149,True
79820764,3362530,,sql,How to retrieve a sub-array from result of array_agg?,"<p>I have a SQL table in postgres 14 that looks something like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>f_key</th>
<th>data1</th>
<th>data2</th>
<th>fit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><code>{'a1', 'a2'}</code></td>
<td><code>null</code></td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td><code>{'b1', 'b2'}</code></td>
<td><code>{'b3'}</code></td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td><code>{'c1', 'c2'}</code></td>
<td><code>null</code></td>
<td>3</td>
</tr>
</tbody>
</table></div>
<p>Note that data1 and data2 are arrays.</p>
<p>I need to query this so that I will get data1 and data2 that have best (highest) fit, but are not null (where possible), grouped by f_key.</p>
<p>So the result would look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>f_key</th>
<th>data1</th>
<th>data2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><code>{'a1', 'a2'}</code></td>
<td><code>{'b3'}</code></td>
</tr>
<tr>
<td>2</td>
<td><code>{'c1', 'c2'}</code></td>
<td><code>null</code></td>
</tr>
</tbody>
</table></div>
<p>My current approach is to use <code>array_agg</code> in this fashion:</p>
<pre class=""lang-sql prettyprint-override""><code>select 
    tt.f_key,
    (array_agg(tt.data1) filter (where tt.data1 is not null))[1] as d1,
    (array_agg(tt.data2) filter (where tt.data2 is not null))[1] as d2
from
   (select * from items order by f_key, fit desc) as tt
group by f_key;
</code></pre>
<p>, but <code>d1</code> and <code>d2</code> return <code>null</code> in this case. What leaves me completely puzzled, is that:</p>
<ul>
<li><code>(array_agg(tt.data1) filter (where tt.data1 is not null)) as d1,</code> returns an array of arrays, as expected</li>
<li><code>(array_agg(tt.data1) filter (where tt.data1 is not null))[1][1] as d1,</code> returns first element of first sub-array, as expected.</li>
</ul>
<p>How do I retrieve the first sub-array from the result of <code>array_agg</code>?</p>
",3,3,0,2025-11-15T11:01:18+00:00,3,164,True
79820914,7404762,,sql,Azure Databricks - use of parameters in SQL DDL in a Notebook,"<p>I am trying to set up a notebook to configure the basics of a catalog and its schemas, permissions etc for our developers can be &quot;set loose&quot;...</p>
<p>I have a parameter set as a widget: <code>v_catalog</code> contains a valid catalog name &quot;dev_catalog&quot;.</p>
<p>I have tried this on serverless and on a cluster, and in case it matters this is &quot;Azure Databricks&quot;.</p>
<p>While the first two commands (<code>USE CATALOG</code>, <code>COMMENT</code>) both work, none of the others (<code>ALTER</code>, <code>GRANT</code>) are working, and the error messages are generally &quot;SYNTAX ERROR&quot; which is not at all helpful.</p>
<p>How can I parameterize the DDL in this notebook?</p>
<p>I would also like to parameterize the <code>AD_GROUP_NAME</code> value too, eventually <em>(and I have checked that <code>AD_GROUP_NAME</code> is correct and it does exist)</em>.</p>
<p>I am rather surprised that <code>current_catalog()</code> does not work. The error here is complaining about the first <code>(</code> in <code>current_catalog()</code> for some reason....</p>
<pre><code> USE CATALOG IDENTIFIER(:v_catalog);
 
 COMMENT ON CATALOG IDENTIFIER(:v_catalog) IS 'This is a comment on my Catalog';
 
 -- All These Fail...
 
 ALTER CATALOG current_catalog() SET OWNER TO `AD_GROUP_NAME`;
 
 ALTER CATALOG IDENTIFIER(:v_catalog) SET OWNER TO `AD_GROUP_NAME`;
 
 ALTER CATALOG :v_actcat SET OWNER TO `AD_GROUP_NAME`;
 
 ALTER CATALOG `:v_actcat` SET OWNER TO `AD_GROUP_NAME`;
 
 ALTER CATALOG SET OWNER TO `AD_GROUP_NAME`;   -- I thought I'd try this but wasnt expecting it to work, and it doesn't.
 
 -- All These Fail...
 GRANT SELECT ON CATALOG current_catalog() TO `AD_GROUP_NAME`;
 
 GRANT SELECT ON CATALOG IDENTIFIER(:v_catalog) TO `AD_GROUP_NAME`;
 
 GRANT SELECT ON CATALOG :v_actcat TO `AD_GROUP_NAME`;
 
 GRANT SELECT ON CATALOG `:v_actcat` TO `AD_GROUP_NAME`;
</code></pre>
",0,0,0,2025-11-15T15:07:57+00:00,0,72,False
79820981,53341,United Kingdom,sql,Cause of and means to avoid &quot;flat BNL join&quot; in mariadb 10.11,"<p>I'm trying to help a user in another <a href=""https://stackoverflow.com/questions/79820232/correlated-queries-without-the-lateral-keyword/79820485#79820485"">stackoverflow question</a> and have bumped by head into a strange behaviour.</p>
<p>As I rarely use MariaDB, I'm opening another question to investigate the behaviour,</p>
<p>Simplified schema...</p>
<pre><code>CREATE TABLE source (
  id          INT,
  some_data   BIGINT,
  PRIMARY KEY (id)
);

CREATE TABLE lookup (
  id          BIGINT,
  a_value     NVARCHAR(32),
  PRIMARY KEY (id)
);
</code></pre>
<p>Simplified query...</p>
<pre><code>SELECT
  *
FROM
  source       AS s
INNER JOIN
  lookup       AS l
    ON l.id = some_function(s.some_data)
</code></pre>
<p>Apparently, due to the function call, mariadb chooses <code>Using join buffer (flat, BNL join)</code></p>
<p><em><strong>If I push the function results into a holding table, mariadb chooses a simple join, and is 1000x faster.</strong></em></p>
<pre><code>CREATE TABLE interim AS
  SELECT
    s.*,
    some_function(s.some_data)  AS lookup_id
  FROM
    source   AS s
;

SELECT
  *
FROM
  interim    AS i
INNER JOIN
  lookup     AS l
    ON l.id = i.lookup_id
;
</code></pre>
<ol>
<li>Why is this happening?</li>
<li>How can it be sensibly avoided?</li>
</ol>
<p><em>(An interim table doesn't seem sensible, and all that's needed is a boring hash join.)</em></p>
<br>
<p>Expanded fiddle with random data, user-defined-function, crude timings, etc...</p>
<ul>
<li><a href=""https://dbfiddle.uk/DDzE_yYd"" rel=""nofollow noreferrer"">https://dbfiddle.uk/DDzE_yYd</a></li>
</ul>
",3,3,0,2025-11-15T17:13:38+00:00,1,112,True
79821363,1093228,"Tehran, Tehran Province, Iran",sql,How to make C# code more readable and maintainable when using Dapper and raw SQL queries in C#?,"<p>When we are using Dapper, we should use raw SQL queries within our code, and if the query is complex with multiple joins and windowed query, it is very difficult to maintain such code.</p>
<p>Putting all those SQL statements inside the database as views/stored procedures is an option, but it has its limitations, especially in debugging.</p>
<p>Is there any other standard method like putting SQL queries inside <code>*.sql</code> files and read those file at compile time?</p>
",2,0,0,2025-11-16T09:26:53+00:00,8,190,True
79821595,31883978,,sql,BigQuery: How to replace 0 values with NULL in a column?,"<p>I’m trying to clean a BigQuery weather dataset where missing values were entered as 0. My UPDATE query to replace 0 with NULL is throwing an error. How can I correctly convert these zeroes to null values?</p>
",-7,0,7,2025-11-16T16:36:59+00:00,1,74,True
79821615,23512643,,sql,SQL: Calculating percentage of time spent in a country,"<p>I have this table in SQL:</p>
<pre><code>CREATE TABLE myt (
    name VARCHAR(20),
    country VARCHAR(20),
    date_arrived DATE,
    date_left DATE
);

INSERT INTO myt (name, country, date_arrived, date_left) VALUES
('Alice',   'USA',     '2019-01-01', '2020-03-01'),
('Bob',     'Canada',  '2020-06-01', '2020-09-01'),
('Bob',     'Mexico',  '2020-10-01', '2021-02-01'),
('Charlie', 'UK',      '2021-06-01', '2021-08-01'),
('Diana',   'France',  '2019-12-01', '2020-07-01'),
('Diana',   'Germany', '2020-08-01', '2020-11-01'),
('Eve',     'USA',     '2020-12-01', '2021-02-01'),
('Eve',     'Canada',  '2021-03-01', '9999-12-31'),
('Frank',   'Mexico',  '2019-06-01', '2019-12-01'),
('Frank',   'USA',     '2020-03-01', '2020-09-01'),
('Frank',   'UK',      '2020-11-01', '9999-12-31'),
('Grace',   'France',  '2019-01-01', '2020-04-01'),
('Grace',   'Germany', '2021-05-15', '2021-07-01'),
('Henry',   'USA',     '2020-05-01', '2020-08-15'),
('Henry',   'Canada',  '2020-08-15', '2020-12-01'),
('Henry',   'USA',  '2020-12-01', '2021-05-01');
</code></pre>
<p><strong>I want to find out the following:</strong></p>
<p>For every person that has data between May-1-2020 and May-1-2021</p>
<ul>
<li><p>What percent of time did they spend in each country during this time period? (for each person, the percentages should sum to 100)</p>
</li>
<li><p>If they have data between this whole time period, I use 365 as a denominator. If they do not, I use the total number of days they have, i.e. max(365, total days)</p>
</li>
<li><p>Its impossible for a person to be in more than 1 country in a given day (but its possible for a person to move back and forth between two countries in the same year)</p>
</li>
</ul>
<p>The final result should look like this:</p>
<pre><code>name   country   in_country   total_days   total_days_spent_in_country
Bob    Canada     42.79%         215                 92
Bob    Mexico     57.21%         215                123
Diana  France     39.87%         153                 61
Diana  Germany    60.13%         153                 92
Eve    USA        50.41%         123                 62
Eve    Canada     49.59%         123                 61
Frank  USA        40.46%         304                123
Frank  UK         59.54%         304                181
Henry  USA        70.41%         365                257
Henry  Canada     29.59%         365                108
</code></pre>
<p>I started and tried to write a series of CTEs corresponding to each logical step to do this:</p>
<pre><code>WITH date_range AS (

    SELECT 
        DATE('2020-05-01') AS period_start,
        DATE('2021-05-01') AS period_end
),
person_countries AS (

    SELECT DISTINCT
        t.name,
        t.country,
        t.date_arrived,
        t.date_left,
        r.period_start,
        r.period_end
    FROM myt t
    JOIN date_range r ON 1=1
    WHERE 

        t.date_arrived &lt; r.period_end 
        AND t.date_left &gt; r.period_start
),
overlap_days AS (
    SELECT
        name,
        country,

        CASE 
            WHEN date_arrived &gt;= period_start THEN date_arrived
            ELSE period_start
        END AS effective_start,
        CASE 
            WHEN date_left &lt;= period_end THEN date_left
            ELSE period_end
        END AS effective_end
    FROM person_countries
),
days_per_country AS (

    SELECT
        name,
        country,

        (effective_end - effective_start) AS days_in_country
    FROM overlap_days
),
person_totals AS (
    SELECT
        name,
        SUM(days_in_country) AS total_days,

        CASE 
            WHEN SUM(days_in_country) &gt;= 365 THEN 365
            ELSE SUM(days_in_country)
        END AS denominator
    FROM days_per_country
    GROUP BY name
),
final_percentages AS (
    SELECT
        d.name,
        d.country,
        d.days_in_country,
        t.denominator,
     
        CAST(d.days_in_country AS DECIMAL(10,2)) * 100.0 / t.denominator AS percentage
    FROM days_per_country d
    JOIN person_totals t ON d.name = t.name
)

SELECT
    name,
    country,
    CAST(percentage AS DECIMAL(5,2)) || '%' AS in_country
FROM final_percentages
ORDER BY name, country;
</code></pre>
<p>But before I could finalize, this is giving me a blank result when I run it:</p>
<pre><code>[1] name         country      date_arrived date_left    period_start period_end  
&lt;0 rows&gt; (or 0-length row.names)
</code></pre>
<p>Can someone please help me fix this?</p>
<hr />
<p>An R solution from <a href=""https://stackoverflow.com/questions/79821231/calculating-the-percent-of-time-spent-in-a-country"">Calculating the percent of time spent in a country</a> :</p>
<pre><code>myt2 |&gt;
    transform(effective_arrival   = pmax(date_arrived, as.Date('2020-05-01')),
              effective_departure = pmin(date_left, as.Date('2021-05-01'))) |&gt;
    subset(effective_arrival &lt; effective_departure) |&gt;  # Changed to strict inequality
    within({
        stay = as.numeric(effective_departure - effective_arrival)
        total_days_spent_in_country = ave(stay, name, country, FUN=sum)
        total_days = ave(stay, name, FUN=sum)
        in_country = sprintf('%.2f%%', total_days_spent_in_country / total_days * 100)
        rm(effective_departure, effective_arrival, stay)
    }) |&gt;
    subset(select = -c(date_arrived, date_left)) |&gt;
    unique(by = c(&quot;name&quot;, &quot;country&quot;))
</code></pre>
<p>Based on the accepted answer below, here is a version that does not use the ratio_to_report() function:</p>
<pre><code>WITH cte AS (
  SELECT *, 
   DAYS_BETWEEN(CASE WHEN date_left &gt;= DATE('2020-05-01')
                     THEN LEAST(date_left, DATE('2021-05-01'))
                END,
                CASE WHEN date_arrived &lt;= DATE('2021-05-01')
                     THEN GREATEST(date_arrived, DATE('2020-05-01'))
                END) AS days_in_country
  FROM myt
),
aggregated AS (
  SELECT 
    Name, 
    Country,
    SUM(days_in_country) AS total_days_spent_in_country
  FROM cte
  WHERE days_in_country IS NOT NULL
  GROUP BY Name, Country
),
with_totals AS (
  SELECT 
    a.Name,
    a.Country,
    a.total_days_spent_in_country,
    SUM(a.total_days_spent_in_country) OVER(PARTITION BY a.Name) AS total_days
  FROM aggregated a
)
SELECT 
  Name,
  Country,
  ROUND(CAST(total_days_spent_in_country AS DECIMAL) * 100.0 / total_days, 2) AS in_country,
  total_days,
  total_days_spent_in_country
FROM with_totals
ORDER BY Name, Country;
</code></pre>
",2,2,0,2025-11-16T17:12:28+00:00,1,136,True
79822122,4934150,,sql,"Calculate difference between two values, including those only appearing once within the partition","<p><strong><a href=""https://dbfiddle.uk/Fea5xW31"" rel=""nofollow noreferrer"">DB&lt;&gt;Fiddle</a></strong></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE inventory (
    id SERIAL PRIMARY KEY,
    stock_date DATE,
    product VARCHAR,
    stock_balance INT
);
INSERT INTO inventory
(stock_date, product, stock_balance)VALUES 
('2025-10-01','prod_01','100'),
('2025-10-01','prod_02','500'),
('2025-10-31','prod_01','800'),
('2025-10-31','prod_02','600'),
('2025-10-31','prod_03','700'),
('2025-10-31','prod_04','400');
</code></pre>
<p><strong>Expected Result:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>product</th>
<th>stock_date</th>
<th>stock_balance</th>
<th>change</th>
</tr>
</thead>
<tbody>
<tr>
<td>prod_01</td>
<td>2025-10-01</td>
<td>100</td>
<td>700</td>
</tr>
<tr>
<td>prod_01</td>
<td>2025-10-31</td>
<td>800</td>
<td>700</td>
</tr>
<tr>
<td>prod_02</td>
<td>2025-10-01</td>
<td>500</td>
<td>100</td>
</tr>
<tr>
<td>prod_02</td>
<td>2025-10-31</td>
<td>600</td>
<td>100</td>
</tr>
<tr>
<td>prod_03</td>
<td>2025-10-01</td>
<td>0</td>
<td>700</td>
</tr>
<tr>
<td>prod_03</td>
<td>2025-10-31</td>
<td>700</td>
<td>700</td>
</tr>
<tr>
<td>prod_04</td>
<td>2025-10-01</td>
<td>0</td>
<td>400</td>
</tr>
<tr>
<td>prod_04</td>
<td>2025-10-31</td>
<td>400</td>
<td>400</td>
</tr>
</tbody>
</table></div>
<p>I want to display for each <code>product</code> in the table the <code>stock_balance</code> on <code>2025-10-01</code> and <code>2025-10-31</code> and calculate the <code>change</code> of the <code>stock_balance</code> between these two dates in a separate column.</p>
<p>So for I have been able to develop this query:</p>
<pre class=""lang-sql prettyprint-override""><code>select t1.product as product
      ,t1.stock_date as stock_date
      ,t1.stock_balance as stock_balance
      ,last_value(t1.stock_balance)over(partition by t1.product 
                                        order by stock_date 
                                        rows between unbounded preceding 
                                                 and unbounded following)
      -first_value(t1.stock_balance)over(partition by t1.product 
                                         order by stock_date
                                         rows between unbounded preceding 
                                                  and unbounded following) as change
from(select product as product
           ,stock_date as stock_date
           ,sum(stock_balance) as stock_balance
     from inventory
     group by 1,2
    ) as t1
group by 1,2,3
order by 1,2,3;
</code></pre>
<p>The query provides the result correctly for <code>prod_01</code> and <code>prod_02</code>. <br>
However, for <code>prod_03</code> and <code>prod_04</code> it is not correct. I assume this is because they appear only on <code>stock_date = 2025-10-31</code>. <br></p>
<p>How do I need to modify the query to also get these products displayed as in the expected results? <br>
<em>(I guess somehow I need to insert an empty row for these products for <code>stock_date = 2025-10-01</code>)</em></p>
",2,2,0,2025-11-17T09:49:41+00:00,3,132,True
79822318,31481891,,sql,Why are my data quality validation rules not triggering for null values in my dataset?,"<p>I’m working on a data quality workflow where I validate incoming records for null or missing values.<br />
Even when a column clearly contains nulls, my rule doesn’t trigger and the record passes validation.</p>
<p>Here’s the logic I’m using:</p>
<pre><code>CASE   
   WHEN column_name IS NULL THEN 'FAIL'  
   ELSE 'PASS'  
END
</code></pre>
<p>But <code>NULL</code> records still return <strong>PASS</strong>.</p>
<p>Things I’ve checked:</p>
<ul>
<li><p>The column datatype is VARCHAR</p>
</li>
<li><p>The source file is CSV</p>
</li>
<li><p>Some values look empty (<code>&quot;&quot;</code>) but not sure if they are treated as NULL</p>
</li>
<li><p><strong>My question:</strong></p>
<p>Is there a difference between empty string and NULL in SQL during validation? If so, how can I reliably detect actual NULL vs whitespace vs empty string?</p>
</li>
</ul>
",0,0,0,2025-11-17T12:55:32+00:00,1,86,True
79822511,9540123,,sql,"Avoid invalid cast inside CASE expression, in a query plan not respecting JOIN","<p>I'm trying to get partitioned tables from a postgres DB. Our partitions have the date in them in various formats which I'm extracting like this</p>
<pre class=""lang-sql prettyprint-override""><code>  SELECT tab.relname AS table_name,
        (CASE
          WHEN relname ~ '(\d{8})$' THEN to_date(right(relname, 8), 'YYYYMMDD')
          WHEN relname ~ '(\d{6})$' THEN to_date(right(relname, 6), 'YYYYMM')
          WHEN relname ~ '(\d{6})_' THEN  to_date(substring(relname from '(\d{6})'), 'YYYYMMDD')
          WHEN relname ~ '(\d{8})_' THEN  to_date(substring(relname from '(\d{8})'), 'YYYYMMDD')
          ELSE NULL
      END)::date AS extracted_date 
    FROM pg_inherits AS inh
    JOIN pg_class AS tab
    ON inh.inhrelid = tab.oid
  WHERE tab.relkind IN ('r','p');
</code></pre>
<p>This works fine. But if I move the CASE expression into the where clause I get an error 'ERROR: date/time field value out of range: &quot;81468193&quot;'</p>
<p>I tried creating a temp table and inserting the data and then querying and it works fine.</p>
<p>I then tried creating a CTE like this</p>
<pre class=""lang-sql prettyprint-override""><code>WITH partitioned_tabs AS
  (
  SELECT
  tab.relname AS table_name
        ,(CASE
          WHEN relname ~ '(\d{8})$' THEN to_date(right(relname, 8), 'YYYYMMDD')
          WHEN relname ~ '(\d{6})$' THEN to_date(right(relname, 6), 'YYYYMM')
          WHEN relname ~ '(\d{6})_' THEN  to_date(substring(relname from '(\d{6})'), 'YYYYMMDD')
          WHEN relname ~ '(\d{8})_' THEN  to_date(substring(relname from '(\d{8})'), 'YYYYMMDD')
          ELSE NULL
      END)::date AS extracted_date 
    FROM pg_inherits AS inh
    JOIN pg_class AS tab
    ON inh.inhrelid = tab.oid
  WHERE tab.relkind IN ('r','p')
  )
SELECT *
FROM partitioned_tabs  AS td 
WHERE td.extracted_date = '2025-01-01';
</code></pre>
<p>I get the same error. There is a table in the DB which matches the pattern in the error, but that table is not partitioned, so if I run this
<code>SELECT * FROM pg_class WHERE relname LIKE '%81468193%';</code> it returns the table but if I run this</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
    FROM pg_inherits AS inh
    JOIN pg_class AS tab
    ON inh.inhrelid = tab.oid
WHERE relname LIKE '%81468193%';
</code></pre>
<p>It returns nothing.
So I got the query plan which is</p>
<pre><code>Gather  (cost=500.43..49454.41 rows=394 width=68)
  Workers Planned: 4
  -&gt;  Nested Loop  (cost=0.42..48950.47 rows=98 width=68)
        -&gt;  Parallel Seq Scan on pg_class tab  (cost=0.00..48760.92 rows=130 width=68)
              Filter: ((relkind = ANY ('{r,p}'::&quot;char&quot;[])) AND (CASE 
                        WHEN (relname ~ '(\d{8})$'::text) THEN to_date((regexp_match((relname)::text, '(\d{8})$'::text))[1], 'YYYYMMDD'::text)
                        WHEN (relname ~ '(\d{6})$'::text) THEN to_date((regexp_match((relname)::text, '(\d{6})$'::text))[1], 'YYYYMM'::text)
                        WHEN (relname ~ '_(\d{6})_'::text) THEN to_date((regexp_match((relname)::text, '_(\d{6})_'::text))[1], 'YYYYMMDD'::text)
                        WHEN (relname ~ '_(\d{8})_'::text) THEN to_date((regexp_match((relname)::text, '_(\d{8})_'::text))[1], 'YYYYMMDD'::text)
                        ELSE NULL::date END = '2025-01-01'::date)
                      )
        -&gt;  Index Only Scan using pg_inherits_relid_seqno_index on pg_inherits inh  (cost=0.42..1.42 rows=1 width=4)
              Index Cond: (inhrelid = tab.oid)
</code></pre>
<p>It seems to me that the parallel worker is trying to query only pg_class and then running into a date error. Is this possible? Am I doing something stupid?
This is postgres 15</p>
",1,1,0,2025-11-17T15:58:48+00:00,1,115,False
79822735,7829640,,sql,Splitting SQL output evenly into 3 HTML columns,"<p>I am trying to take the output of a SQL query and split it evenly into 3 HTML columns such as flexbox divs. Obviously the output total won't always be evenly divisible by 3 and the first 2 totals might need to be rounded up with the remainder in the 3rd column. The total will change over time so creating the 3 totals by dividing the original total will not be set numbers and will need to be dynamic. I would prefer to this with SQL and PHP and not have to use JavaScript.</p>
",2,0,0,2025-11-17T20:09:34+00:00,7,120,True
79822762,12817776,,sql,"Regex function to extract numbers from string-type values, separated by &quot;-&quot;, to get the difference of each two values","<p>I'm 100% new to Regex so I've been floundering about on regex101 trying to figure out how to get my desired output. I am using PostgreSQL to write a query to extract a set of values from the string. Once extracted, I need to convert them to int types and then take the difference between the two values.</p>
<p>A sample of the data I am working with can be found here:<br />
<a href=""https://regex101.com/r/Twkphj/3"" rel=""noreferrer"">https://regex101.com/r/Twkphj/3</a> (Each line break is a new record/value of the data.)
<a href=""https://dbfiddle.uk/ELitHDni"" rel=""noreferrer"">https://dbfiddle.uk/ELitHDni</a></p>
<pre class=""lang-sql prettyprint-override""><code>create table t(id int generated always as identity primary key, data text);
insert into t(data)values
('01-08,24-32')
,('38-70')
,('01-25, 27-38')
,('1-6,13-20,25-32')
,('1-4, 7-8, 11-12')
,('1-83,85-112')
,(NULL)
,('NULL')
,('162-169')
,('145-167, 169-214, 217-218, 247-254, 256-257, 382')
,('01-17, 23-27')
,('73-120, 145-192, 217-264, 289-336, 361-408, 433-480, 505-552, 577-624, 649-696, 721-768')
,('1-33, 37-45');
</code></pre>
<p>The end goal is to get an output like this:</p>
<pre><code>SELECT
data, 
regex(difference of &quot;data&quot; points)
FROM table
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>data</th>
<th>difference (inclusive)</th>
</tr>
</thead>
<tbody>
<tr>
<td>01-08,24-32</td>
<td>8,9</td>
</tr>
<tr>
<td>145-167, 169-214, 217-218, 247-254, 256-257, 382</td>
<td>23, 46, 2, 8, 2, 1</td>
</tr>
</tbody>
</table></div>
<p>From here, I can just use split() if the stakeholder needs to break it down further.</p>
<p>Again I'm not to familiar with regex so regex101 has been great to breakdown and understand why certain &quot;tokens&quot;(?) are used. I think I need to stick to PCRE2 if that matters.</p>
<p>TIA</p>
",5,5,0,2025-11-17T20:44:52+00:00,3,181,True
79822777,13115896,,sql,Postgres Recursive Query Behavior,"<p>I have a table <code>test</code> with two columns, each referencing the other</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID1</th>
<th>ID2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>4</td>
</tr>
</tbody>
</table></div>
<p>Given an <code>id</code>, I need to traverse the chain to return all the rows traversing the chain. Below is the query I have written:</p>
<pre><code>WITH RECURSIVE 
-- backward 
    backward_loop as (
    SELECT id1, id2 
    FROM test t
    WHERE test.id2 = '&lt;id&gt;'
    UNION
    SELECT id1, id2 
    FROM
        test t
        INNER JOIN backward_loop b ON (b.id1 = t.id2)
    ),
     -- forward 
    forward_loop as (
    SELECT id1, id2
    FROM test t
    WHERE test.id1 = '&lt;id&gt;'
    UNION
    SELECT id1, id2
    FROM
        test t
        INNER JOIN forward_loop f ON (f.id2 = t.id1)
)   
SELECT id1, id2
FROM backward_loop bl
UNION ALL
SELECT id1, id2
FROM forward_loop fl;
</code></pre>
<p>E.g. if given 2, I need to return all the rows (2,1), (3,2), (4,3) and (5,4). The query works fine and returns all the rows correctly.</p>
<p>However, when I have the below data, where one of the <code>id</code>s (50) has multiple mappings:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID1</th>
<th>ID2</th>
</tr>
</thead>
<tbody>
<tr>
<td>20</td>
<td>10</td>
</tr>
<tr>
<td>30</td>
<td>20</td>
</tr>
<tr>
<td>40</td>
<td>30</td>
</tr>
<tr>
<td>50</td>
<td>40</td>
</tr>
<tr>
<td><strong>50</strong></td>
<td><strong>60</strong></td>
</tr>
</tbody>
</table></div>
<p>and when I run the query passing 20, I get only (20,10), (30,20), (40,30) and (50,40), but not the row (50,60). When I send in 50, I get back all the rows, but it's an issue when I input any other id.</p>
<p>Any idea how I can get the row (50,60) as well?</p>
",1,2,1,2025-11-17T21:02:47+00:00,1,121,True
79822878,656388,,sql,Creating Materialized view in PG 17 ignores data types in SELECT,"<p>I have the following SQL to create a materialized view and the SQL seems reasonable to me but it's unclear from Postgres docs what should happen in this case.</p>
<p>I am listing the columns when creating the matview and casting to the correct types in the <code>SELECT</code> and all columns work except one, while it's cast to <code>numeric(9, 2)</code> Postgres returns that column as <code>numeric</code> with no precision. The values actually appear to be cast correctly, although the column type is wrong. Here's an excerpt:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE MATERIALIZED VIEW &quot;plus&quot;.&quot;quote_exposure_chars&quot; (... &quot;distance_to_coast_ft&quot;, ...) 
AS 
    SELECT 
        ..., 
        MAX(CASE
                WHEN (qexpcf.field_name = 'distance_to_coast_ft'::text) 
                    THEN (qexpcf.field_value)::numeric(9,2)
                ELSE NULL::numeric
            END) AS ..., 
        distance_to_coast_ft, ...
</code></pre>
<p>Is it something about casting the <em><code>null</code></em> to <code>numeric</code> or not specifying the precision of the new alias column name? The PG doc on matviews seems to gloss over all of this.</p>
",1,2,1,2025-11-17T23:47:16+00:00,1,90,True
79824063,11002366,,sql,Query in SQL - eliminating the rows with multiple values,"<p>I need to eliminate the rows that have two different values in col2 but have the same value in column 3.</p>
<p>What query would select only the last 3 rows and ignore the first two rows of this example table?</p>
<p>Edit: If you look at the highlighted fields, They have same col3 values but different col1 and col 2. I do not want my query to pick these rows as they have two different values in col2 but same value in col3. My query should eliminate these two rows and pick only rows that has only one value in col 2</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
<th>col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>2345</td>
<td>1</td>
<td>4534</td>
</tr>
<tr>
<td>3456</td>
<td>4</td>
<td>4534</td>
</tr>
<tr>
<td>4645</td>
<td>1</td>
<td>9876</td>
</tr>
<tr>
<td>6545</td>
<td>1</td>
<td>5678</td>
</tr>
<tr>
<td>3678</td>
<td>1</td>
<td>2790</td>
</tr>
</tbody>
</table></div>
<p><a href=""https://i.sstatic.net/GToV3PQE.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",-3,2,5,2025-11-19T04:36:30+00:00,1,153,False
79824104,11195677,,sql,Make MySQL evaluate subquery first,"<pre class=""lang-sql prettyprint-override""><code>SELECT    tt.trans_type_name AS transaction_type,
          trans.transaction_time,
          a.trans_action_name AS transaction_action_name,
          trans.transaction_notes
FROM(SELECT Cast(added_on AS CHAR) AS transaction_time,
            notes                  AS transaction_notes,
            trans_type_id          AS trans_type,
            trans_action_id        AS action_id
     FROM   transactions
     WHERE  added_by = 'service_app'
     AND    notes LIKE %(domain)s
     AND    added_on BETWEEN %(transactionstartdate)s 
                         AND %(transactionenddate)s) trans
LEFT JOIN transaction_types tt
ON        trans.trans_type = tt.trans_type_id
LEFT JOIN transaction_actions a
ON        trans.action_id = a.trans_action_id
ORDER BY  trans.transaction_time DESC 
</code></pre>
<p>This is a query I rewritten because the older query had the error:</p>
<blockquote>
<p>The SELECT would examine more than MAX_JOIN_SIZE rows; check your WHERE and use SET SQL_BIG_SELECTS=1 or SET MAX_JOIN_SIZE=# if the SELECT is okay</p>
</blockquote>
<p>The new query(written above) is more faster but I am still getting the <code>MAX_JOIN</code> error. The thing is, I cannot set <code>SET SQL_BIG_SELECTS=1</code>. My wish is evaluating the subquery first because for the values I plug, there are only 3 rows. The join needs to only happen for the 3 rows since it is just replacing the ID with it's corresponding value.</p>
<p>Is there a way? Thanks in advance.</p>
<p>Edit:
Some more clarity - 3 tables <strong>transactions</strong>, <strong>transaction_types</strong> and <strong>transaction_actions</strong>.
The base table is the <strong>transactions</strong> which has more than million rows. It has two IDs which references the other two tables. I want to select the names instead of IDs so joining it.</p>
",1,3,2,2025-11-19T05:58:00+00:00,1,153,False
79824204,9720075,,sql,Permission to view the body of a procedure/view/function from a read-only user on Oracle Database,"<p>I was wondering if there is any privilege or work around that allows a read-only user I created on Oracle to see the DDL of procedure/view/function from third party apps like SQL Developer.</p>
<p>Currently with the given permissions, the read-only user is seeing like this, even though there are objects in another user IMS_CT_OWNER the list is empty.</p>
<p><img src=""https://i.sstatic.net/xehoULiI.png"" alt=""enter image description here"" /></p>
<p>[SQL DEV View]</p>
<p>Read-Only-User Creations Scripts</p>
<pre><code>-------------------------------------------
CREATE USER readonly_user IDENTIFIED BY PASS:

GRANT CREATE SESSION TO readonly_user;  

GRANT SELECT ANY TABLE TO readonly_user;
GRANT SELECT ANY VIEW TO readonly_user; 

GRANT SELECT ON DBA_TABLES TO readonly_user;
GRANT SELECT ON DBA_VIEWS TO readonly_user;

GRANT SELECT ON DBA_SOURCE TO readonly_user;
GRANT SELECT ON ALL_SOURCE TO readonly_user;
</code></pre>
",0,0,0,2025-11-19T08:25:00+00:00,3,47,True
79824438,15005274,,sql,Oracle SQL Test to identify missing rows for unique identifier,"<p>I am trying to find an Oracle SQL to test whether a table (<code>CONTACTS</code>) contains two rows of data for each primary key (PK) called <code>REF_NO</code>.</p>
<p>The table has a PK called <code>REF_NO</code> and a second PK called <code>CONT_IND</code>. <code>CONT_IND</code> can be either 1 or 2 - no other values possible, including <code>NULL</code>.</p>
<p>For each <code>REF_NO</code>, the table should contain:</p>
<ul>
<li>one row where <code>CONT_IND = 1</code></li>
<li>one row where <code>CONT_IND = 2</code></li>
</ul>
<p>I'm trying to find an output that shows:</p>
<pre><code>FAIL: REF_NO 'X' CONT_IND 1 IS MISSING 
</code></pre>
<p>or</p>
<pre><code>FAIL: REF_NO 'X' CONT_IND 2 IS MISSING
</code></pre>
<p>If both rows are present, the output should show:</p>
<pre><code>PASS
</code></pre>
",-2,1,3,2025-11-19T12:23:45+00:00,2,102,True
79824640,3760090,,sql,Grouping of records in case values are null,"<p>We have got a table with a identifier, a key/value pairs and a start and end timestamp which indicates the valid period for the values.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">MASTER_WORK_ORDR_ID</th>
<th style=""text-align: left;"">START_TS</th>
<th style=""text-align: left;"">END_TS</th>
<th style=""text-align: left;"">WORK_ORDR_ID_CTXT</th>
<th style=""text-align: left;"">WORK_ORDR_NUM</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;"">WFM</td>
<td style=""text-align: left;"">NPA 25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407642</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407642</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">714372625</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200409</td>
<td style=""text-align: left;"">NPA</td>
<td style=""text-align: left;"">14743010</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200410</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;"">NPA</td>
<td style=""text-align: left;"">268556282</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407643</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772443</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407643</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772443</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">444207219</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772444</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617407</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">714372625</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772444</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617407</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617408</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590662</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">444207219</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617408</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590662</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590663</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111454</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819-2</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590663</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111454</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">575199896</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111455</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934714</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">444207219</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111455</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934714</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934715</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;"">WFM_INTERNAL</td>
<td style=""text-align: left;"">575199896</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934715</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;"">CALLID</td>
<td style=""text-align: left;"">NPA 25060213905819-2</td>
</tr>
</tbody>
</table></div>
<p>I pivot this to get a more readable format</p>
<pre><code>    SELECT
    MASTER_WORK_ORDR_ID WORK_ORDR_ID 
    ,START_TS 
    ,END_TS 
    ,&quot;'UTS'_VAL&quot; UTS
    ,&quot;'SER'_VAL&quot; SER
    ,&quot;'SPRINT'_VAL&quot; SPRINT 
    ,&quot;'LDB'_VAL&quot; LDB
    ,&quot;'NPA'_VAL&quot; NPA
    ,&quot;'WFM_INTERNAL'_VAL&quot; WFM_INTERNAL
    ,&quot;'INTERNAL'_VAL&quot; INTERNAL
    ,&quot;'ABDTBR'_VAL&quot; ABDTBR 
    ,&quot;'WFMIL'_VAL&quot; WFMIL 
    ,&quot;'WFM'_VAL&quot; WFM
    ,&quot;'CALLID'_VAL&quot; CALLID 
FROM 
    (
    SELECT
        MASTER_WORK_ORDR_ID
        ,START_TS
        ,END_TS 
        ,WORK_ORDR_ID_CTXT 
        ,WORK_ORDR_NUM
    FROM P0_EDV1_ATO.WORK_ORDR_IDENTIFIER_HIST
    WHERE 1=1 
    AND MASTER_WORK_ORDR_ID = 955244397 
    )   s_tab 
    PIVOT   
    (
        Max(WORK_ORDR_NUM) val 
        FOR WORK_ORDR_ID_CTXT IN
            ( 
                'UTS'
                ,'SER'
                ,'SPRINT' 
                ,'LDB'
                ,'NPA'
                ,'WFM_INTERNAL'
                ,'INTERNAL' 
                ,'ABDTBR' 
                ,'WFMIL'
                ,'WFM'
                ,'CALLID' 
            ) 
    ) p_tab 
ORDER BY 1,2,3 DESC;                                                                                                                        
</code></pre>
<p>This gets me to the following</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">WORK_ORDR_ID</th>
<th style=""text-align: left;"">START_TS</th>
<th style=""text-align: left;"">END_TS</th>
<th style=""text-align: left;"">UTS</th>
<th style=""text-align: left;"">SER</th>
<th style=""text-align: left;"">SPRINT</th>
<th style=""text-align: left;"">LDB</th>
<th style=""text-align: left;"">NPA</th>
<th style=""text-align: left;"">WFM_INTERNAL</th>
<th style=""text-align: left;"">INTERNAL</th>
<th style=""text-align: left;"">ABDTBR</th>
<th style=""text-align: left;"">WFMIL</th>
<th style=""text-align: left;"">WFM</th>
<th style=""text-align: left;"">CALLID</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200409</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">14743010</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407642</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200410</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407643</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772443</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772444</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617407</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617408</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590662</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590663</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111454</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">575199896</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819-2</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111455</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934714</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934715</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">575199896</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819-2</td>
</tr>
</tbody>
</table></div>
<p>But this does not get me what I really need.
In fact I need, on each line, all values which are valid for that period of time indicated by start and end ts.  So, the ones that changed and which are valid as from that start_ts but also the ones which are still valid from above since the end_ts is higher than the end_ts of the current record.</p>
<p>The end result should be</p>
<ul>
<li>row 1 has 1 value (can be more) which is valid to the end of time so this should be propagated to all records.</li>
<li>row 2 throws in another value which is only valid till 02/06/2025 12:31:21 but the value of row 3 is also valid in that timeframe.</li>
<li>row 4 changes the value from row 2 and it becomes valid from that time on until eternity so it must be propagated to all records underneath.</li>
<li>etc.</li>
</ul>
<p>Values must be merged together as long as they are valid, that's the bottom line.</p>
<p>It will get me this</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">WORK_ORDR_ID</th>
<th style=""text-align: left;"">START_TS</th>
<th style=""text-align: left;"">END_TS</th>
<th style=""text-align: left;"">UTS</th>
<th style=""text-align: left;"">SER</th>
<th style=""text-align: left;"">SPRINT</th>
<th style=""text-align: left;"">LDB</th>
<th style=""text-align: left;"">NPA</th>
<th style=""text-align: left;"">WFM_INTERNAL</th>
<th style=""text-align: left;"">INTERNAL</th>
<th style=""text-align: left;"">ABDTBR</th>
<th style=""text-align: left;"">WFMIL</th>
<th style=""text-align: left;"">WFM</th>
<th style=""text-align: left;"">CALLID</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200409</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">14743010</td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407642</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200410</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407643</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772443</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.772444</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617407</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617408</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590662</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590663</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111454</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">575199896</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-2</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:44.111455</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934714</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934715</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">575199896</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-2</td>
</tr>
</tbody>
</table></div>
<p>The next step is grouping on all values and taking the min and max timestamps to get the outer timeframe in which values are valid.  You'll notice that there is an overlap.  This is due to the fact that a new version of a work order is created before the old one is completed.</p>
<p>As final result we should get</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">WORK_ORDR_ID</th>
<th style=""text-align: left;"">START_TS</th>
<th style=""text-align: left;"">END_TS</th>
<th style=""text-align: left;"">UTS</th>
<th style=""text-align: left;"">SER</th>
<th style=""text-align: left;"">SPRINT</th>
<th style=""text-align: left;"">LDB</th>
<th style=""text-align: left;"">NPA</th>
<th style=""text-align: left;"">WFM_INTERNAL</th>
<th style=""text-align: left;"">INTERNAL</th>
<th style=""text-align: left;"">ABDTBR</th>
<th style=""text-align: left;"">WFMIL</th>
<th style=""text-align: left;"">WFM</th>
<th style=""text-align: left;"">CALLID</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">02/06/2025 12:31:21.200409</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">14743010</td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;""></td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">02/06/2025 12:27:38.882743</td>
<td style=""text-align: left;"">14/06/2025 17:14:52.617407</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">714372625</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">14/06/2025 17:08:09.407643</td>
<td style=""text-align: left;"">16/06/2025 10:07:21.934714</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">444207219</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-1</td>
</tr>
<tr>
<td style=""text-align: left;"">955.244.397</td>
<td style=""text-align: left;"">16/06/2025 10:05:43.590663</td>
<td style=""text-align: left;"">31/12/9999 23:59:59.999999</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">268556282</td>
<td style=""text-align: left;"">575199896</td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;""></td>
<td style=""text-align: left;"">NPA   25060213905819</td>
<td style=""text-align: left;"">NPA   25060213905819-2</td>
</tr>
</tbody>
</table></div>
",0,0,0,2025-11-19T15:26:04+00:00,2,131,True
79824655,5304058,San fransisco,sql,How to group the data to avoid multiple rows,"<p>I have an employee Manager table where you can see for every deptID, there are multiple rows.
A few rows have null employeename and a few rows have null managername for same DeptID. i would like to aggregate it to keep all employees and Manager tied together instead of separate rows for Employees and Manager.</p>
<p>In below example, for DeptID 24567, Sam and John should have the same manager under Manager Name instead of a separate row for manager.</p>
<pre><code>DROP table #Employee_Manager
Create table #Employee_Manager
(
DeptID int,
EmployeeName varchar(100),
Department varchar(50),
ManagerName varchar(100)
)
   
INSERT INTO #Employee_Manager
    SELECT 24567, 'Sam', 'IT',Null
    UNION
    SELECT 24567, Null, 'IT','Pam'
    UNION
    SELECT 24567, 'John', 'IT',Null
    UNION
    SELECT 26789, 'Mary', 'Sales',Null
    UNION
    SELECT 26789, Null, 'Sales','David'
    UNION
    SELECT 10009, 'Arun', 'Service',Null
    UNION
    SELECT 10009, Null, 'Service','Nancy'
</code></pre>
<p>This is the current output:</p>
<p><a href=""https://i.sstatic.net/824iGceT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/824iGceT.png"" alt=""enter image description here"" /></a></p>
<p>I tried below query, but it just gives me max of employee if there are multiple employees.</p>
<pre class=""lang-sql prettyprint-override""><code>select DeptID,
max(employeename) as employeename,
max(Department) as Department,
max(managername) as managername
from #Employee_Manager 
--where deptID='24567'
group by DeptID
</code></pre>
<p>Expected output:</p>
<p><a href=""https://i.sstatic.net/bmKN4TLU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmKN4TLU.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please guide me here?</p>
",0,1,1,2025-11-19T15:39:11+00:00,3,98,True
79824846,459,United States,sql,Redshift PartiQL for unpivoting the keys and values in a Map typed SUPER,"<p>AWS <a href=""https://docs.aws.amazon.com/redshift/latest/dg/query-super.html"" rel=""nofollow noreferrer"">documentation for querying Redshift's super</a> is very much all in on supers that are arrays.</p>
<p>But, I happen to want a <code>super</code> that is a map, as I'm sure a number of people do, and I would like to then be able to <code>unpivot</code> it, maybe filter with it, maybe join with it.</p>
<p>Because examples I find online generally don't seem to show you how the data they're querying is actually built or inserted for testing, I'll just show you my test case here:</p>
<pre class=""lang-sql prettyprint-override""><code>with s as (
    select 1::INT as session_id,
           '{&quot;token_id&quot;: &quot;token1&quot;}'::SUPER as ids
    union all
    select 2::INT,
           '{&quot;token_id&quot;: &quot;token2&quot;, &quot;user_id&quot;: &quot;me&quot;}'::SUPER
    union all
    select 3::INT,
           '{&quot;user_id&quot;: &quot;you&quot;}'::SUPER
    union all
    select 4::INT,
           '{}'::SUPER
    )
select session_id, ids
from s;
</code></pre>
<p>Giving:</p>
<pre class=""lang-none prettyprint-override""><code>1  &quot;&quot;&quot;{\&quot;&quot;token_id\&quot;&quot;: \&quot;&quot;token1\&quot;&quot;}&quot;&quot;&quot;                          
2  &quot;&quot;&quot;{\&quot;&quot;token_id\&quot;&quot;: \&quot;&quot;token2\&quot;&quot;, \&quot;&quot;user_id\&quot;&quot;: \&quot;&quot;me\&quot;&quot;}&quot;&quot;&quot; 
3  &quot;&quot;&quot;{\&quot;&quot;user_id\&quot;&quot;: \&quot;&quot;you\&quot;&quot;}&quot;&quot;&quot;                              
4  &quot;&quot;&quot;{}&quot;&quot;&quot;                                                      
</code></pre>
<p>Now if you want to <code>UNPIVOT</code> here … well … I maybe see what's wrong, but there just aren't great examples so I tried the following, keeping the with section:</p>
<pre class=""lang-sql prettyprint-override""><code>…
select session_id, ids, JSON_TYPEOF(ids) ids_type, id_name, id_value, JSON_TYPEOF(id_value) id_value_type
from s ss, UNPIVOT ss.ids as id_value at id_name;
</code></pre>
<p>Giving you no rows. Clearly there's an issue, but little indication of what since it's happy with the syntax of the query. I was expecting to get some of the main ids at least.</p>
",0,0,0,2025-11-19T19:11:43+00:00,1,36,False
79825097,17786412,,sql,Procedure to show percentage of nulls in TERADATA SQL ASSISTANT,"<p>I'm working in Teradata SQL Assistant and I need to create a stored procedure that, given a table name, returns the percentage of <code>NULL</code> values for each column in that table.</p>
<p>Right now, I generate the entire query from Python, using a function where I pass the table name and the list of columns. It saves me some time, but it would be much better if I could do all of this directly in a stored procedure.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>column_name</th>
<th>null_percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>col1</td>
<td>74</td>
</tr>
<tr>
<td>col2</td>
<td>32</td>
</tr>
<tr>
<td>....</td>
<td>....</td>
</tr>
</tbody>
</table></div>
",0,0,0,2025-11-20T04:53:55+00:00,5,74,True
79825241,31848846,,sql,How to remove duplicate rows based on array column subset relationship?,"<p>I have a DolphinDB table with an array vector column. I need to remove duplicate rows based on subset relationships within that column.</p>
<p><strong>Sample Input:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>sym</th>
<th>prices</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td><code>[3,4,5,6]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[3,4,5]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[2,4,5,6]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[5,6]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[7,9]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[7,9]</code></td>
</tr>
</tbody>
</table></div>
<p><strong>Expected Output:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>sym</th>
<th>prices</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td><code>[3,4,5,6]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[2,4,5,6]</code></td>
</tr>
<tr>
<td>a</td>
<td><code>[7,9]</code></td>
</tr>
</tbody>
</table></div>
<p><strong>Deduplication Logic:</strong></p>
<ol>
<li><p><strong>Subset Removal:</strong> If a row's <code>prices</code> array is a <strong>subset</strong> (i.e., fully contained) of another row's <code>prices</code> array, remove the subset row. In the example, <code>[3,4,5]</code> is a subset of <code>[3,4,5,6]</code>, so it is removed; similarly, <code>[5,6]</code> is also a subset of <code>[3,4,5,6]</code> and is removed.</p>
</li>
<li><p><strong>Full Duplicate Removal:</strong> If multiple rows have identical prices arrays, keep only one.</p>
</li>
</ol>
<p><strong>What I've Tried:</strong></p>
<p>I considered using group by to remove exact duplicates, but this approach cannot handle subset relationships.</p>
<p><strong>Core Question:</strong><br />
How can I perform this subset-based deduplication?</p>
",0,4,4,2025-11-20T08:00:06+00:00,1,114,False
79825480,21314431,,sql,Adding CHECK Constraint Without Error and Duplication,"<p>I have implemented this code to add a <code>check</code> constraint after creating the table</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE recruitment_data.candidate_experiences 
  ADD CONSTRAINT chk_experience_dates 
    CHECK (   experience_end_date IS NULL 
           OR experience_end_date &gt;= experience_start_date);
</code></pre>
<p>but it gives an error when I run it more than once:</p>
<blockquote>
<p><code>ERROR:  constraint &quot;chk_experience_dates&quot; for relation &quot;candidate_experiences&quot; already exists</code></p>
</blockquote>
<p>Since it is impossible to run <code>ADD CONSTRAINT</code> <strong><code>IF NOT EXISTS</code></strong>, is there any other way to run it twice without creating duplicate constraints, and without having to catch and handle or ignore the error?</p>
<p>What should I do to make this code <strong>re</strong>-runnable?</p>
",-2,1,3,2025-11-20T11:52:45+00:00,1,87,True
79826090,31918491,,sql,Performing product operation on consecutive true segments in a vector,"<p>I have two vectors of equal length, one integer vector and one boolean vector. I want to calculate the product of elements in the integer vector where the corresponding boolean vector has consecutive true segments, while preserving the original values for non-consecutive or single true positions.</p>
<p>Sample input:</p>
<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9]
b = [1, 1, 1, 0, 0, 1, 0, 1, 1]
</code></pre>
<p>Expected output:</p>
<pre><code>[6, 4, 5, 6, 7, 72]
//[1*2*3, 4, 5, 6, 7, 8*9]
</code></pre>
<p>I'm currently solving this problem using a functional programming approach with the following code.</p>
<pre><code>a = 1 2 3 4 5 6 7 8 9
b = 1 1 1 0 0 1 0 1 1
temp=1
nums=[]
for(i in 0:10)
{
    if(b[i]){
        temp*=a[i];
    }
    else{
        if(i==0||(i&gt;0&amp;&amp;b[i-1]==1))
        {
            nums.append!(temp);
        }
        temp=1;
        nums.append!(a[i])
    }
}
print(nums)
</code></pre>
<p>However, I'd like to explore implementing the same solution using SQL programming style in DolphinDB. How can I efficiently implement this segmented product operation?</p>
",0,0,0,2025-11-21T02:57:08+00:00,1,86,False
79826537,31921617,,sql,Calculate percent of total per firm and filter firms above 50%,"<p>I'm trying to calculate the percentage contribution of each firm to the total amount across all firms.</p>
<h3><strong>Database:</strong></h3>
<p><code>google-bigquery</code></p>
<h3><strong>What I need (desired output):</strong></h3>
<ol>
<li><p>Sum <code>usde_haircut_amt</code> per firm</p>
</li>
<li><p>Compute grand total of all firms</p>
</li>
<li><p>Calculate</p>
</li>
</ol>
<pre><code>pct_of_total = firm_total / total_all_firms
</code></pre>
<ol start=""4"">
<li>Return only firms where <code>pct_of_total &gt; 0.50</code> (above 50%)</li>
</ol>
<h3><strong>The SQL I tried (not working):</strong></h3>
<p>I originally attempted a window function:</p>
<pre><code>SELECT 
  firm_name,
  SUM(usde_haircut_amt) AS firm_total,
  SUM(SUM(usde_haircut_amt)) OVER () AS total_all_firms,
  SUM(usde_haircut_amt) / SUM(SUM(usde_haircut_amt)) OVER () AS pct_of_total
FROM my_table
GROUP BY firm_name
HAVING pct_of_total &gt; 0.5;
</code></pre>
<h3><strong>Problem (error):</strong></h3>
<p>BigQuery returns:</p>
<blockquote>
<p>Error: Aggregate function SUM not allowed in HAVING clause when used with window functions</p>
</blockquote>
<h3><strong>What I want:</strong></h3>
<p>A working SQL query that:</p>
<ul>
<li><p>Computes total per firm</p>
</li>
<li><p>Computes total across all firms</p>
</li>
<li><p>Calculates percent of total</p>
</li>
<li><p>Filters firms above 50%</p>
</li>
</ul>
<h3><strong>What I think the query should do conceptually:</strong></h3>
<ul>
<li><p>Aggregate first</p>
</li>
<li><p>Cross join with overall total</p>
</li>
<li><p>Then filter by percentage</p>
</li>
</ul>
<h3><strong>What is the correct SQL to get that result in BigQuery?</strong></h3>
<p>I tried to convert my window function into normal aggregation, but I'm not sure if the approach is correct.</p>
<hr />
",0,1,1,2025-11-21T13:19:44+00:00,1,98,True
79826891,435159,"Fort Collins, CO, United States",sql,"Unallowed GRANT does not raise an exception and doesn&#39;t grant the privilege either, if user already has other privileges on the object","<p>I have a regression test for a PostgreSQL database that checks to make sure a particular user cannot self-assign additional privileges to a particular schema. I do this by logging in as the user &quot;testuser&quot;, then executing this SQL: <a href=""https://dbfiddle.uk/6cKsmzED"" rel=""nofollow noreferrer"">fiddle</a></p>
<pre class=""lang-sql prettyprint-override""><code>GRANT CREATE ON SCHEMA testschema TO testuser;
</code></pre>
<p>In past PostgreSQL versions, this would raise an error (again, this user should not be able to grant permissions on the schema.) With version 17, the SQL <em>does not</em> raise an error (but no additional privilege is granted.)</p>
<p>Using <a href=""https://stackoverflow.com/users/1319998/michal-charemza"">@Michal Charemza</a>'s <a href=""https://stackoverflow.com/a/78466268/5298879"">excellent query</a> from <a href=""https://stackoverflow.com/questions/40759177/postgresql-show-all-the-privileges-for-a-concrete-user"">this question</a> shows that testuser's privileges for <code>testschema</code> to be <code>USAGE</code>, for the database to be <code>CONNECT</code>, and to be inheriting the role <code>users</code>.</p>
<p>Running the same privilege query on the <code>users</code> role shows it has no privileges.</p>
<p>So: under v17, why doesn't <code>testuser</code>'s attempt to <code>GRANT</code> additional privileges on a schema raise an error if <code>testuser</code> only has <code>USAGE</code> privilege on the schema?</p>
",1,1,0,2025-11-21T19:30:52+00:00,1,122,True
79827364,73810,"Helsinki, Finland",sql,Returning multiple columns and count of duplicates with GROUP BY clause,"<p>I'm searching for duplicate values per column and need a count of them and data from some additional columns.</p>
<p>Table sample</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>BillNr</th>
<th>Name</th>
<th>email</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>Shakira</td>
<td>shak@server.co</td>
</tr>
<tr>
<td>1001</td>
<td>Shakira</td>
<td>shak@server.co</td>
</tr>
<tr>
<td>1002</td>
<td>Shakira</td>
<td>shak@server.co</td>
</tr>
<tr>
<td>2220</td>
<td>Beyonce</td>
<td>beyo@mail.tx</td>
</tr>
<tr>
<td>2221</td>
<td>Beyonce</td>
<td>beyo@mail.tx</td>
</tr>
<tr>
<td>3330</td>
<td>Rihanna</td>
<td>riha@host.bb</td>
</tr>
<tr>
<td>3331</td>
<td>RiRi</td>
<td>riha@host.bb</td>
</tr>
<tr>
<td>7000</td>
<td>ImNotDbl</td>
<td>dude@mail.net</td>
</tr>
<tr>
<td>7777</td>
<td>MeNeither</td>
<td>fella@post.com</td>
</tr>
<tr>
<td>8000</td>
<td>HumptyDumpty</td>
<td>mate@msg.org</td>
</tr>
<tr>
<td>9999</td>
<td>NotDblTrbl</td>
<td>buddy@data.eu</td>
</tr>
<tr>
<td>9999</td>
<td>WhyAreWeAllNotSingle</td>
<td>bloke@tele.bb</td>
</tr>
</tbody>
</table></div>
<p>This returns the correct result:</p>
<pre><code>SELECT 
    Name, email, COUNT(email)
FROM 
    table
GROUP BY 
    Name, email
HAVING 
    COUNT(email) &gt; 1 
ORDER BY 
    COUNT(email) DESC
</code></pre>
<p>Now, I need rows with duplicate email. I need to list BillNr which is different on each row and then count occurences of email.</p>
<p>This naturally gives empty recordset. (Thanks @jarlh!)</p>
<pre><code>SELECT 
    BillNr, Name, email, COUNT(email) as CountEmail
FROM 
    table
GROUP BY 
    BillNr, Name, email
HAVING 
    COUNT(email) &gt; 1 
ORDER BY 
    COUNT(email) DESC
</code></pre>
<p>Result should be like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>BillNr</th>
<th>Name</th>
<th>email</th>
<th>CountEmail</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>Shakira</td>
<td>shak@server.co</td>
<td>3</td>
</tr>
<tr>
<td>1001</td>
<td>Shakira</td>
<td>shak@server.co</td>
<td>3</td>
</tr>
<tr>
<td>1002</td>
<td>Shakira</td>
<td>shak@server.co</td>
<td>3</td>
</tr>
<tr>
<td>2220</td>
<td>Beyonce</td>
<td>beyo@mail.tx</td>
<td>2</td>
</tr>
<tr>
<td>2221</td>
<td>Beyonce</td>
<td>beyo@mail.tx</td>
<td>2</td>
</tr>
<tr>
<td>3330</td>
<td>Rihanna</td>
<td>riha@host.bb</td>
<td>2</td>
</tr>
<tr>
<td>3331</td>
<td>RiRi</td>
<td>riha@host.bb</td>
<td>2</td>
</tr>
</tbody>
</table></div>
",-4,0,4,2025-11-22T14:20:48+00:00,3,170,True
79828139,1982778,,sql,Pivot JSON data for multiple rows,"<p>Is it possible to pivot <code>OPENJSON</code> data for multiple rows in T-SQL (on  SQL Server  2022).</p>
<p>I have a case like below, it works fine for single ID (eg. CustID=111). Surely I can go this route processing each row, but probably there is a way to make it more compact and faster. Even free Copilot doesn't know if this possible</p>
<p>Appreciate  your feedback. Please refer to self containing  snippet below.</p>
<pre><code>declare @json nvarchar(max);

with t as (
    select 111 CustID, '{ &quot;xNotes&quot;: &quot;Not shipped&quot;,  &quot;xManager&quot;: &quot;Pavel&quot;, &quot;xType&quot;: &quot;1&quot;}' DataEx
    union all
    select 222 CustID, '{ &quot;xNotes&quot;: &quot;Pending&quot;,  &quot;xManager&quot;: &quot;Maria&quot;, &quot;xType&quot;: &quot;2&quot;}'
)   
select @json = DataEx from t  -- where Custid = 111;

select [key], [value], [type] --, CustID ???
from openjson(@json);

-- output for single row:::
-- key       value     type
-- xNotes    Pending   1
-- xManager  Maria     1
-- xType     2         1
</code></pre>
<p>And this is desired output:</p>
<p><a href=""https://i.sstatic.net/pzSxTbhf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pzSxTbhf.png"" alt=""enter image description here"" /></a></p>
",1,1,0,2025-11-23T22:05:26+00:00,1,198,True
79828257,12139942,,sql,&#39;EXEC&#39; is not processing all of submitted SQL string,"<p>I am creating a <code>varchar(max)</code> string for submission to 'EXEC' and it is less than 1000 bytes long, yet the 'exec' errors out - seems to not get the entire string.</p>
<p>Below is the code that errors out:</p>
<pre><code>create or alter procedure usp_utility_ListFullTextCatalog_items @Database varchar(100)

as

declare @sql varchar(max), @DBName varchar(max)

set @DBName = @Database

set @sql =  cast('' as varchar(max))
             + 'SELECT  t.name as TableName, 
                    c.name as FTCatalogName,
                    i.name as UniqueIdxName,
                    cl.name as ColumnName,
                    cdt.name as DataTypeColumnName '
             + 'from    ' + @DBName + '.sys.tables t inner join '
             + '        ' + @DBName + '.sys.fulltext_indexes fi on '
             + '        ' + 't.[object_id] = fi.[object_id] inner join '
             + '        ' + @DBName + '.sys.fulltext_index_columns ic on '
             + '        ic.[object_id] = t.[object_id] inner join '
             + '        ' + @DBName + '.sys.columns cl on '
             + '            ic.column_id = cl.column_id and '
             + '            ic.[object_id] = cl.[object_id] inner join '
             + '        ' + @DBName + '.sys.fulltext_catalogs c on '
             + '        fi.fulltext_catalog_id = c.fulltext_catalog_id inner join '
             + '        ' + @DBName + '.sys.indexes i on '
             + '            fi.unique_index_id = i.index_id and '
             + '            fi.[object_id] = i.[object_id] left join ' 
             + '        ' + @DBName + '.sys.columns cdt on '
             + '        ic.type_column_id = cdt.column_id and '
             + '        fi.object_id = cdt.object_id'
select len(@sql)
select right(@sql,50)
execute @sql

go

execute usp_utility_ListFullTextCatalog_items 'AdventureWorks2022'
</code></pre>
<hr />
<p>When I run the above, the first 'select' at the bottom of the procedure shows the length of @sql to be 900.</p>
<p>The <code>'execute @sql'</code> statement at the end of the stored procedure produces the below error:</p>
<pre><code>Msg 203, Level 16, State 2, Procedure usp_utility_ListFullTextCatalog_items, Line 33 [Batch Start Line 34]
The name 'SELECT    t.name as TableName, 
                    c.name as FTCatalogName,
                    i.name as UniqueIdxName,
                    cl.name as ColumnName,
                    cdt.name as DataTypeColumnName from AdventureWorks2022.sys.tables t           inner join        AdventureWorks2022.sys.fulltext_indexes fi on       t.[object_id] = fi.[object_id] inner join         AdventureWorks2022.sys.fulltext_index_columns ic on       ic.[object_id] = t.[object_id] inner join         AdventureWorks2022.sys.columns cl on          ic.column_id = cl.column_id and             ic.[object_id] = cl.[object_id] inner join         AdventureWorks2022.sys.fulltext_catalogs c on        fi.fulltext_catalog_id = c.fulltext_catalog_id inne' **_is not a valid identifier._**
</code></pre>
<p>The error message does not show the entire value of @sql, yet the second 'select' at the bottom of the procedure shows the last 50 bytes of @sql to be &quot;= cdt.column_id and   fi.object_id = cdt.object_id&quot; - which is what they are supposed to be.</p>
",3,4,1,2025-11-24T03:55:48+00:00,2,158,True
79829003,347877,,sql,CREATE OR REPLACE FUNCTION in concurrently running transactions,"<p>How does <code>create or replace function</code> in PostgreSQL <strong>14</strong> behave with respect to transactions? If there are multiple concurrent transactions running at <a href=""https://www.postgresql.org/docs/14/transaction-iso.html#XACT-READ-COMMITTED"" rel=""nofollow noreferrer"">read committed</a> isolation level, and trying to <code>create or replace</code> the same function, will it cause any errors?</p>
<p>Is there any chance of the <code>tuple concurrently updated</code> error occurring in this scenario?</p>
",1,1,0,2025-11-24T18:41:02+00:00,2,127,True
79829103,27366805,,sql,SQLAlchemy and partitioning table,"<p>I have tables which are already in Postgres and partitioned. How can I write a query with partioned <code>birth_year</code></p>
<pre><code>stmt = select(UserOrm)
session.execute(stmt)`
</code></pre>
",0,1,1,2025-11-24T21:06:24+00:00,1,66,True
79829177,16063304,,sql,Parse JSON object in SQL with list,"<p>I have a table with data structured like this. Each product ID has a list of element IDs--for each element, there is a dictionary including a list of elements and their assigned IDs. Not every element will have an ID on every product</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">product_id</th>
<th style=""text-align: right;"">element_id</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">product_1</td>
<td style=""text-align: right;"">{&quot;FIRE&quot;: [&quot;1630808&quot;],&quot;WATER&quot;: [&quot;188028&quot;,&quot;234&quot;],&quot;SHADOW&quot;: [&quot;213181&quot;]</td>
</tr>
</tbody>
</table></div>
<p>For each product I'd like to be able to count how many of each element ID appear, in a table like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">product_id</th>
<th>fire_count</th>
<th>water_count</th>
<th>shadow_count</th>
<th style=""text-align: right;"">forest_count</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">product_1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td style=""text-align: right;"">0</td>
</tr>
</tbody>
</table></div>
<p>I've tried using the LATERAL FLATTEN function with KEY and VALUE, but I'm getting duplicate results and wonder if there is a more crisp way of writing this type of query, especially because I also need to count instances where an ID does not appear.</p>
<p>My data is stored in Snowflake and I query it using Snowflake SQL.</p>
<p>Any advice? Thank you!</p>
",0,1,1,2025-11-24T23:11:51+00:00,2,142,True
79829295,3234994,,sql,How to pivot rows into a concatenated column,"<p>I have a query that outputs the below result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">users</th>
<th style=""text-align: left;"">file_consumed</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">John</td>
<td style=""text-align: left;"">orders</td>
</tr>
<tr>
<td style=""text-align: left;"">Martin</td>
<td style=""text-align: left;"">orders</td>
</tr>
<tr>
<td style=""text-align: left;"">Alice</td>
<td style=""text-align: left;"">orders</td>
</tr>
<tr>
<td style=""text-align: left;"">Bob</td>
<td style=""text-align: left;"">payments</td>
</tr>
<tr>
<td style=""text-align: left;"">Alex</td>
<td style=""text-align: left;"">payments</td>
</tr>
<tr>
<td style=""text-align: left;"">Julie</td>
<td style=""text-align: left;"">payments</td>
</tr>
<tr>
<td style=""text-align: left;"">John</td>
<td style=""text-align: left;"">deliveries</td>
</tr>
<tr>
<td style=""text-align: left;"">Bob</td>
<td style=""text-align: left;"">deliveries</td>
</tr>
</tbody>
</table></div>
<p>I want to convert this output to the below format:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">file_consumed</th>
<th style=""text-align: left;"">list_of_users</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">orders</td>
<td style=""text-align: left;"">John,Martin,Alice</td>
</tr>
<tr>
<td style=""text-align: left;"">payments</td>
<td style=""text-align: left;"">Bob,Alex,Julie</td>
</tr>
<tr>
<td style=""text-align: left;"">deliveries</td>
<td style=""text-align: left;"">John,Bob</td>
</tr>
</tbody>
</table></div>
<p>How to achieve this in SQL Server?</p>
",0,1,1,2025-11-25T04:34:55+00:00,2,172,True
79830521,20212947,,sql,Convert the string date type into date type while creating a table,"<p>I am trying to convert the string date type that retrieved from a column.
Below is the scenario</p>
<p>#input</p>
<pre class=""lang-none prettyprint-override""><code>date_str

02/08/24
21/09/2023
25.02.16
null
null
</code></pre>
<p>I need to convert above column into a new column with below format</p>
<pre><code>formated_col
02-aug-24
21-sep-23
25-feb-16
null
null
</code></pre>
<p>'date_str' column is the part of cte table when I used below query, just to test the conversion, it works
for eg:</p>
<pre><code>with test_table as (select * from basetable)
select t.*, 
to_char(to_date(t.date_str, 'DD/MM/YYYY'), 'DD-MON-YY' ) as formatted_date
from test_table t
</code></pre>
<p>this create the expected output like this</p>
<pre><code>formatted_date

02-aug-24
21-sep-23
25-feb-16
null
null
</code></pre>
<p>However when I am trying to create a table out of this, it</p>
<p>like</p>
<pre><code>create table basetable as
with test_table as (select * from basetable)
select t.*, 
to_char(to_date(t.date_str, 'DD/MM/YYYY'), 'DD-MON-YY' ) as formatted_date
from test_table t
</code></pre>
<p>It throws below error</p>
<blockquote>
<p>day of month must be between 1</p>
</blockquote>
<p>i am not able to understand where exactly is the issue?</p>
",1,1,0,2025-11-26T09:32:47+00:00,2,115,True
79831106,1251549,,sql,How split hash-join in Trino into more parts?,"<p>I have a query with join. When Trino has 10 workers it works. When 5 - it fails. As I understand the situation (Trino call this DistributedHashJoon) - it splits entire table into 10 pieces, so smaller piece lead to less resource usage. But what if to split join into 10 piece/tasks but run these all of on 5 workers?</p>
<p>Looks like the same situation, just tasks/splits will &quot;live&quot; a bit longer in coordination queue.</p>
<p>Any ideas how to spli hash join into more pieces?</p>
",0,0,0,2025-11-26T19:36:28+00:00,2,68,True
79831830,21440119,,sql,MySQL SELECT … FOR UPDATE causing table lock during high traffic while generating sequential transaction IDs,"<p>I have a PHP + MySQL project where I need to generate <strong>sequential transaction IDs</strong>.<br />
Each transaction can have <strong>multiple items</strong>, so all rows for that transaction must share the same <code>txn_in</code> value.</p>
<h3><strong>Tables</strong></h3>
<h4><code>stock_inward</code></h4>
<pre><code>ID | txn_in  | material_code | insert_dt
1  | TXN001  | MT001         | 2025-01-13 14:09:08
2  | TXN001  | MT002         | 2025-01-13 14:09:08
3  | TXN001  | MT003         | 2025-01-13 14:09:08
4  | TXN002  | MT002         | 2025-01-13 15:02:37
5  | TXN003  | MT009         | 2025-01-14 11:01:25
6  | TXN003  | MT006         | 2025-01-14 11:01:25
</code></pre>
<h4><code>txn_allot</code></h4>
<pre><code>ID | module   | prefix | session | last_number
1  | STORE_IN | MIN    | 25-26   | 3
</code></pre>
<p><strong>Problem</strong></p>
<p>To generate the next transaction number (<code>TXN004</code>, <code>TXN005</code>, etc.) I am using:</p>
<pre><code>SELECT last_number FROM txn_allot 
WHERE module='STORE_IN' 
FOR UPDATE;
</code></pre>
<p>Then I increment the number and update <code>last_number</code>.</p>
<p>This works, but when multiple users are entering data at the same time,<br />
<strong>the <code>txn_allot</code> table becomes locked</strong>, causing big delays until the lock is released.</p>
<h3><strong>Question</strong></h3>
<p>What is the best way to safely generate sequential, unique transaction IDs <strong>without causing table-level locking</strong> when multiple concurrent requests occur?</p>
<h3><strong>Notes:</strong></h3>
<ul>
<li><p>MySQL database (InnoDB)</p>
</li>
<li><p>PHP backend</p>
</li>
<li><p>Requirement: transaction IDs must be unique and sequential (no duplicates)</p>
</li>
</ul>
",0,0,0,2025-11-27T15:07:07+00:00,7,109,True
79832942,23332429,,sql,How to call Postgres function from .NET?,"<p>I tried this code:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION public.t4(in iid int, OUT n1 text, OUT n2 text)  
LANGUAGE plpgsql
AS $function$
    BEGIN 
            SELECT DISTINCT n1=l.name, n2=l.comment from public.languages l where l.id=iid;
    END;        
$function$;
</code></pre>
<pre class=""lang-cs prettyprint-override""><code> NpgsqlConnection connection = new NpgsqlConnection(ConString);
 connection.Open();
 NpgsqlCommand cmd = new NpgsqlCommand(&quot;SELECT * from public.t4()&quot;, connection);
 cmd.Parameters.Add(new NpgsqlParameter(&quot;iid&quot;, DbType.Int32) { Direction = ParameterDirection.Input });
 cmd.Parameters[0].Value = 18;
 cmd.Parameters.Add(new NpgsqlParameter(&quot;n1&quot;, DbType.String) { Direction = ParameterDirection.Output });
 cmd.Parameters.Add(new NpgsqlParameter(&quot;n2&quot;, DbType.String) { Direction = ParameterDirection.Output });
 cmd.ExecuteNonQuery();
 t1.Text = cmd.Parameters[1].Value.ToString();
 t2.Text = cmd.Parameters[2].Value.ToString();
</code></pre>
<p>And I got an error:</p>
<blockquote>
<p>Npgsql.PostgresException: '42883: function public.t4() does not exist.</p>
</blockquote>
<p>What am I doing wrong?</p>
",1,2,1,2025-11-28T22:10:04+00:00,2,131,True
79832965,742402,,sql,How to use index in simple select,"<p>Table has index on name column:</p>
<pre><code>    CREATE TABLE firma2.klient
    (
        kood character(12) primary key,
         nimi character(100),
       ...
    );
    
   CREATE INDEX IF NOT EXISTS klient_nimi_idx
    ON firma2.klient USING btree
    (nimi COLLATE pg_catalog.&quot;default&quot; ASC NULLS LAST)
    TABLESPACE pg_default;
</code></pre>
<p>Database settings have default values:</p>
<pre><code>enable_indexonlyscan       on
enable_indexscan           on
enable_indexonlyscan       on
enable_indexscan           on
cpu_index_tuple_cost       0.005                                                       
cpu_tuple_cost             0.01                                                    
</code></pre>
<p>Query</p>
<pre><code>SELECT * FROM firma2.klient WHERE nimi='John';
</code></pre>
<p>Runs slowly.</p>
<pre><code>analyze firma2.klient; 
explain analyze select * from firma2.klient where nimi='John'
</code></pre>
<p>Shows that index is not used:</p>
<pre><code>&quot;Seq Scan on klient  (cost=0.00..2287976.20 rows=1 width=4002) (actual time=12769.987..12769.988 rows=0 loops=1)&quot;
&quot;  Filter: (nimi = 'John'::bpchar)&quot;
&quot;  Rows Removed by Filter: 849971&quot;
&quot;Planning Time: 4.751 ms&quot;
&quot;Execution Time: 12770.029 ms&quot;
</code></pre>
<p>How to force Postgres to use index? It probably worked long time but suddenly stopped working today.
Re-started whole windows server but problem persists.</p>
<p>Using</p>
<p>PostgreSQL 17.5 on x86_64-windows, compiled by msvc-19.43.34808, 64-bit</p>
<p>in Windows Server 2022 vers 21H2</p>
",0,1,1,2025-11-28T22:55:14+00:00,0,172,False
79833901,24872728,,sql,Trigger on PostgreSQL updating salary column,"<p>I am having some troubles setting a trigger in PostgreSQL. I have a table <code>Employees</code> with columns such as <code>employee_id</code>, <code>salary</code> and <code>hourly_pay</code>. The <code>salary</code> is annual, so I'd like to change the <code>salary</code> when I change the <code>hourly pay</code>.</p>
<p>For instance, by setting <code>hourly pay</code> a value, the <code>salary</code> must be updated to 2080 times that value. But it seems that I can't make the trigger at all. What is wrong with the following code?</p>
<pre><code>create or replace function salary_update_function()
returns trigger as $salary_update$
begin
    update employees 
    set new.salary = new.hourly_pay*2080;

    return new;
end;
$salary_update$ language plpgsql;

create trigger salary_update_trigger
before update of hourly_pay on employees
for each row execute function salary_update_function();
</code></pre>
",1,1,0,2025-11-30T12:55:13+00:00,2,90,True
79834065,31971711,,sql,How do I design an ERD and SQL schema for a basic banking system (accounts &amp; transactions)?,"<p>I'm working on a small educational project: building a simple banking system that includes customers, accounts, and transactions.</p>
<p>I have read about ERD modeling but I'm not sure how to properly represent the relationships between:</p>
<ul>
<li>Customers</li>
<li>Accounts (savings, checking, etc.)</li>
<li>Transactions (deposits, withdrawals, transfers)</li>
</ul>
<p>My main confusion is:</p>
<ol>
<li>Should each account have many transactions through a 1-to-many relationship?</li>
<li>How should transfers between two accounts be represented in the table structure?</li>
<li>Should I create a separate Transfer table or handle transfers using two transaction records?</li>
</ol>
",1,0,0,2025-11-30T17:17:14+00:00,2,72,True
79835358,7357331,"Chennai, Tamil Nadu, India",sql,SQL: Assign rows to categories based on quota without loops,"<p>I have a table with this data:</p>
<pre><code>account_id | segment | quota 
-----------+---------+------
1          | retail  | 3
2          | retail  | 3
3          | retail  | 3
4          | retail  | 3
4          | premium | 2
5          | premium | 2
</code></pre>
<p>Requirement:
I need to assign each account_id to only one segment based on the quota rules:</p>
<ul>
<li>The quota value represents how many records should be selected for that segment.</li>
<li>Assignment must follow ascending account_id order.</li>
<li>If an account belongs to multiple segments, it should be assigned to the first segment that still has available quota.</li>
<li>Once assigned, the same account_id must not be assigned again.</li>
<li>The solution must be in SQL (any dialect is fine) and must be done WITHOUT LOOPS.</li>
</ul>
<p>Expected output:</p>
<pre><code>account_rid | final_segment
------------+--------------
 1          | retail
 2          | retail
 3          | retail
 4          | premium
 5          | premium
</code></pre>
<p>Question:
How can I write a SQL query to generate this result using window functions or another set-based approach (no procedural loop)?</p>
",0,0,0,2025-12-01T23:27:54+00:00,6,94,True
79835416,31918491,,sql,Filter Rows Within Each Group,"<p>I have a table <code>t</code> containing minute-level data with fields such as <code>datetime</code> (timestamp), <code>stock_id</code> (stock code), and <code>ret</code> (return). My goal is to:</p>
<ol>
<li>First, group by <code>date(datetime)</code> and <code>stock_id</code>, and calculate the moving standard deviation <code>mstdp(ret, 5)</code> within each group, labeling it as <code>st</code>.</li>
<li>Then, group by <code>date(datetime)</code> and <code>stock_id</code> again, and filter rows within each group where <code>st</code> is greater than <code>mean(st) + stdp(st)</code>.</li>
</ol>
<p>This is the minimal data example ：</p>
<pre><code>t=table(1:0, `datetime`stock_id`ret,[STRING,SYMBOL,DECIMAL32(4)])

insert into t values ('2023-01-03 09:30:00', `A, 0.012),
('2023-01-03 10:00:00', `A, 0.008),
('2023-01-03 10:30:00', `A, 0.015),
('2023-01-03 11:00:00', `A, -0.005),
('2023-01-03 11:30:00', `A, 0.020),
('2023-01-03 13:00:00', `A, 0.025),
('2023-01-03 13:30:00', `A, 0.018),
('2023-01-03 09:30:00', `B, 0.005),
('2023-01-03 10:00:00', `B, 0.010),
('2023-01-03 10:30:00', `B, 0.003),
('2023-01-03 11:00:00', `B, 0.015),
('2023-01-03 11:30:00', `B, -0.008),
('2023-01-03 13:00:00', `B, 0.022),
('2023-01-04 09:30:00', `A, 0.009),
('2023-01-04 10:00:00', `A, 0.014),
('2023-01-04 10:30:00', `A, 0.007),
('2023-01-04 11:00:00', `A, 0.019),
('2023-01-04 11:30:00', `A, -0.003),
('2023-01-04 09:30:00', `B, 0.012),
('2023-01-04 10:00:00', `B, 0.008),
('2023-01-04 10:30:00', `B, 0.016),
('2023-01-04 11:00:00', `B, -0.002),
('2023-01-04 11:30:00', `B, 0.011);
</code></pre>
<p>I attempted the following code:</p>
<pre class=""lang-sql prettyprint-override""><code>select myFunc(st) as value
from (
    select datetime, stock_id, mstdp(ret, 5) as st
    from t
    context by date(datetime), stock_id
)
group by date(datetime) as date, stock_id
having st &gt; mean(st) + stdp(st)
</code></pre>
<p>However, this returns an error:</p>
<blockquote>
<p>The HAVING clause after GROUP BY must be followed by a boolean expression.</p>
</blockquote>
<p>I understand that <code>HAVING</code> is typically used for filtering aggregated results, but here I want to evaluate each row's <code>st</code> value within its group rather than filtering aggregated values. How can I correctly implement this type of row-wise filtering within groups in DolphinDB? Should I use <code>WHERE</code> or another approach instead?</p>
",0,1,1,2025-12-02T02:02:40+00:00,2,126,False
79835521,5693776,,sql,What is the method to combine multiple sheets when there are no common fields?,"<p>I am uncertain about how to merge data from multiple sheets using only the file name, especially when the file names vary in case.</p>
<p>C:\Report\overview.csv:</p>
<pre><code>overview.csv.

Issue Name, Issue Type, Issue Priority
Validation: Missing &lt;head&gt; Tag,Issue,High
Validation: Multiple &lt;body&gt; Tags,Issue,High
</code></pre>
<p>C:\Report\validation_missing_head_tag.csv</p>
<pre><code>validation_missing_head_tag.csv

Address,Content Type,Status Code,Status,Indexability
https://g2.com/view ,text/html,200,OK,Indexable
https://g2.com/view1 ,text/html,200,OK,Indexable

validation_multiple_body_tags.csv

Address,Content Type,Status Code,Status,Indexability
https://g2.com/sampleview, text/html,200,OK,Indexable
</code></pre>
<p>Now, I want to join overview.csv with validation_missing_head_tag and validation_multiple_body_tags.</p>
<p>The purpose of this is to merge the columns Issue Type and Issue Priority.</p>
<p>Expected Output:</p>
<pre><code>Issue Name, Issue Type, Issue Priority, Address, Content Type, Status Code,Status, Indeability
validation: Missing &lt;head&gt; Tag, Issue, High, https://g2.com/view ,text/html,200,OK,Indexable
Validation: Missing &lt;head&gt; Tag,Issue,High,https://g2.com/view1 ,text/html,200,OK,Indexable
Validation: Multiple &lt;body&gt; Tags,Issue,High,https://g2.com/sampleview, text/html,200,OK,Indexable
</code></pre>
<p>It would be preferable to combine this using the C# method or SQL to combine those tables.</p>
",-8,0,8,2025-12-02T05:48:24+00:00,1,134,True
79835988,18148586,,sql,Taking the record with the max date with join,"<p>I have Oracle table <code>t1</code> with <code>date</code> column and several other columns, and table <code>t2</code> with <code>date</code> only.</p>
<p>I want to get all <code>t2.date</code> with <code>t1</code> record which have <code>max t1.date &lt;= t2.date</code></p>
<p>if t1 <a href=""https://i.sstatic.net/nSKd8EUP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nSKd8EUP.png"" alt=""enter image description here"" /></a> and t2 <a href=""https://i.sstatic.net/FySXwjPV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FySXwjPV.png"" alt=""enter image description here"" /></a> then result must be <a href=""https://i.sstatic.net/H3JcgBtO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H3JcgBtO.png"" alt=""enter image description here"" /></a></p>
<p>Data Example</p>
<pre><code>with d as (select trunc(sysdate) ate from dual),
    t1 as (select d.ate - 2 dat, 11 val1, 22 val2 from dual, d union all
           select d.ate, 33, 44 from dual, d union all
           select d.ate + 2, 55, 66 from dual, d),
    t2 as (select d.ate - 3 dat from dual, d union all
           select d.ate - 1 from dual, d union all
           select d.ate from dual, d union all
           select d.ate + 3 from dual, d)
 select t2.dat--, 
        --(t1.val1 where t1.dat &lt;= t2.dat and t1.dat = max) val1,
        --(t1.val2 where t1.dat &lt;= t2.dat and t1.dat = max) val2
   from t2
</code></pre>
",1,2,1,2025-12-02T14:38:55+00:00,1,91,True
79836733,14334817,"Cardiff, UK",sql,Latest inspection data only - ORACLE/SQL,"<p>I'm trying to run the following report but adapt it so it only returns the data from the latest inspection (W.ACTFINISH) for each Asset (W.ASSETNUM) but anything i've tried (admittedly with help of co-pilot) is not working. Can anyone help please?</p>
<pre><code>SELECT DISTINCT 
 W.WONUM
,W.ASSETNUM
,W.STATUS
,to_date(W.FNLCONSTRAINT, 'DD-MON-RRRR HH24:MI:SS') AS &quot;FINISH NO LATER THAN DATE&quot;  
,to_date(W.ACTFINISH, 'DD-MON-RRRR HH24:MI:SS') AS &quot;ACTUAL FINISH DATE&quot; 
,a.ka_reading as &quot;Examiner Comments&quot;
,b.ka_reading as &quot;Maintain Recommendations&quot;
,c.ka_reading as &quot;Refurb Recommendations&quot;
,e.ka_reading as &quot;Renew Recommendations&quot;
,d.ka_reading as &quot;GIA System Recommendations&quot;
,f.ka_reading as &quot;Proposed Maintenance&quot;
,g.ka_reading as &quot;Slope Drainage Observed&quot;
,i.ka_reading as &quot;Toe Drainage Observed&quot;
,q.ka_reading as &quot;Crest Drainage Observed&quot;
,s.ka_reading as &quot;Drainage Observed&quot;
FROM tbl_workorder  W
LEFT JOIN tbl_ka_readings_all R ON W.ASSETNUM = R.KA_ASSETNUM
Left outer join tbl_ka_readings_all a on W.WONUM = A.KA_WONUM and A.ka_key = 'Examiner Comments'    
  Left outer join tbl_ka_readings_all b on W.WONUM = B.KA_WONUM and B.ka_key = 'Maintain Recommendations'
  Left outer join tbl_ka_readings_all c on W.WONUM = C.KA_WONUM and C.ka_key = 'Refurb Recommendations'
  Left outer join tbl_ka_readings_all d on W.WONUM = D.KA_WONUM and D.ka_key = 'GIA System Recommendations'
  Left outer join tbl_ka_readings_all e on W.WONUM = E.KA_WONUM and E.ka_key = 'Renew Recommendations'
  Left outer join tbl_ka_readings_all f on W.WONUM = F.KA_WONUM and F.ka_key = 'Proposed Maintenance'
  Left outer join tbl_ka_readings_all g on W.WONUM = G.KA_WONUM and G.ka_key = 'Slope Drainage Observed'
  Left outer join tbl_ka_readings_all i on W.WONUM = I.KA_WONUM and I.ka_key = 'Toe Drainage Observed'
  Left outer join tbl_ka_readings_all q on W.WONUM = Q.KA_WONUM and Q.ka_key = 'Crest Drainage Observed'
  Left outer join tbl_ka_readings_all s on W.WONUM = S.KA_WONUM and S.ka_key = 'Drainage Observed'
where
 W.JPNUM in ('C00200-510-SCG','C00200-510-SET','C00200-510-RSE')
 and lower(W.STATUS) in ('complete','closed','reviewed')
 and W.DESCRIPTION not like '%legacy%'  
 and W.ACTFINISH is not NULL
 and r.ka_reading is not null
 order by assetnum desc, &quot;ACTUAL FINISH DATE&quot; desc
</code></pre>
",0,1,1,2025-12-03T09:38:14+00:00,2,104,True
79837033,765281,,sql,Stored procedure result truncates at 2033 characters,"<p>I am calling a SQL Server stored procedure from C#. It works fine as long as the result is not longer than 2033 characters. If it is longer than 2033 characters, the result is truncated at 2033 characters. Input parameter is json. There does not appear to be any issue with a long input parameter</p>
<p>Here is my call:</p>
<pre class=""lang-cs prettyprint-override""><code>SqlCommand cmdPricing = new SqlCommand(
                   &quot;Pricing.dbo.DEPAR_ALLOWED_JSON &quot;, sqlServer);
cmdPricing.CommandType = System.Data.CommandType.StoredProcedure;

cmdPricing.Parameters.Add(&quot;@claimsJSON&quot;, System.Data.SqlDbType.NVarChar);
            cmdPricing.Parameters.Add(&quot;@ReturnJSON&quot;,System.Data.SqlDbType.Bit);
cmdPricing.Parameters[&quot;@ReturnJson&quot;].Value = 1;

cmdPricing.Parameters[&quot;@claimsJSON&quot;].Value = requestJson;
resultJson  = Convert.ToString(cmdPricing.ExecuteScalar());
</code></pre>
<p>The stored procedure returns JSON and it looks correct up to the 2033rd character, but then it ends.</p>
<p>If I run the query in SQL Server Management Studio, I get expected results (not truncated).</p>
<p>Note:  My question is specific to calling the stored procedure from C# using System.Data.SQLClient</p>
",0,0,0,2025-12-03T15:03:52+00:00,2,114,True
79837102,16187031,,sql,Convert a number stored as a string in SQL to a decimal,"<p>The data management company stored all of the numeric fields in a table I need as varchar, for some reason.  As I work with them to try to get it fixed I still need to do my work so I am trying to convert the varchar to decimal.   I have tried the following (simplified version to focus on the field erroring out) code and got the following errors.</p>
<pre><code>select distinct
    cast([fieldname] as decimal(20,6)) as [fieldname]
from [tablename]
</code></pre>
<blockquote>
<p>Error converting data type varchar to numeric.</p>
</blockquote>
<pre><code>select distinct
    cast(case when [fieldname] is null then 0 else [fieldname] end as decimal(20,6)) as [fieldname]
from [tablename]
</code></pre>
<blockquote>
<p>Conversion failed when converting the varchar value '85187.003136' to data type int.</p>
</blockquote>
<p>To make sure I wasn't crazy I ran the following code, copying the above number stored as a varchar just to see if there were any spaces or similar that I just missed and got no issues.</p>
<pre><code>select cast('85187.003136' as Decimal(20,6))
</code></pre>
<p>What is going wrong here, and why is it trying to convert to data type Int when I told it to do decimal?</p>
",0,0,0,2025-12-03T16:19:33+00:00,1,164,True
79837206,22364991,,sql,Convert Timestamp in dd-mmm-yy hh:mm:ss.sssssssss PM format to yyyy-mm-dd hh:mm:ss,"<p>In SQL Server 2019, I am trying to convert <code>NVARCHAR(max)</code> timestamps (ex. 02-DEC-25 05.27.13.318965000 PM) to datetime (ex. 2025-12-02 17.27.13).  I have tried multiple methods and always seem to get the same error:</p>
<blockquote>
<p>Conversion failed when converting date and/or time from character string.</p>
</blockquote>
<pre class=""lang-sql prettyprint-override""><code>SELECT CAST(CONVERT(DATETIME, '02-DEC-25 05.27.13.318965000 PM', 109) AS DATE) AS ConvertedDate;
</code></pre>
",0,1,1,2025-12-03T18:29:52+00:00,1,117,True
79838006,8772319,,sql,Why does this `select` work in PostgreSQL?,"<pre class=""lang-sql prettyprint-override""><code>select unnest(array[1, 2, 3]), unnest(array[4, 5, 6])
</code></pre>
<p>It produces the following</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>unnest</th>
<th>unnest</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>3</td>
<td>6</td>
</tr>
</tbody>
</table></div>
<p>The docs on <a href=""https://www.postgresql.org/docs/current/queries-table-expressions.html#QUERIES-TABLEFUNCTIONS"" rel=""nofollow noreferrer"">table functions</a> indicate that the proper syntax should be</p>
<pre class=""lang-sql prettyprint-override""><code>select * from unnest(array[1, 2, 3], array[4, 5, 6])
</code></pre>
<p>Is this one of those cases where PostgreSQL is being forgiving, or is there an actual mechanism that's being exploited here?</p>
",3,3,0,2025-12-04T14:11:00+00:00,1,103,True
79838144,31997905,,sql,Joining 2 columns from 2 different tables to replace null values in one of the tables,"<p>I have 2 tables with a column called Participant ID. One table is completely null/empty in said column and the other table is filled. I would like to input the data from the filled table into the empty table. Table 1 has 642 rows and Table 2 has over 6000.</p>
<p>Example of Table 1:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Last Name</th>
<th>First Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>null</td>
<td>Aba</td>
<td>Ase</td>
</tr>
<tr>
<td>null</td>
<td>Flo</td>
<td>Jas</td>
</tr>
</tbody>
</table></div>
<p>Example of Table 2:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Last Name</th>
<th>First Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>53</td>
<td>Aba</td>
<td>Ase</td>
</tr>
<tr>
<td>54</td>
<td>Flo</td>
<td>Jas</td>
</tr>
</tbody>
</table></div>
<p>How can I go about inputting the data from Table 2's ID column into Table 1's ID column to match with the names on Table 1?</p>
<p>I tried a RIGHT Join but it just gave me all of the data for all of the columns but I need to combine the ID columns.</p>
",0,0,0,2025-12-04T16:32:32+00:00,1,70,True
79838278,1238837,,sql,Fix a huge disabled clustered columstore index,"<p>So on SQL Server 2016 production enviroment we have a table with 30b records, about 10 years of data. There is a Clustered ColumnStore Index defined on it. One day due to an accident to cause the index to be in disable state resulting the entire table not accessible with error;</p>
<blockquote>
<p>The query processor is unable to produce a plan because the index 'idx_ClusteredColumnStoreIndex_<em><strong>' on table or view '</strong></em>' is disabled</p>
</blockquote>
<p>So I renamed the original table and created a new table with the same structure so that the daily data load can continue. Now we want to copy historical data from the old to the new table. It looks like the only way to 'enable the index' is to rebuild it. But the rebuild takes a long time and a lot of server resource. After 12 hours we have to abort it as it's impacting the daily operation.</p>
<p>I am considering just drop the index to make it become heap table, or use CREATE CLUSTERED INDEX...WITH (DROP_EXISTING=ON) to convert it to a b-tree. Now I start worrying the space and time we need with either operation. DB report shows this table has about 800Gb data and we only have 500gb left on the data disk. Which way is the best in terms the time and space we need?  We just want the table to be accessible again.</p>
",0,0,0,2025-12-04T19:14:30+00:00,0,17,False
79838609,11804932,,sql,How to enforce dynamic partition pruning in BigQuery TVF using a subquery lookup?,"<p>I have a table-valued function (TVF) that takes an array of IDs as input.</p>
<ol>
<li><p><code>source_table</code>: a very large table (TBs) partitioned by day on <code>capturedTimestamp</code></p>
</li>
<li><p><code>selection_table</code>: a small lookup table that maps <code>id</code> to <code>capturedTimestamp</code>.</p>
</li>
</ol>
<p><strong>The Goal</strong></p>
<p>I want to query the <code>source_table</code> only for the specific <code>capturedTimestamp</code>s associated with the input IDs. In a typical execution, an input array of IDs corresponds to only 5–10 partitions (days) out of thousands of existing partitions.</p>
<p><strong>The Problem</strong></p>
<p>BigQuery is performing a full table scan on the <code>source_table</code> regardless of how few IDs I pass in. It seems unable to prune the partitions dynamically based on the result of the CTE/Subquery lookup.</p>
<p>This is my TVF definition:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE TABLE_FUNCTIONS tvf_test ( ids ARRAY&lt;STRING&gt;) AS (
    WITH selection_cte AS (
      SELECT DISTINCT id
        capturedTimestamp,
      FROM selection_table
      WHERE id IN UNNEST(ids)
    )
    SELECT *
    FROM source_table AS t
      JOIN selection_cte AS s ON s.id = t.id 
    WHERE t.capturedTimestamp IN (
        SELECT DISTINCT capturedTimestamp
        FROM selection_cte
      )
)
</code></pre>
<p>I attempted several variations to force partition pruning, but all resulting query plans show a scan of all partitions (processing TBs of data instead of GBs):</p>
<ol>
<li><p>Direct Join: <code>JOIN selection_cte ON t.capturedTimestamp = s.capturedTimestamp</code></p>
</li>
<li><p><code>IN</code> clause: using <code>WHERE t.capturedTimestamp IN (SELECT ...)</code> (as seen above).</p>
</li>
<li><p>Min/Max range: calculating <code>MIN(capturedTimestamp)</code> and <code>MAX(capturedTimestamp)</code> in the CTE and using <code>WHERE t.capturedTimestamp BETWEEN...</code></p>
</li>
</ol>
<p>After some research, I found out that because my filter relies on a subquery (which depends on the input argument), the planner considers it &quot;unknown&quot; at the start and defaults to scanning everything.</p>
<p>How can I structure this query (within a TVF or alternative strategy) so that BigQuery evaluates the required partitions <em>before</em> scanning the <code>source_table</code>? Is it possible to achieve Dynamic Partition Pruning with a TVF in this scenario? I know I can achieve this by using a stored procedure but I'm leaning to find an alternative approach that doesn't require me to make the logic more complex.</p>
",0,0,0,2025-12-05T07:03:26+00:00,3,46,True
79839016,5334303,,sql,Issue trying to use the ALTER TABLE DROP COLUMN,"<p>I am trying to drop a column from a table. I am getting an error</p>
<blockquote>
<p>Illegal symbol &quot;&quot;</p>
</blockquote>
<p>and I don't really understand it. The only space I see is the extra line. I updated it to put it on one line and still got the error. Any idea what is causing this?  is it data related?</p>
<p>This is my SQL statement:</p>
<pre><code>ALTER TABLE A8SD001.A8SPT014                                          
    DROP COLUMN ASSET_SALE_SETTLEMENT_DATE_ ;
</code></pre>
",0,0,0,2025-12-05T14:40:31+00:00,1,101,False
79839226,11804932,,sql,How to achieve dynamic partition pruning in BigQuery TVF using a subquery lookup?,"<p>I have a Table Valued Function (TVF) that takes an array of IDs as input.</p>
<ul>
<li><code>source_table</code>: A very large table (TBs) partitioned by day on <code>capturedTimestamp</code>.</li>
<li><code>selection_table</code>: A small lookup table that maps <code>id</code> to <code>capturedTimestamp</code>.</li>
</ul>
<p><strong>The Goal</strong>
I want to query the <code>source_table</code> only for the specific <code>capturedTimestamp</code>s associated with the input IDs. In a typical execution, an input array of IDs corresponds to only 5–10 partitions (days) out of thousands of existing partitions.</p>
<p><strong>The Problem</strong>
BigQuery is performing a <strong>full table scan</strong> on the <code>source_table</code> regardless of how few IDs I pass in. It seems unable to prune the partitions dynamically based on the result of the CTE/Subquery lookup.</p>
<p>This is my TVF definition:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE TABLE_FUNCTION my_dataset.tvf_test(ids ARRAY&lt;STRING&gt;) AS (
  WITH selection_cte AS (
    SELECT DISTINCT id, capturedTimestamp
    FROM my_dataset.selection_table
    WHERE id IN UNNEST(ids)
  )
  SELECT t.*
  FROM my_dataset.source_table AS t
  JOIN selection_cte AS s ON s.id = t.id 
  WHERE t.capturedTimestamp IN (
      SELECT DISTINCT capturedTimestamp
      FROM selection_cte
  )
);
</code></pre>
<p>I attempted several variations to force partition pruning, but all resulting query plans show a scan of all partitions (processing TBs of data instead of GBs):</p>
<ul>
<li><strong>Direct Join:</strong> <code>JOIN selection_cte ON t.capturedTimestamp = s.capturedTimestamp</code></li>
<li><strong>IN clause:</strong> Using <code>WHERE t.capturedTimestamp IN (SELECT ...)</code> (as seen above).</li>
<li><strong>Min/Max Range:</strong> Calculating <code>MIN(capturedTimestamp)</code> and <code>MAX(capturedTimestamp)</code> in the CTE and using <code>WHERE t.capturedTimestamp BETWEEN...</code></li>
</ul>
<p>After some research, I found out that because my filter relies on a subquery (which depends on the input argument), the planner considers it &quot;unknown&quot; at the start and defaults to scanning everything.</p>
<p>How can I structure this query (within a TVF or alternative strategy) so that BigQuery evaluates the required partitions <strong>before</strong> scanning the <code>source_table</code>?</p>
<p>Is it possible to achieve Dynamic Partition Pruning with a TVF in this scenario? I know I can achieve this by using a Stored Procedure (calculating variables first), but I am looking for an alternative approach that doesn't require me to make the logic more complex or imperative.</p>
<p>MRE for the above question:</p>
<pre><code>-- 1. Set up a dummy dataset
CREATE SCHEMA IF NOT EXISTS optimization_repro;

-- 2. Create the Large Source Table (Partitioned by Day)
-- Scenario: 1000 IDs, each exists on exactly one specific day.
-- We want to prove that looking up just 2 IDs scans all days.
CREATE OR REPLACE TABLE optimization_repro.source_table
PARTITION BY DATE(capturedTimestamp)
CLUSTER BY id
AS
SELECT
  CONCAT('id_', x) AS id,
  -- Distribute timestamps across 10 different days
  TIMESTAMP_ADD(TIMESTAMP '2024-01-01 10:00:00', INTERVAL MOD(x, 10) DAY) AS capturedTimestamp,
  'payload_data' AS data
FROM UNNEST(GENERATE_ARRAY(1, 1000)) AS x;

-- 3. Create the Lookup Table
-- We only want to look up 2 specific IDs (id_1 and id_2).
-- id_1 is on 2024-01-02, id_2 is on 2024-01-03.
CREATE OR REPLACE TABLE optimization_repro.selection_table AS
SELECT 
  'id_1' AS id, TIMESTAMP('2024-01-02 10:00:00') AS capturedTimestamp 
UNION ALL
SELECT 
  'id_2' AS id, TIMESTAMP('2024-01-03 10:00:00') AS capturedTimestamp;

-- 4. Create the TVF (The &quot;Problematic&quot; Function)
CREATE OR REPLACE TABLE_FUNCTION optimization_repro.tvf_test(input_ids ARRAY&lt;STRING&gt;) AS (
  WITH selection_cte AS (
    SELECT DISTINCT id, capturedTimestamp
    FROM optimization_repro.selection_table
    WHERE id IN UNNEST(input_ids)
  )
  SELECT t.*
  FROM optimization_repro.source_table AS t
  JOIN selection_cte AS s ON s.id = t.id 
  WHERE t.capturedTimestamp = s.capturedTimestamp
);

-- 5. EXECUTION TEST
-- Run this and check the &quot;Job Information&quot; -&gt; &quot;Bytes Processed&quot; or &quot;Execution Plan&quot;.
-- Expected Result: It scans ALL partitions (2024-01-01 to 2024-01-10) 
-- instead of just the 2 partitions required (2024-01-02 and 2024-01-05).
SELECT * FROM optimization_repro.tvf_test(['id_1', 'id_2']);
</code></pre>
",0,0,0,2025-12-05T18:57:47+00:00,0,69,False
79839460,20376176,,sql,UPDATE using jsonb_delete doesn&#39;t remove the target keys,"<p>I'm learning the <code>jsonb</code> data type in PostgreSQL. I executed the following statements:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE my_table (
    id SERIAL PRIMARY KEY,
    data JSONB
);
INSERT INTO my_table (data)
VALUES ('{&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30, &quot;address&quot;: {&quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;}}');
</code></pre>
<p>Then:</p>
<pre class=""lang-sql prettyprint-override""><code>UPDATE my_table SET data = jsonb_delete(data, '{address, state}') 
 WHERE id = 1
 RETURNING *;
</code></pre>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>UPDATE 1
</code></pre>
</blockquote>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>data</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>{&quot;age&quot;: 30, &quot;name&quot;: &quot;John&quot;, &quot;address&quot;: {&quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;}}</td>
</tr>
</tbody>
</table></div>
<p>You can see that <code>address.state</code> field is not removed. Why?</p>
<pre class=""lang-sql prettyprint-override""><code>select version();
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>version</th>
</tr>
</thead>
<tbody>
<tr>
<td>PostgreSQL 18.1 on x86_64-windows, compiled by msvc-19.44.35219, 64-bit</td>
</tr>
</tbody>
</table></div>
",1,1,0,2025-12-06T02:59:10+00:00,2,83,True
79839469,23592847,,sql,Filter for distinct values on the union of multiple columns rather than the intersection,"<p>I'm looking to filter data like the following <a href=""https://dbfiddle.uk/hLmLIDiW"" rel=""nofollow noreferrer"">(fiddle)</a></p>
<p><a href=""https://i.sstatic.net/M2UIJTpB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M2UIJTpB.png"" alt=""neo4jsandbox graph visualisation of example input data"" /></a></p>
<pre class=""lang-sql prettyprint-override""><code>create table t(col1,col2)as values
 ('a',  1)
,('f',  3)
,('d',  4)
,('a',  5)
,('a',  9)
,('a',  8)
,('b',  1)
,('g',  3)
,('a',  1)
,('b',  2)
,('b',  2)
,('b',  3)
,('d',  1)
,('c',  1)
,('g',  null);
</code></pre>
<p>...into a result set like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td>f</td>
<td>3</td>
</tr>
<tr>
<td>d</td>
<td>4</td>
</tr>
<tr>
<td>b</td>
<td>2</td>
</tr>
<tr>
<td>g</td>
<td><em><code>null</code></em></td>
</tr>
</tbody>
</table></div>
<p>Notice in this example that <code>(g, 5)</code> is not included because it does not exist in the original set. It would be included by just matching distinct values of each column arbitrarily, which is not what I'm looking for.</p>
<p>Standard <code>DISTINCT</code> behavior on multiple columns returns rows with a distinct combination of values, which would return all rows in the original set because no rows have the same value of col1 and col2 simultaneously.</p>
<p>Nested <code>DISTINCT</code> filters on each column yields only the first row unless the data can be cleverly sorted.</p>
<p>Instead I'm looking for a filter that yields rows that are unique on all of the chosen columns individually - i.e. a &quot;union&quot; distinct operation rather than an &quot;intersection&quot; distinct operation. Is there an idiomatic or at least scalable way to do this?</p>
<hr />
<h3>Edit</h3>
<p>To clear up confusion, this is the key requirement:</p>
<blockquote>
<p>a filter that yields rows that are unique on all of the chosen columns individually - i.e. a &quot;union&quot; distinct operation rather than an &quot;intersection&quot; distinct operation.</p>
</blockquote>
<p>As some have pointed out, such an operation is dependent on the ordering of the input rows. This is expected and not an issue.</p>
<p>I suspect it may be easier to understand the problem conceptually than by example, but I will attempt to address more edge cases below. Regardless, a valid solution should solve the quoted problem with no additional assumptions based on the example data. To address a few specifically:</p>
<ul>
<li>The number of distinct values in col1 might not equal the number of distinct values in col2.</li>
<li>Values in col1 and col2 might not be co-ordered.</li>
<li>Ideally, null values in either column should be handled as a unique value. But if this makes the problem significantly harder, I'd also be interested in a solution that does not handle this edge case.</li>
<li>Data may not be ordered, but the output may vary depending on the order.</li>
</ul>
<h4>Example Use Case</h4>
<p>Suppose we want to match people from population 1 to people from mutually exclusive population 2. Matching is done by a bidirectional eligibility check that may yield 0-many matches for any given person. Now suppose we want a list of matches (population 1 person ID, population 2 person ID) that contains no duplicate entries for any person of either population, matched if possible or with a null match if not. It's ok that the order of people matters and that the result set may not be optimized for greatest or fewest possible matches or any other condition.</p>
<br>
<p><em><strong>Edit :</strong></em> Important quote from comments below.</p>
<blockquote>
<p>If a value in either column is present in a previous row, eliminate it from the solution set. (a, 5) is eliminated by a in row 1 col 1. (c, 1) is eliminated by 1 in row 1 col 2.</p>
</blockquote>
",0,1,1,2025-12-06T03:30:27+00:00,5,350,True
79840539,31435869,,sql,MySQL: Show last 3 subscription plans with amounts per student in separate columns using PhpMyAdmin,"<p>I am working on a <strong>student subscription database</strong> in MySQL using PhpMyAdmin. I have the following tables:</p>
<p><strong>Students Table:</strong><br />
| student_id | student_name |</p>
<p><strong>Plans Table:</strong><br />
| plan_id | plan_name |</p>
<p><strong>Subscriptions Table:</strong><br />
| subscription_id | student_id | plan_id | amount_paid | payment_date |</p>
<p>I want to write a query that returns <strong>one row per student</strong> showing:</p>
<ul>
<li><p>Student name</p>
</li>
<li><p>Their <strong>last 3 subscription plan names</strong> (ordered by <code>payment_date</code> descending)</p>
</li>
<li><p>Corresponding <strong>amounts paid</strong></p>
</li>
</ul>
<p>For example, the output should look like this:</p>
<p>| student_name | plan_1 | amount_1 | plan_2 | amount_2 | plan_3 | amount_3 |</p>
<p>I tried the following query:</p>
<pre><code>SELECT 
    s.student_name,
    p1.plan_name AS plan_1, sub1.amount_paid AS amount_1,
    p2.plan_name AS plan_2, sub2.amount_paid AS amount_2,
    p3.plan_name AS plan_3, sub3.amount_paid AS amount_3
FROM students s
LEFT JOIN subscriptions sub1 ON s.student_id = sub1.student_id
LEFT JOIN plans p1 ON sub1.plan_id = p1.plan_id
LEFT JOIN subscriptions sub2 ON s.student_id = sub2.student_id
LEFT JOIN plans p2 ON sub2.plan_id = p2.plan_id
LEFT JOIN subscriptions sub3 ON s.student_id = sub3.student_id
LEFT JOIN plans p3 ON sub3.plan_id = p3.plan_id
WHERE sub1.payment_date = (SELECT MAX(payment_date) FROM subscriptions WHERE student_id = s.student_id)
   OR sub2.payment_date = (SELECT MAX(payment_date) FROM subscriptions WHERE student_id = s.student_id)
   OR sub3.payment_date = (SELECT MAX(payment_date) FROM subscriptions WHERE student_id = s.student_id);
</code></pre>
<p><strong>Problem:</strong></p>
<ol>
<li><p>The query duplicates joins and does not correctly pick the <strong>last 3 subscriptions</strong>.</p>
</li>
<li><p>The <code>WHERE</code> condition only selects the latest subscription, so <code>plan_2</code> and <code>plan_3</code> are often wrong or NULL.</p>
</li>
<li><p>Hardcoding <code>plan_1</code>, <code>plan_2</code>, <code>plan_3</code> is <strong>not scalable</strong>.</p>
</li>
</ol>
<p><strong>Questions:</strong></p>
<ol>
<li><p>How can I write a query that correctly returns the <strong>last 3 subscription plans with amounts per student</strong> in a <strong>single row</strong>?</p>
</li>
<li><p>Is there a way to do this <strong>dynamically</strong> without hardcoding columns for each subscription?</p>
</li>
</ol>
",-1,0,1,2025-12-08T00:12:53+00:00,0,28,False
79841070,2691248,"Davao City, Philippines",sql,postgres &quot;&lt;schema.function&gt;&quot; does not exist,"<p>I have a script in postgres to create test data and error <code>Error: relation &quot;schema&quot; does not exist</code> is being thrown. I have verified that the schema and function exists and that I can run the function manually from the psql client but running it from a script gives me that error.</p>
<p>Also the weird thing is that if I login from psql and run the proc:</p>
<pre class=""lang-sql prettyprint-override""><code>select * from schema.function_id_from_name('test');
</code></pre>
<p>at first it will throw that error. But if I set the schema to the correct schema and run the proc it works. Then I can set the schema to <code>public</code>, I can then run the function normally from there. If I put the set schema statements in the script, it will not work but running it manually from the client will it work.</p>
",0,0,0,2025-12-08T14:29:46+00:00,1,65,False
79841348,13090719,,sql,Remove duplicate columns and have them null,"<p>Is there anyway to remove the duplicate rows and just have blank columns as shown below?</p>
<p>I do have:</p>
<p><a href=""https://i.sstatic.net/FrnmH1Vo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FrnmH1Vo.png"" alt=""enter image description here"" /></a></p>
<p>And I want to get this:</p>
<p><a href=""https://i.sstatic.net/Bhl0jxzu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Bhl0jxzu.png"" alt=""enter image description here"" /></a></p>
<p>Final join:</p>
<pre><code>SELECT DISTINCT 
    MST_VISITS.ID_NUM
    ,MST_VISITS.MST_PROVIDER
    ,MST_VISITS.ADMIT_DATE
    ,MST_VISITS.LAST_VISIT
    ,MST_VISITS.TOTAL_UNITS

    ,LESS_7_SUMMARY.PROVIDER_NAME AS LESS_7_PROVIDER_NAME
    ,LESS_7_SUMMARY.SERVICE AS LESS_7_SERVICE
    ,LESS_7_SUMMARY.VISITS AS LESS_7_VISITS

    --,BETWEEN_8_30_SUMMARY.PROVIDER_NAME AS BETWEEN_8_30_PROVIDER_NAME
    --,BETWEEN_8_30_SUMMARY.SERVICE AS BETWEEN_8_30_SERVICE
    --,BETWEEN_8_30_SUMMARY.VISITS AS BETWEEN_8_30_VISITS

    ,BETWEEN_31_180_SUMMARY.PROVIDER_NAME AS BETWEEN_31_180_PROVIDER_NAME
    ,BETWEEN_31_180_SUMMARY.SERVICE  AS BETWEEN_31_180_SERVICE
    ,BETWEEN_31_180_SUMMARY.VISITS AS BETWEEN_31_180_VISITS

    ,BETWEEN_181_365_SUMMARY.PROVIDER_NAME AS BETWEEN_181_365_PROVIDER_NAME
    ,BETWEEN_181_365_SUMMARY.SERVICE AS BETWEEN_181_365_SERVICE
    ,BETWEEN_181_365_SUMMARY.VISITS AS BETWEEN_181_365_VISITS

    --,PRIOR_180_SUMMARY.PROVIDER_NAME AS PRIOR_180_PROVIDER_NAME
    --,PRIOR_180_SUMMARY.SERVICE AS PRIOR_180_SERVICE
    --,PRIOR_180_SUMMARY.VISITS AS PRIOR_180_VISITS
    --,ROW_NUMBER() OVER (PARTITION BY MST_VISITS.ID_NUM ORDER BY MST_VISITS.ADMIT_DATE) AS RN
FROM 
    MST_VISITS
LEFT JOIN 
    LESS_7_SUMMARY ON MST_VISITS.ID_NUM = LESS_7_SUMMARY.ID_NUM
-- LEFT JOIN 
--     BETWEEN_8_30_SUMMARY ON MST_VISITS.ID_NUM = BETWEEN_8_30_SUMMARY.ID_NUM
LEFT JOIN 
    BETWEEN_31_180_SUMMARY ON MST_VISITS.ID_NUM = BETWEEN_31_180_SUMMARY.ID_NUM
LEFT JOIN 
    BETWEEN_181_365_SUMMARY ON MST_VISITS.ID_NUM = BETWEEN_181_365_SUMMARY.ID_NUM
</code></pre>
",-5,0,5,2025-12-08T21:08:36+00:00,0,100,False
79841501,10146441,,sql,Update a value in a mapping table using SQL,"<p>I have tables like below,</p>
<p>fathers</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Jeff</td>
</tr>
<tr>
<td>2</td>
<td>John</td>
</tr>
</tbody>
</table></div>
<p>mothers</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Emy</td>
</tr>
<tr>
<td>2</td>
<td>Sarah</td>
</tr>
</tbody>
</table></div>
<p>siblings</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Andy</td>
</tr>
<tr>
<td>2</td>
<td>Robert</td>
</tr>
</tbody>
</table></div>
<p>people</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Susan</td>
</tr>
<tr>
<td>2</td>
<td>Mark</td>
</tr>
</tbody>
</table></div>
<p>and a mapping table</p>
<p>person_map</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>father_id</th>
<th>mother_id</th>
<th>sibling_id</th>
<th>person_id</th>
<th>active</th>
<th>updated_at</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>t</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I want to write a SQL <code>update</code> query such as <code>person_map</code>'s <code>active</code> column change to <code>f</code> based on the other table values and I have tried the following in <code>postgres</code></p>
<pre><code>update person_map
set active = false::boolean, updated_at = NOW()
from person_map pm

join fathers f on pm.father_id = f.id
join mothers m on pm.mother_id = m.id
join siblings s on pm.sibling_id = s.id
join people p on pm.person_id = p.id

where
  f.name = 'Jeff'
  m.name = 'Emy'
  s.name = 'Andy'
  p.name = 'Susan'
</code></pre>
<p>Above query should only match the first row <code>id = 1</code> of the <code>person_map</code> table however, it matches all.
Could someone let me know what am I missing?</p>
<p>Any help would be highly appreciated</p>
",0,0,0,2025-12-09T02:30:05+00:00,8,130,True
79841768,556958,"Moscow, Russia",sql,Can PostgreSQL optimizer carry `ORDER BY` out of `JOIN` subquery?,"<p>There is a &quot;theoretical&quot; query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM a
JOIN (
    SELECT b.pk, b.not_pk
    FROM b
    ORDER BY b.not_pk
) AS b2
USING (pk)
</code></pre>
<p>and <code>EXPLAIN</code> shows <code>Sort</code> on the <em>whole</em> <code>b</code>.</p>
<p>Can it be carried out of the join (by the optimizer), theoretically?
If so, why it isn't?</p>
",3,3,0,2025-12-09T09:53:12+00:00,2,151,True
79841911,10894456,"NY, USA",sql,Does an index on a column used in an ORDER BY clause help / improve performance in a RDBMS?,"<pre><code>SELECT o
FROM Order o
WHERE o.customerEmail = :email AND o.status = :status
ORDER BY paymentDate DESC
</code></pre>
<p>So does an index on the <code>paymentDate</code> column help here?</p>
",1,0,0,2025-12-09T12:20:33+00:00,5,84,True
79842271,5304058,San fransisco,sql,How to pick second value for same ID with other conditions,"<p>I have a <code>Region</code> table where <code>MapID</code> is the primary key. Each <code>MapID</code> can have <code>EmployeeID</code> or <code>Zip5</code>.</p>
<p>For any <code>MapID</code>, <code>EmployeeID</code> takes the precedence over <code>Zip5</code> i.e. for any <code>MapID</code>, if there is an <code>EmployeeID</code>, then pick <code>EmployeeID</code> value, else pick <code>Zip5</code> value.</p>
<p>I am getting multiple rows for <code>MapID</code> with my code.</p>
<p>Can anyone please help me figure out what the mistake in my code is?</p>
<p>Requirements:</p>
<ol>
<li>For every <code>MapID</code>, check first if <code>EmployeeID</code> exists and if it exists, then check for <code>Business</code>, for every B2 business, Map B1 business as <code>SecondaryRegion</code>.</li>
<li>If <code>EmployeeID</code> is null, then check for <code>Zip5</code> and repeat #1.</li>
<li>For example, for <code>MapID = 9879</code>, there is a valid <code>EmployeeID 100</code> and has both Business B1 and B2. Therefore for this <code>MapID</code>, <code>SecondaryRegion</code> will be <code>South</code>.</li>
<li>However, for <code>MapID = 7890</code> there is only one row for <code>EmployeeID 200</code> but missing B1 business. In this case, <code>SecondaryRegion</code> will be null even if it has two business for <code>zip5</code>. This is because <code>EmployeeID</code> not null will take the precedence.</li>
</ol>
<p>Code:</p>
<pre><code>DROP TABLE IF EXISTS #Region

CREATE TABLE #Region
(
    MapID varchar(10),
    EmployeeID varchar(10),
    Zip5 varchar(10),
    Business Varchar(50),
    Region varchar(50)
)

INSERT INTO #Region
    SELECT '9879', '100', Null,'B1', 'South'
    UNION
    SELECT '9879', '100', Null,'B2', 'South-1'
    UNION
    SELECT '9879', Null, '04987','B2', 'South-1'
    UNION
    SELECT '7890', '200', Null, 'B2','West S San Diago'
    UNION
    SELECT '7890', NULL, '45678','B1', 'West So. CA'
    UNION
    SELECT '7890', NULL, '45678', 'B2','West-1'
    UNION
    SELECT '5678', '400', Null,'B1', 'EastCentral'
    UNION
    SELECT '5678', '400', Null,'B2', 'Central-1'
    UNION
    SELECT '5678', Null, '56333','B1', 'EastCentral-1'
    UNION
    SELECT '5678', Null, '56333','B2', 'East-1'

SELECT DISTINCT t.MapID, t.Business,
    COALESCE(emp.Region, Zip5.Region) AS SecondaryRegion
FROM #Region t
LEFT join #Region emp
    ON t.MapID = emp.mapid
    AND t.EmployeeID = emp.EmployeeID AND emp.Zip5 IS NULL
    AND t.Business = 'B2'
    AND emp.Business = 'B1'
LEFT JOIN #Region zip5
    ON t.MapID = zip5.mapid
    AND t.Zip5 = emp.zip5 AND zip5.EmployeeID IS NULL
    AND t.Business = 'B2'
    AND Zip5.Business = 'B1'
WHERE t.MapID = '7890'
</code></pre>
<p>Expected output:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">mapid</th>
<th>Business</th>
<th>Region</th>
<th>SecondaryRegion</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">5678</td>
<td>B2</td>
<td>Central-1</td>
<td>EastCentral</td>
</tr>
<tr>
<td style=""text-align: center;"">7890</td>
<td>B2</td>
<td>West S San Diago</td>
<td>NULL</td>
</tr>
<tr>
<td style=""text-align: center;"">9879</td>
<td>B2</td>
<td>South-1</td>
<td>South</td>
</tr>
</tbody>
</table></div>
",5,5,0,2025-12-09T18:41:22+00:00,3,124,True
79842407,10894456,"NY, USA",sql,Which index is useful on like clause?,"<pre><code>SELECT o
FROM Order o
WHERE o.customerEmail LIKE %:emailPart%
</code></pre>
<p>are there any RDBMS (PostgreSQL) index type which can improve performance of the query above? (i.e. which index can help on like clause?)</p>
",-1,0,1,2025-12-09T22:14:43+00:00,0,98,False
79842915,30359096,,sql,Fetch First 1 ROWS ONLY for each parameter of IN Clause,"<p>Is it possible to build up a SQL query like below, I want only to get the info if ID_1 and ID_2 is available in one of the two columns. But first match is sufficient.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT COALESCE(A_ID, B_ID)
FROM MY_TABLE
WHERE A_ID IN ('ID_1', 'ID_2')
   OR B_ID IN ('ID_1', 'ID_2')
   FETCH FIRST 1 ROWS ONLY
</code></pre>
<p>The real challenge is how to implement</p>
<pre><code>SELECT ID, A_ID
   FROM MY_TABLE
   WHERE A_ID IN ('ID_1', 'ID_2')
   FETCH FIRST 1 ROWS ONLY
</code></pre>
<p>How can I list ID_1 and ID_2 if both values are present. In above constellation there is only one result line.</p>
",0,1,1,2025-12-10T12:04:16+00:00,2,112,True
79842990,32030466,,sql,Power Pivot for Excel fails to retrieve duplicate row from query,"<p>I'm fetching data from a SQL Server database through a query in Power Pivot / Excel 365.</p>
<p>It's to gather sales data from the rows of all the invoices in a particular time frame so I'm extracting fields like <code>Document_No</code>, <code>Client_No</code>, <code>Item_Code</code>, <code>Amount</code>...</p>
<p>There are invoices with duplicate rows (same day, same item, same client, same price...) since some invoices are an end-of-the-month document and yet those rows show up just once in Power Pivot (and thus won't show the correct values in the Power Pivot Table I need).</p>
<p>This is what shows up:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Posting Date</th>
<th>No</th>
<th>Sell-to Customer No.</th>
<th>Statistic Group 2</th>
<th>Description</th>
<th>Line Amount</th>
<th>No</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>12.34</td>
<td>G010-001</td>
</tr>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>123.45</td>
<td>G015-222</td>
</tr>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>5.68</td>
<td>G012-668</td>
</tr>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>98.76</td>
<td>G020-202</td>
</tr>
</tbody>
</table></div>
<p>The query works as intended in SQL Server Management Studio:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Posting Date</th>
<th>No</th>
<th>Sell-to Customer No.</th>
<th>Statistic Group 2</th>
<th>Description</th>
<th>Line Amount</th>
<th>No</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>12.34</td>
<td>G010-001</td>
</tr>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>123.45</td>
<td>G015-222</td>
</tr>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>5.68</td>
<td>G012-668</td>
</tr>
<tr>
<td><strong>2025-08-12</strong></td>
<td><strong>25-VA001234</strong></td>
<td><strong>001122</strong></td>
<td><strong>006</strong></td>
<td><strong>Locals</strong></td>
<td><strong>123.45</strong></td>
<td><strong>G015-222</strong></td>
</tr>
<tr>
<td>2025-08-12</td>
<td>25-VA001234</td>
<td>001122</td>
<td>006</td>
<td>Locals</td>
<td>98.76</td>
<td>G020-202</td>
</tr>
</tbody>
</table></div>
<p>The structure is as follows:</p>
<pre><code>SELECT
    SALESLINE.[Posting Date],
    SALESHEAD.[No_],
    SALESHEAD.[Sell-to Customer No_],
    CUSTOMER.[Statistic Group 2],
    GENTABST2.[Description],
    SALESLINE.[Line Amount],
    SALESLINE.[No_]
FROM
    dbo.[Sales Invoice Line] AS SALESLINE
LEFT JOIN
    dbo.[Sales Invoice Header] AS SALESHEAD
        ON SALESLINE.[Document No_] = SALESHEAD.[No_]
LEFT JOIN
    dbo.[Customer] AS CUSTOMER
        ON SALESHEAD.[Sell-to Customer No_] = CUSTOMER.[No_]
LEFT JOIN
    dbo.[General Table] AS GENTABST2
        ON CUSTOMER.[Statistic Group 2] = GENTABST2.[Code]
WHERE
    GENTABST2.[TableCode] = 'CUST_STATISTIC' AND
    GENTABST2.[TableCode02] = 'GROUP2' AND
    SALESLINE.[Line Amount] &lt;&gt; 0
</code></pre>
<p>I feel like I'm missing something obvious but can't seem to recognize what.</p>
<p>I've looked for solutions on-line and here, but apparently the common issue is the presence of duplicate rows, not their absence.</p>
<p>As an operating solution I've added a column to the query: <code>Line_number</code>. Thus the records aren't exact duplicates anymore, but I'm still not satisfied by this solution. The records should show up regardless.</p>
",0,0,0,2025-12-10T13:11:09+00:00,0,75,False
79843330,2854333,,sql,Combine Multiple Tables into Single Resultset,"<p>I have 4 tables with data shown here in SQL Server 2019 (RTM) -15.0.2005</p>
<p>Table 1: <code>Buyer</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>123</td>
<td>ABC</td>
</tr>
<tr>
<td>234</td>
<td>XYZ</td>
</tr>
<tr>
<td>456</td>
<td>MNO</td>
</tr>
</tbody>
</table></div>
<p>Table 2: <code>Sell Detail</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Sl No</th>
<th>ID</th>
<th>TYPEID</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>123</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>123</td>
<td>2</td>
</tr>
<tr>
<td>C</td>
<td>123</td>
<td>3`</td>
</tr>
<tr>
<td>D</td>
<td>234</td>
<td>1</td>
</tr>
<tr>
<td>E</td>
<td>456</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>Table 3: <code>Product</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>TYPEID</th>
<th>TNAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SHOE</td>
</tr>
<tr>
<td>2</td>
<td>CHAIR</td>
</tr>
<tr>
<td>3</td>
<td>BOOK</td>
</tr>
</tbody>
</table></div>
<p>Table 4: <code>BillingDetails</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Sl No</th>
<th>QTY</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1</td>
</tr>
<tr>
<td>C</td>
<td>1</td>
</tr>
<tr>
<td>D</td>
<td>1</td>
</tr>
<tr>
<td>E</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>Now, I am trying to merge the records based on type into a single table.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>NAME</th>
<th>TNAME1</th>
<th>QTY1</th>
<th>TNAME2</th>
<th>QTY2</th>
<th>TNAME3</th>
<th>QTY3</th>
</tr>
</thead>
<tbody>
<tr>
<td>123</td>
<td>ABC</td>
<td>SHOE</td>
<td>1</td>
<td>CHAIR</td>
<td>1</td>
<td>BOOK</td>
<td>1</td>
</tr>
<tr>
<td>234</td>
<td>XYZ</td>
<td>SHOE</td>
<td>1</td>
<td>CHAIR</td>
<td>0</td>
<td>BOOK</td>
<td>0</td>
</tr>
<tr>
<td>456</td>
<td>MNO</td>
<td>SHOE</td>
<td>0</td>
<td>CHAIR</td>
<td>1</td>
<td>BOOK</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>I am unable to join the tables independently using inner join but I am not able to map the missing data in result.</p>
<p>Like I am able to get separate rows for SHOE , BOOK and Chair for ABC but not able to merge it into single row . Also, I am unable to get the details for items for which sales is 0.</p>
<p><a href=""https://sqlfiddle.com/sql-server/online-compiler?id=22480a79-cf0d-465b-93d6-0864b9bf4848"" rel=""nofollow noreferrer"">SQL Fiddle Link</a></p>
<p>My attempt:</p>
<p>I did see some post around { WHEN CASE EXISTS } but I was unable to achieve the query. I attempted using inner join and below is the query for same.</p>
<pre><code>SELECT *  
FROM BUYER A 
INNER JOIN SELL_DETAILS B ON A.ID = B.PID
INNER JOIN PRODUCT C ON B.TYPEID = C.TYPEID
INNER JOIN BILLING D ON B.SL = D.SL
</code></pre>
",1,2,1,2025-12-10T18:31:15+00:00,2,154,True
79843802,2854333,,sql,Merge Rows into single column,"<p>I have a table in below format .</p>
<p><img src=""https://i.sstatic.net/LN8Rlbdr.png"" alt=""enter image description here"" /></p>
<p>I am trying to merge columns and convert into row.</p>
<p><img src=""https://i.sstatic.net/TM7YwlCJ.png"" alt=""enter image description here"" /></p>
<p>I tried doing that using PIVOT but it is taking column as entry and not allowing me to pass values.</p>
<p>Here is my sample data:</p>
<pre class=""lang-sql prettyprint-override""><code>-- INIT database
CREATE TABLE SALES (
  VISIT_ID INT ,
  CUSTID INT,
  BOUGHT INT,
  Item Varchar (100),
  QTY INT,
  Free INT,
  Items_Returned INT,
  Reason_Return Varchar (100)
);

INSERT INTO SALES(VISIT_ID, CUSTID, BOUGHT , Item, QTY , Free , Items_Returned , Reason_Return) VALUES (373, 251 ,1 , 'Cake', 2 ,0.5,2,NULL);
INSERT INTO SALES(VISIT_ID, CUSTID, BOUGHT , Item, QTY , Free , Items_Returned , Reason_Return) VALUES (373, 251 ,1 , 'Dark Choclate', 3 ,0.75,2,NULL);
INSERT INTO SALES(VISIT_ID, CUSTID, BOUGHT , Item, QTY , Free , Items_Returned , Reason_Return) VALUES (396, 265 ,1 , 'Olive Oil', 0.5 ,0.5,2,NULL);
INSERT INTO SALES(VISIT_ID, CUSTID, BOUGHT , Item, QTY , Free , Items_Returned , Reason_Return) VALUES (396, 265 ,1 , 'Dark Choclate', 0.2 ,0.2,2,NULL);
INSERT INTO SALES(VISIT_ID, CUSTID, BOUGHT , Item, QTY , Free , Items_Returned , Reason_Return) VALUES (396, 265 ,1 , 'Cold Drink', 0.5 ,0.5,1,'Packing Issue');

-- QUERY database
</code></pre>
<p><a href=""https://sqlfiddle.com/sql-server/online-compiler?id=e33a2210-54cb-42e7-b94f-9dd52844760c"" rel=""nofollow noreferrer"">SQL Fiddle</a></p>
<p>My try which didn't work</p>
<pre><code>SELECT Visi_ID , CustID 
&quot;Cake&quot; as T1 , ISNULL(&quot;Cake&quot;,0) As Q1
&quot;Dark Choclate&quot; as T2 , ISNULL(&quot;Dark Choclate&quot;,0) As Q2
&quot;Olive Oil&quot; as T3 , ISNULL(&quot;Olive Oil&quot;,0) As Q3
&quot;Cold Drink&quot; as T4 , ISNULL(&quot;Cold Drink&quot;,0) As Q4
FROM
 ( SELECT VISIT_ID , CUSTID FROM SALES ) AS SOURCE_TABLE
 PIVOT
 ( SUM(QTY)FOR ITEM IN (&quot;Cake&quot;,&quot;Dark Choclate&quot;,&quot;Olive Oil&quot;,&quot;Cold Drink&quot;)
 ) AS  PivotTable
</code></pre>
<p>This returns the errors:</p>
<blockquote>
<p>Msg 156 Level 15 State 1 Line 2<br />
Incorrect syntax near the keyword 'as'.<br />
Msg 156 Level 15 State 1 Line 7<br />
Incorrect syntax near the keyword 'AS'.</p>
</blockquote>
",-3,0,3,2025-12-11T09:45:43+00:00,1,143,True
79844954,20437977,,sql,Problem in SSIS package in DATA FLOW TASK section,"<p>I am placing the following command in a data flow task in an SSIS 2022 project, and the parameter I have is of type int(32) and the ReceptionDate_Shamsi column is of type char(10), and this causes my package to give an error. What is your solution?</p>
<pre><code>SELECT
    PatientReceptionHeaderPK,
    PatientPK,
    DoctorPK,
    InsuranceTypeCode,
    ReceptionDate_Shamsi
FROM PatientReceptionHeader
WHERE ReceptionDate_Shamsi &gt;= FORMAT(DATEADD(DAY, ?,GETDATE()), 'yyyy/MM/dd', 'fa-IR')
</code></pre>
",0,0,0,2025-12-12T14:11:28+00:00,1,65,False
79844959,23479712,,sql,How can I ensure the uniqueness of folder names in a hierarchical SQL table if the parent column is NULL?,"<p>I am trying to create a table in MySQL to store a file hierarchy. In this table, the <code>parent_folder_id</code> column will reference the <code>id</code> of the parent folder. If a folder does not have a parent, the value of this column will be <code>NULL</code>, which means the folder is the root folder. Each user can have only one folder with a given name under the same parent.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE folders
  (
     id               INT NOT NULL auto_increment,
     user_id          INT NOT NULL,
     parent_folder_id INT DEFAULT NULL,
     folder_name      VARCHAR(255) NOT NULL,
     PRIMARY KEY (id),
     FOREIGN KEY (user_id) REFERENCES users(id),
     FOREIGN KEY (parent_folder_id) REFERENCES folders(id),
     CONSTRAINT uq_user_folder_name UNIQUE KEY (user_id, parent_folder_id,
     folder_name)
  );
</code></pre>
<p>The problem occurs when <code>parent_folder_id</code> is <code>NULL</code>. In this case, MySQL does not perform the comparison as expected, and a user may end up with an infinite number of root folders. How can this be prevented?</p>
",2,2,0,2025-12-12T14:21:44+00:00,2,115,True
79846390,12642829,,sql,How to count the number of matching characters in two strings of the same length?,"<p>Is there a way to count the number of matching characters between two strings (of the same length) in MariaDB?</p>
<p>Like if we have <code>&quot;keyboard&quot;</code> and <code>&quot;ekyboard&quot;</code> then the answer should be 6 matching characters (<code>..yboard</code>). I'm not looking for a Levenshtein function, just how many characters match in the same place in the strings.</p>
<p>I'd like to <code>ORDER BY</code> by the percentage of matching characters (75% in the keyboard example) <code>DESC</code>.</p>
<p>Background:</p>
<p>I need to search for nouns.</p>
<p>First, the original string (str).</p>
<p>And then a generated string that has regular expression character classes (<code>&quot;keyboard&quot; =\&gt; &quot;\[ck\]\[eij\]\[ijy\]\[bp\]\[ao\]\[ao\]\[.\]\[dt\]&quot; =\&gt; r_str)</code>.</p>
<p>(For the strange character classes, it is a mix of Swedish (wrong)spelling and pronunciation).</p>
<p>Then I <code>UNION two SELECT</code> statements.</p>
<p>The first looks for str in the address (<code>LIKE &quot;%str%&quot;</code>) and sets <code>priority=1, percentage=100, loc=start position.</code></p>
<p>The second looks for r_str in the address and should set priority=2, percentage=(percent of matching characters compared to str), loc=start position.</p>
<p>Then <code>GROUP BY noun</code> <code>ORDER BY priority ASC, index ASC, percentage DESC</code>.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * WHERE noun LIKE &quot;%str%&quot;,  priority AS 1, percentage AS 100, LOCATE(&quot;str&quot;, noun) AS loc
UNION
SELECT * WHERE noun RLIKE &quot;r_str&quot;, priority AS 2, percentage AS ???, REGEXP_INSTR(noun, &quot;str&quot;) AS loc
GROUP BY noun
ORDER BY priority, percentage DESC, loc;
</code></pre>
<p>How do I get the number of matching characters between two stings of the same length?</p>
",1,1,0,2025-12-13T05:17:22+00:00,3,179,True
79847000,32051267,,sql,How to exclude records based on conditions in related tables in SQL?,"<p>I have two tables: Patient and Encounters. They are related through an ID for each patient, with the ID appearing as &quot;Id&quot; in the Patient table and as &quot;Patient&quot; in the Encounters table. I need to get the full name and ID of those patients who have never had an 'Encounter Inpatient', with that data being stored in the Description column of the Encounters table. I thought about excluding the patients this way, but it only shows the rows with a value in Description different from that, and not the patients who have never had that value, since several patients have multiple entries in Encounters, each with a different Description.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
   p.Id, p.First, p.Last 
FROM datapatient.Encounters e 
LEFT JOIN datapatient.Patients p 
ON e.Patient=p.Id 
WHERE NOT e.Description=&quot;Encounter Inpatient&quot; 
GROUP BY p.Id;
</code></pre>
",-1,1,2,2025-12-14T12:07:06+00:00,1,114,True
79847129,30487101,,sql,Updating a table with CASE,"<p>In this example I wanted to replace every <code>NULL</code> value in the <code>Quantity</code> column with the mean of the column. I'm using <code>CASE</code> here to achieve that but what ends up happening is instead of replacing <code>NULL</code> with 3 it replaces every cell in the <code>Quality</code> column with <code>NULL</code>.</p>
<p><a href=""https://i.sstatic.net/TMHe3ltJ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I have tried using</p>
<pre><code>WHEN Quality = 'NULL' THEN 3
</code></pre>
<p>or</p>
<pre><code>WHEN Quality IN(-1, ' ', 0) THEN 3 
</code></pre>
<p>but nothing works - they all end up replacing everything with <code>NULL</code>.</p>
<p>What should I be doing instead here?</p>
",-2,1,3,2025-12-14T16:25:57+00:00,1,175,True
79848545,5338127,,sql,Facing issue in Big Query while extracting JSON Value,"<p>We are leveraging BigQuery to create reports, with some column values represented in JSON. Below is a sample payload. I can successfully retrieve the Template value, but the objectType value remains empty. According to my analysis, <code>JSON_Extract</code> or <code>JSON_Value</code> does not work when the value is <code>null</code> keyword. Is there a solution to this issue?</p>
<pre><code>{
“template” : “TEST”,
“partNumber”: null,
“model”: null,
“objectType”: “SHELF”
}
</code></pre>
<pre><code>SELECT
  JSON_VALUE(equipment, '$.template') AS template,
  JSON_VALUE(equipment, '$.objectType') AS objectType from Testing 
</code></pre>
",-1,0,1,2025-12-16T12:49:37+00:00,0,113,False
79848658,3617866,"Dhaka, Bangladesh",sql,GridDB Python query returns empty result set even though rows exist,"<p>I’m using the GridDB Python client to query a container that definitely has rows, but my query returns an empty result set. Inserts succeed, and <code>container.get(row_key)</code> works, so I’m confident data exists.</p>
<p><strong>Environment</strong></p>
<ul>
<li>GridDB server: 5.x</li>
<li>Python: 3.10</li>
<li>griddb-python client installed via pip</li>
<li>Container type: Collection (row-key enabled)</li>
</ul>
<p><strong>Code</strong></p>
<pre><code>import griddb_python as griddb

factory = griddb.StoreFactory.get_instance()

store = factory.get_store({
    &quot;host&quot;: &quot;192.168.1.100&quot;,
    &quot;port&quot;: 31999,
    &quot;cluster_name&quot;: &quot;myCluster&quot;,
    &quot;username&quot;: &quot;admin&quot;,
    &quot;password&quot;: &quot;admin&quot;,
})

con = store.get_container(&quot;sensor_readings&quot;)

# Data exists (this returns a row)
print(con.get(&quot;device-001&quot;))

# But this query returns an empty
query = con.query(&quot;SELECT * WHERE deviceId = 'device-001'&quot;)
rs = query.fetch()
print(&quot;has_next:&quot;, rs.has_next())

while rs.has_next():
    row = rs.next()
    print(row)
</code></pre>
<p>I am expecting at least one row for deviceId = <code>device-001</code>.</p>
<p><strong>But I’m seeing</strong></p>
<ul>
<li>con.get(&quot;device-001&quot;) returns a row (so the container isn’t empty)</li>
<li>rs.has_next() is False immediately</li>
</ul>
<p><strong>Questions</strong></p>
<ol>
<li>In GridDB SQL, do WHERE clauses require an explicit FROM 
Or a different syntax in Python?</li>
<li>Could this be caused by a column type mismatch (e.g., deviceId
stored as BLOB/INTEGER vs string) or case-sensitive column names?</li>
<li>Is there a recommended way to debug the schema/column types from
Python to confirm the stored types?</li>
</ol>
",-1,0,1,2025-12-16T15:20:37+00:00,1,127,True
79848896,16967557,,sql,Why indexes don&#39;t improve performance?,"<p>This query uses my indexes according to the plan, however, they do not give any increase in query execution speed. Absolutely.</p>
<p>I've tried a wide variety of indexes, but none have helped.   I've already been told that the query must have &quot;where&quot; and/or &quot;order by&quot; in order for the indexes to work correctly, but even with them, nothing has changed. There is also a problem with sorting in this query, it takes quite a long time to complete, I don't know what can be done about it.</p>
<p>Indexes:</p>
<pre><code>CREATE INDEX idx_dept_name_id 
ON department (&quot;name&quot;, &quot;ID_department&quot;);

CREATE INDEX idx_inventory_faulty_partial 
ON inventorylist (&quot;inventory_number&quot;) 
WHERE &quot;ID_total&quot; IN (2, 3);

CREATE INDEX idx_auditory_dept_num 
ON auditory (&quot;ID_department&quot;, &quot;number_auditory&quot;);

CREATE INDEX idx_equipment_audit_inv 
ON equipment (&quot;number_auditory&quot;, &quot;inventory_number&quot;);

CREATE INDEX idx_equipment_inv_audit 
ON equipment (&quot;inventory_number&quot;, &quot;number_auditory&quot;);
</code></pre>
<p>The query:</p>
<pre><code>WITH BadDepartments AS (
    SELECT DISTINCT a.&quot;ID_department&quot;
    FROM auditory a
    JOIN equipment e ON a.&quot;number_auditory&quot; = e.&quot;number_auditory&quot;
    JOIN inventorylist il ON e.&quot;inventory_number&quot; = il.&quot;inventory_number&quot;
    WHERE il.&quot;ID_total&quot; IN (2, 3)
),
DeptStats AS (
    SELECT 
        a.&quot;ID_department&quot;,
        COUNT(e.&quot;inventory_number&quot;) as cnt
    FROM auditory a
    JOIN equipment e ON a.&quot;number_auditory&quot; = e.&quot;number_auditory&quot;
    GROUP BY a.&quot;ID_department&quot;
)
SELECT 
    d.&quot;name&quot; AS &quot;division&quot;,
    COALESCE(ds.cnt, 0) AS &quot;num of equip&quot;
FROM department d
LEFT JOIN BadDepartments bd ON d.&quot;ID_department&quot; = bd.&quot;ID_department&quot;
LEFT JOIN DeptStats ds ON d.&quot;ID_department&quot; = ds.&quot;ID_department&quot;
WHERE bd.&quot;ID_department&quot; IS NULL
ORDER BY d.&quot;name&quot; ASC;
</code></pre>
<p>The query plan of my query:</p>
<pre><code>&quot;Gather Merge  (cost=144266.05..192881.18 rows=416672 width=52) (actual time=3277.979..4597.799 rows=500008 loops=1)&quot;
&quot;  Workers Planned: 2&quot;
&quot;  Workers Launched: 2&quot;
&quot;  -&gt;  Sort  (cost=143266.03..143786.87 rows=208336 width=52) (actual time=3155.924..3507.905 rows=166669 loops=3)&quot;
&quot;        Sort Key: d.name&quot;
&quot;        Sort Method: external merge  Disk: 11040kB&quot;
&quot;        Worker 0:  Sort Method: external merge  Disk: 11088kB&quot;
&quot;        Worker 1:  Sort Method: external merge  Disk: 10112kB&quot;
&quot;        -&gt;  Hash Left Join  (cost=102493.05..117738.55 rows=208336 width=52) (actual time=632.838..754.065 rows=166669 loops=3)&quot;
&quot;              Hash Cond: (d.&quot;&quot;ID_department&quot;&quot; = ds.&quot;&quot;ID_department&quot;&quot;)&quot;
&quot;              -&gt;  Hash Anti Join  (cost=13.08..9926.70 rows=208336 width=48) (actual time=0.063..42.340 rows=166669 loops=3)&quot;
&quot;                    Hash Cond: (d.&quot;&quot;ID_department&quot;&quot; = a.&quot;&quot;ID_department&quot;&quot;)&quot;
&quot;                    -&gt;  Parallel Seq Scan on department d  (cost=0.00..7283.37 rows=208337 width=48) (actual time=0.011..11.846 rows=166669 loops=3)&quot;
&quot;                    -&gt;  Hash  (cost=13.07..13.07 rows=1 width=4) (actual time=0.033..0.036 rows=0 loops=3)&quot;
&quot;                          Buckets: 1024  Batches: 1  Memory Usage: 8kB&quot;
&quot;                          -&gt;  Unique  (cost=13.06..13.07 rows=1 width=4) (actual time=0.033..0.035 rows=0 loops=3)&quot;
&quot;                                -&gt;  Sort  (cost=13.06..13.06 rows=1 width=4) (actual time=0.032..0.034 rows=0 loops=3)&quot;
&quot;                                      Sort Key: a.&quot;&quot;ID_department&quot;&quot;&quot;
&quot;                                      Sort Method: quicksort  Memory: 25kB&quot;
&quot;                                      Worker 0:  Sort Method: quicksort  Memory: 25kB&quot;
&quot;                                      Worker 1:  Sort Method: quicksort  Memory: 25kB&quot;
&quot;                                      -&gt;  Nested Loop  (cost=0.97..13.05 rows=1 width=4) (actual time=0.015..0.016 rows=0 loops=3)&quot;
&quot;                                            -&gt;  Nested Loop  (cost=0.55..12.58 rows=1 width=9) (actual time=0.015..0.016 rows=0 loops=3)&quot;
&quot;                                                  -&gt;  Index Only Scan using idx_inventory_faulty_partial on inventorylist il  (cost=0.12..8.14 rows=1 width=11) (actual time=0.015..0.015 rows=0 loops=3)&quot;
&quot;                                                        Heap Fetches: 0&quot;
&quot;                                                  -&gt;  Index Only Scan using idx_equipment_inv_audit on equipment e  (cost=0.42..4.44 rows=1 width=21) (never executed)&quot;
&quot;                                                        Index Cond: (inventory_number = (il.inventory_number)::text)&quot;
&quot;                                                        Heap Fetches: 0&quot;
&quot;                                            -&gt;  Index Scan using u_nom on auditory a  (cost=0.42..0.47 rows=1 width=13) (never executed)&quot;
&quot;                                                  Index Cond: ((number_auditory)::text = (e.number_auditory)::text)&quot;
&quot;              -&gt;  Hash  (cost=98489.63..98489.63 rows=229547 width=12) (actual time=632.183..632.185 rows=699 loops=3)&quot;
&quot;                    Buckets: 262144  Batches: 2  Memory Usage: 2063kB&quot;
&quot;                    -&gt;  Subquery Scan on ds  (cost=87062.75..98489.63 rows=229547 width=12) (actual time=631.126..631.556 rows=699 loops=3)&quot;
&quot;                          -&gt;  HashAggregate  (cost=87062.75..96194.16 rows=229547 width=12) (actual time=631.125..631.472 rows=699 loops=3)&quot;
&quot;                                Group Key: a_1.&quot;&quot;ID_department&quot;&quot;&quot;
&quot;                                Planned Partitions: 4  Batches: 1  Memory Usage: 1617kB&quot;
&quot;                                Worker 0:  Batches: 1  Memory Usage: 1617kB&quot;
&quot;                                Worker 1:  Batches: 1  Memory Usage: 1617kB&quot;
&quot;                                -&gt;  Nested Loop  (cost=0.86..42219.00 rows=700000 width=16) (actual time=0.402..480.119 rows=700000 loops=3)&quot;
&quot;                                      -&gt;  Index Only Scan using idx_equipment_audit_inv on equipment e_1  (cost=0.42..24392.42 rows=700000 width=21) (actual time=0.331..113.335 rows=700000 loops=3)&quot;
&quot;                                            Heap Fetches: 0&quot;
&quot;                                      -&gt;  Memoize  (cost=0.43..0.48 rows=1 width=13) (actual time=0.000..0.000 rows=1 loops=2100000)&quot;
&quot;                                            Cache Key: e_1.number_auditory&quot;
&quot;                                            Cache Mode: logical&quot;
&quot;                                            Hits: 699300  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            Worker 0:  Hits: 699300  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            Worker 1:  Hits: 699300  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            -&gt;  Index Scan using u_nom on auditory a_1  (cost=0.42..0.47 rows=1 width=13) (actual time=0.019..0.019 rows=1 loops=2100)&quot;
&quot;                                                  Index Cond: ((number_auditory)::text = (e_1.number_auditory)::text)&quot;
&quot;Planning Time: 6.017 ms&quot;
&quot;Execution Time: 4618.617 ms&quot;
</code></pre>
",2,2,0,2025-12-16T20:58:02+00:00,2,166,True
79848914,,,sql,What is the efficient way to match a row from Table A where there exists multiple specific rows in Table B,"<p>I'm not sure what the correct or best practice way of handling this would be. I'm trying to avoid having this query hold the resource for too long and I don't have experience with queries like this.</p>
<p>I need rows on table A Where on table B the ID and date match to table A and they have multiple rows (in this case 8) that have specific values in B So I want all rows on Table A Where there exists rows on Table B that match the ID and date AND those 8 specific rows also exist for that ID+date</p>
<p>I tried to do some searching and it seemed like using too many &quot;where exists&quot; clauses can be extremely detrimental to performance.</p>
<p>I'll try to give an example that's a bit of a simplification of the data I'm working with. The actual amount of total columns and data size here is fairly large (legacy financial applications).</p>
<p>Table A example</p>
<pre><code>ID.      DATE     
1.      2025-01-01
2.      2025-01-01
3.      2025-01-01
4.      2025-01-01
....
</code></pre>
<p>Table B example</p>
<pre><code>
ID.      DATE.        Field1.   Field2.  Field3.     Field4.
1.       2025-01-01.  01.       001      01.         001
1.       2025-01-01.  01.       002      03.         003
1.       2025-01-01.  02.       001      03.         001
1.       2025-01-01.  02.       001      01.         001
1.       2025-01-01.  03.       003      03.         003
1.       2025-01-01.  03.       003      03.         001
1.       2025-01-01.  04.       002      02.         001
1.       2025-01-01.  04.       002      02.         002
2.       2025-01-01.  01.       002      02.         001
2.       2025-01-01.  01.       002      02.         001
2.       2025-01-01.  01.       002      02.         001
3.       2025-01-01.  01.       002      02.         001
3.       2025-01-01.  01.       002      02.         001
4.       2025-01-01.  01.       002      02.         001
....
</code></pre>
<p>The values needed for each of the 8 specific combinations for the 4 fields are known. Let's pretend that in the example above, all the values in the Field1 - Field4 are correct and are what we are looking to match on.</p>
<p>So with this in mind, what is the most efficient way of doing a search that will identify rows from table A where there also exists ALL 8 rows on table B with the same ID and DATE. It is important that the matching specifically will only match if ALL 8 total rows exist.</p>
<p>This query will be run through the SPUFI utility on a z/OS IBM mainframe.</p>
",1,0,0,2025-12-16T21:35:07+00:00,4,82,True
79849335,18159572,,sql,Case Expression in Where clause,"<p>Isn't it much easier to write this:</p>
<pre><code>SELECT *
FROM your_table
WHERE CASE
  WHEN CURTIME() BETWEEN '11:01:00' AND '15:00:00' THEN created_at BETWEEN CONCAT(CURDATE(), ' 11:01:00') AND CONCAT(CURDATE(), ' 15:00:00')
  WHEN CURTIME() BETWEEN '15:01:00' AND '17:00:00' THEN created_at BETWEEN CONCAT(CURDATE(), ' 15:01:00') AND CONCAT(CURDATE(), ' 17:00:00')
 ELSE created_at BETWEEN CONCAT(DATE_SUB(CURDATE(), INTERVAL 1 DAY), ' 17:01:00') AND CONCAT(CURDATE(), ' 11:00:00')
 END;
</code></pre>
<p>than this?</p>
<pre><code>SELECT *
FROM your_table
WHERE created_at BETWEEN
(
    CASE
        WHEN CURTIME() BETWEEN '11:01:00' AND '15:00:00' THEN CONCAT(CURDATE(), ' 11:01:00')
        WHEN CURTIME() BETWEEN '15:01:00' AND '17:00:00' THEN CONCAT(CURDATE(), ' 15:01:00')
        ELSE CONCAT(DATE_SUB(CURDATE(), INTERVAL 1 DAY), ' 17:01:00')
    END
)
AND
(
    CASE
        WHEN CURTIME() BETWEEN '11:01:00' AND '15:00:00' THEN CONCAT(CURDATE(), ' 15:00:00')
        WHEN CURTIME() BETWEEN '15:01:00' AND '17:00:00' THEN CONCAT(CURDATE(), ' 17:00:00')
        ELSE CONCAT(CURDATE(), ' 11:00:00')
    END
);
</code></pre>
<p>I get the same results.<br>
On the first one, I can add all the statements I want, not just the <code>created_at</code>, for example adding <code>AND user_id = 1</code>.</p>
<p>Can someone please explain why the 2nd is better.</p>
",0,0,0,2025-12-17T11:03:09+00:00,8,89,True
79849398,28534867,,sql,Dynamic grouping over parameter in athena quik suite SQL,"<p>I am looking for a way to dynamically group rows of a data set. If a defined column name is passed in the parameter, each row should be listed individually. If the column name is not listed, all values should be concatenated according to the other values passed. The ID column is always used for grouping, and then others can be added optionally. This should work for a list of 10 fields. The fields can be selected in a wide variety of combinations. Or is there a way to do this in the dashboard itself with calculated fields?</p>
<p>My code is:</p>
<pre><code>select
  id,
  array_join(array_agg(DISTINCT column1), ', ') AS concat_column1,
  array_join(array_agg(DISTINCT column2), ', ') AS concat_column2,
  count(\*) as zaehler
from  &quot;AwsDataCatalog&quot;.&quot;database&quot;.table i
GROUP BY id, LISTAGG(\&lt;\&lt;parameter\&gt;\&gt;,',')
</code></pre>
<p>But the LISTAGG makes only a comma separated list of strings. This cannot be used as a field list. If i replace the parameter with the real list of Fields it works. This shu</p>
<p>for example:
source</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>column1</th>
<th>column2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>7</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>8</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>9</td>
</tr>
</tbody>
</table></div>
<p>result parameter list: id</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>concat_column1</th>
<th>concat_column2</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1,2</td>
<td>4,5,6</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>2,3</td>
<td>7,8,9</td>
<td>3</td>
</tr>
</tbody>
</table></div>
<p>result parameter list: id,column1</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>concat_column1</th>
<th>concat_column2</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4,5</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>6</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>7</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>8,9</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>result parameter list: id,column1,column2</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>concat_column1</th>
<th>concat_column2</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>6</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>7</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>8</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
<td>9</td>
<td>1</td>
</tr>
</tbody>
</table></div>
",2,2,0,2025-12-17T11:53:04+00:00,1,108,False
79850312,10755553,,sql,Good ways to keep certain rows in case there are multiple unique rows,"<p>I've got a table that includes the following 2 attributes. I want to keep a distinct row per name, and only if <code>status = 'NEW' else if 'CURRENT'</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>NAME</th>
<th>STATUS</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>NEW</td>
</tr>
<tr>
<td>2</td>
<td>NEW</td>
</tr>
<tr>
<td>2</td>
<td>CURRENT</td>
</tr>
<tr>
<td>3</td>
<td>CURRENT</td>
</tr>
<tr>
<td>3</td>
<td>MODIFIED</td>
</tr>
</tbody>
</table></div>
<p>I think I want to inner join the main table with a selection of itself taking only the rows I want to keep. The following statements work, but are these good ways to retrieve these rows? Is there any preferred way in case I'm handling a lot of data?</p>
<pre><code>-- mapping to numeric, taking min, mapping back:
SELECT DISTINCT
    NAME,
    CASE 
            WHEN MIN(
                CASE 
                    WHEN STATUS = 'NEW' THEN 1
                    WHEN STATUS = 'CURRENT' THEN 2
        ELSE Null END
            ) = 1 THEN 'NEW' 
            WHEN MIN(
                CASE 
                    WHEN STATUS = 'NEW' THEN 1
                    WHEN STATUS = 'CURRENT' THEN 2
        ELSE Null END
            ) = 2 THEN 'CURRENT' 
            ELSE Null END AS TMP
FROM MyTable
GROUP BY NAME HAVING TMP IS NOT Null


</code></pre>
<pre><code>-- listagg status
SELECT DISTINCT
    NAME, 
    CASE 
            WHEN listagg(DISTINCT STATUS) WITHIN GROUP (ORDER BY STATUS) LIKE '%NEW%' THEN 'NEW'
            WHEN listagg(DISTINCT STATUS) WITHIN GROUP (ORDER BY STATUS) LIKE '%CURRENT%' THEN 'CURRENT'
        ELSE NULL 
    END AS TMP
FROM myTable
GROUP BY NAME HAVING TMP IS NOT NULL
</code></pre>
",2,2,0,2025-12-18T12:41:43+00:00,1,71,True
79850545,21533009,,sql,Question about `LAST_VALUE` Function Behavior in `GROUP BY` Query in Apache IoTDB,"<p>I'm executing an SQL query with <code>GROUP BY</code> <code>time</code> window in IoTDB 2.0.5 using the <code>LAST_VALUE</code> function. The SQL statement is as follows:</p>
<pre class=""lang-sql prettyprint-override""><code>  SELECT
  `last_value`(AA_01.rcvalue) AS AA_01,
  `last_value`(AA_02.rcvalue) AS AA_02,
  `last_value`(AB_01.rcvalue) AS AB_01,
  `last_value`(AB_02.rcvalue) AS AB_02,
  `last_value`(AC_01.rcvalue) AS AC_01
  FROM `root.connector.modbus_tcp_connector.server_1320_1`
  WHERE `time` &gt;= 2025-10-27 14:30:40 AND `time` &lt;= 2025-10-31 14:58:40
  `GROUP BY` ([2025-10-27 14:47:40, 2025-10-31 14:47:40), 10m)
  HAVING `last_value`(AA_01.rcvalue) IS NOT `NULL`
</code></pre>
<p>However, I found that the returned results don't seem to be the last data point within the current <code>time</code> window, but rather the data from the next <code>time</code> interval.</p>
<p><a href=""https://i.sstatic.net/8ZkpbFTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8ZkpbFTK.png"" alt=""snap"" /></a></p>
<p>Partial raw data is as follows:</p>
<p><a href=""https://i.sstatic.net/G1uRyrQE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/G1uRyrQE.png"" alt=""s2"" /></a>
<a href=""https://i.sstatic.net/Ic8JOtWk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ic8JOtWk.png"" alt=""s3"" /></a></p>
<p>Is there an issue with my SQL writing? How should I modify it to correctly get the last value within each <code>time</code> window?</p>
",-2,0,2,2025-12-18T17:15:59+00:00,0,30,False
79850577,17629737,,sql,Django Can&#39;t find table during insert: LINE 1: SELECT 1 AS &quot;a&quot; FROM &quot;qmgr_def_rules&quot; WHERE &quot;qmgr_def_rules&quot;,"<p>I'm trying to add a row to my model, but it keeps coming back with:</p>
<blockquote>
<p>django.db.utils.ProgrammingError: relation &quot;qmgr_def_rules&quot; does not exist
LINE 1: SELECT 1 AS &quot;a&quot; FROM &quot;qmgr_def_rules&quot; WHERE &quot;qmgr_def_rules&quot;...</p>
</blockquote>
<p>Some context:
I have 3 different applications under the same django project. I have 2 different postgres schemas: rules and django.
The django schema is set to default where all the django specific tables go. The rules schema is for internal data where I can set interval timers, object definitions, etc. I have multiple tables in the rules schema for different types of data.</p>
<p>When I use the admin panel to insert/update/delete on any other table in that schema, it works. Just not def_rules for some odd reason.</p>
<p>A few really odd things:</p>
<ul>
<li>Updates work</li>
<li>Deletions work</li>
<li>Inserts break with error above through admin panel</li>
<li>Inserts work if done through django shell</li>
</ul>
<p>(this is the shell that I used)</p>
<pre><code>from admin_page.models import QmgrDefRules
from django.utils import timezone

# Try to create a test entry with all required fields
try:
    test_entry = QmgrDefRules(
        # Primary key - required
        qmgr='TEST',
        
        # Required boolean fields (NOT NULL in DDL)
        moni_core_enabled=False,
        moni_inventory_enabled=False,
        qreset=True,  # Has default=true in DDL
        dbinsert_0_value=False,  # Has default=false in DDL
        qsg_monitor=False,  # Has default=false in DDL
        dlqh=False,  # Has default=false in DDL
        smds_monitor=False,  # Has default=false in DDL
        monitor_queuestats=False,  # Has default=false in DDL
        monitor_smds=False,  # Has default=false in DDL
        monitor_rba=False,  # Has default=false in DDL
        monitor_psid_bp=False,  # Has default=false in DDL
        monitor_qhandles=False,  # Has default=false in DDL
        monitor_chstatus_p2p=False,  # Has default=false in DDL
        monitor_chstatus_svrconn=False,  # Has default=false in DDL
        monitor_cfstatus=False,  # Has default=false in DDL
        monitor_chstatus_cluschls=False,  # Has default=false in DDL
        
        # Required integer fields (NOT NULL in DDL)
        rule_refresh_interval_in_sec=60,
        restapi_timeout=5,
        restapi_max_retry=5,
        
        # Optional fields (can be NULL in DDL)
        long_retry_wait_in_sec=600,
        bu='bu',  
        location='test',  
        qsg=None,
        day_alert_from=None,
        day_alert_to=None,
        time_alert_from=None,
        time_alert_to=None,
        email_groups='mail@mail.com',
        page_groups=None,
        mq_rest_url='http://url:port/',
        opsmvs_rest_url=None,
        dlqh_qdepth=None,
        dlqh_msgage=None,
        dlq_name=None,
        last_update=timezone.now(),
        date_created=timezone.now()
    )
    
    # Save to the 'rules' database
    test_entry.save(using='rules')
    print(f&quot; Success! Created QMGR: {test_entry.qmgr}&quot;)
    
except Exception as e:
    print(f&quot; Error: {type(e).__name__}: {e}&quot;)
    import traceback
    traceback.print_exc()
</code></pre>
<p>Here's a model and its admin definition that works:</p>
<pre><code>class QdepthRules(models.Model):
    rule_id = models.AutoField(db_column='id', primary_key=True)
    enabled = models.BooleanField(db_column='enabled')  # Checkbox
    qmgr = models.CharField(db_column='qmgr', max_length=4, validators=[qmgrValidator], help_text='All caps (ex: QQQQ)')  # Field name made lowercase.
    queue = models.CharField(db_column='queue', max_length=45, help_text=&quot;Case sensitive. Wildcards allowed.&quot;)  # Field name made lowercase.
    threshold = models.IntegerField(db_column='threshold',default=1, validators=[MinValueValidator(1)], help_text=&quot;How many messages until MONI alerts.&quot;)  # Field name made lowercase.
    day_alert_from = models.IntegerField(db_column='day_alert_from', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    day_alert_to = models.IntegerField(db_column='day_alert_to', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    time_alert_from = models.TimeField(db_column='time_alert_from', blank=True, null=True)  # Field name made lowercase.
    time_alert_to = models.TimeField(db_column='time_alert_to', blank=True, null=True)  # Field name made lowercase.
    email_groups = models.TextField(db_column='email_groups', blank=True, null=True, validators=[emailValidator], help_text=&quot;Format: mail@mail.com&quot;)  # Field name made lowercase.
    page_groups = models.TextField(db_column='page_groups', blank=True, null=True, validators=[pageValidator]&quot;)  # Field name made lowercase.
    repeat_alert_val = models.IntegerField(db_column='repeat_alert_val', default=1, validators=[MinValueValidator(1), MaxValueValidator(240)], help_text=&quot;How many times the alert needs to trip before sending an alert&quot;)  # Field name made lowercase.
    email_page_both = models.CharField(db_column='email_page_both', max_length=45, blank=True, null=True)  # Field name made lowercase.
    last_update = models.DateTimeField(db_column='last_update', blank=True, null=True)  # Field name made lowercase.
    date_created = models.DateTimeField(db_column='date_created', blank=True, null=True)  # Field name made lowercase.

    def __str__(self):
        return f&quot;QMGR: {self.qmgr} - QUEUE: {self.queue} - THRESH: {self.threshold} - ENABLED: {bool(self.enabled)}&quot;

    class Meta:
        managed = False
        db_table = 'qdepth_rules'
        verbose_name_plural = &quot;QDepth_Rules&quot;



---------------------------

from django.contrib import admin

class QDRAdmin(admin.ModelAdmin):
    # fields = (&quot;Title&quot;,)

    field_list = []
    for each in QdepthRules._meta.get_fields():
        field_list.append(each.name)

    list_display = field_list
    list_filter = ('date_created',)
    search_fields = ['qmgr','queue']
    readonly_fields = ('date_created','last_update')
    list_display_links = ['rule_id','qmgr','queue']
    
    def get_queryset(self, request):
        return super().get_queryset(request).using('rules')
    
    def save_model(self, request, obj, form, change):
        obj.save(using='rules')
    
    def delete_model(self, request, obj):
        obj.delete(using='rules')
    
    def save_related(self, request, form, formsets, change):
        form.save_m2m()
</code></pre>
<p>And heres the one that doesn't:</p>
<pre><code>class QmgrDefRules(models.Model):
    qmgr = models.CharField(primary_key=True,db_column='qmgr', max_length=4, validators=[qmgrValidator], help_text='All caps (ex: QQQQ)')  # Field name made lowercase.
    moni_core_enabled = models.BooleanField(db_column='moni_core_enabled')  # Checkbox
    moni_inventory_enabled = models.BooleanField(db_column='moni_inventory_enabled') # Checkbox
    rule_refresh_interval_in_sec = models.IntegerField(db_column='rule_refresh_interval_in_sec', default=60, help_text=&quot;How often (in seconds) MONI checks data against the QMGR.&quot;)  # Field name made lowercase.
    long_retry_wait_in_sec = models.IntegerField(db_column='long_retry_wait_in_sec', default=600)  # Field name made lowercase.
    bu = models.CharField(max_length=4)
    location = models.CharField(max_length=10)
    qsg = models.CharField(db_column='qsg', max_length=10, blank=True, null=True)  # Field name made lowercase.
    qsg_monitor = models.BooleanField(help_text='If QMGR is full repository, enable this to collect shared data. Only enable one QMGR per QSG')
    day_alert_from = models.IntegerField(db_column='day_alert_from', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    day_alert_to = models.IntegerField(db_column='day_alert_to', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    time_alert_from = models.TimeField(db_column='time_alert_from', blank=True, null=True)  # Field name made lowercase.
    time_alert_to = models.TimeField(db_column='time_alert_to', blank=True, null=True)  # Field name made lowercase.
    email_groups = models.TextField(db_column='email_groups', blank=True, null=True, validators=[emailValidator])  # Field name made lowercase.
    page_groups = models.TextField(db_column='page_groups', blank=True, null=True, validators=[pageValidator])  # Field name made lowercase.
    dbinsert_0_value = models.BooleanField(db_column='dbinsert_0_value', help_text=&quot;This will log values into the database even if MSGAGE, CURDEPTH, PUT, and GET are 0. NOTE: THIS SHOULD ONLY BE USED FOR EXTENSIVE TROUBLESHOOTING ONLY.&quot;)  # Checkbox
    qreset = models.BooleanField(db_column='qreset', help_text=&quot;Enables QRESET for the QMGR. Provides PUT/GET data.&quot;)  # Checkbox
    restapi_timeout = models.IntegerField(db_column='restapi_timeout', default=5, help_text=&quot;How long (in seconds) MONI waits for a response from the MQ REST API console.&quot;)  # Field name made lowercase.
    restapi_max_retry = models.IntegerField(db_column='restapi_max_retry', default=5, help_text=&quot;How many times MONI retries to connect to the REST API console if theres an issue connecting before initating a long cooldown.&quot;)  # Field name made lowercase.
    mq_rest_url = models.CharField(max_length=50, blank=True, null=True, help_text=&quot;URL to the MQ REST API console. Used to connect into the QMGR for data collection. (ex: http://url:port/)&quot;)
    opsmvs_rest_url = models.CharField(max_length=50, blank=True, null=True, help_text=&quot;URL to the OPSMVS REST API console. Used to send alerts for email/page.)
    dlqh = models.BooleanField(help_text=&quot;Enables DLQH for the QMGR.&quot;)
    dlqh_qdepth = models.SmallIntegerField(blank=True, null=True, help_text=&quot;QDEPTH at which will trigger the DLQH. Can be used with MSGAGE.&quot;)
    dlqh_msgage = models.SmallIntegerField(blank=True, null=True, help_text=&quot;MSGAGE at which will trigger the DLQH. Can be used with QDEPTH.&quot;)
    dlq_name = models.CharField(blank=True, null=True, help_text=&quot;DLQ for the QMGR (ex: QTN2.DEAD.QUEUE).&quot;)
    last_update = models.DateTimeField(db_column='last_update', blank=True, null=True)  # Field name made lowercase.
    date_created = models.DateTimeField(db_column='date_created', blank=True, null=True)  # Field name made lowercase.
    smds_monitor = models.BooleanField()
    monitor_queuestats = models.BooleanField()
    monitor_smds = models.BooleanField()
    monitor_rba = models.BooleanField()
    monitor_psid_bp = models.BooleanField()
    monitor_qhandles = models.BooleanField()
    monitor_chstatus_p2p = models.BooleanField()
    monitor_chstatus_svrconn = models.BooleanField()
    monitor_cfstatus = models.BooleanField()
    monitor_chstatus_cluschls = models.BooleanField()



    def __str__(self):
        return f&quot;QMGR: {self.qmgr} - RefreshINT: {self.rule_refresh_interval_in_sec} - QSG: {self.qsg} - MONI_CORE_ENABLED: {bool(self.moni_core_enabled)}&quot;

    class Meta:
        managed = False
        db_table = 'qmgr_def_rules'
        verbose_name_plural = &quot;QMGR_DEF_Rules&quot;




---------------------------------------


from django.contrib import admin

class QMGRDRAdmin(admin.ModelAdmin):
    # fields = (&quot;Title&quot;,)

    field_list = []
    for each in QmgrDefRules._meta.get_fields():
        field_list.append(each.name)
        
    # print(field_list)
    # print(QmgrDefRules._meta.get_field('qmgr'))

    list_display = field_list
    list_filter = ('date_created',)
    search_fields = ['qmgr','qsg']
    readonly_fields = ('date_created','last_update')
    list_display_links = ['qmgr','qsg']
    
    def get_queryset(self, request):
        return super().get_queryset(request).using('rules')
    
    def save_model(self, request, obj, form, change):
        obj.save(using='rules')
    
    def delete_model(self, request, obj):
        obj.delete(using='rules')
    
    def save_related(self, request, form, formsets, change):
        form.save_m2m()


</code></pre>
<p>I tried clearing out the migrations and starting from scratch as well, but that didn't seem to fix the problem unless I've done it incorrectly.</p>
",1,1,0,2025-12-18T17:55:34+00:00,0,78,False
79850581,23718712,,sql,Create a trigger with clause to update only one specific row,"<p>Due to a BUG, an application that persists data in an Oracle 19c database is updating a incorrect value in a specific row of a table.<br />
This incorrect value prevents users from accessing that table through the software.</p>
<p>The issue has already been reported, and we are currently waiting for an official fix.</p>
<p>In the meantime, I want to implement a trigger that restores the correct value whenever it is modified.<br />
I have the following code, but I would like to confirm whether this approach is correct, or if there is a more suitable solution.</p>
<pre><code>CREATE OR REPLACE TRIGGER trg_fix_object_flags
AFTER UPDATE ON column_registry
BEGIN
    UPDATE column_registry
       SET object_flags = 32768
     WHERE table_name  = 'ND_204939_DIRTYAREAS'
       AND column_name = 'DIRTYAREA'
       AND object_flags &lt;&gt; 32768;
END;
/
</code></pre>
",0,0,0,2025-12-18T18:06:01+00:00,8,61,True
79851291,25055931,,sql,SQL integration and multiply on huge dataset,"<p>I would like to find an efficient method to multiply 2 array element by element then integrate the result.</p>
<p>My table contain primary key, then 5 lists of real (16 bits) : time, channel A, B, C and D. Each list contains 2000 elements</p>
<p>My problem is that I tried with python or SQL based function like this :</p>
<pre><code>CREATE OR REPLACE FUNCTION trapezoid_mul_fast(
    a float4[],
    b float4[],
    x float8[]
)
RETURNS float8
LANGUAGE SQL
IMMUTABLE
AS $$
    SELECT SUM(
        (x[i] - x[i-1]) * ((a[i]*b[i]) + (a[i-1]*b[i-1])) * 0.5
    )
    FROM generate_subscripts(a, 1) AS i
    WHERE i &gt; 1;
$$;
</code></pre>
<p>then (for example)</p>
<pre><code> SELECT
    id,
    trapezoid_mul_fast(channel_a, channel_b, time_ns) AS int_ab
FROM my_table; 
</code></pre>
<p>But unfortunately it takes way too long to perform. Ideally, I would like to perform that on 300 millions lines, and just retrieving the result of each integration within like ~24h. Could someone tell me where I'm wrong or if it's even possible?</p>
",0,0,0,2025-12-19T16:46:39+00:00,2,114,True
79851461,20543172,,sql,Return rows from a group of rows if a specific value does not exist within the group,"<p>Based on the table values shown below, I am trying to return all rows for each <code>Name</code>, <code>Activity</code>, and <code>Date</code> when 'No Activity' is not in the <code>Activity</code> column for those dates.</p>
<p>I attempted the following but it is not producing the expected result. I receive some values per date but not all.</p>
<pre><code>SELECT
     Date
    ,Name
    ,Activity
FROM 
    testdb
GROUP BY 
    Date
    ,Name
    ,Activity
HAVING 
    SUM(CASE WHEN Activity = 'No Activity' THEN 1 ELSE 0 END) = 0
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Activity</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Patrick</td>
<td>Jury</td>
<td>12/17/2025</td>
</tr>
<tr>
<td>Patrick</td>
<td>Jury</td>
<td>12/17/2025</td>
</tr>
<tr>
<td>Patrick</td>
<td>No Activity</td>
<td>12/17/2025</td>
</tr>
<tr>
<td>Allen</td>
<td>On Call</td>
<td>12/18/2025</td>
</tr>
<tr>
<td>Kevin</td>
<td>PTO</td>
<td>12/18/2025</td>
</tr>
<tr>
<td>Melissa</td>
<td>PTO</td>
<td>12/15/2025</td>
</tr>
<tr>
<td>Melissa</td>
<td>PTO</td>
<td>12/15/2025</td>
</tr>
<tr>
<td>Marliss</td>
<td>On Call</td>
<td>12/8/2025</td>
</tr>
<tr>
<td>Marliss</td>
<td>On Call</td>
<td>12/8/2025</td>
</tr>
<tr>
<td>Marliss</td>
<td>On Call</td>
<td>12/8/2025</td>
</tr>
<tr>
<td>Marliss</td>
<td>No Activity</td>
<td>12/8/2025</td>
</tr>
</tbody>
</table></div>
<p>The expected result would be:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Activity</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Allen</td>
<td>On Call</td>
<td>12/18/2025</td>
</tr>
<tr>
<td>Kevin</td>
<td>PTO</td>
<td>12/18/2025</td>
</tr>
<tr>
<td>Melissa</td>
<td>PTO</td>
<td>12/15/2025</td>
</tr>
<tr>
<td>Melissa</td>
<td>PTO</td>
<td>12/15/2025</td>
</tr>
</tbody>
</table></div>
",3,3,0,2025-12-19T21:39:27+00:00,2,136,True
79851735,21804935,,sql,Inner join performance issue,"<p>I'm experiencing a performance issue with the following query, specifically caused by the join condition</p>
<pre><code>AND edh.EmployeeCode = e.EmployeeCode
</code></pre>
<p>After analyzing the execution plan, I noticed that SQL Server chooses a nested loops join strategy, and I don’t understand why. When I remove this condition, the query runs much faster.</p>
<p>For context, the approximate table sizes are:</p>
<ul>
<li><code>Employees</code>: ~5 million rows</li>
<li><code>Departments</code>: ~20,000 rows</li>
<li><code>Companies</code>: ~600,000 rows</li>
<li><code>Offices</code>: ~20,000 rows</li>
<li><code>EmployeeDepartmentHistory</code>: ~300,000 rows</li>
</ul>
<p>Here is the query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    e.EmployeeName,
    e.EmployeeSurname,
    d.CompanyID,
    e.DepartmentID,
    e.JobTitle,
    o.OfficeName,
    o.OfficeCity
FROM 
    dbo.Employees e
JOIN 
    dbo.Departments d ON d.DepartmentID = e.DepartmentID
JOIN 
    dbo.Companies c ON c.CompanyID = d.CompanyID
JOIN 
    dbo.Offices o ON o.OfficeID = e.OfficeID
JOIN 
    dbo.EmployeeDepartmentHistory edh ON edh.CompanyID = d.CompanyID 
                                      AND edh.EmployeeCode = e.EmployeeCode
WHERE 
    c.ItsOk = 'Y'
GROUP BY
    e.EmployeeName,
    e.EmployeeSurname,
    d.CompanyID,
    e.DepartmentID,
    e.JobTitle,
    o.OfficeName,
    o.OfficeCity;
</code></pre>
",0,2,2,2025-12-20T11:25:55+00:00,1,212,True
79852159,25618935,,sql,How do select ref OID without table object join when type is ref with OID?,"<p>Please see following statements and let me know is it possible to
have ref rowid same as other fields in mentioned select without adding
table object join which is t_tab_object here because ref is with
rowid. Thank you</p>
<pre><code>create type t_type_check as object (item1 number, item2 varchar2 (10))
/
create table t_tab_object of t_type_check
/
insert into t_tab_object
     values (t_type_check (123, 'test'))
/
insert into t_tab_object
     values (t_type_check (789, 'test'))
/
create table t_tab_check
(
    item1    number,
    item2    ref t_type_check with rowid
)
/
insert into t_tab_check
    select 456, ref (t)
      from t_tab_object t
/
select rowidtochar (t_.rowid), t_.item1,deref (t_.item2)
  from t_tab_check t_,t_tab_object t__
  where rowidtochar(t__.rowid)='AAAR1IAAAAAAAZEAAA'
/
</code></pre>
",0,0,0,2025-12-21T08:41:51+00:00,4,77,False
79852308,2962393,,sql,Recursive common table expression initial select search with primary key,"<p>I have a question about initial select in a recursive common table expression
(CTE) on SQLite and how to interpret the output of <code>EXPLAIN QUERY PLAN</code>.</p>
<p>I have constructed a linked list out of <code>node</code> and <code>link</code> tables. I query
a chain of linked nodes using a CTE query <code>chain</code>. I expected the initial
select in the SETUP phase to search the node table using primary key, but
instead it's showing SCAN.</p>
<p>Does that really mean it's potentially scanning the whole node table? What can I do to make it search with primary key?</p>
<p>Here is the query plan output with an annotation:</p>
<pre><code>QUERY PLAN
|--CO-ROUTINE chain
|  |--SETUP
|  |  `--SCAN node  &lt;———— Why is this SCAN?
|  `--RECURSIVE STEP
|     |--SCAN chain
|     |--SEARCH link USING PRIMARY KEY (id_src=?)
|     `--SEARCH node USING INTEGER PRIMARY KEY (rowid=?)
`--SCAN chain
</code></pre>
<p>Here is the SQL code:</p>
<pre><code>CREATE TABLE node
(
    id INTEGER NOT NULL PRIMARY KEY,
    content TEXT
);

CREATE TABLE link
(
    id_src INTEGER NOT NULL,
    id_dst INTEGER NOT NULL,
    PRIMARY KEY (id_src, id_dst),
    FOREIGN KEY (id_src) REFERENCES node (id),
    FOREIGN KEY (id_dst) REFERENCES node (id)
) WITHOUT ROWID;

INSERT INTO node VALUES (1, 'a');
INSERT INTO node VALUES (2, 'b');
INSERT INTO node VALUES (3, 'c');
INSERT INTO node VALUES (4, 'd');
INSERT INTO node VALUES (5, 'e');
INSERT INTO node VALUES (6, 'f');
INSERT INTO node VALUES (7, 'g');
INSERT INTO node VALUES (8, 'h');

INSERT INTO link VALUES (1, 3), (3, 5), (5, 7);
INSERT INTO link VALUES (2, 4), (4, 6), (6, 8);

EXPLAIN QUERY PLAN
WITH RECURSIVE
chain AS
(
    SELECT id AS id_first, id, content
    FROM node

    UNION ALL

    SELECT chain.id_first, node.id, node.content
    FROM chain
    INNER JOIN link ON link.id_src = chain.id
    INNER JOIN node ON node.id = link.id_dst
)
SELECT * FROM chain WHERE id_first = 1;
</code></pre>
",1,1,0,2025-12-21T15:29:09+00:00,1,106,True
79852398,735307,"Yerevan, Armenia",sql,How do I version translations while keeping relationships?,"<p>Some part of a post has translations:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE &quot;posts&quot; (
    &quot;id&quot; int4 NOT NULL,
    &quot;title&quot; varchar,
    PRIMARY KEY (&quot;id&quot;)
);

CREATE TABLE &quot;post_translations&quot; (
    &quot;post_id&quot; int4 NOT NULL,
    &quot;language_code&quot; char(2) NOT NULL,
    &quot;content&quot; text DEFAULT NULL,
    CONSTRAINT &quot;post_translations_post_id_fkey&quot; FOREIGN KEY (&quot;post_id&quot;) REFERENCES &quot;posts&quot;(&quot;id&quot;),
    PRIMARY KEY (&quot;post_id&quot;,&quot;language_code&quot;)
);
</code></pre>
<p>Posts with translations (<a href=""https://sqlfiddle.com/postgresql/online-compiler?id=4474db98-5528-4c19-b08e-9257f4ca8855"" rel=""nofollow noreferrer"">SQL Fiddle</a>):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM &quot;posts&quot;
INNER JOIN &quot;post_translations&quot; ON &quot;post_translations&quot;.&quot;post_id&quot; = &quot;posts&quot;.&quot;id&quot;
</code></pre>
<p>I need to also keep a draft version of a post with draft translations. The user can delete the post or a specific translation. They will either commit the draft version and make it live (committing the post with its translations) or discard the draft. Each post can have at most one draft.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE &quot;posts&quot; (
    &quot;id&quot; int4 NOT NULL,
    &quot;title&quot; varchar,
    &quot;draft_id&quot; int4 DEFAULT NULL,
    &quot;deleted_at&quot; TIMESTAMP WITHOUT TIME ZONE DEFAULT NULL,
    CONSTRAINT &quot;draft_id_unique&quot; UNIQUE (&quot;draft_id&quot;),
    CONSTRAINT &quot;post_draft_id&quot; FOREIGN KEY (&quot;draft_id&quot;) REFERENCES &quot;posts&quot;(&quot;id&quot;),
    PRIMARY KEY (&quot;id&quot;)
);
</code></pre>
<p><code>draft_id</code> references the live version while indicating this row is a draft. When editing a post I insert a draft version if it's not already draft; otherwise I update the draft. On post removal I create a new draft, indicating it is deleted. Different parts of the application are either working with only the live version of the posts or the most recent version (either live or draft).</p>
<p>To query the most recent version of posts (<a href=""https://sqlfiddle.com/postgresql/online-compiler?id=86a60365-7190-42ae-bdce-526029a98c28"" rel=""nofollow noreferrer"">SQL Fiddle</a>):</p>
<pre class=""lang-sql prettyprint-override""><code>WITH
  &quot;drafts&quot; (&quot;id&quot;, &quot;title&quot;, &quot;draft_id&quot;) AS (
    SELECT
      *
    FROM
      &quot;posts&quot;
    WHERE
      &quot;draft_id&quot; IS NOT NULL
  )
SELECT
  *
FROM
  &quot;drafts&quot;
UNION ALL
SELECT *
FROM &quot;posts&quot;
WHERE &quot;draft_id&quot; IS NULL AND &quot;id&quot; NOT IN (SELECT &quot;draft_id&quot; FROM &quot;drafts&quot;)
</code></pre>
<p>How do I version <code>post_translations</code> while keeping the relationships and ability to query posts with all translations (either recent or live)?</p>
",1,1,0,2025-12-21T18:39:15+00:00,1,65,False
79853487,32090566,,sql,Crystal Reports summing within a sub group,"<p>In Crystal Reports XI, I'm trying to sum within a sub group, I've grouped by two fields.</p>
<p>Is normally sum by doing something like:</p>
<pre><code>Sum({@Formula_Field},{Group_1_field}) 
</code></pre>
<p>But I've realised this returns all of the formula field within the first grouping. I was wondering if there was a way to sum only then values within the subgroup of group 2 within each of the group 1 groups. I need to have a calculated value that is it's own field for each subgroup which is the same for each line of the subgroup once calculated.</p>
<pre><code>    Column_1  Column_2  Column_3 
Grouping_1
    Grouping_2
Row_1 10        20       30
Row_2 10        10       10
    Grouping_2
Row_3 5         5        5
Grouping_1
    Grouping_2
Row_4 8         8        9
Grouping_1
    Grouping_2
Row_5 6        7         6
</code></pre>
<p>Using my formula above for the first grouping it return depending on which column was your formula field:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Row</th>
<th>Column_1</th>
<th>Column_2</th>
<th>Column_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>25</td>
<td>35</td>
<td>45</td>
</tr>
<tr>
<td>2</td>
<td>25</td>
<td>35</td>
<td>45</td>
</tr>
<tr>
<td>3</td>
<td>25</td>
<td>35</td>
<td>45</td>
</tr>
<tr>
<td>4</td>
<td>8</td>
<td>8</td>
<td>9</td>
</tr>
<tr>
<td>5</td>
<td>6</td>
<td>7</td>
<td>6</td>
</tr>
</tbody>
</table></div>
<p>Ideally I want to know how to sum only the rows in the second grouping something like:</p>
<pre><code>Sum({@Formula_Field},({Group_1_field}&amp;{Group_2_field})) 
</code></pre>
<p>Although I know that code is nonsense just to hopefully illustrate what I'm trying to achieve, the outcome when it works would give:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Row</th>
<th>Column_1</th>
<th>Column_2</th>
<th>Column_3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>20</td>
<td>30</td>
<td>40</td>
</tr>
<tr>
<td>2</td>
<td>20</td>
<td>30</td>
<td>40</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>4</td>
<td>8</td>
<td>8</td>
<td>9</td>
</tr>
<tr>
<td>5</td>
<td>6</td>
<td>7</td>
<td>6</td>
</tr>
</tbody>
</table></div>
<p>The difference being between row 1/2 &amp; 3 where there is a different subgroup between the rows meaning the summing happens across only the first two rows and then separately for row 3 the new sub group.</p>
<p>Thank you for any advice!</p>
",2,2,0,2025-12-23T08:58:27+00:00,1,40,True
79853766,8772319,,sql,How do you translate (+) joins when they appear across multiple tables?,"<p>I have the following query in a legacy Oracle database. (I've simplified it a bit; the real query has a lot of columns that are common across all joins which I've collapsed to one <code>COL_1</code>). I'm trying to translate it into a &quot;standard&quot; query in a new PostgreSQL database.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    *
FROM
    A,
    B,
    C,
    D
WHERE
    A.COL_1 = B.COL_1
    AND C.COL_1 = B.COL_1
    AND C.COL_2 = B.COL_2
    AND A.COL_1 = D.COL_1(+)  
    AND C.COL_1 = D.COL_1(+)  
    AND C.COL_2 = D.COL_2(+)  
    AND B.COL_1 = D.COL_1(+)  
    AND B.COL_2 = D.COL_2(+)
</code></pre>
<p>This is what I have:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    *
FROM
    A
    JOIN B ON
        A.COL_1 = B.COL_1
    JOIN C ON
        C.COL_1 = B.COL_1
        AND C.COL_2 = B.COL_2
    LEFT JOIN D ON
        A.COL_1 = D.COL_1
        AND C.COL_1 = D.COL_1
        AND C.COL_2 = D.COL_2
        AND B.COL_1 = D.COL_1
        AND B.COL_2 = D.COL_2
</code></pre>
<p>What I'm not sure about is whether I'm handling the <code>LEFT JOIN</code> right (the two queries are, as far as I can tell, returning the same data, but that could be dumb luck). What is the correct interpretation of the <code>(+)</code> syntax when it is matched across multiple tables?</p>
",2,2,0,2025-12-23T15:06:47+00:00,3,116,True
79853954,32091931,,sql,SQLite fails to find newly inserted row,"<p>I'm using SQLite to track Fabrics and create projects that use those Fabrics. Two tables point to each other so Fabrics knows what projects they belong to and projects know what Fabrics are a part of it. While adding a new project I want to cycle through the Fabrics that are added and append the new project ID. However, when I attempt to get the ID from the newly created project, SQLite can't find a column with the name.</p>
<p>I use the following button press for creating a new project where the <code>Name</code> column in the <code>Project</code> table needs to be unique:</p>
<pre><code>projectButton.setOnClickListener {
    if(!projectName.text.isEmpty()){
        val fabricToProject = buildChecked(checkedList).toTypedArray()
        val result: Long = fabricDB.projectCreate(fabricToProject,projectName.text.toString())
        val projectID = fabricDB.getProjectID(projectName.text.toString())
        for(x in fabricToProject){
            fabricDB.fabricUpdate(x,projectID)
        }

        if(result.toInt() == -1){
            Toast.makeText(applicationContext,&quot;Project create failed, is your project name unique?&quot;,Toast.LENGTH_LONG).show()
        }else{
            Toast.makeText(applicationContext,&quot;Project created!&quot;,Toast.LENGTH_SHORT).show()
        }
    }
    else{
        Toast.makeText(applicationContext,&quot;Please enter a project name.&quot;,Toast.LENGTH_LONG).show()
    }
}
</code></pre>
<p><code>projectCreate()</code> works; when I load my activity that loads them into views, I can see all projects:</p>
<pre><code>fun projectCreate(fabricIDs: Array&lt;Int&gt;, name: String): Long {
    val dbW: SQLiteDatabase=this.writableDatabase
    val dbR: SQLiteDatabase=this.readableDatabase
    var idString = &quot;&quot;
    val contentValuesProject = ContentValues()
    contentValuesProject.put(Pcol4,name)
    for(x in fabricIDs){
        idString += &quot;$x,&quot;
    }
    idString = idString.dropLast(1)
    contentValuesProject.put(Pcol2,idString)
    val result: Long = dbW.insert(PtableName,null,contentValuesProject)
    return result
}
</code></pre>
<p>I attempted to pull the project's ID from within <code>projectCreate()</code> and add it to the <code>Fabric</code> table's rows, but when that could not find the column I created <code>getProjectID()</code> to run a <code>SELECT</code> query:</p>
<pre><code>fun getProjectID(name:String): Int{
    val dbR: SQLiteDatabase=this.readableDatabase
    val projectID = DatabaseUtils.stringForQuery(dbR,&quot;SELECT ID FROM Project WHERE Name is $name&quot;,null)
    return projectID.toInt()
}
</code></pre>
<p>If I add Project1 it crashes the activity:</p>
<blockquote>
<p>(1) no such column: Project1 in &quot;SELECT ID FROM Project WHERE Name is Project1&quot;</p>
</blockquote>
<p>If I then open my project list activity (which loads from the same table) I can see Project1. The difference is my project list activity uses a cursor since I need to loop through the table, whereas <code>getProjectID()</code> uses <code>readableDatabase()</code> since I only pull a single value.</p>
",0,1,1,2025-12-23T20:03:13+00:00,1,78,True
79854081,31647154,,sql,Filtering dataset by geolocation data using SQL in BigQuery,"<p>I trying to determine if NCAA basketball games have an affect on liquor sales in the surrounding areas around the stadiums where the teams play their games. I am writing a query that filters and sorts the Iowa Liquor Sales dataset to include on the areas around the schools. Full query at the end of the post.</p>
<p>In the Iowa Liquor sales dataset it contains a column for the geography location which is formatted like <code>POINT(long,lat)</code>. Here is my <code>SELECT</code> statement for the geo data, I have seen different types of commands for what to use and I chose this one as it works before I add the where clause.</p>
<pre><code>ST_ASTEXT(store_location) AS store_location,
</code></pre>
<p>I have researched how to use the geo data and I have come up with a filter for the <code>WHERE</code> clause, the purpose of this filter is have only the stores that are within 5 miles of the sports arena.</p>
<p>This portion of the <code>WHERE</code> clause is after a date filter.</p>
<pre><code>  AND store_location IS NOT NULL
  AND (ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-93.634821,42.021009), 8050)
    OR ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-91.554604,41.663778), 8050)
    OR ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-93.652720,41.604430), 8050)
    OR ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-92.467260,42.514382), 8050))
</code></pre>
<p>I am receiving an error.</p>
<p>This error appears only after I put in the geolocation filter in the WHERE clause.</p>
<blockquote>
<p>++Query error: No matching signature for function ST_GEOGFROMTEXT
Argument types: GEOGRAPHY
Signature: ST_GEOGFROMTEXT(STRING, [BOOL])
Argument 1: Unable to coerce type GEOGRAPHY to expected type STRING
Signature: ST_GEOGFROMTEXT(STRING, [oriented =&gt; BOOL], [planar =&gt; BOOL], [make_valid =&gt; BOOL])
Argument 1: Unable to coerce type GEOGRAPHY to expected type STRING at [24:19] at [5:1]++</p>
</blockquote>
<p>Attempted fixes: Tried different variations in the select clause on the <code>store_location</code> using different commands. Tried using different commands in the <code>WHERE</code> clause.</p>
<p>My question is how would I fix this and what the changes do and why I do it that way. For instance should I use a CTE, or try something else in the <code>WHERE</code> clause?</p>
<pre><code>SELECT
  ST_ASTEXT(store_location) AS store_location
FROM `bigquery-public-data.iowa_liquor_sales.sales`
WHERE
store_location IS NOT NULL
  AND (ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-93.634821,42.021009), 8050)
    OR ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-91.554604,41.663778), 8050)
    OR ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-93.652720,41.604430), 8050)
    OR ST_DWITHIN(ST_GEOGFROMTEXT(store_location), ST_GEOGPOINT(-92.467260,42.514382), 8050))
</code></pre>
",3,3,0,2025-12-24T02:10:07+00:00,1,62,True
79854149,1136807,,sql,Using REGEXP instead of AND condition,"<p>I am writing a query where the user can enter a <code>comma(,)</code> separated value which would be used as an <code>AND</code> condition on a single column - lets say <code>address</code> - for refined search.</p>
<p>For example:</p>
<pre class=""lang-sql prettyprint-override""><code>/**
 * Search: Gurugram, Haryana, India
 */

SELECT * FROM users
WHERE (
    address LIKE '%Gurugram%'
    AND address LIKE '%Haryana%'
    AND address LIKE '%India%'
)
</code></pre>
<p>Although, the query is producing the desired result as required, but I was wondering if it could be used with <code>REGEXP</code> instead of adding multiple <code>AND</code> conditions, something like:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM users
WHERE address REGEXP 'Gurugram,Haryana,India'
-- OR -- 
WHERE address REGEXP 'Gurugram+Haryana+India'
-- OR --
</code></pre>
<p>Is it possible this way?</p>
",3,3,0,2025-12-24T05:49:42+00:00,1,89,True
79854159,12108591,"Bengaluru, Karnataka, India",sql,Prisma is giving error while trying to migrate it or push to NeonDB,"<blockquote>
<p>The datasource property <code>url</code> is no longer supported in schema files. Move connection URLs for Migrate to <code>prisma.config.ts</code> and pass either <code>adapter</code> for a direct database connection or <code>accelerateUrl</code> for Accelerate to the <code>PrismaClient</code> constructor. See <a href=""https://pris.ly/d/config-datasource"" rel=""nofollow noreferrer"">https://pris.ly/d/config-datasource</a> and <a href=""https://pris.ly/d/prisma7-client-configPrisma"" rel=""nofollow noreferrer"">https://pris.ly/d/prisma7-client-configPrisma</a></p>
</blockquote>
<p>My prisma.schema file:</p>
<pre><code>// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

// Looking for ways to speed up your queries, or scale easily with your serverless or edge functions?
// Try Prisma Accelerate: https://pris.ly/cli/accelerate-init

generator client {
  provider = &quot;prisma-client&quot;
  output   = &quot;../app/generated/prisma&quot;
}

datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}

model User {
  id    Int     @id @default(autoincrement())
  email String  @unique
  name  String?
  posts Post[]
}

model Post {
  id        Int     @id @default(autoincrement())
  title     String
  content   String?
  published Boolean @default(false)
  authorId  Int
  author    User    @relation(fields: [authorId], references: [id])
}
</code></pre>
<p>And prisma.config.ts file</p>
<pre><code>
// This file was generated by Prisma and assumes you have installed the following:
// npm install --save-dev prisma dotenv
import &quot;dotenv/config&quot;;
import { defineConfig, env } from &quot;prisma/config&quot;;

export default defineConfig({
  schema: &quot;prisma/schema.prisma&quot;,
  migrations: {
    path: &quot;prisma/migrations&quot;,
  },
  engine: &quot;classic&quot;,
  datasource: {
    url: env(&quot;DATABASE_URL&quot;),
  },
});
</code></pre>
",0,0,0,2025-12-24T06:13:29+00:00,1,58,False
79854842,32001271,,sql,How to optimize SELECT * queries on wide tables?,"<p>I'm a MySQL DBA moving to PolarDB IMCI. I know we can force columnstore queries like:</p>
<pre><code>SELECT /*+ SET_VAR(cost_threshold_for_imci=0) */ COUNT(*) FROM t1 WHERE a &gt; 1;
</code></pre>
<p>or disable them:</p>
<pre><code>SELECT /*+ SET_VAR(USE_IMCI_ENGINE=OFF) */ COUNT(*) FROM t1 WHERE a &gt; 1;
</code></pre>
<p>But when I run queries like this on wide tables (500+ columns):</p>
<pre><code>SELECT * FROM wide_table
WHERE analytic_column &gt; 1000
ORDER BY timestamp_col
LIMIT 1000;
</code></pre>
<p>The columnstore filtering is fast, but fetching all columns causes massive read amplification. How to optimize such SELECT * queries on wide tables in PolarDB?</p>
",0,1,1,2025-12-25T12:11:40+00:00,1,60,True
79854966,1873075,,sql,Best database book for 10-thousand-foot flyer?,"<p>I'm grokking data analytics as part of the Google DA course, and I realize I'm going to need a bit more about databases than the course teaches.</p>
<p>Any ideas for a great introductory database book?  I'm looking for ways for setting up online DB instances [cheaply!] without bogging my laptop down with myriad RDBMS programs.  I'd also like to practice my SQL kungfu without all the usual storage baggage.  Thanks!</p>
",0,0,0,2025-12-25T17:20:54+00:00,3,67,True
79855026,30636598,,sql,How to recursively inject WHERE clauses into deep subqueries and nested expressions?,"<p>I am building a SQL interceptor in Java to implement multi-tenant isolation. My goal is to parse incoming SQL statements using JSqlParser and inject a mandatory predicate <code>tenant_id = '123'</code> into every table access. Assume every table has a <code>tenant_id</code> column.</p>
<p>I have a working prototype that handles simple <code>SELECT</code>, <code>UPDATE</code>, and <code>DELETE</code> statements. However, my current approach involves manually checking specific expression types such as <code>InExpression</code>, <code>ExistsExpression</code>, <code>SubSelect</code>, and others.</p>
<p><strong>Current Approach (Manual)</strong></p>
<p>I am currently manually unwrapping expressions. This works for top-level queries but gets messy quickly:</p>
<pre class=""lang-java prettyprint-override""><code>public void modifyPlainSelect(PlainSelect plainSelect, String simId) {
    // 1. Add to main table
    if (plainSelect.getFromItem() instanceof Table) {
        addWhereCondition(plainSelect, simId);
    }
    
    // 2. Manually check WHERE clause for subqueries
    Expression where = plainSelect.getWhere();
    if (where instanceof InExpression inExpr) {
        // Manually handle the right side
        if (inExpr.getRightItemsList() instanceof SubSelect) {
             modifySelect(((SubSelect) inExpr.getRightItemsList()).getSelectBody());
        }
    } 
    // I have to add manual checks for Exists, Between, BinaryExpression, etc...
}
</code></pre>
<p><strong>The Problem</strong></p>
<p>This fails to cover complex scenarios automatically. For example:</p>
<p>Input:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM orders o 
WHERE o.customer_id IN (
    SELECT c.id FROM customers c 
    WHERE c.status = (SELECT s.code FROM status s WHERE s.active = 1)
)
</code></pre>
<p>Desired Output:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM orders o 
WHERE o.simulation_id = '123' -- Injected
AND o.customer_id IN (
    SELECT c.id FROM customers c 
    WHERE c.simulation_id = '123' -- Injected (Recursion Level 1)
    AND c.status = (
        SELECT s.code FROM status s 
        WHERE s.simulation_id = '123' -- Injected (Recursion Level 2)
        AND s.active = 1
    )
)
</code></pre>
<p><strong>My Question</strong></p>
<p>Is there a standard pattern in JSqlParser, such as <code>StatementVisitor</code>, <code>TablesNamesFinder</code>, or <code>ExpressionDeParser</code>, that allows me to do the following:</p>
<ol>
<li><p>Visit <em>every</em> node in the AST automatically, with recursion handled by the library.</p>
</li>
<li><p>Identify whenever a <code>Table</code> is accessed, whether in <code>FROM</code>, <code>JOIN</code>, <code>UPDATE</code>, or <code>DELETE</code>.</p>
</li>
<li><p>Inject the <code>WHERE</code> clause into the parent <code>SELECT</code> or <code>UPDATE</code> object.</p>
</li>
</ol>
<p>I want to avoid writing manual traversal logic for every possible expression type to ensure there are no blind spots.</p>
",2,2,0,2025-12-25T20:36:11+00:00,1,121,False
79681731,30907777,,postgresql,Cannot Change the Port of the postgresql server in Mac OS,"<p>I'm using Mac OS 13.6.1 and psql (PostgreSQL) 15.12.</p>
<p>I am trying to change port of the Postgres server.</p>
<p><strong>What I've done</strong></p>
<p>I found the config_file using psql shell like this</p>
<pre><code># show config_file;
                   config_file                   
-------------------------------------------------
 /opt/homebrew/var/postgresql@15/postgresql.conf
(1 row)
</code></pre>
<p>and then I edited the port and listen_addresses like this</p>
<pre><code>listen_addresses = '*'      # what IP address(es) to listen on;
                    # comma-separated list of addresses;
                    # defaults to 'localhost'; use '*' for all
                    # (change requires restart)
port = 5433             # (change requires restart)
</code></pre>
<p>after that I did <code>sudo brew services postgresql restart</code>
but when I check it was still running in port 5432 instead of 5433.
Why does this happen and how do I fix this?</p>
<p>P.S. I also tried setting listen_addresses to localhost and it still didn't work</p>
",2,2,0,2025-06-27T09:41:14+00:00,2,136,True
79681749,11356098,,postgresql,sqlalchemy insert PRAGMA statement while dealing with postgresql database,"<p>I'm trying to migrate a small table on local SQLite database to a remote postgresql server, but I'm getting error message &quot;(psycopg2.errors.SyntaxError) syntax error at or near &quot;PRAGMA&quot; LINE 1: PRAGMA foreign_keys=ON&quot;. This is my code:</p>
<pre class=""lang-py prettyprint-override""><code>from sqlalchemy import create_engine, select, insert
import sqlalchemy.orm as so
import psycopg2
import pandas as pd
from config import Config
import os
from dotenv import load_dotenv
from app.models import Grupo

# Creating engines
load_dotenv()
sqlite_engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)
postgres_url = f&quot;postgresql+psycopg2://{os.getenv('PGUSER')}:{os.getenv('PGPASSWORD')}@{os.getenv('PGHOST')}:{os.getenv('PGPORT')}/{os.getenv('PGDATABASE')}&quot;
postgres_engine = create_engine(postgres_url)

# Reading table from SQLite
stmt = select(Grupo)
df_Grupo = pd.read_sql(sql=stmt, con=sqlite_engine)
# a small, non important change
df_Grupo['DT_COMPETENCIA'] = df_Grupo['DT_COMPETENCIA'].str[:4] + '-' + df_Grupo['DT_COMPETENCIA'].str[4:]

# Writing table on postgresql
with so.Session(postgres_engine) as pg_session:
    try:
        pg_session.execute(insert(Grupo), df_Grupo.to_dict(orient='records'))
        pg_session.commit()
    except Exception as e:
        print(f&quot;Erro na migração da tabela Grupo: \n{e}&quot;)
</code></pre>
<p>This is the output:</p>
<pre class=""lang-none prettyprint-override""><code>Erro na migração da tabela Grupo: 
(psycopg2.errors.SyntaxError) syntax error at or near &quot;PRAGMA&quot;
LINE 1: PRAGMA foreign_keys=ON
        ^

(Background on this error at: https://sqlalche.me/e/20/f405)
</code></pre>
<p>An this is the complete table:</p>
<pre class=""lang-none prettyprint-override""><code>
CO_GRUPO    NO_GRUPO    DT_COMPETENCIA
0   01  Ações de promoção e prevenção em saúde  2024-06
1   02  Procedimentos com finalidade diagnóstica    2024-06
2   03  Procedimentos clínicos  2024-06
3   04  Procedimentos cirúrgicos    2024-06
4   05  Transplantes de orgãos, tecidos e células   2024-06
5   06  Medicamentos    2024-06
6   07  Órteses, próteses e materiais especiais 2024-06
7   08  Ações complementares da atenção à saúde 2024-06
8   09  Procedimentos para Ofertas de Cuidados Integrados   2024-06
</code></pre>
<p>This is the error traceback</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 146, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 3300, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 712, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py&quot;, line 179, in _do_get
    with util.safe_reraise():
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py&quot;, line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py&quot;, line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 674, in __init__
    self.__connect()
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 914, in __connect
    )._exec_w_sync_on_first_run(self.dbapi_connection, self)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/event/attr.py&quot;, line 483, in _exec_w_sync_on_first_run
    self(*args, **kw)
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/event/attr.py&quot;, line 495, in __call__
    fn(*args, **kw)
  File &quot;/home/vado/python-projects/SUSano/src/app/__init__.py&quot;, line 20, in set_sqlite_pragma
    cursor.execute(&quot;PRAGMA foreign_keys=ON&quot;)
psycopg2.errors.SyntaxError: syntax error at or near &quot;PRAGMA&quot;
LINE 1: PRAGMA foreign_keys=ON
        ^




The above exception was the direct cause of the following exception:


Traceback (most recent call last):
  File &quot;/tmp/ipykernel_189447/3532388728.py&quot;, line 3, in &lt;module&gt;
    pg_session.execute(insert(Grupo), df_Grupo.to_dict(orient='records'))
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py&quot;, line 2351, in execute
    return self._execute_internal(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py&quot;, line 2226, in _execute_internal
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py&quot;, line 2095, in _connection_for_bind
    return trans._connection_for_bind(engine, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;&lt;string&gt;&quot;, line 2, in _connection_for_bind
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/orm/state_changes.py&quot;, line 139, in _go
    ret_value = fn(self, *arg, **kw)
                ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py&quot;, line 1189, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 3276, in connect
    return self._connection_cls(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 148, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 2440, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 146, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py&quot;, line 3300, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 712, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py&quot;, line 179, in _do_get
    with util.safe_reraise():
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py&quot;, line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/impl.py&quot;, line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 674, in __init__
    self.__connect()
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/pool/base.py&quot;, line 914, in __connect
    )._exec_w_sync_on_first_run(self.dbapi_connection, self)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/event/attr.py&quot;, line 483, in _exec_w_sync_on_first_run
    self(*args, **kw)
  File &quot;/home/vado/python-projects/SUSano/.venv/lib/python3.12/site-packages/sqlalchemy/event/attr.py&quot;, line 495, in __call__
    fn(*args, **kw)
  File &quot;/home/vado/python-projects/SUSano/src/app/__init__.py&quot;, line 20, in set_sqlite_pragma
    cursor.execute(&quot;PRAGMA foreign_keys=ON&quot;)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near &quot;PRAGMA&quot;
LINE 1: PRAGMA foreign_keys=ON
        ^


(Background on this error at: https://sqlalche.me/e/20/f405)
</code></pre>
",1,1,0,2025-06-27T10:01:51+00:00,1,54,True
79682047,30547358,,postgresql,Why does psycopg2 still allow SQL injection with dynamically constructed table names,"<p>I'm developing a multi-tenant Python web application where users need to query different database tables based on their client context. The challenge is constructing secure SQL queries when table names must be determined dynamically at runtime. The core problem is that while psycopg2's parameterized queries effectively prevent SQL injection for data values, they cannot be used for table or column names. When I attempt to parameterize a table name using standard placeholders, PostgreSQL throws a syntax error because identifiers (table/column names) cannot be parameterized in the same way as values. This leaves me in a situation where I must construct SQL strings dynamically, which traditionally opens the door to SQL injection attacks. For example, if an attacker can control the table name input, they could potentially inject malicious SQL code that gets executed alongside the intended query.</p>
<p>I've discovered psycopg2's <code>sql.Identifier()</code> function which claims to safely handle dynamic identifiers, but I need to understand its limitations and whether additional validation layers are necessary. The application handles sensitive financial data across multiple clients, so security is paramount, but I also need to maintain good performance and code maintainability.</p>
<pre><code>import psycopg2

def get_user_data(connection, table_name, user_id):
    query = f&quot;SELECT * FROM {table_name} WHERE user_id = %s&quot;
    cursor = connection.cursor()
    cursor.execute(query, (user_id,))
    return cursor.fetchall()

#See an example of the security risk:
malicious_table = &quot;users; DROP TABLE users; --&quot;
get_user_data(conn, malicious_table, 123)  #This would execute the DROP command 
</code></pre>
<pre class=""lang-py prettyprint-override""><code>I have tried doing the direct parameterization:

cursor.execute(&quot;SELECT * FROM %s WHERE user_id = %s&quot;, (table_name, user_id))
</code></pre>
<p>I expected the query to execute safely with the table name properly escaped.</p>
<p>But then the actual result: <br />
<code>psycopg2.errors.SyntaxError: syntax error at or near &quot;$1&quot;</code></p>
<p>PostgreSQL doesn't allow parameterized identifiers.<br />
I have tried F-string formatting, also tried using sql.Identifier as seen below:</p>
<pre><code>from psycopg2 import sql  
query = sql.SQL(&quot;SELECT * FROM {} WHERE user_id = %s&quot;).format(sql.Identifier(table_name)) 
cursor.execute(query, (user_id,))
</code></pre>
<p>This works, but I am unsure about edge cases</p>
<p>The the whitelist + sql.Identifier() is seen below</p>
<pre><code>ALLOWED_TABLES = {'client_1_transactions', 'client_2_transactions'}
if table_name not in ALLOWED_TABLES:
    raise ValueError(f&quot;Invalid table: {table_name}&quot;)
</code></pre>
<p>I am thinking the whitelist + sql.Identifier() is an overkill,  not sure though.<br />
I would love to know the following:</p>
<ul>
<li><p>Is <code>psycopg2.sql.Identifier()</code> truly safe against all forms of SQL injection for table/column names, or are there edge cases I should be aware of?</p>
</li>
<li><p>What's the recommended approach for validating table names before using them in dynamic queries? Should I maintain a whitelist, or is there a more elegant solution?</p>
</li>
<li><p>Are there any performance implications when using <code>sql.SQL()</code> and <code>sql.Identifier()</code> compared to regular parameterized queries?</p>
</li>
</ul>
",-2,1,3,2025-06-27T13:39:45+00:00,1,112,True
79682321,12190005,,postgresql,PostgreSQL `null` value in JSON column not filtered by `IS NOT NULL` as expected,"<p>(PostgreSQL version: 15.10)</p>
<p>I am trying to query a table with a nullable json-type column. Here is the simplified table description:</p>
<pre><code>db=&gt; \d tablename
                          Table &quot;public.tablename&quot;
   Column           |            Type             | Collation | Nullable |   
--------------------+-----------------------------+-----------+----------
 id                 | uuid                        |           | not null |
 group_id           | uuid                        |           |          |
 field_id           | uuid                        |           |          |
 mapping_key        | character varying           |           |          |
 sub_field_mappings | json                        |           |          |
 option_mappings    | json                        |           |          |
 time_created       | timestamp without time zone |           |          |
 time_updated       | timestamp without time zone |           |          |
</code></pre>
<p>Here is the query I was trying to run:</p>
<pre><code>db=&gt;
SELECT id, option_mappings from tablename
where option_mappings IS NOT NULL limit 10;

    id    |     option_mappings
----------+-------------------------
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | null
 &lt;UUID&gt; | [Actual JSON data here]
</code></pre>
<p>I'm confused: why am I getting rows with <code>null</code> values when I specifically queried with <code>IS NOT NULL</code>? The same behavior happened with the <code>sub_field_mappings</code> column. As a sanity check, I also did this:</p>
<pre><code>db=&gt; SELECT id, option_mappings from tablename where option_mappings IS NULL limit 10;
</code></pre>
<p>and it returned nothing. Okay, so the <code>null</code> I am seeing is not a <code>NULL</code> value?</p>
<p>I ran this:</p>
<pre><code>db=&gt; SELECT json_typeof(option_mappings) from tablename where option_mappings IS NOT NULL limit 10;
 json_typeof 
-------------
 null
 null
 null
 null
 null
 null
 null
 null
 null
 array
(10 rows)
</code></pre>
<p>At this point, I was losing my mind. HOW CAN A VALUE WITH TYPE <code>'null'</code> NOT BE <code>NULL</code>?</p>
<h2>Update</h2>
<p>I still have no idea why my query was not filtering rows with <code>null</code> values as expected, but this query did what I wanted:</p>
<pre><code>db=&gt; SELECT id, option_mappings from tablename where json_typeof(option_mappings) != 'null' limit 10;
</code></pre>
<p>I still have no idea why a <code>null</code> value in a JSON field did not equate to <code>NULL</code> in a <code>WHERE</code> clause. If anyone has any insights here, that would be appreciated!</p>
",0,0,0,2025-06-27T17:48:06+00:00,1,80,True
79682355,5560898,"Salem, MA, USA",postgresql,PostgreSQL &quot;_uuid&quot; data type,"<p>I am looking at some table DDL and I noticed we have two different data types.</p>
<ul>
<li>uuid</li>
<li>_uuid (yes, it's literally &quot;_uuid&quot;)</li>
</ul>
<p>I have never seen this before, what is the difference between them?</p>
",4,5,1,2025-06-27T18:19:43+00:00,1,142,True
79682530,30912515,,postgresql,ADO.NET failure to acquire connection in an SQL Server to PostgreSQL ETL (DTS),"<p>I'm creating in VisualStudio for Application 2022 an ETL that is migrating data from a MS-SQL Server table to another in PostgreSQL DataBase. I create the ADO.NET Destination connection (server IP, user, pass) to PostgreSQL using &quot;Npgsql PostgreSQL Integration Extension&quot; is &quot;success&quot;. But on execution time I run a task that is a single &quot;Truncate table&quot; on the Postgre DB, I get the Output error:</p>
<ul>
<li>Error: ADO.NET Destination has failed to acquire the connection...with the following message: 'Could not create a managed connection manager.'</li>
<li>Error: SSIS Pipeline: ADO.NET Destination failed validation and returned error code 0xC0208452.</li>
<li>Error: SSIS Pipeline: One or more component failed validation.</li>
<li>Error: There were errors during task validation.</li>
</ul>
<p>I notice that in the &quot;ADO.NET Destination Editor&quot; window, it loads no PostgreSQL Tables on the Tables/View combo. Any suggestions? Thanks in advance!</p>
",0,0,0,2025-06-27T22:06:04+00:00,0,77,False
79682704,1949581,"Seattle, WA",postgresql,Copy large table (millions of records) into a duplicate table efficiently,"<p>I know this type of question has been answered with suggested uses of <code>CREATE TABLE FROM</code> or <code>INSERT INTO SELECT</code>.</p>
<p>My question is about performance for these.  If I have a table with million+ rows, and I use <code>INSERT INTO SELECT</code>, how can I evaluate performance and resource utilization.</p>
<p>My concern is if these operations live in a single transaction, won't that potentially degrade the server and starve other operations?</p>
",1,1,0,2025-06-28T04:15:37+00:00,1,195,True
79682969,22255471,,postgresql,Can&#39;t migrate prisma schema encountering P1000: Authentication failed against database server on docker postgres database,"<p>I can't migrate my prima schema to my database. it said
<a href=""https://i.sstatic.net/xFFPHKoi.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>here is my docker-compose.yml</p>
<pre><code>version: &quot;3.8&quot;
services:
  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
      POSTGRES_DB: mydb
    ports:
      - &quot;5432:5432&quot;
    volumes:
      - postgres-data:/var/lib/postgresql/data
  pgadmin:
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: root
    ports:
      - &quot;5050:80&quot;
    depends_on:
      - postgres
volumes:
  postgres-data:

</code></pre>
<p>here is my schema.prisma</p>
<pre><code>generator client {
  provider = &quot;prisma-client-js&quot;
  output   = &quot;../app/generated/prisma&quot;
}

datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}

model User {
  id         Int      @id @default(autoincrement())
  email      String   @unique
  first_name String
  last_name  String
  role       String
  birth_day  DateTime
  create_at  DateTime @default(now())
}

model User_Log {
  log_id     Int      @id @default(autoincrement())
  user_id    Int
  field_name String
  old_value  String
  new_value  String
  update_by  Int // user's id
  update_at  DateTime @default(now())
}

model Inventory {
  item_id   Int    @id @default(autoincrement())
  item_name String
  quantity  Int
  location  String
}

model Inventory_Log {
  log_id     Int      @id @default(autoincrement())
  item_id    Int
  field_name String
  old_value  String
  new_value  String
  update_by  Int // user's id
  update_at  DateTime @default(now())
}
</code></pre>
<p>here is my .env file</p>
<pre><code>DATABASE_URL=&quot;postgresql://myuser:mypassword@localhost:5432/mydb?schema=public&quot;
</code></pre>
<p>i've tried creating new connection in pgadmin and setting connection options as followed</p>
<p>Host name/address: postgres
Port: 5432
Maintenance database: postgres
Username: myuser
Password: mypassword</p>
",0,0,0,2025-06-28T11:47:24+00:00,0,43,False
79683892,855415,,postgresql,Issue over 1600 columns when create multi column in postgres using laravel,"<p>I have use <strong>laravel 11</strong> and <strong>postgres 17</strong>, when I use import file to create column for table in database (file only have ~500 rows, which means it will create about 500 new columns for the table). But I get error <code>SQLSTATE[54011]: Too many columns: 7 ERROR:  tables can have at most 1600 columns</code> even though my table originally only had 18 columns. Here my functions:</p>
<pre class=""lang-php prettyprint-override""><code>DB::beginTransaction();
try {
   ... read data from file ...
   foreach ($data as $record) {
      $sql = &quot;ALTER TABLE \&quot;$record[0]\&quot; ADD COLUMN IF NOT EXISTS \&quot;$record[1]\&quot; $record[2]($record[3]);&quot;
      \Log::debug('insert &gt;&gt;&gt; ' . $sql);
      DB::statement($sql);
      # this query below I use to check number of column in table after insert.      
      $qlCountCol = &quot;SELECT count(*) as temp FROM information_schema.columns WHERE table_schema = 'public' AND table_name = 'custom_object_bc1d0996-7911-41c0-bfda-374680f6e614'&quot;; 
      $respo = DB::select($qlCountCol);
      \Log::debug($obj-&gt;table_name . ' &gt;&gt;&gt;&gt;&gt;&gt; col &gt;&gt;&gt;&gt; ' . json_encode($respo[0]));
   }
   DB::commit();
} catch (\Exception $e) {
   DB::rollBack();
   report($e);
   throw $e;
}
</code></pre>
<p>And I get log as below:</p>
<pre><code>....
[2025-06-30 01:24:37] local.DEBUG: insert &gt;&gt;&gt; ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_e0ab6fa8-e4f7-4bee-b8ed-1d0bb4975118&quot; BIGINT  ;  
[2025-06-30 01:24:37] local.DEBUG: custom_object_bc1d0996-7911-41c0-bfda-374680f6e614 &gt;&gt;&gt;&gt;&gt;&gt; col &gt;&gt;&gt;&gt; {&quot;temp&quot;:385}  
[2025-06-30 01:24:37] local.DEBUG: insert &gt;&gt;&gt; ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_739b4b6f-4c49-43d0-8840-b4559fec8601&quot; BIGINT  ;  
[2025-06-30 01:24:37] local.DEBUG: custom_object_bc1d0996-7911-41c0-bfda-374680f6e614 &gt;&gt;&gt;&gt;&gt;&gt; col &gt;&gt;&gt;&gt; {&quot;temp&quot;:386}  
[2025-06-30 01:24:37] local.DEBUG: insert &gt;&gt;&gt; ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_1a43292d-e691-4d73-a3d6-52f65822dfce&quot; BIGINT  ;  
[2025-06-30 01:24:37] local.DEBUG: custom_object_bc1d0996-7911-41c0-bfda-374680f6e614 &gt;&gt;&gt;&gt;&gt;&gt; col &gt;&gt;&gt;&gt; {&quot;temp&quot;:387}  
[2025-06-30 01:24:37] local.DEBUG: insert &gt;&gt;&gt; ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_4cc4dcf0-216e-4260-b7ee-00e4192bbaf7&quot; BIGINT  ;  
[2025-06-30 01:24:37] local.DEBUG: custom_object_bc1d0996-7911-41c0-bfda-374680f6e614 &gt;&gt;&gt;&gt;&gt;&gt; col &gt;&gt;&gt;&gt; {&quot;temp&quot;:388}  
[2025-06-30 01:24:37] local.DEBUG: insert &gt;&gt;&gt; ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_0f4a5850-da64-4d9f-88b6-e5d676b4b3dc&quot; BIGINT  ;  
[2025-06-30 01:24:37] local.DEBUG: custom_object_bc1d0996-7911-41c0-bfda-374680f6e614 &gt;&gt;&gt;&gt;&gt;&gt; col &gt;&gt;&gt;&gt; {&quot;temp&quot;:389}  
[2025-06-30 01:24:37] local.DEBUG: insert &gt;&gt;&gt; ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_de47c89a-29c7-4180-a1da-b4d143fa8b1c&quot; BIGINT  ;
[2025-06-30 01:24:37] local.ERROR: SQLSTATE[54011]: Too many columns: 7 ERROR:  tables can have at most 1600 columns (Connection: default, SQL: ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot; ADD COLUMN IF NOT EXISTS &quot;field_de47c89a-29c7-4180-a1da-b4d143fa8b1c&quot; BIGINT  ;) {&quot;userId&quot;:1,&quot;exception&quot;:&quot;[object] (Illuminate\\Database\\QueryException(code: 54011): SQLSTATE[54011]: Too many columns: 7 ERROR:  tables can have at most 1600 columns (Connection: tenant, SQL: ALTER TABLE \&quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614\&quot; ADD COLUMN IF NOT EXISTS \&quot;field_de47c89a-29c7-4180-a1da-b4d143fa8b1c\&quot; BIGINT  ;) at /src/vendor/laravel/framework/src/Illuminate/Database/Connection.php:825)
</code></pre>
<p>I don't know why my table just have 389 columns but I getting error over 1600 columns of postgres. Please explain for me why I'm geting this error and how to resolve it. Thanks.</p>
<p><strong>PS:</strong>
I think is not error of PHP and laravel. I have try insert with psql query as below I getting same errors. (limit char post I can't paste full query but it simple as below)</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot;
    ADD COLUMN IF NOT EXISTS &quot;field_b23058d5-de8b-4b99-a405-a3500b66d33a&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_fb93ff3e-7ced-48fc-9b42-cde892151c68&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_e1d56b40-f37c-406a-8703-ab1a6d0930ff&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_cb6b05da-3882-4c85-aa49-d0bdd0b0641f&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_bc797a62-cb29-413c-bf64-bd32ac82cb4e&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_892c85e9-e740-4b8a-ac37-b98a1c2678a4&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_ac87a2dd-a679-4716-95a2-d3abf1b32886&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_dd425cb6-4ee3-4b4f-9403-37ec424e59f0&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_e365f94f-4beb-40bd-9ac8-6339391a1c11&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_2db92de6-9419-44e1-bfdc-757c6bde52da&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_12915b57-fd61-40d1-93dd-dd1994baec4e&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_f88ed3f7-04cf-4db9-8297-7bc44f1c094c&quot; JSONB,
    ADD COLUMN IF NOT EXISTS &quot;field_e9a28e0f-ede5-445f-8f07-96617ef759cf&quot; VARCHAR(255),
    ADD COLUMN IF NOT EXISTS &quot;field_3b859643-b359-4adc-b3c4-9f1ac3b2fd41&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_c74b07c2-8311-4027-83c4-b745d88c3960&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_b357fe9a-f50c-446d-ba65-71db4b7bcb20&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_38cd0f1f-a26d-4b8b-84c3-a04051c5e2e4&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_1451940a-43e8-4935-bcc2-b2a732260d21&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_145d9521-6483-4c90-aef9-42aa2c8cf1e2&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_e1806caa-67bc-44fa-95a3-bb5fd63d92fd&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_7675886b-ff57-4b92-8911-585c0783e192&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_735797bc-7644-47c7-886b-5f01ec6b1558&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_d015393b-6ae9-4af6-8b21-ad2b92c90f7b&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_14e382ac-6247-4867-a3f8-8528fd019fc9&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_705be07f-a407-4892-b367-675d1b87cc0d&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_144cf94c-bb50-456b-82d5-6805cb4c59b7&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_0ff561b2-596f-46ae-a290-d3cf7a9b256c&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_9281c4c2-91fd-49cc-9575-729dce9bf669&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_2bf72685-0515-481f-8e0b-b64d1b242c61&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_d811281c-5821-4145-806c-1bd66f573f99&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_88e3d912-fe60-41a8-b030-d3fa18a426d4&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_93d47e04-9414-4c48-adaf-ab7d366d7efa&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_d299459b-d4e3-47b6-a855-36d11634195c&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_9115abd9-89d1-4888-92ec-d9ade862077a&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_e06a4cc4-5581-4623-b2f8-714652ce574b&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_8da5a451-f91a-4465-b527-6e99aeaf5bb8&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_aa8938f2-4b55-4036-bd3a-c05d8b93a191&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_5078b490-ee17-4bce-9d01-47eba010ebe2&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_dc9ce45f-3692-42d7-afd8-2ab7af1709dd&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_3af0dcc1-e624-431e-96ba-5fae69e4762f&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_71fbee40-5b13-4ef8-b3f7-7f496d944867&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_aaf1d70d-4e7b-4735-80f5-ade6956cdbc3&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_dc820da3-539b-4c6d-a108-7061b9aef69c&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_abe56663-b39e-4bb4-89ce-b1b70503df17&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_8d7db4ba-786c-4d62-b360-ed2df1c6969d&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_deba0917-a1cf-4343-b18e-127d22aacdca&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_b1bf0bb4-e3ba-495f-a397-94e7be7b89cd&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_033e1090-0ebd-4f80-b82e-858b39296bf9&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_fdba15df-a89c-4943-aaa6-85c1111e697e&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_59a03046-4b87-4093-bceb-489739e8793f&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_faaa39f7-69c8-4dd7-b9d6-937531b08870&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_f0dd7ac8-0469-4143-8e68-60959ebee538&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_f946f78c-b7d1-4827-b34a-0bb9483a9c19&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_8126f4d7-4d4f-48f2-8823-5a4e8a80693c&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_7935ff59-63e9-4da0-ad1a-51efe6a0ba58&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_193d1cb4-c383-4d2d-848c-aa6f5f8e571e&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_4678d5b9-7e14-4366-b225-39df6b826401&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_19d2f6b3-912a-43d4-8945-8624d6cfbc90&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_e1eade5b-b2f5-4987-990a-69d6f1225c42&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_9a78d50e-6283-411a-8905-54217fcb74c1&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_e3ab0679-b426-45bf-af4c-702a1910bb1c&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_90545d3d-1d7a-4c35-a48f-286ab4aa0752&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_9212dfc2-86ac-42b5-8ab7-e84d6a8ee98e&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_b6b9c15b-68f2-400b-b993-a8a3225c18a8&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_fb73c25f-6fcb-40fb-8f5d-097ecb251611&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_9d356167-ff6f-4787-8478-f6d879ae089b&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_b6dd4ccd-a092-4133-97ed-6692a0f994f0&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_cba4e187-97af-4e7a-a13f-d8164bd977bf&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_c0738d8c-ea33-47cb-91b6-29187c95b6d4&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_77a16feb-3b35-45fc-ab2f-aeb286606d87&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_43adf597-3f90-475c-9ea0-a01b671dd07e&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_da46313b-eef8-49a6-a868-43b6df107c32&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_35e22a68-fc8f-45f2-8157-3bf9cd118a0a&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_63089c3f-9741-41c2-b729-069116a35b22&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_8157276f-08a5-41bd-a5e9-31fb2f20cff6&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_bf42f320-6ba6-470f-bc7e-81f71cf3f7ef&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_9e3b04d7-fbe0-4605-ab9c-263dce772508&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_f9a11b2a-7929-4133-a31e-3544e34c1efb&quot; BIGINT,
    ADD COLUMN IF NOT EXISTS &quot;field_c00f53d1-8aa8-4242-b55f-0bf3fc8d3b9e&quot; BIGINT, ....
</code></pre>
<p>with table have structs as belows</p>
<pre class=""lang-sql prettyprint-override""><code>create table &quot;custom_object_bc1d0996-7911-41c0-bfda-374680f6e614&quot;
(
    id                bigserial
        primary key,
    user_managed_id   uuid         default gen_random_uuid(),
    name              varchar(255),
    owned_by          bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_owned_by_for
            references users
            on delete cascade,
    layout_id         bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_field_layout
            references custom_field_layouts
            on delete cascade,
    role_id           bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_role_id_fore
            references roles
            on delete cascade,
    path_id           bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_path_id_fore
            references paths
            on delete cascade,
    corporate_number  varchar(255),
    marksetil_label   jsonb,
    timset_label      jsonb,
    created_at        timestamp(0) default CURRENT_TIMESTAMP,
    created_by        bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_created_by_f
            references users
            on delete cascade,
    updated_at        timestamp(0) default CURRENT_TIMESTAMP,
    updated_by        bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_updated_by_f
            references users
            on delete cascade,
    deleted_at        timestamp(0),
    deleted_by        bigint
        constraint custom_object_bc1d0996_7911_41c0_bfda_374680f6e614_deleted_by_f
            references users
            on delete cascade,
    system_updated_at timestamp(0),
    settings          jsonb
);
</code></pre>
<p><strong>PS2:</strong>
I tested it and it is correct according to this comment <a href=""https://stackoverflow.com/questions/29387569/table-can-have-at-most-1600-columns-in-postgres-openerp#39130447"">table can have at most 1600 columns in postgres openerp</a></p>
",0,0,0,2025-06-29T16:40:42+00:00,1,173,True
79683899,29417292,,postgresql,NestJS TypeORM Environment Variables Not Loading Despite dotenv Configuration,"<p>I'm working on a NestJS application with TypeORM and PostgreSQL. My environment variables are not being loaded when the application starts, even though I can see dotenv injecting them in the console output. This results in a &quot;password authentication failed&quot; error because the database configuration receives undefined values.
Here's what I see in the logs:</p>
<pre><code>[dotenv@17.0.0] injecting env (5) from .env – 🔐 encrypt with dotenvx: https://dotenvx.com
[Nest] 11840  - 06/29/2025, 4:18:13 PM     LOG [NestFactory] Starting Nest application...
[Nest] 11840  - 06/29/2025, 4:18:13 PM   ERROR [TypeOrmModule] Unable to connect to the database. Retrying (1)...
error: password authentication failed for user &quot;user&quot;
</code></pre>
<p>When I debug the environment variables in my app.module.ts, they all show as undefined:
Raw Environment Variables:
POSTGRES_HOST: undefined
POSTGRES_PORT: undefined
POSTGRES_USER: undefined
POSTGRES_PASSWORD: undefined
POSTGRES_DATABASE: undefined
All env keys containing POSTGRES: []
Current Configuration
My .env file (located in project root):
env</p>
<blockquote>
<p>POSTGRES_HOST=localhost POSTGRES_PORT=5433 POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres POSTGRES_DATABASE=course-management</p>
</blockquote>
<p>My app.module.ts:
typescript</p>
<pre><code>import { Module } from '@nestjs/common';
import { TypeOrmModule } from '@nestjs/typeorm';
import { AppController } from './app.controller';
import { AppService } from './app.service';
import { CourseModule } from './course/course.module';
import { ScoreModule } from './score/score.module';

// Debug logging
console.log('Environment Variables:', {
  host: process.env.POSTGRES_HOST,
  port: process.env.POSTGRES_PORT,
  user: process.env.POSTGRES_USER,
  database: process.env.POSTGRES_DATABASE,
  passwordSet: !!process.env.POSTGRES_PASSWORD
});

@Module({
  imports: [
    TypeOrmModule.forRoot({
      type: 'postgres',
      host: process.env.POSTGRES_HOST || 'localhost',
      port: parseInt(process.env.POSTGRES_PORT || '5433'),
      password: String(process.env.POSTGRES_PASSWORD || ''),
      username: process.env.POSTGRES_USER || 'postgres',
      entities: [],
      database: process.env.POSTGRES_DATABASE || 'course-management',
      synchronize: true,
      logging: true,
    }),
    CourseModule,
    ScoreModule,
  ],
  controllers: [AppController],
  providers: [AppService],
})
export class AppModule {}
</code></pre>
<p>My main.ts:
typescript</p>
<pre><code>import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
import * as dotenv from 'dotenv';

dotenv.config();

async function bootstrap() {
  const app = await NestFactory.create(AppModule);
  await app.listen(3000);
}
bootstrap();
</code></pre>
<p>What I Expected vs What Happened
Expected: The environment variables should be loaded and available when app.module.ts is evaluated, allowing TypeORM to connect to PostgreSQL with the correct credentials.
What Actually Happened:</p>
<p>dotenv shows it's injecting 5 variables from .env
But when app.module.ts tries to read them, they're all undefined
TypeORM tries to connect with default/undefined values
Connection fails with &quot;password authentication failed for user 'user'&quot; (should be 'postgres')</p>
<p>What I've Tried</p>
<p>Verified .env file location: It's in the project root directory
Checked .env file format: No spaces around equals signs
Added explicit string conversion: String(process.env.POSTGRES_PASSWORD)
Added debug logging: Confirms all variables are undefined
Installed dotenv and types: npm install dotenv @types/dotenv
Verified database credentials: Can connect manually with psql using the same credentials</p>
<p>The issue seems to be a timing problem where app.module.ts is evaluated before the environment variables are loaded, despite calling dotenv.config() in main.ts.
Questions</p>
<p>Why are the environment variables undefined in app.module.ts even though dotenv claims to inject them?
What's the correct way to ensure environment variables are available during module initialization in NestJS?
Is there a better pattern than using dotenv directly with NestJS and TypeORM?</p>
<p>Environment:</p>
<blockquote>
<p>NestJS 10.x TypeORM PostgreSQL dotenv 17.0.0 Node.js (Windows)</p>
</blockquote>
",2,2,0,2025-06-29T16:51:30+00:00,4,583,True
79683923,24888000,,postgresql,Pg SQL Output tree hierarchy as JSON,"<p>I need to return a JSON tree by departments from the database, having a table of departments. I have a table like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>parent_id</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>null</td>
<td>Отдел 1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>Отдел 1.1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>Отдел 1.1.1</td>
</tr>
<tr>
<td>3</td>
<td>null</td>
<td>Отдел 2</td>
</tr>
<tr>
<td>4</td>
<td>3</td>
<td>Отдел 2.1</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>Отдел 1.1.1.1</td>
</tr>
</tbody>
</table></div>
<p>I need to write a recursive query on this table and get the result as JSON:</p>
<pre><code>      {[
  {&quot;departmentId&quot;: 0,
   &quot;departmentName&quot;: &quot;Отдел 1&quot;,
        &quot;children&quot;: [
          {&quot;departmentId&quot;: 1,
           &quot;departmentName&quot;: &quot;Отдел 1.1&quot;,
           &quot;children&quot;: [
               {
                 &quot;departmentId&quot;: 2,
                 &quot;departmentName&quot;: &quot;Отдел 1.1.1&quot;,
                 &quot;children&quot;: [
                     {
                       &quot;departmentId&quot;: 5,
                       &quot;departmentName&quot;: &quot;Отдел 1.1.1.1&quot;,
                       &quot;children&quot;: &quot;null&quot;
                     }
                   ]
               }
             ]
          }],
          {&quot;departmentId&quot;: 3,
           &quot;departmentName&quot;: &quot;Отдел 2&quot;,
           &quot;children&quot;:[
             {&quot;departmentId&quot;: 4,
              &quot;departmentName&quot;: &quot;Отдел 2.1&quot;,
              &quot;children&quot;: &quot;null&quot;
             }
          ]
         }
  }
]}
</code></pre>
<p>After looking at recursive queries on the internet, I made the following query:</p>
<pre><code>    WITH recursive parents AS (
 
  SELECT
    id, name, parent_id
  FROM departments
  WHERE
    id = '798c66092ef24ec488ef308291947913'
 
  UNION ALL
 
  SELECT
    dep.id, dep.name, dep.parent_id
  FROM departments dep
  JOIN parents ON parents.parent_id = dep.id
)
 
SELECT
  *
FROM parents
WHERE id != '798c66092ef24ec488ef308291947913'
order by id;
</code></pre>
<p>But this query returns only the parents of the department by id. That is, it searches for all the parents of the passed department.
How can I pass it so that it returns me such a JSON?
I tried to do it in different ways, looked through different examples, but I can’t get myself such a JSON.
Could you please tell me how to write such a query?</p>
",2,2,0,2025-06-29T17:37:21+00:00,3,77,True
79684344,8762009,,postgresql,Cannot connect RDS that was previously private to EC2 or local,"<p>I know questions like this have been asked before but even if I follow the suggested advices, I cannot get a solution.</p>
<p>Previously I had a private RDS instance with postgres that was working fine with a Django web application hosted in an EC2 instance in my VPC.</p>
<p>Now I have the need to do a pg_dump from my local machine, so I thought of changing the public accesibility so I did. Since I had no succes but the website was still working I looked for answers and turns that I needed to accept inbound traffic in the security groups and in ACL. so I did and now not even in my local nor in my EC2 I can connect to the db, so the website stopped working. Any clues? I've been on this all weekend. Thanks.</p>
",0,0,0,2025-06-30T06:56:22+00:00,1,46,True
79684449,4473615,"Bangalore, Karnataka, India",postgresql,"After updating a trigger in PostgreSQL, it updates all the records instead of specific records","<p>I have a trigger to update a field when there is an update performed on a table. But the trigger updates all the records in the table instead of specific records.</p>
<p>Trigger</p>
<pre><code>create or replace trigger table_update_trigger
after update of
column_1, column_2, column_3, column_4
on table_1
for each row
execute function table_update();
</code></pre>
<p>Function</p>
<pre><code>create or replace function table_update()
returns trigger language plpgsql
as $$
begin
update table_1 set changed_date = current_date, changed_user = current_user;
return new;
end;
$$;
</code></pre>
<p>While firing update statement,
<code>update table_1 set column_1 = 'DEF' where column_1 = 'ABC';</code> records in the entire table gets updated for changed_date and changed_user.</p>
<p>How can I fix it?</p>
",0,0,0,2025-06-30T08:27:15+00:00,2,81,True
79684960,22786821,,postgresql,TimeScaleDb/Postgres: Materialized Views(COGG): GROUP BY: group by certain field values,"<p>What I'm currently doing is this:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    time_bucket('60 min', raw_data.timestamp) AS time_60min,
    COUNT(raw_data.vehicle_class) AS &quot;count&quot;,
    raw_data.vehicle_class AS &quot;vehicle_class&quot;
FROM bma_raw_data_ode AS raw_data
WHERE raw_data.vehicle_class IN ('car', 'bus', 'van', 'motorbike')
GROUP BY time_60min, raw_data.vehicle_class
ORDER BY time_60min
</code></pre>
<p>All entries in <code>raw_data.vehicle_class</code> have one of 12 given values (<code>car</code>, <code>bus</code>, <code>person</code>, <code>bicycle</code> and so on) - think of it as an enum, as it can only be one of these 12 values.</p>
<p>What the query above returns at the moment is the count of each vehicle_class in raw_data within an hour:</p>
<pre><code>&quot;2025-06-10 19:00:00+00&quot;    1   &quot;bus&quot;
&quot;2025-06-10 19:00:00+00&quot;    4   &quot;motorbike&quot;
&quot;2025-06-10 19:00:00+00&quot;    126 &quot;car&quot;
&quot;2025-06-10 19:00:00+00&quot;    3   &quot;van&quot;
</code></pre>
<p>Now, what I'd like to achieve is: Not having separate <code>car</code> and <code>van</code> rows per hour, but <strong>a single row for the sum of both counts</strong>.</p>
<p>So, the result I'm looking for would be someting like:</p>
<pre><code>&quot;2025-06-10 19:00:00+00&quot;    1   &quot;bus&quot;
&quot;2025-06-10 19:00:00+00&quot;    4   &quot;motorbike&quot;
&quot;2025-06-10 19:00:00+00&quot;    129 &quot;car + van&quot;
</code></pre>
<p>Can you point be in the right direction? Can I achieve this directly in <code>COUNT(raw_data.vehicle_class) AS &quot;count&quot;,</code>, can it be done by a SELECT in the GROUP BY statement or is some fancy join on a virtual table the right way?</p>
<p>Thanks</p>
",1,1,0,2025-06-30T15:04:08+00:00,1,76,True
79685161,29888271,,postgresql,How to extract JSONB array values from a Supabase Postgres table and insert them into another table in React Native?,"<p>I'm working on a React Native app with Supabase as my backend.</p>
<p>My products table has a <em>style_tags</em> column in <em>JSONB array</em> format, for example:</p>
<pre><code>[
  { &quot;tag&quot;: &quot;streetwear&quot;, &quot;weight&quot;: 0.8 },
  { &quot;tag&quot;: &quot;summer&quot;, &quot;weight&quot;: 0.5 }
]
</code></pre>
<p>I want to extract each object in the array, and insert its <strong>tag</strong> field into the <strong>style_tags</strong> column of another table called user_style_tags_scores, and its <strong>weight</strong> field into that table's <strong>score</strong> column.</p>
<p>What's the way to fetch the JSONB array and insert each tag-weight pair into another table in Supabase?</p>
<p>I tried fetching the data using:</p>
<pre><code>const { data: productData } = await supabase
  .from('products')
  .select('style_tags')
  .eq('id', id);
</code></pre>
<p>but I'm not sure how to loop through the JSONB array and then insert each of its elements into the user_style_tags_scores with the right columns (style_tags and scores)</p>
",0,0,0,2025-06-30T18:30:52+00:00,0,99,False
79685195,903061,"Traverse City, MI, United States",postgresql,Converting JSON column to multiple columns,"<p>I've got a postgres (version 14.5) table with a column called &quot;observations&quot; that looks something like this:</p>
<pre><code>{
 &quot;fl_1&quot;:{&quot;answers&quot;:{&quot;yes&quot;:true},&quot;comment&quot;:&quot;lorem ipsum&quot;},
 &quot;fl_2&quot;:{&quot;answers&quot;:{&quot;yes&quot;:true},&quot;comment&quot;:&quot;blah&quot;},
 &quot;fl_3&quot;:{&quot;answers&quot;:{&quot;no&quot;:true},&quot;comment&quot;:&quot;yada&quot;},
 &quot;fl_4&quot;:{&quot;answers&quot;:{&quot;na&quot;:true}
}
</code></pre>
<p>There are 13 flags that may be present, and I need to convert the &quot;answers&quot; element for them to 13 different columns with a 1 if the answer is &quot;yes&quot;, a 0 if the answer is &quot;no&quot;, and NULL otherwise. For the above input, the desired output would be</p>
<pre><code>fl_1  fl_2  fl_3  fl_4
   1     1     0  NULL
</code></pre>
<p>I can do that with something like</p>
<pre><code>case 
  when json_extract_path(observations, 'fl_1', 'answers', 'yes') is true then 1
  when json_extract_path(observations, 'fl_1', 'answers', 'no') is true then 0
  else NULL
end as fl_1
-- and repeat for all 13 flags
</code></pre>
<p>But is there a better way? Something more efficient to run and/or more efficient for me to write?</p>
",0,0,0,2025-06-30T19:00:53+00:00,2,71,True
79685280,18586873,,postgresql,Filtering by Dates in drizzle ORM,"<h3>How can I use Drizzle to query records within a specific time frame defined by two timestamps?</h3>
<p>I'm using PostgreSQL and I searched and it seems the only thing available is <code>between()</code> method, which only works for <strong>integers</strong>, and I can't use it for <strong>strings</strong> or <strong>dates</strong>. I want to say like:</p>
<pre class=""lang-ts prettyprint-override""><code>const customers = await db
        .select()
        .from(customers)
        .where(between(customers.created, startDate, endDate));
</code></pre>
<p>here is my schema of customers in drizzle:</p>
<pre class=""lang-ts prettyprint-override""><code>export const customers = pgTable(&quot;customers&quot;,{
    ...
    created: timestamp({ withTimezone: true, mode: &quot;string&quot; }).defaultNow(),
  },
);
</code></pre>
<p>I tried using raw SQL, but it's not a typesafe way to do this:</p>
<pre class=""lang-ts prettyprint-override""><code>await db.execute(sql`
      SELECT * 
      FROM customers 
      WHERE created BETWEEN ${startDate} AND ${endDate}
    `)  
</code></pre>
",1,2,1,2025-06-30T20:53:37+00:00,1,1122,False
79686134,14478298,,postgresql,How to add order clause in sequelize-cursor-pagination,"<p>I'm using the <a href=""https://www.npmjs.com/package/sequelize-cursor-pagination"" rel=""nofollow noreferrer""><code>sequelize-cursor-pagination</code></a> package to implement pagination in my Node-Postgres application. I need to order my results based on a field from an included (associated) model, but the package doesn't allow this.<br />
<strong><code>NB:</code></strong><code> The order option format only supports the ['field'] and ['field', 'DESC'] variations (field name and the optional order direction). For example, ordering by an associated model's field won't work.</code></p>
<pre class=""lang-js prettyprint-override""><code>  const queryDict = {
    where: {
      orgPk,
    },
    include: [...(associations.includeControl ? [{ model: db.Control, as: 'control', where: { orgPk } }] : [])],
  }
</code></pre>
<p>I've attempted to add an <code>order</code> clause using <code>Sequelize Literal</code></p>
<pre class=""lang-js prettyprint-override""><code>    order: associations.includeControl ? [[Sequelize.literal('CAST(&quot;control&quot;.&quot;number&quot; AS INTEGER)'), 'ASC']] : [],
</code></pre>
<p>But this works only for the first page, app crashes when second page is requested<br />
<code>errors: [ &quot;message&quot;: &quot;missing FROM-clause entry for table \&quot;undefined\&quot;&quot;,</code></p>
",1,1,0,2025-07-01T13:28:17+00:00,1,80,False
79687130,5113754,Kollam Kerala India,postgresql,How I can do strapi DB migration to existing DB in production environment?,"<p>In production env , there is one Database .
And from local I changed some table structure and added new table also . How I can migrate the new changes of DB to prod env . I ma using using PostgreSQL DB</p>
",0,0,0,2025-07-02T08:54:38+00:00,0,44,False
79687140,17018285,,postgresql,using databricks trying to update a row in postgres,"<p>I wanted to update a row in postgres table</p>
<ol>
<li><p>when i am using pyspark in databricks, i am easily able and read and overwrite the data.</p>
</li>
<li><p>when i am using psycopg2-binary library i am getting this error.</p>
</li>
</ol>
<p>initially i thought its a security or access issue but then why i am able to overwrite using pyspark also i checked in aws the required permission is provided to the security group and 0.0.0.0/0 ip is allowed</p>
<p>postgres is hosted on aws server.</p>
<p>i am attaching the code and the error screenshot.</p>
<p>#code</p>
<pre><code>import psycopg2

host = &quot;myhostname.rds.amazonaws.com&quot;
port = 5432
database = &quot;mydbname&quot;
user = &quot;postgres&quot;
password = &quot;mypassword&quot;

conn = psycopg2.connect(
    host=host,
    port=port,
    dbname=database,
    user=user,
    password=password
)
cur = conn.cursor()

sql = &quot;&quot;&quot;
UPDATE your_table
SET cohort_name = 'testing_new_approch'
WHERE cohort_id = 'a9fe83a3-7aa2-4905-9184-06326b543764';
&quot;&quot;&quot;

cur.execute(sql)


conn.commit()

cur.close()
conn.close()
</code></pre>
<p>#error: OperationalError: connection to server at &quot;myhostname.rds.amazonaws.com&quot; (10.99.77.257), port 5432 failed: Connection refused
Is the server running on that host and accepting TCP/IP connections?</p>
<p>this code is running</p>
<p>-</p>
<pre><code>db_table = 'rwd_cohort.t_client_cohort_bridge_test1'
(updated_df.write
    .format(&quot;jdbc&quot;)
    .option(&quot;url&quot;, url)
    .option(&quot;dbtable&quot;, db_table)
    .option(&quot;user&quot;, db_user)
    .option(&quot;password&quot;, db_password)
    .option(&quot;driver&quot;, &quot;org.postgresql.Driver&quot;)
    .mode(&quot;overwrite&quot;)
  .save()
)
</code></pre>
<p>same server</p>
<p>#versions i am using</p>
<p>psycopg2_binary-2.9.10
PostgreSQL 15.12
Python 3.13.3</p>
<p>can someone tell me where i am going wrong</p>
",0,0,0,2025-07-02T09:01:25+00:00,0,54,False
79688106,2057516,"Princeton, NJ",postgresql,Why aren&#39;t my Django Postgres `ArrayAgg` members sorting?,"<p>I'm exploring the use of <code>ArrayAgg</code> and I don't understand why <code>'histidine-[13C6,15N3]'</code> doesn't occur before <code>'isoleucine-[13C6,15N1]'</code> in this example:</p>
<pre><code>In [25]: for i in Infusate.objects.annotate(tns=ArrayAgg(&quot;tracer_links__tracer__name&quot;, order_by=&quot;tracer_links__tracer__name&quot;)).order_by(&quot;tns&quot;).distinct():
    ...:     print(i.tns)
    ...: 
['inosine-[15N4]']
['isoleucine-[13C6,15N1]', 'leucine-[13C6,15N1]', 'valine-[13C5,15N1]']
['isoleucine-[13C6,15N1]', 'lysine-[13C6,15N2]', 'phenylalanine-[13C9,15N1]', 'threonine-[13C4,15N1]', 'tryptophan-[13C11]', 'histidine-[13C6,15N3]']
</code></pre>
<p>The records/rows are sorted based on the first value, which is great, but that first value isn't sorted WRT the other values in the list.</p>
<p>For that matter, why doesn't the order of the members of the list change at all when I negate the <code>order_by</code> in the <code>ArrayAgg</code>?  According to what I read, this should sort the list items.  Am I blind?  What am I doing wrong?</p>
<pre><code>In [25]: for i in Infusate.objects.annotate(tns=ArrayAgg(&quot;tracer_links__tracer__name&quot;, order_by=&quot;-tracer_links__tracer__name&quot;)).order_by(&quot;tns&quot;).distinct():
    ...:     print(i.tns)
    ...: 
['inosine-[15N4]']
['isoleucine-[13C6,15N1]', 'leucine-[13C6,15N1]', 'valine-[13C5,15N1]']
['isoleucine-[13C6,15N1]', 'lysine-[13C6,15N2]', 'phenylalanine-[13C9,15N1]', 'threonine-[13C4,15N1]', 'tryptophan-[13C11]', 'histidine-[13C6,15N3]']
</code></pre>
",1,1,0,2025-07-02T21:33:19+00:00,2,66,True
79688150,15084061,,postgresql,Do PostgreSQL cursors operate on snapshots?,"<p>I declare a cursor in PostgreSQL, open it &amp; start fetching data in batches.</p>
<p>Does the cursor operate on a &quot;snapshot&quot; (=copy) of the table made at the time of cursor opening, or is it susceptible to non-repeatable reads (or perhaps even dirty reads?).</p>
<p>E.g.</p>
<ol>
<li><p>Cursor opens at timestamp T1=00:00:01.</p>
</li>
<li><p>First batch is read, all data corresponds to state T1.</p>
</li>
<li><p>In the meantime, another transaction committed changes to all data at timestamp T2=00:00:05.</p>
</li>
<li><p>Second batch is read, <strong>will the fetched data correspond to T1 or T2</strong>?</p>
</li>
</ol>
<p>I know that many DB systems have their cursor operate on &quot;immutable copies&quot; rather than the actual table, but cannot find any information on how it's handled in PostgreSQL.</p>
",3,3,0,2025-07-02T22:28:55+00:00,2,109,True
79688301,28622821,,postgresql,MyBatis Dynamic SQL with PostgreSQL Throws Syntax Error When Certain Parameters Are Combined,"<p>I'm facing an issue with MyBatis dynamic SQL generation. When combining certain search parameters, the SQL it generates causes a PostgreSQL error:</p>
<blockquote>
<p>org.postgresql.util.PSQLException: ERROR: syntax error at or near
&quot;flight_number&quot;</p>
</blockquote>
<p>This happens only when I pass airlineCode, flightNumber, and flightDate together (or airlineCode + flightNumber). But if I pass just flightNumber, or airlineCode with a flight date range, it works fine.</p>
<pre><code>&lt;select id=&quot;fetchAllUpdAirRetroactiveCostList&quot; resultMap=&quot;airUpdRetroactiveCostVOs&quot;&gt;
    SELECT
        airline_code,
        contract_code,
        flight_number,
        flight_date,
        contract_route_code,
        airline_fare_basis_code,
        currency,
        adult_cost,
        child_cost,
        infant_cost,
        cabin_class,
        ops_req_id,
        unique_retro_id
    FROM ITRVL_AIR_RETRO_UPD
    &lt;where&gt;
        TENANT_ID = :tenant_id
        &lt;if test=&quot;searchVO.airlineCode != null and searchVO.airlineCode != ''&quot;&gt;
            AND airline_code = #{searchVO.airlineCode}
        &lt;/if&gt;
        &lt;if test=&quot;searchVO.contractCode != null and searchVO.contractCode != ''&quot;&gt;
            AND contract_code = #{searchVO.contractCode}
        &lt;/if&gt;
        &lt;if test=&quot;searchVO.flightNumber != null&quot;&gt;
            AND flight_number = #{searchVO.flightNumber}
        &lt;/if&gt;
        &lt;if test=&quot;searchVO.fromDate != null and searchVO.toDate != null&quot;&gt;
            AND flight_date BETWEEN CAST(#{searchVO.fromDate} AS DATE) AND CAST(#{searchVO.toDate} AS DATE)
        &lt;/if&gt;
    &lt;/where&gt;
&lt;/select&gt;
</code></pre>
<p>flightNumber is an Integer in the Java searchVO.
I suspect MyBatis is generating SQL like AND flight_number = when parameters aren't fully populated.
How can I safely handle optional parameters in MyBatis so that it doesn’t generate malformed SQL like AND flight_number =?</p>
<p>Is there a better way to conditionally build dynamic SQL for PostgreSQL in MyBatis?</p>
",0,0,0,2025-07-03T04:32:27+00:00,0,81,False
79688989,29935472,,postgresql,Is it bad to fork for input null or not null within a single query?,"<p>I have a table <code>users</code> with a column <code>department_id</code>. Every user has a <code>department_id</code>.</p>
<p>I also have a dashboard if an Admin logs in I check which department_id he has and then I make a call to show only the users that have the same department id like the admin.</p>
<p>So but if an admin has no department_id I want to show users from all departments. So the super admin has the role that he can see all users. How can I make it now that I say if there is no department_id then show all users?</p>
<p>Currently I solved like this:</p>
<pre><code>SELECT u.id 
FROM users u
INNER JOIN department_users du
ON du.user_id = u.id
WHERE CASE WHEN $2 IS NULL THEN (u.department_id IS NULL OR u.department_id &gt; 0) ELSE u.department_id = $2 END
GROUP BY u.id
</code></pre>
<p>Department_id is an integer. I want to show all users when he has no department_id wherever department_id is NULL or have a number. If he has an department_id I want to show the users only from the department_id.</p>
<p>It works, but is the way correct or can I do it better?<br />
I have like 50 queries like this.</p>
",1,1,0,2025-07-03T14:15:40+00:00,1,80,True
79689016,8558811,,postgresql,Database field error when launching SpringBoot integration tests with maven command (using PostgreSQLContainer),"<p>I have this situation with my Spring Boot application and the management of tests:</p>
<p>The application is running correctly. But for its unit tests, regular tests are OK, while integration tests are giving me some issues.</p>
<p>In order to be able to operate with a database for the tests, I do the following in a base class from which all integration tests inherit:</p>
<pre><code>public abstract class IntegrationTestBase {

  protected static PostgreSQLContainer&lt;?&gt; postgres = null;

  private static boolean databaseStarted = false;

  @BeforeAll
  public static void beforeAll() {
    if (!databaseStarted) {
      postgres = new PostgreSQLContainer&lt;&gt;(&quot;postgres:16-alpine&quot;)
        .withReuse(true);

      postgres.start();
      databaseStarted = true;

      Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; {
        // Ideally this should stop the DB only after executing all classes
        postgres.stop();
      }));
    }
  }
}
</code></pre>
<p>I can launch a test individually through a command like this:</p>
<pre><code>mvn test -Dtest=&quot;UserServiceIT&quot;
</code></pre>
<p>But when I try to launch all of them:</p>
<pre><code>mvn failsafe:integration-test
</code></pre>
<p>I get the following error:</p>
<pre><code>org.springframework.beans.factory.BeanCreationException:
Error creating bean with name 'entityManagerFactory' defined in class path resource
[org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]:
Invocation of init method failed; nested exception is javax.persistence.PersistenceException:
[PersistenceUnit: default] Unable to build Hibernate SessionFactory;
nested exception is org.hibernate.tool.schema.spi.SchemaManagementException:
Schema-validation: wrong column type encountered in column [item_id] in table [Item];
found [int8 (Types#BIGINT)], but expecting [int4 (Types#INTEGER)]
</code></pre>
<p>The class &quot;Item&quot; comes from an external jar and therefore I am unable to change its type, but I can tell you it is defined as a long.</p>
<p>I have been trying to make workarounds such as trying to redefine the entityManagerFactory but that only makes things more complicated. After all the tests are working when executing individually.</p>
<p>Do you know what could be causing this? Am I launching the tests the right way?</p>
",0,0,0,2025-07-03T14:32:19+00:00,1,55,False
79689783,186202,"Rennes, France",postgresql,How to use Alembic to create Postgresql indexes concurrently?,"<p>I would like to create an index on a large table.</p>
<p>I know that one should use CREATE INDEX CONCURRENTLY for that.</p>
<p>When trying to do so with Alembic I get this error:</p>
<pre><code>sqlalchemy.exc.InternalError: (psycopg.errors.ActiveSqlTransaction) CREATE INDEX CONCURRENTLY cannot run inside a transaction block
</code></pre>
<p>How can I create the index concurrently with Alembic?</p>
",1,1,0,2025-07-04T07:00:37+00:00,1,249,True
79690550,23305162,,postgresql,Insert record into many-to-many relationship,"<p>I want to create a database for my blog website where I have many posts and categories. A post can have many categories and a category can have many posts. I couldn't figure out how to implement it in PostgreSQL.</p>
<p>This is the <code>category</code> table:</p>
<pre><code>CREATE TABLE category (
   category_name VARCHAR(255) PRIMARY KEY
);
</code></pre>
<p>And this is the <code>Post</code> table:</p>
<pre><code>CREATE TABLE post (
   post_id INTEGER NOT NULL PRIMARY KEY,
   title VARCHAR(255),
   category_name VARCHAR(255) REFERENCES category (category_name)
                 ON UPDATE CASCADE ON DELETE CASCADE,
   date DATE
);
</code></pre>
<p>I find it difficult to <code>INSERT</code> a record.</p>
<p>The following doesn't work:</p>
<pre><code>INSERT INTO post(post_id, title, category_name, date)
VALUES (1, 'How to create a website',['technology','website'], '2025-06-04');
</code></pre>
<p>I could've inserted some data into the <code>category</code> table, but only 1 value works at a time. How do I add multiple values?</p>
",0,2,2,2025-07-04T17:54:16+00:00,2,162,True
79690717,15385022,,postgresql,Race condition safe check of maximum allowable rows in database with JPA (Postgres),"<p>Is the following approach to limiting the maximum number of characters a user can make actually race condition safe (and reasonably performant)? My thought process is the query with lock will halt duplicate create requests until the locked create request is done, and then the duplicate request will see the correct count. One thing I am worried about is if there are no characters, there are no rows to lock, and nothing will block other requests.</p>
<p>If this is not a safe approach, what are some approaches I can take to actually accomplish this? I would prefer Java approaches but setting a constraint at the DB level is fine too, especially if it is best for performance.</p>
<p>My repository:</p>
<pre><code>@Repository
public interface CharacterRepository extends JpaRepository&lt;PlayerCharacter, BigInteger&gt; {
    @Lock(LockModeType.PESSIMISTIC_WRITE)
    long countByUserId(BigInteger userId);
}
</code></pre>
<p>In my service layer:</p>
<pre><code>    @Transactional
public List&lt;PlayerCharacter&gt; createCharacter(PlayerCharacterRequest character) {
    // ...
    if (charRepo.countByUserId(loggedInUserId) &gt;= MAX_CHARACTERS_PER_USER) {
        // error out
    }
    charRepo.save(newPlayerCharacter);
    // ...
}
</code></pre>
",0,0,0,2025-07-04T23:21:13+00:00,0,79,False
79692177,30979176,,postgresql,Fetching Issue : Prisma findMany() Returns null in GET /all Route but Fetch by ID Works,"<p>Title:
Prisma findMany returns null with PostgreSQL, Cloudflare Hono setup (other routes work)</p>
<p>Body:</p>
<p>I am building a project using PostgreSQL, Prisma, Hono, deployed on Cloudflare.</p>
<p>My <code>/all</code> route is supposed to fetch all blog posts with a GET request. However, the route returns unexpected <code>null</code> for blogs:</p>
<p><strong>This is the response:</strong>
<a href=""https://i.sstatic.net/Qs4UbiGn.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/Qs4UbiGn.png</a></p>
<hr />
<p><strong>Relevant Code &amp; Setup:</strong></p>
<p><strong>The <code>/all</code> Route:</strong></p>
<pre class=""lang-js prettyprint-override""><code>postRouter.get('/all', auth, async (c) =&gt; {
  const prisma = new PrismaClient({
    datasourceUrl: c.env.DATABASE_URL,
  }).$extends(withAccelerate());

  try {
    const blogs = await prisma.post.findMany();
    return c.json({ blogs });
  } catch (err) {
    c.status(500);
    return c.json({ error: err.message, message: &quot;Error fetching blogs&quot; });
  }
});
</code></pre>
<hr />
<p><strong>Prisma Schema:</strong></p>
<pre class=""lang-none prettyprint-override""><code>model User {
  id     String  @id @default(uuid())
  email  String  @unique
  name   String
  password String
  posts  Post[]
}

model Post {
  id          String  @id @default(uuid())
  title       String
  description String
  published   Boolean @default(false)
  author      User    @relation(fields: [author_id], references: [id])
  author_id   String
}
</code></pre>
<hr />
<p><strong>What I've Tried:</strong></p>
<ul>
<li><code>DATABASE_URL</code> works, other routes (fetch by ID, create) function properly</li>
<li>Database is generated, migrations applied</li>
<li><code>prisma.post.findMany()</code> should return an array (confirmed from docs)</li>
<li>No Prisma errors in terminal</li>
<li>Double-checked route logic — seems correct</li>
<li>Tested in Postman, consistent issue</li>
</ul>
<hr />
<p><strong>Additional Context:</strong></p>
<ul>
<li>First time building with Cloudflare Workers and PostgreSQL</li>
<li>I suspect the issue could be a silly return mismatch, but I'm stuck</li>
</ul>
<p><strong>Screenshots:</strong></p>
<ul>
<li><a href=""https://i.sstatic.net/Qs4UbiGn.png"" rel=""nofollow noreferrer"">API Response</a></li>
<li><a href=""https://i.sstatic.net/9QEecnnK.png"" rel=""nofollow noreferrer"">Route Code</a></li>
<li><a href=""https://i.sstatic.net/gYZCHO7I.png"" rel=""nofollow noreferrer"">Prisma Schema</a></li>
</ul>
<hr />
<p><strong>Repo for full context:</strong>
<a href=""https://github.com/kartiktyagi2001/Podium"" rel=""nofollow noreferrer"">https://github.com/kartiktyagi2001/Podium</a></p>
<hr />
<p><strong>Question:</strong>
Why does <code>findMany()</code> return <code>null</code> for <code>/all</code> route even though:</p>
<ul>
<li>DB connection works</li>
<li>Other routes behave correctly</li>
<li>Prisma setup looks valid</li>
</ul>
<p>Could this be related to:</p>
<ul>
<li>Cloudflare + Prisma interaction?</li>
<li>Response structure mistake?</li>
<li>Something specific to PostgreSQL handling in Hono/Cloudflare Workers?</li>
</ul>
<p>Any suggestions, debugging steps, or ideas are welcome. Thanks!</p>
",0,0,0,2025-07-06T23:38:52+00:00,0,38,False
79692355,4594495,,postgresql,How can you use the TimescaleDB extension with Azure Postgres Flexible server for Postgres v17.*,"<p>I am attempting to use the <code>TimescaleDB</code> extension with Azure Postgres but I keep getting this error</p>
<blockquote>
<p>Because timescaledb isn't a trusted extension, only members of &quot;azure_pg_admin&quot; are allowed to use COMMENT ON EXTENSION timescaledb</p>
</blockquote>
<h3>Steps to produce:</h3>
<ul>
<li><p>I create the server using the Azure portal and create the server admin</p>
</li>
<li><p>I then go to server parameters and add <code>TimescaleDB</code> to <code>azure.extensions</code> and <code>shared_preload_libraries</code></p>
</li>
</ul>
<p>Finally, I create a database on the server and connect with the server admin and run this <code>CREATE EXTENSION timescaledb;</code> to which I get this error</p>
<blockquote>
<p>Because timescaledb isn't a trusted extension, only members of &quot;azure_pg_admin&quot; are allowed to use COMMENT ON EXTENSION timescaledb</p>
</blockquote>
<p>Again, I am using the server admin and can verify that that user does have the azure_pg_admin role!</p>
",-1,0,1,2025-07-07T05:51:48+00:00,1,501,False
79692703,9839568,,postgresql,Upgrade PostgreSQL TimescaleDB on Windows,"<p>I have PostgreSQL v17.0 and TimescaleDB v2.18.0 installed on a windows 11 64bit machine. Now I want to upgrade TimescaleDB to v2.20.3, but the upgrade procedure doesn't work, PostgreSQL still reports TimescaleDB v2.18.0. I am using the TimescaleBD community edition for both 2.18.0 and 2.20.3.</p>
<p>That's what I am doing:</p>
<ol>
<li><p>Stop the postgresql service</p>
</li>
<li><p>Run TimescaleDB v2.20.3 setup tool 'setup.exe' with Administrator privileges</p>
</li>
<li><p>After completion I checked PostgreSQL`s lib directory, the file 'timescaledb.dll&quot; has been updated and there are the new v2.20.3 DLLs (timescaledb-2.20.3.dll/timescaledb-tsl-2.20.3.dll) and the old v2.18.0 ones (timescaledb-2.18.0.dll/timescaledb-tsl-2.18.0.dll)</p>
</li>
<li><p>Start the postgresql service</p>
</li>
<li><p>Start psql and run the following commands</p>
<ul>
<li><p>\dx =&gt; reports TimescaleDB 2.18.0</p>
</li>
<li><p>ALTER EXTENSION timescaledb UPDATE</p>
</li>
<li><p>ALTER EXTENSION timescaledb UPDATE TO VERSION '2.20.3'</p>
</li>
<li><p>\dx Still reports timescaledb v2.18.0</p>
</li>
</ul>
</li>
</ol>
<p>What am I missing?</p>
<p>Best regards,
Guido</p>
",0,0,0,2025-07-07T10:29:24+00:00,0,92,False
79692938,7809641,"Boston, MA, USA",postgresql,How to decrypt postgresql bytea data as text properly?,"<p>I am using <code>pgcrypto</code> to encrypt passwords in a table filled with a list of usernames and passwords. The username is of type <code>character varying</code> and the password column is of type <code>bytea</code></p>
<p>For example, I am encrypting the passwords using the command:</p>
<p><code>INSERT INTO user_passwords VALUES ('user1', pgp_sym_encrypt('password1', 'key1'));</code></p>
<p>output:</p>
<pre><code> username |                                                                         password                                                                         
----------+----------------------------------------------------------------------------------------------------------------------------------------------------------
 user1    | \xc30d04070302a835dce62e2ad16661d23a013eddc5c3a823d74771cc5b0de43705bd86f768005178885069f3cbf6cd9913afa0dbb57a009cb3589b8cfb68d4ce58f97789a12c4918de73d8
(1 row)
</code></pre>
<p>And then decrypting with the command:</p>
<p><code>SELECT pgp_sym_decrypt(password, 'key1') FROM user_passwords WHERE username='user1';</code></p>
<p>output:</p>
<pre><code>pgp_sym_decrypt 
-----------------
 password1
(1 row)
</code></pre>
<p>However, the postgresql documentation (<a href=""https://www.postgresql.org/docs/current/pgcrypto.html"" rel=""nofollow noreferrer"">https://www.postgresql.org/docs/current/pgcrypto.html</a>) states that:</p>
<blockquote>
<p>Decrypting bytea data with pgp_sym_decrypt is disallowed. This is to avoid outputting invalid character data. Decrypting originally textual data with pgp_sym_decrypt_bytea is fine.</p>
</blockquote>
<p>How am I supposed to retrieve the data as text? <code>pgp_sym_decrypt_bytea</code> returns bytea data
output:</p>
<pre><code>mydatabase=# SELECT pgp_sym_decrypt_bytea(password, 'key1') FROM user_passwords WHERE username='user1';
 pgp_sym_decrypt_bytea 
-----------------------
 \x70617373776f726431
(1 row)
</code></pre>
",1,1,0,2025-07-07T13:33:00+00:00,2,122,True
79693444,13079675,,postgresql,Does SQL&#39;s IN operator hash its input?,"<p>I'm wondering about the performance of Postgres' <code>IN</code> operator. Specifically, does it hash the values it's passed?</p>
<p>Example:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM table WHERE col IN (1, 2, 3)
</code></pre>
<p>versus</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM table WHERE col in (1, 1, 1, 2, 2, 3, 3, 3, 3)
</code></pre>
<p>Are these two similar in performance? That is, does <code>IN</code> hash both lists its given?</p>
<p>Another example with a subquery (note the <code>DISTINCT</code> keyword):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM table
WHERE col IN (SELECT DISTINCT col FROM another_table WHERE ...)
</code></pre>
<p>Is this any better (or worse) than doing:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM table
WHERE col IN (SELECT col FROM another_table WHERE ...)
</code></pre>
<p>From my understanding:</p>
<ul>
<li>Doing <code>SELECT DISTINCT ...</code> in the subquery is actually extra work, and not performant. I'm not sure why though, I was just told this.</li>
<li>Is it because when we do <code>DISTINCT ...</code> we also construct our own hash table (similar to <code>IN</code>)?</li>
</ul>
",3,3,0,2025-07-07T21:52:00+00:00,2,99,True
79693600,16029814,,postgresql,PostgreSQL !~* not working as expected on nulls,"<p>I just noticed that my SQL query didn't grab a row that should be included, and it's from this part of the query: <code>WHERE tasks.description !~* 'test'</code>.</p>
<p>I have this filter to exclude the tasks that have 'test' in their descriptions, since those tasks just appeared in the database from testing.</p>
<p>I was surprised to see that the description in the row that didn't get included is None.</p>
<p>I have successfully updated this line of code to <code>WHERE tasks.description !~* 'test' OR tasks.description IS NULL</code>, but I am still wondering why my initial code filters out the rows with null descriptions. Does anyone understand why this is?</p>
",2,2,0,2025-07-08T02:40:02+00:00,2,79,True
79693816,24926800,,postgresql,"Why do the pg_temp_### and pg_toast_temp_### schemas keep growing, and why are they not being dropped or released automatically?","<p>We've observed a significant number of temporary schemas with names like pg_temp_* and pg_toast_temp_* accumulating in both the Sandbox and Dev environments.</p>
<p>Why do the pg_temp_### and pg_toast_temp_### schemas keep growing, and why are they not being dropped or released automatically?  Please investigate the root cause of this behavior—whether it's due to long-running sessions, unclosed transactions, or improper cleanup by applications or tools.</p>
<p>Could you please provide a better solution for the issue?</p>
",0,0,0,2025-07-08T07:17:46+00:00,1,444,True
79693948,15633902,,postgresql,AuthJS / Next-Auth using middleware getting error,"<p>I am developing an application to learn NextJS and getting deeper to its concepts.</p>
<p>Im following a course that teaches most needed things to build a full-stack application using NextJS.</p>
<p>The stack I'm using :
NextJS, Prisma, Postgresql, AuthJS</p>
<p>I'm consistently getting this error when I bring up my development server with :
<code>npm run dev</code></p>
<p><code>TypeError: Cannot read properties of undefined (reading 'modules')</code></p>
<p>This error only appears when I use middleware to protect some routes in my app.</p>
<p>These are files and information you may need to help me:</p>
<p>package.json</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;next-match&quot;,
  &quot;version&quot;: &quot;0.1.0&quot;,
  &quot;private&quot;: true,
  &quot;scripts&quot;: {
    &quot;dev&quot;: &quot;next dev --turbopack&quot;,
    &quot;build&quot;: &quot;next build&quot;,
    &quot;start&quot;: &quot;next start&quot;,
    &quot;lint&quot;: &quot;next lint&quot;
  },
  &quot;dependencies&quot;: {
    &quot;@auth/prisma-adapter&quot;: &quot;^2.10.0&quot;,
    &quot;@heroui/react&quot;: &quot;^2.7.11&quot;,
    &quot;@hookform/resolvers&quot;: &quot;^5.1.1&quot;,
    &quot;@prisma/client&quot;: &quot;^6.11.1&quot;,
    &quot;bcrypt&quot;: &quot;^6.0.0&quot;,
    &quot;framer-motion&quot;: &quot;^12.19.1&quot;,
    &quot;next&quot;: &quot;15.3.4&quot;,
    &quot;next-auth&quot;: &quot;^5.0.0-beta.29&quot;,
    &quot;react&quot;: &quot;^19.0.0&quot;,
    &quot;react-dom&quot;: &quot;^19.0.0&quot;,
    &quot;react-hook-form&quot;: &quot;^7.58.1&quot;,
    &quot;react-icons&quot;: &quot;^5.5.0&quot;,
    &quot;react-toastify&quot;: &quot;^11.0.5&quot;,
    &quot;zod&quot;: &quot;^3.25.67&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;@eslint/eslintrc&quot;: &quot;^3&quot;,
    &quot;@types/bcrypt&quot;: &quot;^5.0.2&quot;,
    &quot;@types/node&quot;: &quot;^20&quot;,
    &quot;@types/react&quot;: &quot;^19&quot;,
    &quot;@types/react-dom&quot;: &quot;^19&quot;,
    &quot;autoprefixer&quot;: &quot;^10.4.21&quot;,
    &quot;eslint&quot;: &quot;^9&quot;,
    &quot;eslint-config-next&quot;: &quot;15.3.4&quot;,
    &quot;postcss&quot;: &quot;^8.5.6&quot;,
    &quot;prisma&quot;: &quot;^6.10.1&quot;,
    &quot;tailwindcss&quot;: &quot;3.4.17&quot;,
    &quot;typescript&quot;: &quot;^5&quot;
  }
}
</code></pre>
<p>auth.ts</p>
<pre class=""lang-js prettyprint-override""><code>import NextAuth from &quot;next-auth&quot;;
import authConfig from &quot;@/auth.config&quot;;
import { PrismaAdapter } from &quot;@auth/prisma-adapter&quot;;
import { prisma } from &quot;./lib/prisma&quot;;

export const { handlers, auth, signIn, signOut } = NextAuth({
  callbacks: {
    session: async ({ token, session }) =&gt; {
      if (token.sub &amp;&amp; session.user) {
        session.user.id = token.sub;
      }
      return session;
    },
  },
  adapter: PrismaAdapter(prisma),
  session: { strategy: &quot;jwt&quot; },
  ...authConfig,
});
</code></pre>
<p>auth.config.ts</p>
<pre class=""lang-js prettyprint-override""><code>import Credentials from &quot;next-auth/providers/credentials&quot;;
import type { NextAuthConfig } from &quot;next-auth&quot;;
import { loginSchema } from &quot;./lib/schemas/login-form-schema&quot;;
import { getUserByEmail } from &quot;./app/actions/auth-actions&quot;;
import { compare } from &quot;bcrypt&quot;;

export default {
  providers: [
    Credentials({
      name: &quot;credentials&quot;,
      authorize: async (creds) =&gt; {
        const validated = loginSchema.safeParse(creds);

        if (validated.success) {
          const { email, password } = validated.data;
          const user = await getUserByEmail(email);

          if (!user || !(await compare(password, user.password))) return null;

          return user;
        }

        return null;
      },
    }),
  ],
} satisfies NextAuthConfig;
</code></pre>
<p>middleware.ts</p>
<pre class=""lang-js prettyprint-override""><code>import { NextResponse } from &quot;next/server&quot;;
import { AUTH_ROUTES, PUBLIC_ROUTES } from &quot;./routes&quot;;
import { auth } from &quot;./auth&quot;;

export default auth((req) =&gt; {
  const { nextUrl } = req;
  const isLoggedIn = !!req.auth;

  const isPublic = PUBLIC_ROUTES.includes(nextUrl.pathname);
  const isAuthRoute = AUTH_ROUTES.includes(nextUrl.pathname);

  if (isPublic) {
    return NextResponse.next();
  }

  if (isAuthRoute) {
    if (isLoggedIn) {
      return NextResponse.redirect(new URL(&quot;/members&quot;, nextUrl));
    }
    return NextResponse.next();
  }

  if (!isPublic &amp;&amp; !isLoggedIn) {
    return NextResponse.redirect(new URL(&quot;/login&quot;, nextUrl));
  }

  return NextResponse.next();
});

export const config = {
  matcher: [&quot;/((?!api|_next/static|_next/image|favicon.ico).*)&quot;],
};
</code></pre>
<pre><code>Operating System:
  Platform: darwin
  Arch: arm64
  Version: Darwin Kernel Version 24.5.0: Tue Apr 22 19:48:46 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8103
  Available memory (MB): 8192
  Available CPU cores: 8
Binaries:
  Node: 22.17.0
  npm: 10.9.2
  Yarn: N/A
  pnpm: 10.12.4
Relevant Packages:
  next: 15.3.4 // There is a newer version (15.3.5) available, upgrade recommended! 
  eslint-config-next: 15.3.4
  react: 19.1.0
  react-dom: 19.1.0
  typescript: 5.8.3
Next.js Config:
  output: N/A
</code></pre>
<p>Course repo: <a href=""https://github.com/TryCatchLearn/next-match"" rel=""nofollow noreferrer"">https://github.com/TryCatchLearn/next-match</a></p>
<h2>What I tired:</h2>
<ul>
<li>At first I was using pnpm, I changed to npm to match the course instructions</li>
<li>I split configuration as you see in the code</li>
<li>I changed prisma-adapter and next-auth to match course version</li>
</ul>
<p>Nothing worked.</p>
<p>Thanks &lt;3</p>
",2,2,0,2025-07-08T08:55:46+00:00,2,603,True
79694578,16320430,Tanzania,postgresql,How can I configure Hibernate to generate DDL only for entities with @Table(schema = &quot;...&quot;)?,"<p>I'm using <strong>Spring Boot with Hibernate 6+</strong> and <strong>PostgreSQL</strong>, and I want Hibernate to <strong>generate tables only for entities that explicitly declare a schema using</strong> <code>@Table(schema = &quot;schema_name&quot;)</code>.</p>
<p>For example:</p>
<pre class=""lang-java prettyprint-override""><code>@Entity
@Table(name = &quot;users&quot;, schema = &quot;tenant1&quot;)
public class User {

}
</code></pre>
<p>This entity should result in table creation under the tenant1 schema.</p>
<p>However, for this entity:</p>
<pre class=""lang-java prettyprint-override""><code>@Entity
public class Customer {
 
}
</code></pre>
<p>I don't want Hibernate to generate a table at all, because it doesn't declare a schema.</p>
<p><strong>What I've tried:</strong></p>
<p>Setting <code>hibernate.hbm2ddl.create_namespaces=true</code>
This only ensures the schema exists, but doesn't prevent table creation for schema-less entities.
<strong>Note</strong>: I am not using any migrations tool just hibernate ddl.</p>
",0,0,0,2025-07-08T16:56:20+00:00,0,57,False
79695510,19084192,,postgresql,JpaRepository delete not executing with @SQLDelete,"<p>Im using JpaRepository and performing soft-delete with <code>@SQLDelete</code> annotation. The database use Postgresql.</p>
<p>After some changes, this mechanism stopped working as intended.</p>
<p>Here the class that I'd like to delete:</p>
<pre><code>@Entity
@Table(name = &quot;X&quot;)
@NoArgsConstructor
@Getter
@Setter
@JsonIgnoreProperties(ignoreUnknown = true)
@SQLDelete(sql = &quot;UPDATE X SET status = 'DELETED' WHERE id = ?&quot;, check = ResultCheckStyle.COUNT)
@Where(clause = &quot;status != 'DELETED'&quot;)
public class X extends BasicEntity {
...
}
</code></pre>
<p>Performing a delete with:</p>
<pre><code>@Repository
public interface XRepository extends JpaRepository&lt;X, Long&gt;, JpaSpecificationExecutor&lt;X&gt;
</code></pre>
<p><code>xRepository.delete(x);</code></p>
<p>Since I added this relationship to the X entity:</p>
<pre><code>@OneToOne(mappedBy = &quot;x&quot;)
private Y y;
</code></pre>
<pre><code>CREATE MATERIALIZED VIEW IF NOT EXISTS Y...
</code></pre>
<pre><code>@Entity
@Table(name = &quot;Y&quot;)
@AllArgsConstructor
@NoArgsConstructor
@Getter
@Setter
public class Y implements Serializable {

    @Id
    private Long xId;

    @OneToOne
    @MapsId
    @JoinColumn(name = &quot;x_id&quot;)
    private X x;
}
</code></pre>
<p>When I perform the delete, the X status attribute is not set to DELETE.</p>
<p>I'm sure that the code that call <code>xRepository.delete</code> is executed, but even with the additional logs that show the query executed, there is no trace of UPDATE or DELETE query.</p>
<p>Why does this happen? How can I restore the correct behaviour?</p>
<p>Cheers.</p>
",0,0,0,2025-07-09T11:05:54+00:00,0,47,False
79695526,1290385,"Budapest, Magyarorsz&#225;g",postgresql,"As IIS Express the API can connect to the dockerized database, but unable if running as docker","<p>I have a .Net API, which is able to connect to the dockerized database as IIS Express with this connectionString:</p>
<pre><code>  &quot;ConnectionStrings&quot;: {
&quot;DefaultConnection&quot;: &quot;Host=localhost;Port=5433;Database=postgres;Username=postgres;Password=1&quot;
  },
</code></pre>
<p>Running it as a Container results in <strong>TypeError: NetworkError when attempting to fetch resource.</strong>.</p>
<p>I use this connectionString when I run it as a Container:</p>
<pre><code>  &quot;ConnectionStrings&quot;: {
&quot;DefaultConnection&quot;: &quot;Host=postgres-db;Port=5432;Database=postgres;Username=postgres;Password=1&quot;
  },
</code></pre>
<p>I attached both container to a network:</p>
<pre><code>C:\Users\fesat\Desktop\sample\db&gt;docker network inspect my_network
[
    {
        &quot;Name&quot;: &quot;my_network&quot;,
        &quot;Id&quot;: &quot;736299b9a97134e8f67c5a722e343871bad2bb9fc41400105e0fbfbeaffb3fe5&quot;,
        &quot;Created&quot;: &quot;2025-07-09T10:04:35.298329138Z&quot;,
        &quot;Scope&quot;: &quot;local&quot;,
        &quot;Driver&quot;: &quot;bridge&quot;,
        &quot;EnableIPv4&quot;: true,
        &quot;EnableIPv6&quot;: false,
        &quot;IPAM&quot;: {
            &quot;Driver&quot;: &quot;default&quot;,
            &quot;Options&quot;: {},
            &quot;Config&quot;: [
                {
                    &quot;Subnet&quot;: &quot;172.19.0.0/16&quot;,
                    &quot;Gateway&quot;: &quot;172.19.0.1&quot;
                }
            ]
        },
        &quot;Internal&quot;: false,
        &quot;Attachable&quot;: false,
        &quot;Ingress&quot;: false,
        &quot;ConfigFrom&quot;: {
            &quot;Network&quot;: &quot;&quot;
        },
        &quot;ConfigOnly&quot;: false,
        &quot;Containers&quot;: {
            &quot;40316085dba522987ccb0c816718b8258ab14d5c2035eb69379794a8f070f4ad&quot;: {
                &quot;Name&quot;: &quot;Pokemon_API&quot;,
                &quot;EndpointID&quot;: &quot;944636efdc267e0c05f3c6b96826ed2f8f016b06a06a87f6c400c6b400cd9f13&quot;,
                &quot;MacAddress&quot;: &quot;ba:91:ca:cd:37:a2&quot;,
                &quot;IPv4Address&quot;: &quot;172.19.0.3/16&quot;,
                &quot;IPv6Address&quot;: &quot;&quot;
            },
            &quot;ce18f0f94eb76fbeb3f9d2530ca3a0bb88ef57c90ad04b91fd3053551b6d3774&quot;: {
                &quot;Name&quot;: &quot;postgres-db&quot;,
                &quot;EndpointID&quot;: &quot;1be8e2138789e2ec2cd4eac9a3bb0ae8a536eacb6733249c2c940e9d47c27c2e&quot;,
                &quot;MacAddress&quot;: &quot;ba:f9:9f:9e:fb:08&quot;,
                &quot;IPv4Address&quot;: &quot;172.19.0.2/16&quot;,
                &quot;IPv6Address&quot;: &quot;&quot;
            }
        },
        &quot;Options&quot;: {
            &quot;com.docker.network.enable_ipv4&quot;: &quot;true&quot;,
            &quot;com.docker.network.enable_ipv6&quot;: &quot;false&quot;
        },
        &quot;Labels&quot;: {}
    }
]
</code></pre>
<p>This is the yml for my database:</p>
<pre><code>version: '3.8'

services:
  postgres:
    image: postgres:15
    container_name: postgres-db
    restart: always
    ports:
      - &quot;5433:5432&quot;
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: 1
      POSTGRES_DB: postgres
    volumes:
      - pgdata:/var/lib/postgresql/data
      # - ./init.sql:/docker-entrypoint-initdb.d/init.sql  # ha szeretnél induláskor SQL-t betölteni

volumes:
  pgdata:
</code></pre>
<p>I'm not sure what else I should provide. Could you please advise me on what additional information you need?</p>
",0,0,0,2025-07-09T11:18:56+00:00,0,29,False
79695994,30281089,,postgresql,TimescaleDB installation on postgres 14,"<p>I'm struggling to properly install TimescaleDB on my existing postgres 14 database.
On my other machine I have postgres 16 and I installed it flawlessly, everything worked from the start but on psql14 it was installing in the wrong schema for some reason:</p>
<p>I had pgivm and the create extension command was installing the extension in the pgivm schema instead of the right one (I assume it's the public).
After changing it to the public schema I ran the query &quot;SELECT proname FROM pg_proc WHERE proname ILIKE '%timescale%' LIMIT 5;&quot; and it only shows the timescaledb_pre_restore and  timescaledb_post_restore functions available.</p>
<p>I can't create hypertables, call pgivm.timescaledb_version, etc.</p>
<p>Is my DB borked?</p>
<p>EDIT:
I've reinstalled the whole postgres server, it creates the extension automatically on the public schema but the error persists.
As asked in the comments, I changed the schema by dropping the extensions and creating it adding &quot;WITH SCHEMA public&quot;.</p>
",0,0,0,2025-07-09T17:03:20+00:00,0,238,False
79696122,14305251,"Paris, France",postgresql,"UnicodeDecodeError when connecting to PostgreSQL using psycopg2, despite UTF-8 encoding everywhere","<p>I'm trying to connect to a local PostgreSQL database using psycopg2 in Python. Here's the code I'm using:</p>
<pre><code>import psycopg2
    
    params = {
        'dbname': 'database_name',
        'user': 'user_name',
        'password': 'mypassword',
        'host': 'localhost',
    }
    
    for k, v in params.items():
        print(f&quot;{k}: {v} (type={type(v)}, encoded={v.encode('utf-8')})&quot;)
    
    try:
        conn = psycopg2.connect(**params)
        print(&quot;Connexion OK&quot;)
        conn.close()
    except Exception as e:
        print(&quot;Connexion Erro&quot;)
        print(type(e), e)
</code></pre>
<p>The printed output confirms that all parameters are strings and UTF-8 encoded. However, I still get the following error:</p>
<pre><code>    Connexion Erro
&lt;class 'UnicodeDecodeError'&gt; 'utf-8' codec can't decode byte 0xe9 in position 103: invalid continuation byte
</code></pre>
<p>I also checked the server and client encoding on PostgreSQL using:</p>
<pre><code>   SHOW server_encoding;
   SHOW client_encoding;
</code></pre>
<p>Both return UTF8.</p>
<p>Given that all inputs are UTF-8 and the database is configured for UTF-8, I don't understand why this error occurs.</p>
<p>Has anyone encountered this before or has an idea of where this byte 0xe9 might come from? What else should I check?</p>
<p>EDIT : I’m running the code on Windows 11 (64-bit) with Python 3.11.9.</p>
<p>The default language of my Windows system is French.</p>
<p>I do not run the Django application in Chrome. I only use Chrome to view the pages generated by Django views.</p>
<p>I have other browsers available for testing the application.
I mentioned Chrome because after a browser update, it triggered an automatic reboot of my PC, which caused an abrupt shutdown of all servers (Django, database, etc.). The issues started right after this event, whereas everything was working fine before.</p>
<p>The code snippet is part of the Django application. However, even when isolating and running this code on its own, the problem persists. Based on my observations, the issue seems to be specifically related to the database connection</p>
",0,0,0,2025-07-09T18:50:19+00:00,0,118,False
79697286,31011927,,postgresql,How to text-search in PostgreSQL when using RLS?,"<p>Over the past week, I’ve been working on implementing text search in a NestJS application using PostgreSQL. The challenge is that the app supports multiple tenants, all sharing the same tables, so we use Row-Level Security (RLS) to isolate data between tenants.</p>
<p>I want to perform simple text searches using LIKE and ILIKE, but the catch is: the data is stored case-sensitive in the database, and I want the searches to be case-insensitive.I also would like to extend this in the future to do some more complex searching.</p>
<p>At first, I used GIN indexes because I read they are well-suited for text search in PostgreSQL.</p>
<p>When I tested queries as a user without Row-Level Security (RLS), the indexes were used as expected. However, when I ran the same queries as a user with RLS enabled, the GIN indexes were ignored.</p>
<p>After some research I learned that, due to “leakproof” requirements in PostgreSQL RLS, GIN indexes cannot be used in queries because they are not leakproof.</p>
<p>Now I’m wondering: is it even possible to do efficient text search while using RLS? And what are the recommended approaches to handle this scenario?</p>
",3,3,0,2025-07-10T15:24:32+00:00,0,100,False
79697557,6319675,"Lima, Peru",postgresql,Confluent Cloud - Kafka Sink Postgres - type UUID,"<p>I am using CC and Postgres Sink connector.</p>
<p>I am facing the issue:</p>
<blockquote>
<p>Sink Connector for PostgreSQL fails to delete record if key type is UUID</p>
</blockquote>
<blockquote>
<p>java.sql.SQLException: Exception chain:<br />
java.sql.BatchUpdateException:<br />
Batch entry 0 DELETE FROM &quot;my_table&quot; WHERE &quot;my_id&quot; = ('e9157f76-543f-5230-b62f-807f3ea3af22') AND &quot;start_ts&quot; = ('2025-07-05 05:53:23.408+00')<br />
was aborted: ERROR: operator does not exist: uuid = character varying<br />
Hint: No operator matches the given name and argument types. You might need to add explicit type casts.</p>
<p>Position: 93  Call getNextException to see other errors in the batch.</p>
</blockquote>
<p>I found more people facing the same, <a href=""https://github.com/confluentinc/kafka-connect-jdbc/issues/1084"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Can't figure out how I can fix it. Any clues?</p>
",-1,0,1,2025-07-10T20:12:20+00:00,1,67,False
79698322,22722326,,postgresql,PostgreSQL Force Index Scan,"<p>I have a table with below columns in PostgreSQL.</p>
<pre><code>id,
location,
store,
flag1,
flag2
</code></pre>
<p>Here id, location, store are contributing to Primary Key. So by default postgreSQL creates an index on it.</p>
<p>I have created two more index apart from primary key index as below :</p>
<ol>
<li>id</li>
<li>location, id</li>
</ol>
<p>Now when I run the below query :</p>
<pre><code>EXPLAIN ANALYZE
SELECT *
FROM my_table
WHERE location = '1' AND id = '1';
</code></pre>
<p>It should have used the index with <code>location, id</code>, but it uses primary key index.</p>
<p>Now when I run below query, it uses <code>id</code> index, which is expected.</p>
<pre><code>EXPLAIN ANALYZE
SELECT *
FROM my_table
WHERE id = '1';
</code></pre>
<p>I am not sure on how can I force the indexing or the behaviour of PostgreSQL is correct &amp; efficient one. On paper, it looks like <code>location, id</code> would be better choice for the query.</p>
<p>Can some help on the issue/doubt I have ?
Also if this is the expected behaviour, can you provide some query which will use <code>location,id</code> index ?</p>
",0,1,1,2025-07-11T12:17:26+00:00,1,94,False
79698383,17605387,,postgresql,Liquibase/Hibernate Error: Unable to load class,"<p>I'm having problems with Liquibase in my Spring Backend project. Let me start saying that I managed to have it working in an older project, but in this one I used docker and the structure is a bit more complex.</p>
<p>Docker structure is:</p>
<ul>
<li>backend-primary</li>
<li>postgres-primary</li>
<li>backend-secondary</li>
<li>postgres-secondary</li>
<li>zookeeper</li>
<li>kafka</li>
</ul>
<blockquote>
<p>beaf57fc6129   central-server-backend-secondary   &quot;java -jar app.jar&quot;
5 minutes ago   Up 5 minutes   0.0.0.0:8444-&gt;8443/tcp,
[::]:8444-&gt;8443/tcp   central-server-backend-secondary-1</p>
<p>d096b48a8d59   central-server-backend-primary     &quot;java -jar app.jar&quot;
5 minutes ago   Up 5 minutes   0.0.0.0:8443-&gt;8443/tcp,
[::]:8443-&gt;8443/tcp   central-server-backend-primary-1</p>
<p>9a1c36c6ea0f   confluentinc/cp-kafka:7.4.0<br />
&quot;/etc/confluent/dock…&quot;   5 minutes ago   Up 5 minutes<br />
0.0.0.0:9092-&gt;9092/tcp, [::]:9092-&gt;9092/tcp   kafka</p>
<p>d2213968e3b6   postgres:15<br />
&quot;docker-entrypoint.s…&quot;   5 minutes ago   Up 5 minutes<br />
0.0.0.0:5432-&gt;5432/tcp, [::]:5432-&gt;5432/tcp   postgres-primary</p>
<p>42b39c40ab3f   confluentinc/cp-zookeeper:7.4.0<br />
&quot;/etc/confluent/dock…&quot;   5 minutes ago   Up 5 minutes<br />
0.0.0.0:2181-&gt;2181/tcp, [::]:2181-&gt;2181/tcp   zookeeper</p>
<p>5161a764be9d   dpage/pgadmin4                     &quot;/entrypoint.sh&quot;<br />
5 minutes ago   Up 5 minutes   0.0.0.0:8081-&gt;80/tcp, [::]:8081-&gt;80/tcp
pgadmin</p>
<p>ff7b1838e9fa   postgres:15<br />
&quot;docker-entrypoint.s…&quot;   5 minutes ago   Up 5 minutes<br />
0.0.0.0:5433-&gt;5432/tcp, [::]:5433-&gt;5432/tcp   postgres-secondary</p>
</blockquote>
<p>Now I'm trying to run a liquibase:diff in backend-primary with this command:</p>
<pre><code>docker run --rm -it -v ${PWD}:/app -w /app --network central-server_central-net maven:3.9.0 mvn &quot;-Dliquibase.classpath=target/classes:target/dependency&quot; liquibase:diff
</code></pre>
<p>Honestly, I'm not a great expert of Docker nor Liquibase, but I understood that I can't simply run <code>mvn liquibase:diff</code> from IntelliJ terminal cause databse is inside a Docker Network so it can't be reached from outside (again, not sure why, but this I've read online).</p>
<p>BTW, when I run that command it works, downloads lots of stuff until here:</p>
<blockquote>
<p>[INFO] Envers integration enabled? : true [INFO] No LoadTimeWeaver
setup: ignoring JPA class transformer [WARNING] HHH90000021:
Encountered deprecated setting
[hibernate.temp.use_jdbc_metadata_defaults], use
[hibernate.boot.allow_jdbc_metadata_access] instead [WARNING]
HHH90000025: PostgreSQLDialect does not need to be specified
explicitly using 'hibernate.dialect' (remove the property setting and
it will be selected by default) [INFO] HHH10001005: Database info:
Database JDBC URL [undefined/unknown]
Database driver: undefined/unknown
Database version: 12.0
Autocommit mode: undefined/unknown
Isolation level: 
Minimum pool size: undefined/unknown
Maximum pool size: undefined/unknown [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO]
------------------------------------------------------------------------ [INFO] Total time:  37.316 s [INFO] Finished at: 2025-07-11T12:48:22Z
[INFO]
------------------------------------------------------------------------ [ERROR] Failed to execute goal
org.liquibase:liquibase-maven-plugin:4.27.0:diff (default-cli) on
project backend-primary: [ERROR] Error setting up or running
Liquibase: [ERROR]
org.hibernate.boot.registry.classloading.spi.ClassLoadingException:
Unable to load class [com.starcrest.backend_primary.entity.Account]:
Could not load requested class :
com.starcrest.backend_primary.entity.Account: Throwable [ERROR] -&gt;
[Help 1] [ERROR] [ERROR] To see the full stack trace of the errors,
re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X
switch to enable full debug logging. [ERROR] [ERROR] For more
information about the errors and possible solutions, please read the
following articles: [ERROR] [Help 1]
<a href=""http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException"" rel=""nofollow noreferrer"">http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException</a></p>
</blockquote>
<p>As you can see, it says:</p>
<blockquote>
<p>Unable to load class [com.starcrest.backend_primary.entity.Account]:</p>
</blockquote>
<p>So it DOES finds my entities, it just can't load those for some reason ! Can you help ?
I'm providing my Account entity now:</p>
<pre><code>@Entity @Table(name = &quot;account&quot;)
public class Account {
    @Id @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(unique = true)
    private String username;

    @Column(unique = true)
    private String email;

    @Column
    private String password;

    @Temporal(TemporalType.TIMESTAMP) @Column(nullable = false, updatable = false)
    private Date creationDate;

    @Temporal(TemporalType.TIMESTAMP) @Column(nullable = false, updatable = true)
    private Date lastLoginDate;

    @Column(nullable = false)
    private boolean isVerified;

    @Enumerated(EnumType.STRING) @Column(nullable = false)
    private Genre genre;

    @Enumerated(EnumType.STRING) @Column(nullable = false)
    private Country country;

    @Enumerated(EnumType.STRING) @Column(nullable = false)
    private Language language;

    @Temporal(TemporalType.DATE) @Column(nullable = true)
    private Date birthDate;

    @Enumerated(EnumType.STRING) @Column(nullable = false)
    private AccountRole role;

    @Temporal(TemporalType.TIMESTAMP) @Column(nullable = true, updatable = true)
    private Date lastBanEnd;

    @Column(nullable = false)
    private boolean isDeleting;

    @Temporal(TemporalType.TIMESTAMP) @Column(nullable = true, updatable = true)
    private Date lastDeletingDate;


    // CONSTRUCTORS
    public Account() {
        this.creationDate = new Date();
        this.lastLoginDate = new Date();
        this.isVerified = false;
        this.genre = Genre.NOT_SPECIFIED;
        this.country = Country.NOT_SPECIFIED;
        this.language = Language.NOT_SPECIFIED;
        this.role = AccountRole.NOT_SPECIFIED;
        this.isDeleting = false;
    }

    public Account(String username, String email, String password) {
        this.username = username;
        this.email = email;
        this.password = password;
        this.creationDate = new Date();
        this.lastLoginDate = new Date();
        this.isVerified = false;
        this.genre = Genre.NOT_SPECIFIED;
        this.country = Country.NOT_SPECIFIED;
        this.language = Language.NOT_SPECIFIED;
        this.role = AccountRole.NOT_SPECIFIED;
        this.isDeleting = false;
    }

    public Account(AccountRegistrationDTO dto, AccountRole role) {
        this.username = dto.getUsername();
        this.email = dto.getEmail();
        this.password = dto.getPassword();
        this.creationDate = new Date();
        this.lastLoginDate = new Date();
        this.isVerified = false;
        this.genre = Genre.NOT_SPECIFIED;
        this.country = Country.NOT_SPECIFIED;
        this.language = Language.NOT_SPECIFIED;
        this.role = role;
        this.isDeleting = false;
    }

    // GETTERS
    public Long getId() { return id; }
    public String getUsername() { return username; }
    public String getEmail() { return email; }
    public String getPassword() { return password; }
    public Date getCreationDate() { return creationDate; }
    public Date getLastLoginDate() { return lastLoginDate; }
    public boolean getIsVerified() { return isVerified; }
    public Genre getGenre() { return genre; }
    public Country getCountry() { return country; }
    public Language getLanguage() { return language; }
    public Date getBirthDate() { return birthDate; }
    public AccountRole getRole() { return role; }
    public Date getLastBanEnd() { return lastBanEnd; }
    public boolean getIsDeleting() { return isDeleting; }
    public Date getLastDeletingDate() { return lastDeletingDate; }

    // SETTERS
    public void setUsername(String username) { this.username = username; }
    public void setEmail(String email) { this.email = email; }
    public void setPassword(String password) { this.password = password; }
    public void setGenre(Genre genre) { this.genre = genre; }
    public void setCountry(Country country) { this.country = country; }
    public void setLanguage(Language language) { this.language = language; }
    public void setBirthDate(Date birthDate) { this.birthDate = birthDate; }
    public void setLastBanEnd(Date lastBanEnd) { this.lastBanEnd = lastBanEnd; }



    // UPDATERS
    public void verifyAccount() { this.isVerified = true; }
    public void updateLastLoginDate() { this.lastLoginDate = new Date(); }

    public void startDeleting(){
        isDeleting = true;
        lastDeletingDate = new Date();
    }

    public void cancelDeleting(){
        isDeleting = false;
    }


    // UTILITIES
    public String getFormattedCreationDate() {return DateUtil.formatDateDetailed(creationDate);}
    public String getFormattedLastLoginDate() {return DateUtil.formatDateDetailed(lastLoginDate);}
    public String getFormattedBirthDate() {return birthDate != null ? DateUtil.formatDateSimple(birthDate) : null;}

    public void setBirthDateFromString(String birthDateStr) throws ParseException {
        if(birthDateStr == null || birthDateStr.isEmpty()) {this.birthDate = null;}
        else{this.birthDate = DateUtil.parseDateSimple(birthDateStr);}
    }

    public void setLastBanEndFromString(String lastBanEndStr) throws ParseException {
        if(lastBanEndStr == null || lastBanEndStr.isEmpty()) { this.lastBanEnd = null; }
        else { this.lastBanEnd = DateUtil.parseDateSimple(lastBanEndStr); }
    }

    public boolean isBanned() { return lastBanEnd != null &amp;&amp; lastBanEnd.after(new Date()); }
}
</code></pre>
<p>Adding the pom as requested:</p>
<pre><code>    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;3.5.0&lt;/version&gt;
        &lt;relativePath/&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.starcrest&lt;/groupId&gt;
    &lt;artifactId&gt;backend-primary&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;backend-primary&lt;/name&gt;
    &lt;description&gt;Main backend for Starcrest central server&lt;/description&gt;
    &lt;url/&gt;

    &lt;licenses&gt;
        &lt;license/&gt;
    &lt;/licenses&gt;

    &lt;developers&gt;
        &lt;developer/&gt;
    &lt;/developers&gt;

    &lt;scm&gt;
        &lt;connection/&gt;
        &lt;developerConnection/&gt;
        &lt;tag/&gt;
        &lt;url/&gt;
    &lt;/scm&gt;

    &lt;properties&gt;
        &lt;java.version&gt;21&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;spring-kafka-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
            &lt;artifactId&gt;spring-security-crypto&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt;
            &lt;artifactId&gt;jjwt-api&lt;/artifactId&gt;
            &lt;version&gt;0.12.6&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt;
            &lt;artifactId&gt;jjwt-impl&lt;/artifactId&gt;
            &lt;version&gt;0.12.6&lt;/version&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt;
            &lt;artifactId&gt;jjwt-jackson&lt;/artifactId&gt;
            &lt;version&gt;0.12.6&lt;/version&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.github.vladimir-bukhtoyarov&lt;/groupId&gt;
            &lt;artifactId&gt;bucket4j-core&lt;/artifactId&gt;
            &lt;version&gt;7.6.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.liquibase&lt;/groupId&gt;
            &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;
            &lt;version&gt;4.27.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.liquibase.ext&lt;/groupId&gt;
            &lt;artifactId&gt;liquibase-hibernate6&lt;/artifactId&gt;
            &lt;version&gt;4.27.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
            &lt;artifactId&gt;gson&lt;/artifactId&gt;
            &lt;version&gt;2.10.1&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;annotationProcessorPaths&gt;
                        &lt;path&gt;
                            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                            &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
                        &lt;/path&gt;

                        &lt;path&gt;
                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
                        &lt;/path&gt;
                    &lt;/annotationProcessorPaths&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;argLine&gt;-XX:+EnableDynamicAgentLoading&lt;/argLine&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.liquibase&lt;/groupId&gt;
                &lt;artifactId&gt;liquibase-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;4.27.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;propertyFile&gt;src/main/resources/liquibase.properties&lt;/propertyFile&gt;
                &lt;/configuration&gt;
                &lt;dependencies&gt;
                    &lt;dependency&gt;
                        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                        &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
                        &lt;version&gt;3.2.5&lt;/version&gt;
                    &lt;/dependency&gt;

                    &lt;dependency&gt;
                        &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
                        &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
                        &lt;version&gt;42.7.3&lt;/version&gt;
                    &lt;/dependency&gt;

                    &lt;dependency&gt;
                        &lt;groupId&gt;org.hibernate.orm&lt;/groupId&gt;
                        &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt;
                        &lt;version&gt;6.6.15.Final&lt;/version&gt;
                    &lt;/dependency&gt;

                    &lt;dependency&gt;
                        &lt;groupId&gt;jakarta.persistence&lt;/groupId&gt;
                        &lt;artifactId&gt;jakarta.persistence-api&lt;/artifactId&gt;
                        &lt;version&gt;3.1.0&lt;/version&gt;
                    &lt;/dependency&gt;

                    &lt;dependency&gt;
                        &lt;groupId&gt;org.liquibase.ext&lt;/groupId&gt;
                        &lt;artifactId&gt;liquibase-hibernate6&lt;/artifactId&gt;
                        &lt;version&gt;4.27.0&lt;/version&gt;
                    &lt;/dependency&gt;
                &lt;/dependencies&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;

    &lt;/build&gt;
&lt;/project&gt;
</code></pre>
<p>This is docker-compose.yaml</p>
<pre><code>version: '3.8'

services:
  postgres-primary:
    image: postgres:15
    container_name: postgres-primary
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_PRIMARY_DB}
    volumes:
      - pgdata-primary:/var/lib/postgresql/data
    ports:
      - &quot;5432:5432&quot;
    networks:
      - central-net

  postgres-secondary:
    image: postgres:15
    container_name: postgres-secondary
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_SECONDARY_DB}
    volumes:
      - pgdata-secondary:/var/lib/postgresql/data
    ports:
      - &quot;5433:5432&quot;
    networks:
      - central-net

  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
    volumes:
      - pgadmin-data:/var/lib/pgadmin
    ports:
      - &quot;8081:80&quot;
    networks:
      - central-net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
      - zookeeper-secrets:/etc/zookeeper/secrets
    ports:
      - &quot;2181:2181&quot;
    networks:
      - central-net

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
      - kafka-secrets:/etc/kafka/secrets
    ports:
      - &quot;9092:9092&quot;
    networks:
      - central-net

  backend-primary:
    build: ./backend-primary
    environment:
      SPRING_DATASOURCE_USERNAME: ${POSTGRES_USER}
      SPRING_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres-primary:5432/${POSTGRES_PRIMARY_DB}
      SSL_KEYSTORE_PASSWORD: ${SSL_KEYSTORE_PASSWORD}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      SMTP_USERNAME: ${SMTP_USERNAME}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
    volumes:
      - backend-primary-data:/app/data
    ports:
      - &quot;8443:8443&quot;
    depends_on:
      - postgres-primary
      - kafka
    networks:
      - central-net

  backend-secondary:
    build: ./backend-secondary
    environment:
      SPRING_DATASOURCE_USERNAME: ${POSTGRES_USER}
      SPRING_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres-secondary:5432/${POSTGRES_SECONDARY_DB}
      SSL_KEYSTORE_PASSWORD: ${SSL_KEYSTORE_PASSWORD}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      SMTP_USERNAME: ${SMTP_USERNAME}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
    volumes:
      - backend-secondary-data:/app/data
    ports:
      - &quot;8444:8443&quot;
    depends_on:
      - postgres-secondary
      - kafka
    networks:
      - central-net

  liquibase-primary:
    image: liquibase/liquibase
    container_name: liquibase-primary
    depends_on:
      - postgres-primary
    volumes:
      - ./liquibase/primary:/liquibase/changelog
    working_dir: /liquibase/changelog
    environment:
      LIQUIBASE_COMMAND_URL: jdbc:postgresql://postgres-primary:5432/${POSTGRES_PRIMARY_DB}
      LIQUIBASE_COMMAND_USERNAME: ${POSTGRES_USER}
      LIQUIBASE_COMMAND_PASSWORD: ${POSTGRES_PASSWORD}
    entrypoint: [ &quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot; ]
    networks:
      - central-net

networks:
  central-net:
    driver: bridge

volumes:
  pgdata-primary:
  pgdata-secondary:
  pgadmin-data:
  zookeeper-data:
  zookeeper-log:
  zookeeper-secrets:
  kafka-data:
  kafka-secrets:
  backend-primary-data:
  backend-secondary-data:
</code></pre>
",0,0,0,2025-07-11T13:04:55+00:00,1,197,False
79699931,1525425,,postgresql,Why is the PostgreSQL concat_ws function not immutable?,"<p>In PostgreSQL the function <a href=""https://www.postgresql.org/docs/17/functions-string.html"" rel=""nofollow noreferrer""><code>concat_ws</code></a> accepts only character, character varying, and text data types. Why is it not immutable?</p>
<p>An <a href=""https://www.postgresql.org/docs/9.1/functions-string.html"" rel=""nofollow noreferrer"">older documentation</a> has the note that the string functions previously accepted other data types converting them to string. In such a scenario it's clear that the function cannot be marked immutable because e.g. date conversion will differ on settings. But now it's only string data types.</p>
<p>I also found <a href=""https://www.postgresql.org/message-id/3361.1410026366%40sss.pgh.pa.us"" rel=""nofollow noreferrer"">this old post</a> which should no longer apply.
Could it be a backwards compability reason?</p>
<p>The function not being immutable means I can for example not use it in a generated column.</p>
",3,4,1,2025-07-13T11:43:54+00:00,2,177,True
79701104,1103606,,postgresql,"function to_timestamp(bytea, unknown) does not exist","<p>I have this PostgreSQL table:</p>
<pre><code>CREATE TABLE IF NOT EXISTS logs
(
    id bigint NOT NULL,
    inserted_date timestamp(6) with time zone NOT NULL,
    account_id character varying(255) COLLATE pg_catalog.&quot;default&quot;,
    CONSTRAINT logs_pkey PRIMARY KEY (id)
)
</code></pre>
<p>I want to use this query which is working with Oracle:</p>
<pre><code>select al from logs al where (al.insertedDate &gt;= to_timestamp(:startDate,'YYYY-MM-DD HH24:MI:SS'))) 
(:endDate is null or (al.insertedDate &lt;= to_timestamp(:endDate,'YYYY-MM-DD HH24:MI:SS')))) order by al.insertedDate asc&quot;)
</code></pre>
<p>I get error when I send empty startDate and endDate:</p>
<pre><code>ERROR: function to_timestamp(bytea, unknown) does not exist
  Hint: No function matches the given name and argument types. You might need to add explicit type casts.
  Position: 314
  Location: File: parse_func.c, Routine: ParseFuncOrColumn, Line: 629
  Server SQLState: 42883] [n/a]
</code></pre>
<p>Do you know how I can fix this?</p>
<pre><code>public interface LogRepository extends JpaRepository&lt;Log, Long&gt; {

  @Query(&quot;select al from logs al where (al.insertedDate &gt;= to_timestamp(:startDate,'YYYY-MM-DD HH24:MI:SS'))) 
    (:endDate is null or (al.insertedDate &lt;= to_timestamp(:endDate,'YYYY-MM-DD HH24:MI:SS')))) order by al.insertedDate asc&quot;)&quot;)
  List&lt;Log&gt; findByStartAndEndDate(.....@Param(&quot;startDate&quot;) String startDate, @Param(&quot;endDate&quot;) String endDate);

}
</code></pre>
",0,0,0,2025-07-14T15:47:05+00:00,1,81,False
79701246,2703209,,postgresql,Set-returning functions are not allowed in WHERE,"<p>I am accessing a PostgreSQL database form Java using JDBC.</p>
<p>I would like to return rows from a table <code>messages</code> where <code>id</code> matches one of multiple values, which are determined in Java code. The list of IDs to select is not anywhere in the database, and the number of IDs may vary (can be as many as 300).</p>
<p>I had this working with HSQLDB, where I would run a parametrized query</p>
<pre><code>select * from message where id in (unnest(?))
</code></pre>
<p>and populate the parameter with a <code>java.sql.Array</code> of <code>String</code> values matching the IDs I need.</p>
<p>With PostgreSQL I get an <code>org.postgresql.util.PSQLException</code> exception saying</p>
<blockquote>
<p>ERROR: set-returning functions are not allowed in WHERE</p>
</blockquote>
<p>What is the correct way to write this statement so it will work on PostgreSQL, preferably with minimal changes to the surrounding Java code?</p>
",2,2,0,2025-07-14T18:26:17+00:00,4,182,True
79701641,11528178,"Karachi, Pakistan",postgresql,Change of status from active to inactive in backend,"<p>In my application admin can toggle company, employees, products status from active to inactive. I simply keep status flag on all the entities and change it accordingly by saving a moderation log per toggle.</p>
<p>The problem came when the admin inactive the company then all of his employees should also gets inactive, and all of the products of those employees should also be inactive, How to handle this cascade type inactivation and then activation ?</p>
",-2,0,2,2025-07-15T06:17:19+00:00,1,93,True
79702140,1103606,,postgresql,"function to_timestamp(bytea, unknown) does not exist when null value is sent","<p>I have this JPQL query in Spring Boot with PostgreSQL:</p>
<pre><code>public interface LogRepository extends JpaRepository&lt;Log, Long&gt; {

  @Query(&quot;select al from Log al where &quot; +
      + &quot;((:startDate is null or (al.insertedDate &gt;= to_timestamp(nullif(:startDate,''),'YYYY-MM-DD HH24:MI:SS'))) and &quot;
      + &quot;(:endDate is null or (al.insertedDate &lt;= to_timestamp(nullif(:endDate,''),'YYYY-MM-DD HH24:MI:SS')))) &quot;
      + &quot;order by al.insertedDate asc&quot;)
  List&lt;AuditLog&gt; findByStartAndEndDate(@Param(&quot;startDate&quot;) String startDate, @Param(&quot;endDate&quot;) String endDate);
</code></pre>
<p>When I send empty startDate and endDate(null as value) I get error:</p>
<pre><code>ERROR: function to_timestamp(bytea, unknown) does not exist
  Hint: No function matches the given name and argument types. You might need to add explicit type casts.
  Position: 314
  Location: File: parse_func.c, Routine: ParseFuncOrColumn, Line: 629
  Server SQLState: 42883] [n/a]
</code></pre>
<p>How I can solve this issue? I want to be able to send empty value for dates?</p>
",0,0,0,2025-07-15T13:25:14+00:00,1,59,True
79702285,31047070,,postgresql,All MapStruct fields are encrypted with PasswordEncoder instead of only the password,"<p>I’m developing an application with <strong>Spring Boot</strong> and facing an issue using <strong>MapStruct</strong> to map a DTO (<code>RegisterRequest</code>) to an entity (<code>Usuario</code>) and save the data in a <strong>PostgreSQL</strong> database.</p>
<p>My goal is for <strong>only</strong> the <code>password</code> field to be encrypted with <strong>PasswordEncoder</strong> (using BCrypt), while the other fields (<code>firstName</code>, <code>lastName</code>, <code>email</code>, <code>role</code>) should be mapped directly from the DTO.</p>
<p>However, the implementation generated by <strong>MapStruct</strong> is encrypting <strong>all</strong> the fields, which causes a length error:<br />
<code>value too long for type character varying(30)</code></p>
<p>on the firstName column, which has a 30-character limit, while the generated hashes are approximately 60 characters long.</p>
<p><strong>What I want to do</strong></p>
<p>To avoid doing the mapping manually, I wanted to use <strong>MapStruct</strong> (as I had done before with other mappings) to delegate this work and reduce boilerplate code. The flow I needed was:</p>
<ul>
<li><p>Receive a POST request with a <code>RegisterRequest</code> containing <code>email</code>, <code>password</code>, <code>firstName</code>, and <code>lastName</code>.</p>
</li>
<li><p>Map these fields to a <code>Usuario</code> entity using <strong>MapStruct</strong>.</p>
</li>
<li><p>Encrypt only the <code>password</code> field using <strong>PasswordEncoder</strong>.</p>
</li>
<li><p>Assign a constant value <code>&quot;User&quot;</code> to the <code>role</code> field.</p>
</li>
<li><p>Save the <code>Usuario</code> in the database without any length errors.</p>
</li>
</ul>
<hr />
<p><strong>Relevant Code</strong></p>
<pre><code>@Mapper(componentModel = MappingConstants.ComponentModel.SPRING)
public interface UsuarioRegisterMapper {

    @Mapping(target = &quot;rol&quot;, constant = &quot;User&quot;)
    @Mapping(target = &quot;id&quot;, ignore = true)
    @Mapping(target = &quot;contrasena&quot;, expression = &quot;java(encodePassword(request.contrasena(), passwordEncoder))&quot;)
    Usuario toEntity(RegisterRequest request, @Context PasswordEncoder passwordEncoder);

    default String encodePassword(String rawPassword, @Context PasswordEncoder passwordEncoder) {
        return passwordEncoder.encode(rawPassword);
    }
}
</code></pre>
<p>the implementation automatically generated by MapStruct:</p>
<pre><code>@Generated(
    value = &quot;org.mapstruct.ap.MappingProcessor&quot;,
    date = &quot;2025-07-14T21:19:15-0600&quot;,
    comments = &quot;version: 1.5.5.Final, compiler: javac, environment: Java 23.0.2 (Oracle Corporation)&quot;
)
@Component
public class UsuarioRegisterMapperImpl implements UsuarioRegisterMapper {

    @Override
    public Usuario toEntity(RegisterRequest request, PasswordEncoder passwordEncoder) {
        if (request == null) {
            return null;
        }

        Usuario usuario = new Usuario();

        usuario.setNombre(encodePassword(request.nombre(), passwordEncoder));
        usuario.setApellido(encodePassword(request.apellido(), passwordEncoder));
        usuario.setEmail(encodePassword(request.email(), passwordEncoder));
        usuario.setRol(encodePassword(&quot;User&quot;, passwordEncoder));
        usuario.setContrasena(encodePassword(request.contrasena(), passwordEncoder));

        return usuario;
    }
}
</code></pre>
<pre><code>package com.proyectoUno.security.dto;

public record RegisterRequest(
        String email,
        String contrasena,
        String nombre,
        String apellido
) {}
</code></pre>
<pre><code>@Service
public class AuthService {
    // ... (injections)

    public AuthResponse register(RegisterRequest request) {
        if (usuarioRepository.findByEmail(request.email()).isPresent()) {
            throw new EntidadDuplicadaException(&quot;Email is already associated with an account&quot;, &quot;email&quot;, Collections.singletonList(request.email()));
        }

// This is where I use the mapper to skip the manual process of instantiating the class and calling .set for each value
        Usuario usuario = usuarioRegisterMapper.toEntity(request, this.passwordEncoder);

        System.out.println(&quot;Usuario before saving: &quot; + usuario);
        usuarioRepository.save(usuario);

        UserDetails userDetails = new CustomUserDetails(usuario);
        String jwtToken = jwtService.generateAccessToken(userDetails);
        String refreshToken = jwtService.generateRefreshToken(userDetails);
        return new AuthResponse(jwtToken, refreshToken);
    }
    // ...
}
</code></pre>
<p>Postman:</p>
<pre><code>{
  &quot;email&quot;: &quot;mario.rodriguez@gmail.com&quot;,
  &quot;contrasena&quot;: &quot;soyMario1050&quot;,
  &quot;nombre&quot;: &quot;Mario&quot;,
  &quot;apellido&quot;: &quot;Rodriguez&quot;
}
</code></pre>
<p>After sending the request from Postman, the bug appeared. To verify it was caused by the Mapper configuration, I did some quick debugging to check the mapper’s output.</p>
<pre><code>Usuario before saving: Usuario{id=null, 
  nombre='$2a$10$SKiTvYYJK.vZPetwOtY1OOBMCz6m15.bSUCZzk67Q..Ybs0h0n6nu', 
  apellido='$2a$10$h/PjGAv8aF7sGdCMo7jK/.CaHfcS.e1bHGIM28bb/RIYd/t1CL0jy', 
  email='$2a$10$ookM1PA26edcSu0mt4FnZegsvg/Cm3S0zdp5aRmfY/e1pcJ7TqT8K', 
  contrasena='$2a$10$aMIQJAo/pX7TEWhbKwtj/O0x/yuy8eqkfMVBfY7..fmnnmcwLo1e.', 
  rol='$2a$10$WV1kZvIx.j/UZfCSmcPMoOvOMFXXUM.qDKIarD5rIOLAHThULlUKK', 
  fechaRegistro=null, activo=false, prestamo=[]}
</code></pre>
<p>And that’s when the issue became clear: the mapper wasn’t just handling the <code>password</code> field — it was applying the default method to <strong>all</strong> fields, even though it was only supposed to be used for the <code>password</code>.</p>
<p><strong>What I’ve tried</strong></p>
<ul>
<li><p>I added explicit mappings in <code>UsuarioRegisterMapper</code> for <code>nombre</code>, <code>apellido</code>, and <code>email</code> (like <code>@Mapping(target = &quot;nombre&quot;, source = &quot;request.nombre&quot;)</code>), but the generated implementation still applies <code>encodePassword</code> to all fields.</p>
</li>
<li><p>I cleaned and recompiled the project with <code>mvn clean install</code>, but the problem persists.</p>
</li>
<li><p>I verified that the <code>usuario</code> table has the correct column lengths (e.g., <code>nombre</code> as <code>varchar(30)</code>), but the generated hashes exceed this limit.</p>
</li>
</ul>
<p><strong>Question</strong></p>
<ul>
<li><p>Why is MapStruct encrypting all fields with <code>PasswordEncoder</code> instead of limiting it to the <code>password</code> field?</p>
</li>
<li><p>How can I fix the configuration or implementation so that only <code>password</code> is encrypted and the other fields are mapped directly?</p>
</li>
<li><p>Is there a bug in MapStruct version 1.5.5.Final with Java 23.0.2?</p>
</li>
</ul>
<p>Any help or suggestions would be greatly appreciated. Thanks in advance.</p>
",1,1,0,2025-07-15T15:13:07+00:00,2,197,True
79702717,1742777,"Gaithersburg, MD. USA",postgresql,Group by query to calculate percentage,"<p>I have an PostgreSQL table called <code>my_table</code> that looks like this:</p>
<pre><code>       col_a |                         col_b |
-------------+-------------------------------+
           A | 2025-04-28 16:23:55.961 -0400 |
             | 2024-03-27 17:17:08.100 -0400 |
           C | 2024-03-27 18:57:23.194 -0500 |
           B | 2025-04-28 17:44:51.647 -0500 |
             | 2023-04-28 10:47:30.667 -0400 |
</code></pre>
<p>I want to find out for each day in <code>col_b</code>, what percentage of rows have <code>col_a=null</code></p>
<p>So the result should look like this:</p>
<pre><code>      b_date | percentage |
-------------+------------+
  2023-04-28 |         66 |
  2024-03-27 |         50 |
</code></pre>
<p>This is the query I tried, but it is not quite right:</p>
<pre><code>select 
    col_b::date as b_date, 
    (count(col_a=null) * 100)/count(*) as percentage
from my_table
group by b_date
order by b_date asc
</code></pre>
",2,2,0,2025-07-15T22:19:27+00:00,2,87,True
79702867,30988813,,postgresql,SELECT all columns except one in PostgreSQL,"<p>While learning, I thought of a use case where I might have to show all the columns and it's data except one particular column. For example I might want to hide the id column in a select query.</p>
<p>Is there a way where I can simply write <code>SELECT * (EXCEPT id) FROM table_name</code> instead of writing out all the column names and excluding the id column manually? Let's say I have a lot of columns.
I didn't find any solid current info on this, I saw SnowFlake allows the <code>EXCEPT</code> or <code>EXCLUDE</code> keyword. Is there any particular reason for that (if PostgreSQL doesn't allow)?</p>
",-1,1,2,2025-07-16T03:12:17+00:00,3,1036,True
79703388,6910789,,postgresql,use activerecord-import to upsert only when values have changed,"<p>How to use activerecord-import to upsert only when values have changed?
Body:
I'm using the activerecord-import gem in a Rails 6 app to bulk upsert data into a PostgreSQL database. Here's an example of how I'm using it:</p>
<pre class=""lang-rb prettyprint-override""><code>User.import(
  [:id, :name, :age],
  [
    [1, &quot;Alice&quot;, 30],
    [2, &quot;Bob&quot;, 25]
  ],
  on_duplicate_key_update: {
    conflict_target: [:id],
    columns: [:name, :age]
  }
)
</code></pre>
<p>This works as expected for inserting and updating based on id. However, I noticed that even if the values for a given row haven't changed, PostgreSQL still performs an UPDATE.
For performance reasons (especially with large datasets), I want to avoid unnecessary updates and only update if any of the fields have actually changed.</p>
<p>Is there a way to do this using activerecord-import? Ideally something like:</p>
<pre class=""lang-sql prettyprint-override""><code>WHERE users.name IS DISTINCT FROM EXCLUDED.name OR users.age IS DISTINCT FROM EXCLUDED.age
</code></pre>
<p>Can activerecord-import support this kind of conditional update? If so, what would the syntax look like?</p>
",0,0,0,2025-07-16T12:14:07+00:00,1,90,True
79703499,31055557,,postgresql,"Postgresql, Postgis, QGIS in container launched from charliecloud","<p>I need to migrate my work for geospatial processing (using mainly qgis processing and postgis functions from python scripts) to a HPC cluster. As neither qgis nor postgis are installed on the HPC I figured I need to setup those two in a container. For building and launching the containers I am limited to charliecloud as docker is not available on the HPC. Regarding containers I am an absolute beginner.</p>
<p>So far I have tried three approaches:</p>
<ol>
<li>Container from Qgis specific docker image and install Postgis</li>
<li>Container from Postgis specific docker image and install Qgis</li>
<li>Basic Ubuntu container and install Qgis and Postgis</li>
</ol>
<p>Option 1 and 2 did not work as the very specific docker images are not suited to run other applications. Option 3 works fine for qgis but when I try to install postgis using apt-get install -y postgresql I get the error messages:</p>
<pre><code>debconf: delaying package configuration, since apt-utils is not installed
dpkg: error: requested operation requires superuser privilege
E: Sub-process /usr/bin/dpkg returned an error code (2)
</code></pre>
<p>running the same command with sudo gives the following messages:</p>
<pre><code>sudo: /etc/sudo.conf is owned by uid 4288686, should be 0
sudo: The &quot;no new privileges&quot; flag is set, which prevents sudo from running as root.
sudo: If sudo is running in a container, you may need to adjust the container configuration to disable the flag.
</code></pre>
<p>I did some research on the &quot;no new privileges&quot; flag but only found solutions for docker, not for charliecloud.</p>
<p>I used a very basic dockerfile for the setup of the ubuntu container as I wanted to do the installations of qgis and postgis myself inside the container to be better able to trace the progress and potential errors.</p>
<pre><code>FROM ubuntu:24.04

ENV DEBIAN_FRONTEND=noninteractive
ENV LANG=C.UTF-8

RUN locale-gen en_US.UTF-8
ENV LANG=en_US.UTF-8
ENV LANGUAGE=en_US:en
ENV LC_ALL=en_US.UTF-8

RUN mkdir -p /dss /lrz /gpfs
</code></pre>
<p>The container was built using ch-image build --force -t image_name -f Dockerfile /path</p>
<p>I have installed postgis and qgis inside the container using</p>
<pre><code>apt-get update
apt-get install -y qgis 
apt-get update 
apt-get install -y postgresql 
</code></pre>
<p>Another option I was considering is setting up two containers, one specific for qgis and one specific for postgis. After doing some research I could just not figure out how to get one container so access the other using charliecloud. There is a lot of information available on using docker, but I just can't use it.</p>
<p>My prefered option is the setup of qgis and postgis within one container. As I'm a beginner with containers I'm open to any other suggestions and hints aswell.</p>
",0,0,0,2025-07-16T13:38:10+00:00,0,50,False
79704987,17131804,,postgresql,.NET - TestContainers many instance PostgreSql conflict,"<p>I'm writing integration tests in my .NET Web API. I'm using XUnit for this. I am using factory and collection. And everything works as expected. I run factory there I have used PostgreSql database using test container. I transfer the connection string to the application, where the migration starts. Everything goes through and it's great.</p>
<p>However, the moment I added a second factory and started up the PostgreSql factory again it still worked, but all of a sudden it started happening that when I run all the tests with a different factory it stumbles and the tests crash on the following error.</p>
<blockquote>
<p>Message: <br />
Npgsql.PostgresException : 23505: duplicate key value violates unique constraint &quot;pg_type_typname_nsp_index&quot;</p>
<p>DETAIL: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.</p>
</blockquote>
<p>Stack Trace:</p>
<pre><code>NpgsqlConnector.ReadMessageLong(Boolean async, DataRowLoadingMode dataRowLoadingMode, Boolean readingNotifications, Boolean isReadingPrependedMessage)
IValueTaskSource&lt;TResult&gt;.GetResult(Int16 token)
NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)
NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)
NpgsqlDataReader.NextResult()
NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)
NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)
NpgsqlCommand.ExecuteNonQuery(Boolean async, CancellationToken cancellationToken)
NpgsqlCommand.ExecuteNonQuery()
RelationalCommand.ExecuteNonQuery(RelationalCommandParameterObject parameterObject)
&lt;13 more frames...&gt;
WebApplicationFactory`1.CreateHost(IHostBuilder builder)
WebApplicationFactory`1.ConfigureHostBuilder(IHostBuilder hostBuilder)
WebApplicationFactory`1.EnsureServer()
WebApplicationFactory`1.CreateDefaultClient(DelegatingHandler[] handlers)
WebApplicationFactory`1.CreateDefaultClient(Uri baseAddress, DelegatingHandler[] handlers)
WebApplicationFactory`1.CreateClient(WebApplicationFactoryClientOptions options)
WebApplicationFactory`1.CreateClient()
</code></pre>
<p>It looks to me like the tests are connecting to the wrong database. I tried setting database names, ports etc but nothing helped. Of course the error doesn't pop up every time the tests run, but only occasionally. Unfortunately I couldn't find much information on the internet and it seems to me that I must be doing something extremely wrong.</p>
<p><code>FullSetupWriteFactory</code> (second factory &quot;Read&quot; looks the same)</p>
<pre><code>public class FullSetupWriteFactory : WebApplicationFactory&lt;Program&gt;, IAsyncLifetime
{
    private readonly PostgreSqlContainer _postgresContainer;

    public FullSetupWriteFactory()
    {
        var dbName = $&quot;db_{Guid.NewGuid():N}&quot;;

        _postgresContainer = new PostgreSqlBuilder()
            .WithDatabase(dbName)
            .Build();
    }

    protected override void ConfigureWebHost(IWebHostBuilder builder)
    {
        System.Environment.SetEnvironmentVariable(&quot;ConnectionStrings__DbConnection&quot;, _postgresContainer.GetConnectionString());

        builder.ConfigureServices(services =&gt;
        {
        });
    }

    public async Task InitializeAsync()
    {
        await _postgresContainer.StartAsync();
    }

    public new async Task DisposeAsync()
    {
        await _postgresContainer.StopAsync();
        await _postgresContainer.DisposeAsync();
    }
}
</code></pre>
<p><code>FullSetupWriteCollection</code>:</p>
<pre><code>[CollectionDefinition(Name)]
public class FullSetupWriteCollection : ICollectionFixture&lt;FullSetupWriteFactory&gt;
{
    public const string Name = &quot;FullSetupWrite&quot;;
}
</code></pre>
<p><code>OrderControllerTest</code>:</p>
<pre><code>[Collection(FullSetupWriteCollection.Name)]
public partial class OrderControllerTest
{
    Test1....
    Test2....
}
</code></pre>
<p><code>ItemControllerTest</code>:</p>
<pre><code>[Collection(FullSetupReadCollection.Name)]
public partial class OrderControllerTest
{
    Test1....
    Test2....
}
</code></pre>
<p>I run the migration in <code>Program.cs</code> with a simple command.</p>
<pre><code>_dbContext.Database.Migrate();
</code></pre>
<p>Thank you very much in advance</p>
",1,1,0,2025-07-17T14:40:26+00:00,1,114,True
79705221,4695280,,postgresql,Doctrine migration ignores UniqueConstraint,"<p>Tech stack: Symfony 7.3 + Doctrine 2 + PostgreSQL.</p>
<p>I would like to create unique index for two columns - <code>app_id</code> and <code>user_id</code> - for <code>user_mobile_apps</code> table.</p>
<p>Entity:</p>
<pre class=""lang-php prettyprint-override""><code>declare(strict_types=1);

namespace App\Entity;

use App\Repository\UserMobileAppRepository;
use Doctrine\DBAL\Types\Types;
use Doctrine\ORM\Mapping as ORM;

#[ORM\Entity(repositoryClass: UserMobileAppRepository::class)]
#[ORM\Table(name: '`user_mobile_apps`', uniqueConstraints: [
    new ORM\UniqueConstraint(name: 'uniq_app_user', columns: ['app_id', 'user_id'])
])]
class UserMobileApp
{
    use BlameableTimestampableTrait;

    #[ORM\Id]
    #[ORM\GeneratedValue]
    #[ORM\Column]
    private ?int $id = null;

    #[ORM\Column(name: &quot;app_id&quot;, type: Types::BIGINT)]
    private ?int $appId = null;

    #[ORM\Column(nullable: true)]
    private ?\DateTimeImmutable $mobileAppLastLogin = null;

    #[ORM\Column(options: ['default' =&gt; false])]
    private ?bool $mobileAppRegisteredInApp = null;

    #[ORM\Column(nullable: true, options: ['default' =&gt; false])]
    private ?bool $mobileAppPushMessagesAccepted = null;

    #[ORM\Column(length: 500, nullable: true)]
    private ?string $mobileAppPushToken = null;

    #[ORM\Column(length: 32, nullable: true)]
    private ?string $mobileAppStravaTempCode = null;

    #[ORM\ManyToOne(inversedBy: 'userMobileApp')]
    #[ORM\JoinColumn(name: &quot;user_id&quot;, nullable: false)]
    private ?User $user = null;

    public function getId(): ?int
    {
        return $this-&gt;id;
    }

    public function getAppId(): ?int
    {
        return $this-&gt;appId;
    }

    public function setAppId(int $appId): static
    {
        $this-&gt;appId = $appId;

        return $this;
    }

    public function getMobileAppLastLogin(): ?\DateTimeImmutable
    {
        return $this-&gt;mobileAppLastLogin;
    }

    public function setMobileAppLastLogin(?\DateTimeImmutable $mobileAppLastLogin): static
    {
        $this-&gt;mobileAppLastLogin = $mobileAppLastLogin;

        return $this;
    }

    public function isMobileAppRegisteredInApp(): ?bool
    {
        return $this-&gt;mobileAppRegisteredInApp;
    }

    public function setMobileAppRegisteredInApp(bool $mobileAppRegisteredInApp): static
    {
        $this-&gt;mobileAppRegisteredInApp = $mobileAppRegisteredInApp;

        return $this;
    }

    public function isMobileAppPushMessagesAccepted(): ?bool
    {
        return $this-&gt;mobileAppPushMessagesAccepted;
    }

    public function setMobileAppPushMessagesAccepted(bool $mobileAppPushMessagesAccepted): static
    {
        $this-&gt;mobileAppPushMessagesAccepted = $mobileAppPushMessagesAccepted;

        return $this;
    }

    public function getMobileAppPushToken(): ?string
    {
        return $this-&gt;mobileAppPushToken;
    }

    public function setMobileAppPushToken(?string $mobileAppPushToken): static
    {
        $this-&gt;mobileAppPushToken = $mobileAppPushToken;

        return $this;
    }

    public function getMobileAppStravaTempCode(): ?string
    {
        return $this-&gt;mobileAppStravaTempCode;
    }

    public function setMobileAppStravaTempCode(?string $mobileAppStravaTempCode): static
    {
        $this-&gt;mobileAppStravaTempCode = $mobileAppStravaTempCode;

        return $this;
    }

    public function getUser(): ?User
    {
        return $this-&gt;user;
    }

    public function setUser(?User $user): static
    {
        $this-&gt;user = $user;

        return $this;
    }
}
</code></pre>
<p>The problem is migration file doesn't contain SQL for unique index, when I execute <code>php bin/console make:migration</code> command.</p>
<p>What I've tried (nothing works):</p>
<pre class=""lang-php prettyprint-override""><code>#[ORM\Table(name: '`user_mobile_apps`', uniqueConstraints: [
    new ORM\UniqueConstraint(name: 'uniq_app_user', columns: ['app_id', 'user_id'])
])]
</code></pre>
<pre class=""lang-php prettyprint-override""><code>#[ORM\Table(name: '`user_mobile_apps`', uniqueConstraints: [
    new ORM\UniqueConstraint(name: 'uniq_app_user', fields: ['appId', 'user'])
])]
</code></pre>
<pre class=""lang-php prettyprint-override""><code>#[ORM\Table(name: '`user_mobile_apps`')]
#[ORM\UniqueConstraint(name: 'uniq_app_user', columns: ['app_id', 'user_id'])]
</code></pre>
<pre class=""lang-php prettyprint-override""><code>#[ORM\Table(name: '`user_mobile_apps`')]
#[ORM\UniqueConstraint(name: 'uniq_app_user', fields: ['appId', 'user'])]
</code></pre>
<pre class=""lang-php prettyprint-override""><code>#[ORM\Table(name: '`user_mobile_apps`')]
#[ORM\UniqueConstraint(name: 'uniq_app_user', columns: ['app_id', 'user_id'])]
#[UniqueEntity(fields: ['appId', 'user'])]
</code></pre>
<p>Also without <code>name</code> option for <code>UniqueConstraint</code>.</p>
<p>Before each attempt I called the commands:</p>
<pre class=""lang-bash prettyprint-override""><code>php bin/console doctrine:cache:clear-metadata
php bin/console cache:clear
</code></pre>
<p>Indexes, that are already defined for <code>user_mobile_apps</code> table:</p>
<p><a href=""https://i.sstatic.net/pBWkrnmf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBWkrnmf.png"" alt=""Indexes"" /></a></p>
",1,1,0,2025-07-17T17:44:40+00:00,2,107,False
79705615,4044009,,postgresql,Downloaded postgres fresh with Homebrew on Mac but cannot connect,"<p>I install postgres with homebrew and verify the postgres server is running:</p>
<pre><code>brew install postgresql@14
brew services restart postgresql@14
psql --version
-&gt; psql (PostgreSQL) 14.18 (Homebrew)
</code></pre>
<p>I try to connect to postgres via <code>psql</code> and it expects a password. I read online that it shouldn't need a password, so I just press enter. This doesn't work:</p>
<pre><code>&lt;MYUSER&gt;@Mac postgresql@14 % psql          
Password for user &lt;MYUSER&gt;: 
psql: error: connection to server on socket &quot;/tmp/.s.PGSQL.5432&quot; failed: fe_sendauth: no password supplied
</code></pre>
<ul>
<li>My user password doesn't work either</li>
<li>I also try to connect via <code>psql -U postgres</code> but that expects a password too which I do not know.</li>
</ul>
<p>My username does match my macOS username, so as far as I know: the peer authentication mechanism (connecting without a password) should work. I read that this setting is in the file <strong>pg_hba.conf</strong>, which on my computer is located at: <code>/opt/homebrew/var/postgresql@14/pg_hba.conf</code>. As far as I can tell, all these settings are as expected so there is nothing to change:</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD

# &quot;local&quot; is for Unix domain socket connections only
local   all             all                                     trust
# IPv4 local connections:
host    all             all             127.0.0.1/32            trust
</code></pre>
<p>I'm not sure what to try next. How do I connect to the postgres server? If that requires changing the password, I'm having trouble figuring out how to do that. I do see similar questions but they are very old.</p>
<p>Additional details:</p>
<ul>
<li>macOS version 15.5</li>
</ul>
",0,0,0,2025-07-18T02:42:07+00:00,1,145,True
79705869,9400754,,postgresql,PostgreSQL solution to work easier on a specific row with multiple columns,"<p>I have a table where I want to do some statistics on each rows different columns. For example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>NW2020</th>
<th>NW2021</th>
<th>NW2022</th>
<th>NW2023</th>
<th>NW2024</th>
<th>CA2020</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>-299</td>
<td>4000</td>
<td>21</td>
<td>-325</td>
<td>2544</td>
<td>55</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>54</td>
<td>244</td>
<td>-5</td>
<td>-54</td>
<td>325</td>
<td>874</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>How can I have a table with the frequency of negative values of each Net Worth?
Is there a way that I can ask for &quot;NW%&quot; columns? (a kind of GROUP BY but by columns and not by rows)</p>
<p>The only (UGLY) solution I had in mind was this one :</p>
<pre><code>SELECT *, 
((CASE NW2020 &lt; 0 THEN 1 ELSE 0 END) + (CASE WHEN NW2021 &lt; 0 THEN 1 ELSE 0 END) + (CASE WHEN NW2022 &lt; 0 THEN 1 ELSE 0 END) + ... )::double precision/
    (CASE WHEN NW2020 is not null THEN 1 ELSE 0 END) + (CASE WHEN NW2021 is not null THEN 1 ELSE 0 END) + (CASE WHEN NW2022  is not null THEN 1 ELSE 0 END) + ... )as freq_negative
FROM a
</code></pre>
<p>(I know <code>coalesce</code> but it doesn't short it enough for the amount of columns I have.</p>
<p>I had the same issue calculating the average and the standard deviation.</p>
<p>Is my DB and table organization bad?</p>
<p>Thanks in advance</p>
<p><strong>EDIT</strong> : The solution for future convenience was to implement Normalisation into my schema. See @richard-huxton 's comment.</p>
",0,0,0,2025-07-18T08:05:17+00:00,4,128,True
79707789,30999422,,postgresql,Postgres indexing fails in Django,"<p>Tried to set db_index=True, HashIndex and BrinIndex, nothing works, indexes by Seq Scan, there are 1000 records in the database, all migrations are completed. Model code:</p>
<pre><code>from django.db import models
from django.utils import timezone
from django.contrib.postgres.indexes import BrinIndex, HashIndex


class Contact(models.Model):
    phone = models.CharField(max_length=50, unique=True)
    address = models.CharField(max_length=50)

    def __str__(self):
        return self.phone


class Department(models.Model):
    name = models.CharField(max_length=255)
    description = models.TextField(null=True, blank=True)

    def __str__(self):
        return self.name


class Employee(models.Model):
    first_name = models.CharField(max_length=100)
    last_name = models.CharField(max_length=100)
    about = models.CharField(max_length=10000,db_index=True)
    age = models.SmallIntegerField(null=True)
    created = models.DateTimeField(default=timezone.now)
    work_experience = models.SmallIntegerField(default=0, null=True)
    contact = models.OneToOneField(Contact, on_delete=models.CASCADE, null=True)
    department = models.ForeignKey(Department, on_delete=models.CASCADE, default=None, null=True)

    class Meta:
        indexes = (
            BrinIndex(fields=('created',), name=&quot;hr_employee_created_ix&quot;,
                    pages_per_range=2
                    ),
        )

    def __str__(self):
        return f'{self.first_name} {self.last_name}'
</code></pre>
<p>I tried this filters:</p>
<pre><code>employees = Employee.objects.filter(created__year__lte=2022)
employees = Employee.objects.filter(about__contains='Test')
</code></pre>
",1,2,1,2025-07-20T06:46:36+00:00,1,73,True
79708564,3123056,,postgresql,Query takes too long if json column is included in select query of postgres,"<p>I have a table in a PostgreSQL database that includes a JSON column. When I run a SELECT query excluding this JSON column, the result is returned in under 5 seconds. However, including the JSON column in the selected columns causes the query to slow down significantly—sometimes taking 20 seconds or even over a minute.</p>
<p>The JSON column contains varying keys across rows and may include many nested objects within a single row. Importantly, this column is never used in WHERE conditions, but it is always included in the SELECT list, like:</p>
<p>SELECT id, asofdate, json_column, attributeid
FROM table_name
WHERE asofdate &gt; '2025-07-01';</p>
<p>How can I improve the performance of this query?</p>
",0,0,0,2025-07-21T05:02:47+00:00,1,126,True
79709133,9112151,,postgresql,Problem with multiple Query in view of FastAPI application endpoint,"<p>I'm trying to develop filtering/ordering/pagination functionality for FastAPI applications. For now I'm facing difficulty with separating filtering and sorting. The code below generates undesirable swagger:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI, Query
from pydantic import BaseModel
from sqlalchemy import Select, create_engine, MetaData, select
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column

app = FastAPI()

url = &quot;postgresql+psycopg2://postgres:postgres@localhost:5432/database&quot;
engine = create_engine(url, echo=True)


class Base(DeclarativeBase):
    metadata = MetaData()


class User(Base):
    __tablename__ = &quot;users&quot;
    id: Mapped[int] = mapped_column(primary_key=True)
    first_name: Mapped[str]
    last_name: Mapped[str]


class FilterSet(BaseModel):
    first_name: str | None

    def filter_queryset(self, query: Select) -&gt; Select:
        conditions = self.model_dump(exclude_unset=True)
        # apply ordering
        return query


class Ordering(BaseModel):
    ordering: list[str] | None = None

    def order_queryset(self, query: Select) -&gt; Select:
        self.ordering
        # apply ordering
        return query


@app.get(&quot;/&quot;)
async def index(filterset: FilterSet = Query(), ordering: Ordering = Query()):
    query = select(User)
    query = filterset.filter_queryset(query)
    query = ordering.order_queryset(query)
    # request database
</code></pre>
<p>The bad swagger:</p>
<p><a href=""https://i.sstatic.net/TMDrQCJj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TMDrQCJj.png"" alt=""enter image description here"" /></a></p>
<p>Is it possible to fix it without combining FilterSet and Ordering classes into a single class?</p>
",0,0,0,2025-07-21T13:21:51+00:00,1,69,True
79709658,31104263,,postgresql,Postgresql MCP server not running. Tool showing error,"<p>Here is the code I am running for mcp-server on cursor -</p>
<pre><code>from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass

import asyncpg
from mcp.server.fastmcp import FastMCP, Context

# Async PostgreSQL wrapper using asyncpg
class Database:
    def __init__(self, pool: asyncpg.Pool):
        self.pool = pool

    @classmethod
    async def connect(cls) -&gt; &quot;Database&quot;:
        pool = await asyncpg.create_pool(
            user=&quot;postgres&quot;,
            password=&quot;rohan&quot;,
            database=&quot;company&quot;,
            host=&quot;localhost&quot;,
            port=5432,
        )
        return cls(pool)

    async def disconnect(self):
        await self.pool.close()

    async def query(self, query: str) -&gt; list:
        async with self.pool.acquire() as conn:
            try:
                return await conn.fetch(query)
            except Exception as e:
                print(f&quot;Query error: {e}&quot;)
                return []

    async def fetch_schema(self) -&gt; list:
        query = &quot;&quot;&quot;
        SELECT table_name FROM information_schema.tables
        WHERE table_schema = 'public'
        &quot;&quot;&quot;
        return await self.query(query)


@dataclass
class AppContext:
    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&gt; AsyncIterator[AppContext]:
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        await db.disconnect()


mcp = FastMCP(&quot;PostgresMCPServer&quot;, lifespan=app_lifespan)

@mcp.tool(&quot;fetch_schema&quot;)
async def fetch_schema(ctx: Context) -&gt; str:
    db = ctx.request_context.lifespan_context.db
    schema = await db.fetch_schema()
    return str([record[&quot;table_name&quot;] for record in schema])


@mcp.tool(&quot;fetch_all_tables&quot;)
async def fetch_all_tables(ctx: Context) -&gt; str:
    db = ctx.request_context.lifespan_context.db
    query = &quot;SELECT * FROM information_schema.tables WHERE table_schema='public'&quot;
    tables = await db.query(query)
    return str(tables)


@mcp.tool(&quot;run_query&quot;)
async def run_query(ctx: Context, query: str) -&gt; str:
    db = ctx.request_context.lifespan_context.db
    result = await db.query(query)
    return str(result)

if __name__ == &quot;__main__&quot;:
    # Initialize and run the server
    mcp.run()
</code></pre>
<p>Here are the libraries i have installed and they are showing up on pyproject.toml file as well -</p>
<pre><code>mcp
httpx
mcp[cli]
asyncpg
</code></pre>
<p>Here is how i am adding the mcp to use as a tool -</p>
<pre><code>&quot;PostgresMCPServer&quot;: {
      &quot;command&quot;: &quot;uv&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--with&quot;,
        &quot;mcp[cli]&quot;,
        &quot;mcp&quot;,
        &quot;run&quot;,
        &quot;C:/mylocalpath/server.py&quot;
      ]
    }
</code></pre>
<p>When I run the server.py, the tool shows error on cursor as well as claude desktop. Here are some error logs from claude -</p>
<pre><code>2025-07-21T12:05:43.760Z [PostgresMCPServer] [info] Initializing server... { metadata: undefined }
2025-07-21T12:05:43.801Z [PostgresMCPServer] [info] Using MCP server command: C:\Users\Rohan\AppData\Local\com.Langflow\uv\uv.exe with args and path: {
  metadata: {
    args: [
      'run',
      '--with',
      'mcp[cli]',
      'mcp',
      'run',
      'C:/Users/Rohan/Desktop/MCP_newProjects/postgress-mcp/server.py',
      [length]: 6
    ],
    paths: [
      'C:\\Program Files\\nodejs',
      'C:\\Python312\\Scripts\\',
      'C:\\Python312\\',
      'C:\\WINDOWS\\system32',
      'C:\\WINDOWS',
      'C:\\WINDOWS\\System32\\Wbem',
      'C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\',
      'C:\\WINDOWS\\System32\\OpenSSH\\',
      'C:\\Program Files\\nodejs\\',
      'C:\\ProgramData\\chocolatey\\bin',
      'C:\\Terraform',
      'C:\\Program Files\\Amazon\\AWSCLIV2\\',
      'C:\\Program Files\\PuTTY\\',
      'C:\\Program Files\\Git\\cmd',
      'C:\\ProgramData\\miniconda3\\Library\\bin\\conda.bat',
      'C:\\ProgramData\\miniconda3\\Scripts\\conda.exe',
      'C:\\ProgramData\\miniconda3\\condabin\\conda.bat',
      'C:\\ProgramData\\miniconda3',
      'C:\\ProgramData\\miniconda3\\Library\\bin',
      'C:\\ProgramData\\miniconda3\\Scripts',
      '',
      'c:\\Users\\Rohan\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin',
      'c:\\Users\\Rohan\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin',
      'C:\\Users\\Rohan\\AppData\\Local\\com.Langflow\\uv',
      'C:\\Users\\Rohan\\.local\\bin',
      'C:\\Users\\Rohan\\AppData\\Local\\Microsoft\\WindowsApps',
      'C:\\Users\\Rohan\\AppData\\Roaming\\npm',
      'C:\\Users\\Rohan\\AppData\\Local\\Programs\\Microsoft VS Code\\bin',
      'C:\\Terraform',
      'C:\\ProgramData\\miniconda3\\Library\\bin\\conda.bat',
      'C:\\ProgramData\\miniconda3\\Scripts\\conda.exe',
      'C:\\ProgramData\\miniconda3\\condabin\\conda.bat',
      'C:\\ProgramData\\miniconda3',
      'C:\\ProgramData\\miniconda3\\Library\\bin',
      'C:\\ProgramData\\miniconda3\\Scripts',
      'C:\\Users\\Rohan\\AppData\\Local\\Programs\\Ollama',
      'C:\\Users\\Rohan\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin',
      [length]: 37
    ]
  }
} %o
2025-07-21T12:05:43.812Z [PostgresMCPServer] [info] Server started and connected successfully { metadata: undefined }
2025-07-21T12:05:43.861Z [PostgresMCPServer] [info] Message from client: {&quot;method&quot;:&quot;initialize&quot;,&quot;params&quot;:{&quot;protocolVersion&quot;:&quot;2025-06-18&quot;,&quot;capabilities&quot;:{},&quot;clientInfo&quot;:{&quot;name&quot;:&quot;claude-ai&quot;,&quot;version&quot;:&quot;0.1.0&quot;}},&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;id&quot;:0} { metadata: undefined }
Failed to run server
Traceback (most recent call last):
  File &quot;C:\Users\Rohan\AppData\Local\uv\cache\archive-v0\PQBi79xgW5oSnoZKAozUC\Lib\site-packages\mcp\cli\cli.py&quot;, line 343, in run
    server = _import_server(file, server_object)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Rohan\AppData\Local\uv\cache\archive-v0\PQBi79xgW5oSnoZKAozUC\Lib\site-packages\mcp\cli\cli.py&quot;, line 141, in _import_server
    spec.loader.exec_module(module)
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 999, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 488, in _call_with_frames_removed
  File &quot;C:\Users\Rohan\Desktop\MCP_newProjects\postgress-mcp\server.py&quot;, line 5, in &lt;module&gt;
    import asyncpg
ModuleNotFoundError: No module named 'asyncpg'
2025-07-21T12:05:48.536Z [PostgresMCPServer] [info] Server transport closed { metadata: undefined }
2025-07-21T12:05:48.537Z [PostgresMCPServer] [info] Client transport closed { metadata: undefined }
2025-07-21T12:05:48.538Z [PostgresMCPServer] [info] Server transport closed unexpectedly, this is likely due to the process exiting early. If you are developing this MCP server you can add output to stderr (i.e. `console.error('...')` in JavaScript, `print('...', file=sys.stderr)` in python) and it will appear in this log. { metadata: undefined }
2025-07-21T12:05:48.539Z [PostgresMCPServer] [error] Server disconnected. For troubleshooting guidance, please visit our [debugging documentation](https://modelcontextprotocol.io/docs/tools/debugging) { metadata: { context: 'connection', stack: undefined } }
2025-07-21T12:05:48.541Z [PostgresMCPServer] [info] Client transport closed { metadata: undefined }
</code></pre>
<p>In the error log, I can see issue with asyncpg import but its already installed and there no error on IDE as well, only when i try to add it as mcp tool. Here is pyproject.toml file -</p>
<pre><code>[project]
name = &quot;postgress-mcp&quot;
version = &quot;0.1.0&quot;
description = &quot;Add your description here&quot;
readme = &quot;README.md&quot;
requires-python = &quot;&gt;=3.12&quot;
dependencies = [
    &quot;asyncpg&gt;=0.30.0&quot;,
    &quot;httpx&gt;=0.28.1&quot;,
    &quot;mcp[cli]&gt;=1.12.0&quot;,
]
</code></pre>
<p>Can someone please help in identifying the mistake.</p>
<p>I ran the code in server.py and expected that the mcp tool would work but instead its showing connection error everywhere.</p>
",2,2,0,2025-07-21T21:31:42+00:00,0,329,False
79709712,2060899,"Belo Horizonte, Brazil",postgresql,503 &quot;Slowdown&quot; error when using Supabase — has anyone experienced this?,"<p>I'm facing a recurring issue with my application that uses Supabase as the backend.</p>
<p>From time to time, I get the following error when making requests to the Supabase API:</p>
<pre><code>503 Service Unavailable — Slowdown
</code></pre>
<p>This happens intermittently, even with what I believe is a low volume of requests. I've already checked the following:</p>
<p>No functions are running in infinite loops or being triggered excessively.</p>
<p>The app does not seem to be hitting the rate limits of the free plan.</p>
<p>The database doesn't show consistent high CPU or memory usage in the logs.</p>
<p>I’d like to know:</p>
<p>Has anyone else experienced this specific &quot;slowdown&quot; error with Supabase?</p>
<p>Is there a reliable way to diagnose what’s causing this?</p>
<p>Any recommendations on how to mitigate it? Should I implement retries with exponential backoff, or would upgrading the plan be a better path?</p>
<p>Appreciate any insights or shared experiences that could help!</p>
",0,0,0,2025-07-21T23:20:08+00:00,1,159,True
79711835,402281,,postgresql,Stateless REST API to poll new records,"<p>I'm working on this application that records operations performed within the application as <em>events</em>. Each operation is recorded as a separate row in the table <code>events</code>. The row is written in the same serializable database transaction that's also performing the operation.</p>
<p>I would like to add an API that allows polling this table in a way that returns exactly the rows that have been added since the last call. The API should be stateless, meaning that the server does not need to keep track of which rows have already been returned. Instead, the client should pass some information (cookie/cursor) in the request that allows the server to figure out which rows are new. I'm having trouble figuring out a scheme that works reliably.</p>
<p>This is what I currently have:</p>
<p>The database schema currently looks like this (reduced example):</p>
<pre class=""lang-sql prettyprint-override""><code>create table events (
    id int generated by default as identity primary key,
    message varchar,
    timestamp timestamptz
);
</code></pre>
<p>And rows are inserted like this. The ID is generated automatically by the sequence object attached to the <code>id</code> column:</p>
<pre class=""lang-sql prettyprint-override""><code>insert into events (message, timestamp) values ('hello', now());
</code></pre>
<p>The API allows passing an <code>after_id</code> parameter, which limits the result to events with an ID larger than the passed value.</p>
<pre class=""lang-none prettyprint-override""><code>// GET /v1/events?after_id=123
{
  &quot;events&quot;: [
    {
      id: 124,
      message: &quot;...&quot;,
      timestamp: &quot;...&quot;,
    },
    // ...
  ]
}
</code></pre>
<p>The problem with this is that even though the ID of each row is generated by a sequence object, the database doesn't guarantee that transactions will commit in the order of the IDs that they got from the sequence. For example:</p>
<ul>
<li>Transaction 1 starts.</li>
<li>Transaction 1 gets ID 125 from the sequence.</li>
<li>Transaction 2 starts.</li>
<li>Transaction 2 gets ID 126 from the sequence.</li>
<li>Transaction 2 commits.</li>
<li>API is accessed with <code>after_id=124</code> and returns the row with ID 126.</li>
<li>Transaction 1 commits.</li>
<li>API is accessed with <code>after_id=126</code> and returns no rows.</li>
</ul>
<p>In this example, the row with ID 125 has silently been skipped.</p>
<p><strong>What schemes exist to solve this problem?</strong></p>
<p>I could of course force all transaction to run in sequence, e.g. by locking the <code>events</code> table, but that would impact performance by quite a lot, as often there are dozens of such transactions running in parallel.</p>
",1,1,0,2025-07-23T11:55:44+00:00,1,77,True
79711840,31123713,,postgresql,"Postgres Pro 14.4 pg_proaudit 1.1: DML triggers not created for all_dml,table rules in syslog/csvlog modes","<p>I’m running PostgreSQL Pro 14.4 with the pg_proaudit 1.1 extension on Astra Linux. I’ve configured auditing of all DDL and DML and redirected audit output to syslog (rsyslog listening on local0 tag AUDIT). DDL auditing works, but no DML triggers are ever created on new or existing tables, so INSERT/UPDATE/DELETE never appear in the audit log.</p>
<p>PostgreSQL Pro Enterprise 14.4, compiled with gcc 8.3.0 on Astra Linux SE</p>
<p>Extension version: 1.1 (SELECT extversion FROM pg_extension WHERE extname='pg_proaudit';)</p>
<p>pg_proaudit was configured as follows:</p>
<ol>
<li><p>Reset any existing audit rules</p>
<pre><code>SELECT pg_proaudit_reset();
</code></pre>
</li>
<li><p>Enable auditing of all DDL at the database level</p>
<pre><code>SELECT pg_proaudit_set_object('all_ddl','database');
</code></pre>
</li>
<li><p>Enable auditing of all DML at the table level</p>
<pre><code>SELECT pg_proaudit_set_object('all_dml','table');
</code></pre>
</li>
<li><p>Persist your new rules</p>
<pre><code>SELECT pg_proaudit_save();
</code></pre>
</li>
<li><p>Reload the config so the rules take effect immediately</p>
<pre><code>SELECT pg_proaudit_reload();
</code></pre>
</li>
<li><p>Verify that your rules are active</p>
<pre><code>SELECT * FROM pg_proaudit_show();
</code></pre>
<p>The expected output:</p>
<pre class=""lang-none prettyprint-override""><code> db_name | event_type | object_type | object_oid | role_name
---------+------------+-------------+------------+----------
  *      | all_ddl    | database    |          0 | *
  *      | all_dml    | table       |          0 | *
</code></pre>
</li>
<li><p>Create a test table to trigger the row‐level trigger creation</p>
<pre><code>CREATE TABLE audit_test(id int);
</code></pre>
</li>
<li><p>Check that the pg_proaudit row‐triggers were created</p>
<pre><code>SELECT tgname, tgrelid::regclass
FROM pg_trigger
WHERE tgname LIKE 'audit\_%';
</code></pre>
<p>Expected: one or more rows, e.g. <code>audit_row_ddl</code> and <code>audit_row_dml</code> on <code>audit_test</code></p>
</li>
<li><p>Perform some DML to generate audit entries</p>
<pre><code>INSERT INTO audit_test VALUES (1);

UPDATE audit_test SET id = 2;

DELETE FROM audit_test;
</code></pre>
</li>
</ol>
<p>This is how I checked if anything was logged:</p>
<pre class=""lang-bash prettyprint-override""><code># tail the syslog
tail -f /var/log/pg_proaudit.log
journalctl -f -t AUDIT
</code></pre>
",0,0,0,2025-07-23T12:01:12+00:00,0,50,False
79711921,10424501,,postgresql,Using a script variable to generate a stored procedure in Postgresql,"<p>I would like to use a script variable to set the data type, so that I can generate a few functions with different types, like so:</p>
<p><code>\set bucket_data_type DATE</code></p>
<p>Then run a script to generate this function:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION insert(
    ids BIGINT[],
    types TEXT[],
    buckets :bucket_data_type[]
) RETURNS VOID
LANGUAGE PLPGSQL
AS $$
BEGIN
   EXECUTE 
    FORMAT('INSERT INTO myTable(id, type, buckets)
    SELECT _.id, _.type, _.bucket
    FROM(
        SELECT unnest(%L::bigint[]) AS monitor_id,
               unnest(%L::text[]) AS feature_type,
               unnest(%L::'|| :bucket_data_type ||'[]) AS bucket
        ) _
        ON CONFLICT DO NOTHING;',
           ids, types, buckets);
END
$$;
</code></pre>
<p>and then again with <code>\set bucket_data_type TIMESTAMP</code>, for example.</p>
<p>It works for the parameter definitions, but <em>not</em> the SQL inside the FORMAT block.</p>
<p>I am expecting something like</p>
<pre><code>\set bucket_data_type DATE

&lt;run create script as per above&gt;
</code></pre>
<p>to return</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION insert(
    ids BIGINT[],
    types TEXT[],
    buckets DATE[]
) RETURNS VOID
LANGUAGE PLPGSQL
AS $$
BEGIN
   EXECUTE 
    FORMAT('INSERT INTO myTable(id, type, buckets)
    SELECT _.id, _.type, _.bucket
    FROM(
        SELECT unnest(%L::bigint[]) AS monitor_id,
               unnest(%L::text[]) AS feature_type,
               unnest(%L::DATE[]) AS bucket
        ) _
        ON CONFLICT DO NOTHING;',
           ids, types, buckets);
END
$$;
</code></pre>
<p>but I'm unable to get <code>unnest(%L::DATE[]) AS bucket</code> to appear.</p>
<p>I either get format errors like:</p>
<pre><code>ERROR:  syntax error at or near &quot;:&quot;
LINE 15:                unnest(%L::'|| :bucket_data_type ||'[]) AS bu...
</code></pre>
<p>or I simply get <code>:bucket_data_type</code> as a string inside the execute block.</p>
<p>I've tried <code>SELECT set_config('tsp.bucket_data_type', :'bucket_data_type', false);</code> as well (as this works in DO BLOCKS where things are string literals).</p>
<p>Is this even possible?</p>
",3,3,0,2025-07-23T12:55:56+00:00,3,120,True
79712336,31127060,,postgresql,How do I preserve data integrity with a linking table in PostgreSQL?,"<p>I have these tables in my PostgreSQL database:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE companies (
    id INT PRIMARY KEY,
    name VARCHAR(255) NOT NULL UNIQUE
);

-- Employees belong to one company
CREATE TABLE employees (
    id INT PRIMARY KEY,
    companyId INT NOT NULL,
    employeeName VARCHAR(255) NOT NULL,
    FOREIGN KEY (companyId) REFERENCES companies(id)
);

-- Projects Belong to one company
CREATE TABLE projects (
    id INT PRIMARY KEY,
    companyId INT NOT NULL,
    projectName VARCHAR(255) NOT NULL,
    FOREIGN KEY (companyId) REFERENCES companies(id)
);
</code></pre>
<p>Now I need to create a table that links an employee to a project:</p>
<pre class=""lang-sql prettyprint-override""><code>-- The linking table (with the problem)
CREATE TABLE assignments (
    id INT PRIMARY KEY,
    employeeId INT NOT NULL,
    projectId INT NOT NULL,
    FOREIGN KEY (employeeId) REFERENCES employees(id),
    FOREIGN KEY (projectId) REFERENCES projects(id)
);
</code></pre>
<p>The issue is that an employee from one company, can be linked to a project from another company, which I do not want.</p>
<p>How do I enforce the employee and project to be from the same company?</p>
",0,0,0,2025-07-23T17:08:45+00:00,1,84,True
79712364,9819342,,postgresql,See INSERT/UPDATE rows in table from only current uncommitted transaction,"<p>In postgres, is there a way to see what rows in a table are from the current, uncommitted transaction only (without knowing specifics like key values, etc.)? Basically imagine doing some work on a table and then wanting to ask &quot;before I commit, let me see what the changes are?&quot;</p>
",0,0,0,2025-07-23T17:39:40+00:00,1,64,False
79712748,1102726,"New York, NY, United States",postgresql,dotnet ef doesn&#39;t seem to be using password provided by UseNpgsql with Mac Postgres.app,"<p>I have a web application:</p>
<pre class=""lang-cs prettyprint-override""><code>var builder = WebApplication.CreateBuilder(args);

// Add services to the container.

builder.Services.AddControllersWithViews();

builder.Services.AddDbContext&lt;MyApplicationDbContext&gt;(options =&gt; 
    options.UseNpgsql(builder.Configuration.GetConnectionString(&quot;PSQLConnection&quot;))
);
</code></pre>
<p>I have a design-time DB context factory:</p>
<pre class=""lang-cs prettyprint-override""><code>public class MyApplicationDbContextFactory : Microsoft.EntityFrameworkCore.Design.IDesignTimeDbContextFactory&lt;MyApplicationDbContext&gt;
{
    public MyApplicationDbContext CreateDbContext(string[] args = null)
    {
        var configuration = new ConfigurationBuilder()
            .SetBasePath(Directory.GetCurrentDirectory())
            .AddJsonFile(&quot;appsettings.Development.json&quot;)
            .Build();

        var optionsBuilder = new DbContextOptionsBuilder&lt;MyApplicationDbContext&gt;();
        var connectionString = configuration.GetConnectionString(&quot;PSQLConnection&quot;);
        optionsBuilder.UseNpgsql(connectionString);

        return new MyApplicationDbContext(optionsBuilder.Options);
    }
}
</code></pre>
<p>I had allowed connection from VS Code without a password in Postgres.app. I ran <code>dotnet ef migrations add InitialCreate</code> and that seemed to have worked, but then <code>dotnet ef database update</code> failed with <code>42501: permission denied for schema public</code>. I noticed intentionally putting the wrong password in the connection string while troubleshooting didn't change the error message, so I turned off allowing connections with no password. I'm now getting <code>Postgres.app rejected &quot;trust&quot; authentication</code>.</p>
<p>My database is pretty empty. All I'd done is:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE DATABASE MyApplication;
CREATE USER MyApplication WITH ENCRYPTED PASSWORD 'password';
GRANT ALL PERMISSIONS ON DATABASE MyApplication TO MyApplication;
</code></pre>
<p>I would like the EF migration to happen with the user in the connection string. How do I get this to work?</p>
",1,1,0,2025-07-24T02:56:54+00:00,1,68,True
79713003,5714031,,postgresql,Perl and Postgresl table name with dash,"<p>This line works fine under linux :</p>
<pre><code>PGPASSWORD=***** psql --username=**** -h ***** -d **** -p **** -P pager=off -t --csv -c &quot;select \&quot;Nom\&quot;, \&quot;Path\&quot; from \&quot;vVMware-Vms\&quot; where \&quot;Date\&quot; = '2025-07-22';&quot;
</code></pre>
<p>When I try to embed it in a perl script, it outputs me an error. Tried many separators (single-quote, double-quote) without success.</p>
<p>In my perl script, I create my string wih this line (but tried many other things) :</p>
<pre><code>$Cmd = qq(PGPASSWORD=**** psql --username=**** -h ***** -d ***** -p **** -P pager=off -t --csv -c &quot;select \&quot;Nom\&quot;, \&quot;Path\&quot; from \&quot;vVMware-Vms\&quot; where \&quot;Date\&quot; = '2025-07-22';&quot;);
</code></pre>
<p>When printing it (with Dumper), I got :</p>
<pre><code>$VAR1 = 'PGPASSWORD=**** psql --username=**** -h **** -d **** -p **** -P pager=off -t --csv -c &quot;select &quot;Nom&quot;, &quot;Path&quot; from &quot;vVMware-Vms&quot; where &quot;Date&quot; = \'2025-07-22\';&quot;';
</code></pre>
<p>And when running, I got this message :</p>
<pre><code>ERROR:  syntax error at or near &quot;-&quot;
LIGNE 1 : select Nom, Path from vVMware-Vms where Date = '2025-07-22';
                                       ^
</code></pre>
<p>And of course, I cannot change the table name and don't want to use DBI module.</p>
<p>Any clue ?</p>
",0,1,1,2025-07-24T08:22:24+00:00,2,135,True
79713329,99904,,postgresql,Sort respecting diacritics (PostgreSQL),"<p>Can I get PostgreSQL to sort rows by a string column respecting the accents?</p>
<p>I found out that it's possible to define a custom collation having &quot;ks&quot; (colStrength) set to &quot;level2&quot;, which would mean that it's accent-sensitive.</p>
<p>However, when I try to actually sort using that collation, the order seem to be accent-insensitive.</p>
<p>There is an <a href=""https://peter.eisentraut.org/blog/2023/05/16/overview-of-icu-collation-settings#colstrength"" rel=""nofollow noreferrer"">extensive blog post</a> about this by a PostgreSQL developer, let's use the same ICU locale) like so:</p>
<pre><code>CREATE TABLE test (string text);
INSERT INTO test VALUES ('bar'), ('bat'), ('bär');
CREATE COLLATION &quot;und1&quot; (provider = icu, deterministic = false, locale = 'und-u-ks-level1');
CREATE COLLATION &quot;und2&quot; (provider = icu, deterministic = false, locale = 'und-u-ks-level2');
CREATE COLLATION &quot;und3&quot; (provider = icu, deterministic = false, locale = 'und-u-ks-level3');
SELECT * FROM test ORDER BY string collate &quot;und1&quot;;
SELECT * FROM test ORDER BY string collate &quot;und2&quot;;
SELECT * FROM test ORDER BY string collate &quot;und3&quot;;
</code></pre>
<p>All three collations give me the same order: <code>bar</code> &lt; <code>bär</code> &lt; <code>bat</code>, although an accent-sensitive order would be <code>bar</code> &lt; <code>bat</code> &lt; <code>bär</code></p>
<p>Do I misunderstand the collation capabilities? Is there a way to get an accent-sensitive order?</p>
<p>Also, is there a way to see what options are there for the default built-in collations? I don't see, for example, the used &quot;ks&quot; level in the <code>pg_collation</code> table data.</p>
",4,4,0,2025-07-24T12:11:41+00:00,2,129,True
79713632,22851958,,postgresql,Is it possible to set default values for custom parameters in a Heroku database?,"<p>My application uses local parameters extensively. I'd like to set defaults for these, by running e.g. <code>ALTER DATABASE &lt;NAME&gt; SET &quot;app.role&quot; = '';</code>. This works fine in my local database, but when I attempt to run that in my Heroku-hosted database the query fails with the message <code>permission denied to set parameter &quot;app.extra_permissions&quot;</code>.</p>
<p>Is it possible to do this, or does Heroku lock users out of setting defaults for custom parameters?</p>
",0,0,0,2025-07-24T15:47:19+00:00,0,36,False
79714471,26834782,,postgresql,Efficient way to partially match stringified JSON in a string column using Prisma (without full JSON match),"<p>I'm trying to find a record where a column (payload) stores a stringified JSON object. The column type is <strong>string</strong> (as defined in Prisma schema) — so it's not a JSON or JSONB type.</p>
<p>I want to check if a subset of the JSON content exists in the string, without matching the entire object.</p>
<p>Here's a sample value stored in the payload column:</p>
<pre><code>{
  &quot;triggerEvent&quot;: &quot;RESERVATION_EXPIRED&quot;,
  &quot;id&quot;: 16,
  &quot;eventTypeId&quot;: 3,
  &quot;userId&quot;: 4,
  &quot;slotUtcStartDate&quot;: &quot;2025-07-25T03:30:00.000Z&quot;,
  &quot;slotUtcEndDate&quot;: &quot;2025-07-25T04:00:00.000Z&quot;,
  &quot;uid&quot;: &quot;014cbb69-fa4b-421b-8c6d-af0ac7f4184e&quot;,
  &quot;releaseAt&quot;: &quot;2025-07-24T20:56:33.000Z&quot;,
  &quot;isSeat&quot;: false
}

</code></pre>
<p>I'm only interested in this part of the object:</p>
<pre><code>&quot;eventTypeId&quot;: 3,
&quot;userId&quot;: 4,
&quot;slotUtcStartDate&quot;: &quot;2025-07-25T03:30:00.000Z&quot;,
&quot;slotUtcEndDate&quot;: &quot;2025-07-25T04:00:00.000Z&quot;,
&quot;uid&quot;: &quot;014cbb69-fa4b-421b-8c6d-af0ac7f4184e&quot;
</code></pre>
<p>Here’s my current Prisma-based query:</p>
<pre><code>const { id, releaseAt, isSeat, ...rest } = slot;
const restString = JSON.stringify(rest);
const searchString = restString.slice(1, -1); // remove outer {}

const isWebhookScheduledTriggerExists = await prisma.webhookScheduledTriggers.findFirst({
  where: {
    payload: {
      contains: searchString,
    },
  },
});
</code></pre>
<h3>The Problem:</h3>
<p>I was told this approach is inefficient and not production-safe.</p>
<h3>Constraint:</h3>
<p>I cannot convert the column to JSON/JSONB or change its type — it must remain a string.</p>
<h3>My Question:</h3>
<p>Is there a more efficient or reliable way to match a partial stringified JSON object in a string column using Prisma or raw SQL?</p>
",0,0,0,2025-07-25T08:59:56+00:00,1,102,False
79714539,19485612,,postgresql,How to set default &#39;&#39; to varchar field when creating a table,"<p>When I create a table as below in dbeaver:</p>
<pre class=""lang-sql prettyprint-override""><code>create table test 
(
    id varchar(36) not null primary key,
    key varchar(120) not null default ''
);
</code></pre>
<p>Then view table ddl of test, it shows field 'key' with default value : NULL::character varying.</p>
<p>Use sql to check:</p>
<pre class=""lang-sql prettyprint-override""><code>select column_name, column_default, is_nullable 
from information_schema.columns
where table_name = 'test'
</code></pre>
<p>It also shows field 'key' column_default NULL::character varying.</p>
",0,0,0,2025-07-25T09:54:31+00:00,2,126,False
79714668,21642986,,postgresql,Problem with INSERT and UPDATE Trigger in case of INSERT ON CONFLICT in Postgres,"<p>I have a problem regarding ON INSERT BEFORE triggers firing on rows that are inserted via <em>insert on conflict update</em> upserts.</p>
<p>I have a postgres (v.17) database with a base table or super table that 5 other tables have a one-to-one relation to or &quot;extend&quot; (a sort of class table inheritance). So for every row in one of this four tables there must be a corresponding row in the base table.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE super_table (
   id BIGINT  GENERATED ALWAYS AS IDENTITY PRIMARY KEY
   -- other fields
);

CREATE TABLE sub_table1(
    id BIGINT  GENERATED ALWAYS AS IDENTITY PRIMAR KEY,
    super_id BIGINT REFERENCES super_table UNIQUE
   -- other fields
);
-- other sub_tables ...
</code></pre>
<p>This class table inheritance warrants that several rules are fullfilled:</p>
<ul>
<li>every row in a subtable always has a corresponding row in the super table</li>
<li>for a given row in a subtable the super_table_id always stays the same</li>
<li>if a row in a subtable is deleted the corresponding row in the super table is deleted</li>
<li>... and some more</li>
</ul>
<p>Relevant with regard to my problem are the first two rules. To ensure that they are fullfilled I use two triggers. An on insert and and on update trigger.</p>
<pre class=""lang-sql prettyprint-override""><code>On every of the 5 tables an ON INSERT trigger creates a new row in the base table and returns the id:
IF NEW.super_id IS NULL THEN
    INSERT INTO super_table (type, created_at, updated_at, is_deletable)
    VALUES ('some_type', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, false)
    RETURNING id INTO NEW.super_id;
END IF;
RETURN NEW;

</code></pre>
<p>Correspondingly there's an ON UPDATE Trigger that ensures that no matter what base_id is always set to NEW.super_id = OLD.super_id to guarantee that there will be only one row in the super_table in any of the subtables.</p>
<h1>Problem:</h1>
<p>My problem arises when I do Batch inserts with ON CONFLICT:
That leads to both triggers firing, also on rows that are updated. The reason is that a before insert trigger will fire during insert on conflict operations before a conflict arises if a unique constraint is violated and postgres trys an update.
This behaviour leads to a situation where for all inserts that resolve to updates the On insert trigger fires and creates a new row in the super table. Then the conflict arises and the operatio is aborted and postgres tries an update. That leads to this new super_table_id not inserted in the row in the subtable. Instead the on update trigger then ensures that the OLD.super_table_id is used. So integrity is preserved and for updated rows the super_table_id stays the same but a lot of usless rows are created in the super_table.</p>
<p>I thought about adding a new field <code>super_table_id_created BOOLEAN</code> to every subtable and make <code>super_table_id NULL</code> so that I could change the ON INSERT trigger to AFTER which would  ensure that it fires during INSERT ON CONFLICT operations only after a conflict arises and thus only for rows that are actually inserted not updated. The on update trigger could then use this field.</p>
<p>Does anyone know a solution that either:</p>
<ul>
<li>allows me to maintain the super_table and subtables without triggers?</li>
<li>or uses a differen approach to ensure integrity on a database level?</li>
</ul>
",1,1,0,2025-07-25T11:44:59+00:00,1,186,True
79715514,23305548,,postgresql,TypeORM save() overwrites nullable fields with null in NestJS — how to prevent this?,"<p>I’m working on a NestJS project using TypeORM. I have an entity with some nullable columns, for example:</p>
<pre><code>@Entity()
export class User {
  @PrimaryGeneratedColumn()
  id: number;

  @Column({ nullable: true })
  profilePicture?: string;

  @Column()
  name: string;
}

</code></pre>
<p>When I partially update a User and call repository.save(), the nullable fields that are not included in the update object are being set to null. For example:</p>
<pre><code>await userRepository.save({
  id: 1,
  name: 'Updated Name'
});

</code></pre>
<p>After this, the profilePicture column becomes NULL in the database, even though I didn’t provide it in the update object.</p>
<p>I know one solution is to use update() instead of save(), like this:</p>
<pre><code>await userRepository.update(1, { name: 'Updated Name' });
</code></pre>
<p>But my project is large and already uses save() in many places, so changing all of them is not practical.</p>
<p>My question:
How can I prevent save() from overwriting nullable fields with null when they are not included in the update object? Is there a configuration option or recommended pattern for this in TypeORM?</p>
",0,0,0,2025-07-26T08:11:19+00:00,0,74,False
79715575,22010759,,postgresql,Why do I get a &quot;FATAL: password authentication failed for user &quot;${DB_USERNAME}&quot;&quot; error in my application YAML file using GitHub Actions secrets?,"<p>I have a workflow which looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>on: push

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      DB_NAME: ${{ secrets.DB_NAME }}
      DB_USERNAME: ${{ secrets.DB_USERNAME }}
      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up JDK 24
        uses: actions/setup-java@v4
        with:
          java-version: '24'
          distribution: 'temurin'
          cache: maven
      - name: Build with Maven
        run: mvn clean install
</code></pre>
<p>Also I have an <code>application.yml</code> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>spring:
  application:
    name: application

  datasource:
    url: jdbc:postgresql://localhost:5432/${DB_NAME}
    username: ${DB_USERNAME}
    password: ${DB_PASSWORD}

  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
</code></pre>
<p>When I run <code>mvn clean install</code> the execution fails with the following message:</p>
<blockquote>
<p>Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user &quot;${DB_USERNAME}&quot;</p>
</blockquote>
<p>Therefore, I tried passing the variables using the <code>-D</code> option and it works fine <em>locally</em>:</p>
<pre class=""lang-bash prettyprint-override""><code>mvn clean install -DDB_NAME=dbname -DDB_USERNAME=dbuser -DDB_PASSWORD=pass
</code></pre>
<p>But when I try to run it like that:</p>
<pre class=""lang-yaml prettyprint-override""><code>...
  - name: Build with Maven
    run: mvn clean install -DDB_NAME=${{ secrets.DB_NAME }} -DDB_USERNAME=${{ secrets.DB_USERNAME }} -DDB_PASSWORD=${{ secrets.DB_PASSWORD }}
</code></pre>
<p>I still get the same error.</p>
<p>The secrets on GitHub are set correctly.<br />
Spring version - 6.2.8.</p>
",0,1,1,2025-07-26T09:48:37+00:00,2,146,False
79717133,24883464,,postgresql,How to add a stream image to the database using CommunityToolkit MVVM,"<p>Please help me figure out how to add an image to the database.
By clicking the &quot;TakePhoto_Tapped&quot; button with a plus, I add a photo from the camera to AvataView, but by clicking the &quot;Next&quot; button, I need to save this photo to the database. I think this screenshot will make it clearer. The screenshot corresponds to the code.
<a href=""https://i.sstatic.net/Wid9ZRuw.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><strong>XAML</strong></p>
<pre><code>`&lt;toolkit:AvatarView HeightRequest=&quot;400&quot;
                                            WidthRequest=&quot;280&quot;
                                            Background=&quot;#F9F9F9&quot;
                                            x:Name=&quot;currenPhoto&quot;
                                            ImageSource=&quot;{Binding Photoprofile}&quot;&gt;
                            
                            &lt;Grid&gt;
                                &lt;Image Source=&quot;add.png&quot; HeightRequest=&quot;26&quot;&gt;
                                    &lt;Image.GestureRecognizers&gt;
                                        &lt;TapGestureRecognizer Tapped=&quot;TakePhoto_Tapped&quot;/&gt;
                                    &lt;/Image.GestureRecognizers&gt;
                                &lt;/Image&gt;
                            &lt;/Grid&gt;

                        &lt;/toolkit:AvatarView&gt;`
</code></pre>
<p><strong>Method for working with the stream:</strong></p>
<pre><code>private async void TakePhoto_Tapped(object sender, EventArgs e)
{
FileResult photo = await MediaPicker.Default.CapturePhotoAsync(new           MediaPickerOptions  
        {
            Title = &quot;Select your photo&quot;
        }); 

        if (photo != null)
        {
            var stream = await photo.OpenReadAsync();
            currenPhoto.ImageSource = ImageSource.FromStream(() =&gt;  stream);

        }
    }
</code></pre>
<p><strong>This is my view model:</strong></p>
<pre><code>public partial class VMSteptree : ObservableObject
    {
        [ObservableProperty]
        private byte _photoprofile;

        [RelayCommand]

        public static void AddAsync()
        {
           DbMeeto dba = new();

           Auchman auchman = new()
           {
              Photoprofile = new()
           };

            dba.Auchmans.Add(auchman);
        }
    }
</code></pre>
<p><strong>Command tied to the button:</strong></p>
<pre><code> &lt;Button Text=&quot;Далее&quot;
         TextColor=&quot;White&quot;
         FontSize=&quot;Body&quot;
         CornerRadius=&quot;20&quot;
         FontAttributes=&quot;Bold&quot;
         x:Name=&quot;Gotostepfor&quot;
         Background=&quot;#4C25D9&quot;
         Clicked=&quot;Gotostepfor_Clicked&quot;
         Command=&quot;{Binding AddAsyncCommand}&quot;&gt;

           &lt;Button.Triggers&gt;
              &lt;EventTrigger Event=&quot;&quot;&gt;
                   &lt;aninmbtn:BtnAnimation/&gt;
               &lt;/EventTrigger&gt;
           &lt;/Button.Triggers&gt;

 &lt;/Button&gt;
</code></pre>
<p><strong>My expectation:</strong>
I expect that the image downloaded from the stream will be transferred to the vie model through data binding and then saved to it through the command on the button.</p>
<p><strong>But what's really going on:</strong>
When the command is executed, no record is created in the database. I want to add that i use Postgres. And I also connected the necessary packages and directives using and the rest of the data is saved as it should, except for the images.</p>
",-1,0,1,2025-07-28T09:43:59+00:00,2,138,True
79717514,14728414,,postgresql,Flink SQL Job: com.starrocks.data.load.stream.exception.StreamLoadFailException: Could not get load state because,"<p>I'm encountering a Flink job failure and would appreciate any input on what might be misconfigured:</p>
<pre><code>2025‑07‑28 17:30:52
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
...
Caused by: java.lang.Exception: Failed to abort transactions with label postgres_test‑test‑0‑1
...
Caused by: com.starrocks.data.load.stream.exception.StreamLoadFailException: Could not get load state because of incorrect response status code 404, label: postgres_test‑test‑0‑1, response body: &lt;HTML&gt;&lt;HEAD&gt;
&lt;TITLE&gt;404 Not Found&lt;/TITLE&gt;
&lt;/HEAD&gt;&lt;BODY&gt;
&lt;H1&gt;Not Found&lt;/H1&gt;
&lt;/BODY&gt;&lt;/HTML&gt;
</code></pre>
<p>Context:</p>
<blockquote>
<p>Checkout project repository: <a href=""https://github.com/iman-sandbox/flink-postgres-to-starrocks"" rel=""nofollow noreferrer"">https://github.com/iman-sandbox/flink-postgres-to-starrocks</a></p>
</blockquote>
<blockquote>
<p>Flink CDC pipeline reading from Postgres and writing to StarRocks with exactly-once semantics.</p>
</blockquote>
<blockquote>
<p>The sink label prefix is postgres_test.</p>
</blockquote>
<blockquote>
<p>Errors occur during transaction abort during job restart.</p>
</blockquote>
<p>Details:</p>
<blockquote>
<p>Flink version: 1.17.2</p>
</blockquote>
<blockquote>
<p>Flink connector version: 1.2.10</p>
</blockquote>
<blockquote>
<p>StarRocks version: 3.5.2</p>
</blockquote>
<blockquote>
<p>Sink config includes sink.label-prefix='postgres_test', sink.wait-for-continue.timeout-</p>
</blockquote>
<blockquote>
<p>ms='60000', and semantic exactly-once.</p>
</blockquote>
<p><strong>Questions:</strong></p>
<blockquote>
<p>What does the Recovery is suppressed by NoRestartBackoffTimeStrategy indicate in my context is it due to missing restart strategy or disabled checkpointing?</p>
</blockquote>
<blockquote>
<p>Why might the sink aborter fail to retrieve the load state (404)? Could this be caused by misconfigured sink properties or missing endpoints?</p>
</blockquote>
<blockquote>
<p>Any recommendations or config tweaks (Flink or StarRocks) to ensure cleanup of lingering transactions and successful restart?
Is there any docker image to use simply for my use case [Real-time data syncing from Postgres to Starrocks by FlinkSQL]</p>
</blockquote>
<p>Here is Apache Flink Jobmanager dashboard screenshot:
<a href=""https://i.sstatic.net/6AcHwYBM.png"" rel=""nofollow noreferrer"">Apache Flink Jobmanager dashboard</a></p>
<p>Here is Apache Flink Jobmanager Logs:</p>
<pre><code>Caused by: com.starrocks.data.load.stream.exception.StreamLoadFailException: Could not get load state because of incorrect response status code 404, label: postgres_test-test-0-1, response body: &lt;HTML&gt;&lt;HEAD&gt;
&lt;TITLE&gt;404 Not Found&lt;/TITLE&gt;
&lt;/HEAD&gt;&lt;BODY&gt;
&lt;H1&gt;Not Found&lt;/H1&gt;
&lt;/BODY&gt;&lt;/HTML&gt;

    at com.starrocks.data.load.stream.DefaultStreamLoader.getLabelState(DefaultStreamLoader.java:471)
    at com.starrocks.data.load.stream.DefaultStreamLoader.getLoadStatus(DefaultStreamLoader.java:444)
    at com.starrocks.connector.flink.table.sink.LingeringTransactionAborter.tryAbortTransaction(LingeringTransactionAborter.java:172)
    ... 17 more
2025-07-28 14:00:52,104 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 07781516ad3e3fbe257d044cab1baf2d has been registered for cleanup in the JobResultStore after reaching a terminal state.
2025-07-28 14:00:52,107 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'insert-into_default_catalog.default_database.starrocks_test' (07781516ad3e3fbe257d044cab1baf2d).
2025-07-28 14:00:52,109 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2025-07-28 14:00:52,109 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor 172.18.0.5:43843-1eeb2e because: Stopping JobMaster for job 'insert-into_default_catalog.default_database.starrocks_test' (07781516ad3e3fbe257d044cab1baf2d).
2025-07-28 14:00:52,110 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [06fab36829286332376141a41e5e864c].
2025-07-28 14:00:52,111 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 5b0f69b6fc769dadd140c550530829aa: Stopping JobMaster for job 'insert-into_default_catalog.default_database.starrocks_test' (07781516ad3e3fbe257d044cab1baf2d).
2025-07-28 14:00:52,112 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 07781516ad3e3fbe257d044cab1baf2d from the resource manager.
</code></pre>
<p>Any help or pointers would be great.</p>
",0,0,0,2025-07-28T14:57:17+00:00,0,76,False
79717954,1525311,,postgresql,support for PGvector in datanucleus,"<p>I can find very little information on support for <a href=""https://github.com/pgvector/pgvector-java"" rel=""nofollow noreferrer"">PGvector</a> (and the corresponding <a href=""https://github.com/pgvector/pgvector"" rel=""nofollow noreferrer"">vector type in postgresql</a>) in <strong>datanucleus</strong>. It seems like no one is working on supporting this new &quot;datastore type&quot; in the postgresql rdbms plugin.</p>
<p>I do not want to map the java PGvector to a supported type (e.g. <code>float[]</code>), but want it to remain the <code>vector</code> type in the db table, so a <em>custom mapping</em> does not seem the answer. Nor do i want to make my own (new) <em>datastore adapter</em>. However, it seems like my only hope is to write a new datastore adapter that <em>extends the existing</em> postgresql adapter.</p>
<p>Are there any suggestions or existing solutions I have missed?</p>
",1,1,0,2025-07-28T22:49:16+00:00,1,53,True
79718323,18588777,,postgresql,How to generate CockroachDB-like unique_rowid() in PostgreSQL?,"<p>I was using CockroachDB previously, where I used <code>unique_rowid()</code> as my primary key, which generates a 64-bit timestamp-based unique ID (18-digit, time-sortable).</p>
<p>I've now migrated to PostgreSQL and want to replicate the same behavior—i.e., generate a primary key in PostgreSQL that:</p>
<ul>
<li>Is a 64-bit <code>BIGINT</code></li>
<li>Is time-sortable (newer entries have larger IDs)</li>
<li>Is unique even under high-concurrency</li>
</ul>
<p>What’s the best way to replicate <code>unique_rowid()</code> in PostgreSQL directly in the database.</p>
",1,1,0,2025-07-29T08:40:52+00:00,1,121,False
79718500,1130020,,postgresql,Need a regex_match checks if word doesn&#39;t starts with letters R or W and contains 4 letters and 3 numbers,"<p>I need a match if in my text column I can find a word that contains 4 characters followed with 3 digits, and the word did not started with the letter R or W (if possible on Postgres).</p>
<p>I have made already a regexp_match, but it doesn't figured out what i need.</p>
<p>Like :</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>column</th>
<th>should be returned</th>
</tr>
</thead>
<tbody>
<tr>
<td>Thomas Hawk AQWS456</td>
<td>OK</td>
</tr>
<tr>
<td>Cecile Star RQWS456</td>
<td>KO</td>
</tr>
<tr>
<td>Mickey Mouse WQWS456</td>
<td>KO</td>
</tr>
<tr>
<td>Donald Duck SQWS456</td>
<td>OK</td>
</tr>
</tbody>
</table></div>
<p>Here's my regex code : <code>(?:[A-Za-z]{3,4}\d{3})</code></p>
<p><a href=""https://regex101.com/r/0DYIDs/2"" rel=""nofollow noreferrer"">https://regex101.com/r/0DYIDs/2</a></p>
",0,2,2,2025-07-29T11:11:59+00:00,3,222,True
79718607,1777221,,postgresql,Terminating connection due to conflict with recovery,"<p>I understand the theory behind &quot;Conflict with recovery&quot; errors and the various <a href=""https://stackoverflow.com/questions/14592436/postgresql-error-canceling-statement-due-to-conflict-with-recovery?rq=2"">PG configs to address them</a>.
The error sometimes shows up as &quot;Cancelling Statement due to conflict with recovery&quot; but most times shows up as &quot;terminating connection due to conflict with recovery&quot;.</p>
<p>In which situation does Postgres throw &quot;Cancelling a statement&quot; vs &quot;Terminating Connection&quot;?
My code is able to handle &quot;Cancelled statement&quot; errors fairly gracefully but the &quot;Terminating Connection&quot; errors show up as ConnectionDoesNotExistError on the client app and are subsequently causing SQLAlchemy Garbage Collection errors.</p>
",0,0,0,2025-07-29T12:30:32+00:00,1,100,False
79718709,26576605,,postgresql,I don&#39;t understand why my test database is not being cleared and how to fix it,"<p>I'm trying to write integration tests for my FastAPI registration endpoint using SQLModel and an async PostgreSQL test database but after succesful register database don't cleaned up even i have</p>
<pre><code>await conn.run_sync(SQLModel.metadata.drop_all) 
</code></pre>
<p>My current code</p>
<p>conftest.py</p>
<pre><code>import pytest
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlmodel import SQLModel
from httpx import AsyncClient
from main import app
from backend.app.database.database import get_session

TEST_DATABASE_URL = &quot;postgresql+asyncpg://test_user:test_pass@localhost:5432/test_db&quot;

engine_test = create_async_engine(TEST_DATABASE_URL, echo=True)
TestingSessionLocal = async_sessionmaker(
    bind=engine_test, expire_on_commit=False, class_=AsyncSession
)


async def override_get_session():
    async with TestingSessionLocal() as db:
        yield db


app.dependency_overrides[get_session] = override_get_session


@pytest.fixture(scope=&quot;session&quot;)
async def setup_database():
    async with engine_test.begin() as conn:
        await conn.run_sync(SQLModel.metadata.drop_all)  # Очищаем после тестов

        await conn.run_sync(SQLModel.metadata.create_all)  # Создаем таблицы
    yield
    async with engine_test.begin() as conn:
        await conn.run_sync(SQLModel.metadata.drop_all)  # Очищаем после тестов


@pytest.fixture(scope=&quot;module&quot;)
async def client():
    async with AsyncClient(app=app, base_url=&quot;http://test&quot;) as c:
        yield c


@pytest.fixture(scope=&quot;session&quot;)
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

</code></pre>
<p>test_auth.py</p>
<pre><code>import pytest
from httpx import AsyncClient


@pytest.mark.asyncio
async def test_register_user(
    client: AsyncClient,
):
    response = await client.post(
        &quot;/api/v01/register/&quot;,
        data={
            &quot;username&quot;: &quot;testusername&quot;,
            &quot;email&quot;: &quot;test@example.com&quot;,
            &quot;password&quot;: &quot;strongpassword&quot;,
            &quot;role&quot;: &quot;reader&quot;,
        },
    )

    assert response.status_code == 302
    assert response.headers[&quot;location&quot;] == &quot;/profile/&quot;

</code></pre>
<p>I'm using Python 3.8.10.</p>
<p>I tried many different options with chatgpt but it's all time don't work or giving me errors like
&quot;RuntimeError: Task &lt;Task pending name='Task-4' coro=&lt;test_register_user() running at c:\ &quot;</p>
",-1,0,1,2025-07-29T13:37:57+00:00,0,166,False
79718748,28146417,,postgresql,How to disable sequence,"<p>I need to disable a sequence on a table, insert some values and re-enable.
I'm searching for the Postgres equivalent of</p>
<pre><code>set identity_insert dbo.myTable ON/OFF
</code></pre>
<p>as I can do it in SQL Server.</p>
<p>I have a table myTable on schema pippo in Postgres database. I need to disable the sequence on the column &quot;id&quot;, insert some rows, then re-enable the sequence (identity)</p>
<pre><code>CREATE TABLE test.product ( id int4 GENERATED ALWAYS AS IDENTITY( INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 START 893 CACHE 1 NO CYCLE) NOT NULL, id_prod uuid NULL, prod_type varchar(255) NULL
</code></pre>
<p>I need to insert some values from a tool that cannot use the clause OVERRIDING SYSTEM VALUE</p>
",2,2,0,2025-07-29T14:03:51+00:00,1,128,True
79718887,2071807,"London, United Kingdom",postgresql,"Filter Django RangeField by comparing to a point, not to another range","<p>The <a href=""https://docs.djangoproject.com/en/5.2/ref/contrib/postgres/fields/#comparison-functions"" rel=""nofollow noreferrer"">PostgreSQL specific model fields</a> docs are very specific about how to compare one <code>RangeField</code> to another range. But how do you compare a range to a single point?</p>
<p>For example, if I've got a model with <code>valid_range=DateTimeRangeField</code>, and I want to find all instances which are no longer valid, I need to do something like:</p>
<pre class=""lang-py prettyprint-override""><code>from django.utils import timezone as tz

MyProduct.objects.filter(valid_range__lt=tz.now())
</code></pre>
<p>But this isn't allowed. I thought I could use <code>fully_lt</code> but that's not allowed with a particular date either.</p>
<p>How do I filter a <code>DateTimeRangeField</code> to find instances whose ranges ended before a certain datetime?</p>
",0,0,0,2025-07-29T15:42:37+00:00,1,115,False
79719154,23023095,,postgresql,Why am I getting a return of false instead of true on the ts_query for this string,"<p>I'm learning about full-text search in postgres using ts_vector &amp; ts_query, by analyzing texts in strings.</p>
<p>But I'm getting a false return but instead I should be getting a true output. Here is my code:</p>
<pre><code>db_two=# select
db_two-# to_tsvector(
db_two(# 'I ate a lot of ice cream today.'
db_two(# ) @@ to_tsquery('eat') result;
 result 
--------
 f
(1 row)

db_two=#
</code></pre>
<p>Am I missing something here, I'm still new to postgres</p>
",2,2,0,2025-07-29T19:44:19+00:00,1,66,True
79719836,6523288,,postgresql,What is the intended usage of hypersistence-utils JsonBinaryType,"<p>Suppose I have the following table definition in postgres</p>
<pre class=""lang-sql prettyprint-override""><code>create table foo_entity
(
    id                varchar(255) not null primary key,
    jsonb_column      jsonb
);
</code></pre>
<p>I am trying to modify the table using a native query via entity manager like so</p>
<pre class=""lang-java prettyprint-override""><code>EntityManager em; // its obtained somehow
List&lt;String&gt; strings = List.of(&quot;foo&quot;, &quot;bar&quot;);
Query q = em.createNativeQuery(&quot;update foo_entity set jsonb_colimn = :value&quot;);
q.setParameter(&quot;value&quot;, new TypedParameterValue&lt;&gt;(
   new JsonBinaryType(new ObjectMapper()),
   strings
));
q.executeUpdate();
</code></pre>
<p>But I am getting an error that the provided value was of type <code>bytea</code> rather than the intended <code>jsonb</code>.</p>
<pre class=""lang-none prettyprint-override""><code>2025-07-30T13:53:09.547+03:00 DEBUG 13275 --- [demo1] [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : JDBC exception executing SQL [        update foo_entity set jsonb_column = ?
] [n/a]

org.postgresql.util.PSQLException: ERROR: column &quot;jsonb_column&quot; is of type jsonb but expression is of type bytea
  Hint: You will need to rewrite or cast the expression.
  Position: 46
</code></pre>
<p>Meanwhile if I provide a string scalar into the value, rather than an array type I get a different error</p>
<pre class=""lang-none prettyprint-override""><code>2025-07-30T13:54:57.683+03:00 DEBUG 13526 --- [demo1] [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : JDBC exception executing SQL [        update foo_entity set jsonb_column = ?
] [n/a]

org.postgresql.util.PSQLException: ERROR: column &quot;jsonb_column&quot; is of type jsonb but expression is of type character varying
  Hint: You will need to rewrite or cast the expression.
  Position: 46
</code></pre>
<p>How do I ensure that the resulting value is properly written as <code>jsonb</code> type?</p>
<p>At the time of writing i'm using Hibernate 6.6 and Hypersistence-utils 63 3.10.1</p>
",1,1,0,2025-07-30T10:57:49+00:00,1,125,True
79720297,7613649,"Belgrade, Serbia",postgresql,Cursor pagination with conditional sorting,"<p>I need to implement <a href=""https://stackoverflow.com/a/70520457/7613649"">keyset pagination</a> (sometimes referenced as cursor-based or seek method) in Postgres with the following ordering rules:</p>
<ol>
<li>Rows where <code>group_name = :priorityGroup</code> come first (<code>:priorityGroup</code> is a dynamic parameter).</li>
<li>Then sort by <code>name</code>, then by <code>id</code> (<code>id</code> is an unique key).</li>
</ol>
<p>Here, the cursor is a reference to the last retrieved row, indicating the position from which the next page should be fetched.</p>
<p>Previously, a query without conditional sorting worked well:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT id, name, group_name 
FROM record
WHERE (name, id) &gt; (:cursorName, :cursorId)
ORDER BY name, id
LIMIT :limit
</code></pre>
<p>Latency was low, because the index on (<code>name</code>, <code>id</code>) was used by both <code>WHERE</code> and <code>ORDER</code>. <br />
Now, the requirement is to prioritize a specific group (<code>group_name = :priorityGroup</code>)</p>
<p>Details:</p>
<ol>
<li>Millions of rows. If index is not used, latency is low</li>
<li>Keyset pagination only (e.g. limit/offset or server-side cursors (<code>DECLARE … CURSOR</code>) are not options)</li>
<li>PostgreSQL 15</li>
</ol>
<h4>The test cases</h4>
<p>They could be useful to test suggestions.</p>
<p>DDL:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE record (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    group_name VARCHAR(255) NOT NULL
);
CREATE INDEX name_and_id_idx ON record (name, id);
</code></pre>
<p>Sample data:</p>
<pre><code>| id | name | group_name |
|----|------|------------|
|  1 | A    | Group 1    |
|  2 | A    | Group 2    |
|  3 | A    | Group 2    |
|  4 | B    | Group 1    |
|  5 | B    | Group 2    |
|  6 | C    | Group 2    |
</code></pre>
<p>Input: <code>priorityGroup='Group 2', cursorId=2, limit=2</code> -&gt; Expected: <code>[record(id=3), record(id=5)]</code>.<br />
Input: <code>priorityGroup='Group 2', cursorId=5, limit=2</code> -&gt; Expected: <code>[record(id=6), record(id=1)]</code>.<br />
Input: <code>priorityGroup='Group 2', cursorId=1, limit=2</code> -&gt; Expected: <code>[record(id=4)]</code>.</p>
",4,4,0,2025-07-30T16:44:21+00:00,1,178,True
79721483,3485361,,postgresql,ExecuteUpdateAsync seems not to be sent to database,"<p>I have a table in a Postgres database that I update in C# for test cases.</p>
<pre class=""lang-cs prettyprint-override""><code>var updated = await db.PublicConfigurationRoletypeWaitingTime
                      .Where(x =&gt; x.TypeId.Contains((int)personRoleType))
                      .ExecuteUpdateAsync(x =&gt; x.SetProperty(y =&gt; y.Waitingtime, waitingTime.Value));
</code></pre>
<p>After that the <code>Databasecontext</code> gets disposed and the actual test starts.</p>
<p>I noticed that my tests failed sometimes, since there was a wrong time.</p>
<p>So I added assertions after updating the database:</p>
<pre class=""lang-cs prettyprint-override""><code>var updated = await db.PublicConfigurationRoletypeWaitingTime
                  .Where(x =&gt; x.TypeId.Contains((int)personRoleType))
                  .ExecuteUpdateAsync(x =&gt; x.SetProperty(y =&gt; y.Waitingtime, waitingTime.Value));

// was it updated?
updated.Should().Be(1, $&quot;Waiting time for {personRoleType} should be updated&quot;);
// yes one row was changed

// get the entry from the db - The same where clause then above
var test = await db.PublicConfigurationRoletypeWaitingTime
                       .Where(x =&gt; x.TypeId.Contains((int)personRoleType))
                       .ToListAsync();

// should be exactly one entry, not something like have multiple matching entries and returning a random one
test.Should().HaveCount(1);
// yes

// Ok maybe it has old values cached, so reload the entity
var entity = db.Entry(test.First());
await entity.ReloadAsync();

// Now it should have the correct value from the database
test.First().Waitingtime.Should().Be(waitingTime.Value);

// NOPE it failed
</code></pre>
<p>For context, that is the class for the table:</p>
<pre class=""lang-cs prettyprint-override""><code>[Table(&quot;configuration_roletype_waiting_time&quot;)]
public partial class PublicConfigurationRoletypeWaitingTime
{
    [Key]
    [Column(&quot;id&quot;)]
    public global::System.Int32 Id { get; set; }

    [Column(&quot;type_id&quot;)]
    public global::System.Collections.Generic.List&lt;System.Int32&gt; TypeId { get; set; } = null!;

    [Column(&quot;waitingtime&quot;)]
    public global::System.TimeSpan Waitingtime { get; set; }

    [Column(&quot;notification_time&quot;)]
    public global::System.TimeSpan NotificationTime { get; set; }

    [Column(&quot;notification_text&quot;)]
    public global::System.String NotificationText { get; set; } = null!;

    [Column(&quot;safe_deletion_threshold&quot;)]
    public global::System.Int32 SafeDeletionThreshold { get; set; }

    [Column(&quot;import_deletion_threshhold&quot;)]
    public global::System.TimeSpan? ImportDeletionThreshhold { get; set; }

    [Column(&quot;import_deletion_faild_requests&quot;)]
    public global::System.Int32? ImportDeletionFaildRequests { get; set; }

    [Column(&quot;import_deletion_faild_requests_threshhold&quot;)]
    public global::System.Int32? ImportDeletionFaildRequestsThreshhold { get; set; }

    [Column(&quot;import_threshhold_check&quot;)]
    public global::System.TimeSpan? ImportThreshholdCheck { get; set; }
}
</code></pre>
<p>And here is the table:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE public.configuration_roletype_waiting_time 
(
    id integer NOT NULL,
    type_id integer[] NOT NULL,
    waitingtime interval NOT NULL,
    notification_time interval NOT NULL,
    notification_text text NOT NULL,
    safe_deletion_threshold integer NOT NULL,
    import_deletion_threshhold interval,
    import_deletion_faild_requests integer,
    import_deletion_faild_requests_threshhold integer,
    import_threshhold_check interval
);
</code></pre>
<p>What am I missing? I'm at a loss. I could load the entity, modify it and then perform a <code>db.SaveChanges()</code> but there are some places I use <code>ExecuteUpdateAsync</code> and I would prefer to not change that.</p>
<p>I must have some fundamental misunderstanding how <code>ExecuteUpdateAsync</code> works...</p>
<hr />
<p><strong>Edit</strong>:</p>
<p>Excedently cut of the <code>updated</code> variable in the second code block…</p>
",2,2,0,2025-07-31T14:57:54+00:00,1,118,True
79721866,12004994,,postgresql,Clickhouse race condition on materialized views,"<p>I’m using PostgreSQL as the source database with Change Data Capture (CDC) enabled via publications. I have two related tables:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"" data-babel-preset-react=""false"" data-babel-preset-ts=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>-- Table 1: orders
CREATE TABLE orders (
  id UUID PRIMARY KEY,
  customer_id UUID,
  amount NUMERIC,
  created_at TIMESTAMP DEFAULT now()
);

-- Table 2: payments
CREATE TABLE payments (
  id UUID PRIMARY KEY,
  order_id UUID REFERENCES orders(id),
  payment_method TEXT,
  status TEXT,
  paid_at TIMESTAMP DEFAULT now()
);</code></pre>
</div>
</div>
</p>
<p>Both tables are part of a publication:</p>
<p><code>CREATE PUBLICATION my_pub FOR TABLE orders, payments;</code></p>
<p>I’m inserting data into both tables in a single Postgres transaction</p>
<p>In ClickHouse, I’m consuming this CDC stream (via Kafka) and have a materialized view on the orders table that performs a join with the payments table to enrich the data:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"" data-babel-preset-react=""false"" data-babel-preset-ts=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>CREATE MATERIALIZED VIEW orders_mv TO enriched_orders AS
SELECT
  o.id AS order_id,
  o.customer_id,
  o.amount,
  p.payment_method,
  p.status
FROM orders AS o
LEFT JOIN payments AS p ON o.id = p.order_id;</code></pre>
</div>
</div>
</p>
<p>Question:</p>
<p>Since the CDC messages for both orders and payments are emitted from a single Postgres transaction, is it possible that the materialized view in ClickHouse is triggered when orders is inserted, but the matching payments record has not yet been processed or is unavailable?</p>
<p>In other words, can this lead to a race condition or incomplete join, and if so, what are the best practices to avoid this? Should I avoid joining directly in the MV and instead defer the enrichment step?</p>
",0,0,0,2025-07-31T21:42:52+00:00,1,97,True
79721893,686621,,postgresql,Include duplicate rows with postgresql &#39;WHERE IN&#39; clause with lists containing duplicate id&#39;s,"<p>I am wondering if there is a way to suppress the automatic deduping that happens with a postgresql SELECT statement using a <code>WHERE id IN (list)</code> clause, where <code>list</code> will contain duplicate ids. What I want is something like this statement that will return 4 results including duplicates:</p>
<pre><code>SELECT * FROM table WHERE id IN (102, 103, 103, 104)
</code></pre>
<p>But the actual result is 3 rows returned:
((102, a, b)(103, c, d)(104, e, f))</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th><code>table.id</code></th>
<th><code>table.column1</code></th>
<th><code>table.column2</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>102</td>
<td>a</td>
<td>b</td>
</tr>
<tr>
<td>103</td>
<td>c</td>
<td>d</td>
</tr>
<tr>
<td>104</td>
<td>e</td>
<td>f</td>
</tr>
</tbody>
</table></div>
<p>while I would like 103 to appear twice in the return from the Select statement:
((102, a, b)(103, c, d)(103, c, d)(104, e, f))</p>
<p>I know that I can just handle the duplicates in my python code but I assume there is a way to handle this in Postgres. Thanks for any help.</p>
",3,5,2,2025-07-31T22:22:46+00:00,2,183,True
79722524,31186206,,postgresql,Error with Ktor and Exposed with LocalDate field in Postgres (ClassNotFoundException: kotlinx.datetime.Instant),"<p>I have a small test table in Postgres with a DATE field, and data class in Ktor app. When mapping the database table row result to TestItem the 'updated' LocalDate field generated the error.</p>
<pre><code>import DatabaseFactory.dbQuery
import io.ktor.http.HttpHeaders
import io.ktor.http.HttpMethod
import io.ktor.http.HttpStatusCode
import io.ktor.serialization.kotlinx.json.json
import io.ktor.server.application.*
import io.ktor.server.engine.*
import io.ktor.server.netty.*
import io.ktor.server.plugins.contentnegotiation.ContentNegotiation
import io.ktor.server.plugins.cors.routing.CORS
import io.ktor.server.plugins.defaultheaders.DefaultHeaders
import io.ktor.server.response.respond
import io.ktor.server.routing.get
import io.ktor.server.routing.route
import io.ktor.server.routing.routing
import kotlinx.datetime.LocalDate
import kotlinx.serialization.Serializable
import org.jetbrains.exposed.v1.core.Column
import org.jetbrains.exposed.v1.core.ResultRow
import org.jetbrains.exposed.v1.core.StdOutSqlLogger
import org.jetbrains.exposed.v1.core.Table
import org.jetbrains.exposed.v1.datetime.date
import org.jetbrains.exposed.v1.jdbc.Database
import org.jetbrains.exposed.v1.jdbc.selectAll
import org.jetbrains.exposed.v1.jdbc.transactions.transaction

object DatabaseFactory{
    fun init() {
        val driver = &quot;org.postgresql.Driver&quot;
        val url = &quot;jdbc:postgresql://192.168.0.XXX:5432/mydb&quot;
        val user = &quot;postgres&quot;
        val password = &quot;postgres&quot;

        val db = Database.connect(url, driver, user = user, password = password)
        transaction {
            addLogger(StdOutSqlLogger)
        }
    }
    fun &lt;T&gt; dbQuery(block: () -&gt; T): T =
        transaction { block() }
}

object TestTable: Table (name=&quot;test&quot;) {
    val id: Column&lt;Int&gt; = integer(&quot;id&quot;)
    val updated: Column&lt;LocalDate&gt; = date(&quot;updated&quot;)
}

@Serializable
data class TestItem(
    val id: Int,
    val updated: LocalDate
)

interface DAOFacade {
    suspend fun getId(id: Int): TestItem?
}

class DAOFacadeImpl : DAOFacade {
    private fun resultRowToTestId(row: ResultRow): TestItem = TestItem(
        id = row[TestTable.id],
        updated = row[TestTable.updated]
    )

    override suspend fun getId(id: Int): TestItem? {
        val foundId = dbQuery {
            TestTable
                .selectAll()
                .where { TestTable.id eq id }
                .map(::resultRowToTestId)
                .firstOrNull()
        }
        return foundId
    }
}

val dao: DAOFacade = DAOFacadeImpl()

fun main() {
    embeddedServer(Netty, host = &quot;0.0.0.0&quot;, port = 8080, module = Application::module)
        .start(wait = true)
}

fun Application.module() {
    configureSerialization()
    DatabaseFactory.init()
    configureRouting()
    configureHTTP()
}

fun Application.configureSerialization() {
    install(ContentNegotiation) {
        json()
    }
}

fun Application.configureRouting() {
    routing {
        route(&quot;/&quot;) {
            get(&quot;/{id}&quot;) {
                val requestId = call.parameters[&quot;id&quot;] ?: &quot;&quot;
                val idFound: TestItem? = dao.getId(requestId.toInt())

                if (idFound == null) {
                    call.respond(HttpStatusCode.BadRequest, message = &quot;Not Found&quot;)
                } else {
                    call.respond(HttpStatusCode.OK, message = idFound)
                }
            }
        }
    }
}

fun Application.configureHTTP() {
    install(DefaultHeaders) {
        header(&quot;X-Engine&quot;, &quot;Ktor&quot;) // will send this header with each response
    }
    install(CORS) {
        allowMethod(HttpMethod.Options)
        allowHeader(HttpHeaders.Authorization)
        allowHeader(&quot;MyCustomHeader&quot;)
        anyHost() // @TODO: Don't do this in production if possible. Try to limit it.
    }
}
</code></pre>
<p>My build.gradle.kts:</p>
<pre class=""lang-kts prettyprint-override""><code>plugins {
    kotlin(&quot;jvm&quot;) version &quot;2.2.0&quot;
    kotlin(&quot;plugin.serialization&quot;) version &quot;2.2.20-Beta1&quot;
    id(&quot;io.ktor.plugin&quot;) version &quot;3.2.2&quot;
    application
}

application {
    mainClass.set(&quot;MainKt&quot;)
}

group = &quot;net.karpenkov&quot;
version = &quot;1.0-SNAPSHOT&quot;

repositories {
    mavenCentral()
}

dependencies {
    testImplementation(kotlin(&quot;test&quot;))
    dependencies {
        implementation(&quot;org.slf4j:slf4j-api:2.0.3&quot;)
        implementation(&quot;org.jetbrains.exposed:exposed-core:1.0.0-beta-4&quot;)
        implementation(&quot;org.jetbrains.exposed:exposed-dao:1.0.0-beta-4&quot;)
        implementation(&quot;org.jetbrains.exposed:exposed-jdbc:1.0.0-beta-4&quot;)
        implementation(&quot;org.jetbrains.exposed:exposed-kotlin-datetime:1.0.0-beta-4&quot;)
        implementation(&quot;io.ktor:ktor-server-core-jvm:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-serialization-kotlinx-json-jvm:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-content-negotiation-jvm:3.2.3&quot;)
        implementation(&quot;org.postgresql:postgresql:42.5.1&quot;)
        implementation(&quot;io.ktor:ktor-server-cors-jvm:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-default-headers:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-host-common-jvm:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-auth-jvm:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-netty-jvm:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-auth:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-sessions:3.2.3&quot;)
        implementation(&quot;io.ktor:ktor-server-freemarker:3.2.3&quot;)
        implementation(&quot;ch.qos.logback:logback-classic:1.4.11&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-datetime:0.7.1&quot;)
        implementation(&quot;io.ktor:ktor-server-status-pages:2.1.2&quot;)
        implementation(&quot;com.typesafe:config:1.4.2&quot;)
    }
}

tasks.test {
    useJUnitPlatform()
}

kotlin {
    jvmToolchain(24)
}

ktor {
    development = true
    fatJar {
        archiveFileName.set(&quot;testLocaleDate.jar&quot;)
    }
}
</code></pre>
<p>Run .jdks/azul-24.0.1/bin/java from Idea IDE</p>
<p>Error (from <code>./gradlew run</code>):</p>
<pre class=""lang-none prettyprint-override""><code>Caused by: java.lang.ClassNotFoundException: kotlinx.datetime.Instant
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:580)
Caused by: java.lang.ClassNotFoundException: kotlinx.datetime.Instant

java.lang.NoClassDefFoundError: kotlinx/datetime/Instant
    at org.jetbrains.exposed.v1.datetime.KotlinLocalDateColumnType.longToLocalDate(KotlinDateColumnType.kt:229)
    at org.jetbrains.exposed.v1.datetime.KotlinLocalDateColumnType.valueFromDB(KotlinDateColumnType.kt:202)
    at org.jetbrains.exposed.v1.datetime.KotlinLocalDateColumnType.valueFromDB(KotlinDateColumnType.kt:187)
    at org.jetbrains.exposed.v1.core.ResultRow.rawToColumnValue(ResultRow.kt:99)
    at org.jetbrains.exposed.v1.core.ResultRow.getInternal$lambda$5$lambda$4$lambda$3(ResultRow.kt:87)
    at org.jetbrains.exposed.v1.core.vendors.DatabaseDialectKt.withDialect(DatabaseDialect.kt:152)
    at org.jetbrains.exposed.v1.core.ResultRow.getInternal$lambda$5(ResultRow.kt:86)
    at org.jetbrains.exposed.v1.core.ResultRow$ResultRowCache.cached(ResultRow.kt:218)
</code></pre>
<p>I can't find any documentation to troubleshoot this simple code. What might be a problem?</p>
",2,2,0,2025-08-01T12:42:06+00:00,2,222,True
79722528,15992045,,postgresql,How to add a relationship to a GeneratedField in Django,"<h3>Context</h3>
<p>I have a model:</p>
<pre class=""lang-py prettyprint-override""><code>class Article(models.Model):
    id = models.UUIDField(default=uuid4, editable=False, unique=True, primary_key=True)
    paper_id: UUID | None
    paper = models.ForeignKey[&quot;Paper&quot;](
        &quot;Paper&quot;,
        on_delete=models.CASCADE,
        related_name=&quot;articles&quot;,
        null=True,
        blank=True,
    )
    website_id: UUID | None
    website = models.ForeignKey[&quot;Website&quot;](
        &quot;Website&quot;,
        on_delete=models.CASCADE,
        related_name=&quot;articles&quot;,
        null=True,
        blank=True,
    )
</code></pre>
<p>I then have a view that unifies <code>Paper</code> and <code>Website</code>:</p>
<pre class=""lang-py prettyprint-override""><code>class Source(pg.View):
    sql = &quot;&quot;&quot;
        SELECT ...
        FROM papers

        UNION ALL

        SELECT ...
        FROM websites
        ;
    &quot;&quot;&quot;

    id = models.UUIDField(unique=True, primary_key=True)

    # ...

</code></pre>
<h3>Question</h3>
<p>I'd like to make it so from a <code>Source</code> model I can get the <code>articles</code> (e.g. <code>source.articles.all()</code>). I would also like to be able to use <code>article.source</code> to get to the source.</p>
<p>Ideally this should be done using Django models as other libraries (e.g. Graphene) can autogenerate code based on regular Django fields.</p>
<p>How can I do that?</p>
<h3>What have I tried?</h3>
<p>I've tried to add this field to <code>Article</code>:</p>
<pre class=""lang-py prettyprint-override""><code>    source_id = models.GeneratedField(
        expression=Coalesce(F(&quot;paper_id&quot;), F(&quot;website_id&quot;)),
        output_field=models.UUIDField(),
        db_persist=False,
    )
    source = models.ForeignKey[&quot;Source&quot;](
        &quot;Source&quot;,
        on_delete=models.DO_NOTHING,
        related_name=&quot;articles&quot;,
        db_column=&quot;source_id&quot;,
        to_field=&quot;id&quot;,
        null=True,
        editable=False,
    )
</code></pre>
<p>But when creating the migration I get:</p>
<blockquote>
<p>(models.E006) The field 'source' clashes with the field 'source_id' from model 'Article'.</p>
</blockquote>
",0,0,0,2025-08-01T12:47:21+00:00,1,59,True
79722656,20263044,,postgresql,Postgres bypasses filtered join,"<p>I'm running into an issue with Postgres and EAV queries.<br />
I have a basic Entity-Attribute-Value setup where all values are stored in a single <code>attribute_value</code> table with a text column <code>data</code>. When querying, I cast <code>data</code> on the fly depending on the attribute type.</p>
<p><a href=""https://i.sstatic.net/cwxytfcg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cwxytfcg.png"" alt=""enter image description here"" /></a></p>
<p>A typical query might look like this (fetching buildings where surface area &gt; 200):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM &quot;entity&quot;
LEFT OUTER JOIN &quot;entity_attribute&quot; surface ON (
  &quot;entity&quot;.&quot;id&quot; = surface.&quot;entity_id&quot; AND (surface.&quot;attribute_id&quot; = 2315)
)
LEFT OUTER JOIN &quot;attribute_value&quot; ON (
  surface.&quot;id&quot; = &quot;attribute_value&quot;.&quot;entity_attribute_id&quot;
)
WHERE (&quot;attribute_value&quot;.&quot;data&quot;)::integer &gt; 200
</code></pre>
<p>Attribute 2315 represents the surface and holds only valid integers.</p>
<p>The above works well almost every time, but once or twice per year, the app crashes with <em>ERROR:  invalid input syntax for type integer: &quot;some-non-integer-value&quot;</em></p>
<p>It looks to me the planner tried to cast a <code>data</code> that doesn't belong to <em>surface</em>.<br />
My best guess is the planner chose to resolve condition on <code>attribute_value.data</code> first and ended up casting the entire table.</p>
<p>The query plan looks correct to me:<br />
<a href=""https://i.sstatic.net/oTntFNwA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oTntFNwA.png"" alt=""enter image description here"" /></a></p>
<p>Am I wrong assuming a join filter is always applied before the where clause ?<br />
If not, why doesn’t this crash more often ?<br />
I understand casting in the WHERE clause isn't ideal, but I’m currently not able to change the schema.<br />
My current workaround is:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM &quot;entity&quot;
LEFT OUTER JOIN &quot;entity_attribute&quot; surface ON (
  &quot;entity&quot;.&quot;id&quot; = surface.&quot;entity_id&quot; AND (surface.&quot;attribute_id&quot; = 2315)
)
LEFT OUTER JOIN &quot;attribute_value&quot; ON (
  surface.&quot;id&quot; = &quot;attribute_value&quot;.&quot;entity_attribute_id&quot;
)
WHERE (CASE WHEN surface.&quot;attribute_id&quot; = 2315 THEN &quot;attribute_value&quot;.&quot;data&quot; ELSE NULL END)::integer &gt; 200
</code></pre>
<p>This seems to prevent the crash but it feels awkward and it difficult to maintain across the whole application</p>
",0,1,1,2025-08-01T14:44:36+00:00,2,89,True
79722820,16018798,,postgresql,Can I enforce the BigQuery table schema when transfering from PostgreSQL with Google Cloud Dataflow?,"<p>I'm trying to create a job to mirror a view that I have in my PostgreSQL DB to a BigQuery table in my Google Cloud Project through Dataflow, I created the job using the &quot;Job builder&quot;, and I've got the following YAML generated from it:</p>
<pre class=""lang-yaml prettyprint-override""><code>pipeline:
  transforms:
    - name: otto-bq-sbx-postgresql-source
      type: ReadFromPostgres
      config:
        jdbc_url: 'jdbc:postgresql://host:5432/otto'
        read_query: SELECT * FROM public.&quot;Carlos_PowerBi&quot;
        username: user
        password: pass
        type: postgres
    - name: otto-bq-sbx-bigquery-update-param
      type: ReadFromBigQuery
      config:
        table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi'
        fields:
          - lastMessage
    - name: otto-bq-sbx-incremental-transform
      type: Sql
      config:
        query: |-
          SELECT *
          FROM
            input1 AS source
          WHERE
            source.&quot;lastMessage&quot; &gt; (
              SELECT COALESCE(MAX(lastMessage), TIMESTAMP '1970-01-01')
              FROM input0
            )
      input:
        input0: otto-bq-sbx-bigquery-update-param
        input1: otto-bq-sbx-postgresql-source
    - name: otto-bq-sbx-sink
      type: WriteToBigQuery
      input: otto-bq-sbx-incremental-transform
      config:
        table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi'
        num_streams: 1
</code></pre>
<p>So, in summary, I want the job to incrementally update the BQ table's data based on the lastMessage column.</p>
<p>Another point is that I'm connecting to the DB through a VPC, which apparently is working fine (I've correctly configured the connection in the job parameters).</p>
<p>However, when I run the job I run into an error where JDBC tells me that it couldn't figure out the correct type of a field:</p>
<pre class=""lang-none prettyprint-override""><code>ValueError: Failed to decode schema due to an issue with Field proto:
&quot;name: \&quot;proposalSent\&quot;
type {
  nullable: true
  logical_type {
    urn: \&quot;beam:logical_type:javasdk_bit:v1\&quot;
    payload: \&quot;\\202SNAPPY\\000\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\002a\\301\\007\\360@\\254\\355\\000\\005sr\\000*org.apache.beam.sdk.io.jdbc.LogicalTypes$1%\\263\\224\\246%\\031\\260\\355\\002\\000\\000xr\\000?N9\\000 schemas.l\\t9Dtypes.PassThroughL\\t\\030\\001Q\\270\\210\\324\\331\\211\\313P\\033\\263\\002\\000\\004L\\000\\010argumentt\\000\\022Ljava/lang/Object;L\\000\\014a\\r \\001:\\034t\\000.Lorg/\\t\\266\\000/\\001\\266\\020/sdk/\\r}\\004/S\\005\\205\\024$Field\\0010\\020;L\\000\\tf\\021\\rDq\\000~\\000\\003L\\000\\nidentifier6s\\000\u003cString;xpt\\000\\000sr\\0006n\\346\\000$AutoValue_\\ts\\000_\\025sh9\\304m\\364S\\243\\227P\\002\\000\\010L\\000\\025collectionEle\\001\\346\\001\\226\\r\\211\\000\\013-+\\001\\023\\010t\\0000\\216\\331\\000\\000L9E$;L\\000\\nmapKey\\001@\\rS\\014\\014map\\005\\227\\035\\024,\\010metadatat\\000\\017)aXutil/Map;L\\000\\010nullablet\\000\\023\\t\\035%~\\030Boolean!?\\010row\\t\\343\\010t\\000$\\212\\243\\000\\001T!\\374\\030Namet\\000-\\2122\\000\\000$\\001\\254\\001/\\020;xr\\000,nu\\001\\t\\2109\\3360\\013PLl[\\357\\3103\\002\\000\\000xp\\001\\001\\014sr\\000\\036AC\\000.\\001\\342\\004.C5|Ds$EmptyMapY6\\024\\205Z\\334\\347\\320\\0053\\014sr\\000\\021\\005/\\020lang.\\r\\3648\\315 r\\200\\325\\234\\372\\356\\002\\000\\001Z\\000\\005v!\\344\\034xp\\000p~r\\000+\\212\\234\\000\\021\\314\\000\\000\\r\\001\\000\\022e1\\000\\016\\031f\\014Enum\\r\\034\\005\\035(pt\\000\\006STRINGs!\\304\\020\\007pppp\\001\\t\\000\\020\\001\\005\\010\\022p~\\001\\007@\\023t\\000\\007BOOLEANt\\000\\003BIT\&quot;
   representation {
     atomic_type: BOOLEAN
   }
   argument_type {
     atomic_type: STRING
   }
   argument {
     atomic_value {
       string: \&quot;\&quot;
     }
   }
 }
}
</code></pre>
<p>I've created the BQ table matching the types of my PostgreSQL view, so I have no clue why this is happening. Also, my BQ table is currently empty, could this be a problem too?</p>
<p>Just to make sure I'm not missing anything, here are the view types:</p>
<p><a href=""https://i.sstatic.net/BOyG92Uz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BOyG92Uz.png"" alt=""View types"" /></a></p>
<p>And here are my BQ table types, matching the ones of the view:</p>
<p><a href=""https://i.sstatic.net/iVwHL0fj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iVwHL0fj.png"" alt=""BQ table types"" /></a></p>
<p>Here is my most recent attempt as a YAML:</p>
<pre class=""lang-yaml prettyprint-override""><code>pipeline:
  transforms:
    - name: otto-bq-sbx-postgresql-source
      type: ReadFromPostgres
      config:
        jdbc_url: 'jdbc:postgresql://host:5432/otto'
        read_query: &gt;-
          SELECT
            &quot;idConversa&quot;::INTEGER AS &quot;idConversa&quot;,
            &quot;dataConversa&quot;::TIMESTAMP AS &quot;dataConversa&quot;,
            &quot;estadoFunil&quot;::TEXT AS &quot;estadoFunil&quot;,
            &quot;tags&quot;::TEXT AS &quot;tags&quot;,
            &quot;propostaEnviada&quot;::BOOLEAN AS &quot;propostaEnviada&quot;,
            &quot;termoEnviado&quot;::BOOLEAN AS &quot;termoEnviado&quot;,
            &quot;termoAssinado&quot;::BOOLEAN AS &quot;termoAssinado&quot;,
            &quot;idCanal&quot;::INTEGER AS &quot;idCanal&quot;,
            &quot;nomeCanal&quot;::TEXT AS &quot;nomeCanal&quot;,
            &quot;nomeCliente&quot;::TEXT AS &quot;nomeCliente&quot;,
            &quot;numeroCliente&quot;::TEXT  AS &quot;numeroCliente&quot;,
            &quot;ultimaMensagem&quot;::TIMESTAMP AS &quot;ultimaMensagem&quot;,
            &quot;infoExtra&quot;::TEXT AS &quot;infoExtra&quot;
          FROM
            public.&quot;Carlos_PowerBi&quot;
        username: user
        password: pass
        type: postgres
    - name: otto-bq-sbx-bigquery-update-param
      type: ReadFromBigQuery
      config:
        table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi'
        fields:
          - ultimaMensagem
    - name: otto-bq-sbx-incremental-transform
      type: Sql
      config:
        query: &gt;-
          SELECT
            source.&quot;idConversa&quot;::INTEGER AS &quot;idConversa&quot;,
            source.&quot;dataConversa&quot;::TIMESTAMP AS &quot;dataConversa&quot;,
            source.&quot;estadoFunil&quot;::TEXT AS &quot;estadoFunil&quot;,
            source.&quot;tags&quot;::TEXT AS &quot;tags&quot;,
            source.&quot;propostaEnviada&quot;::BOOLEAN AS &quot;propostaEnviada&quot;,
            source.&quot;termoEnviado&quot;::BOOLEAN AS &quot;termoEnviado&quot;,
            source.&quot;termoAssinado&quot;::BOOLEAN AS &quot;termoAssinado&quot;,
            source.&quot;idCanal&quot;::INTEGER AS &quot;idCanal&quot;,
            source.&quot;nomeCanal&quot;::TEXT AS &quot;nomeCanal&quot;,
            source.&quot;nomeCliente&quot;::TEXT AS &quot;nomeCliente&quot;,
            source.&quot;numeroCliente&quot;::TEXT AS &quot;numeroCliente&quot;,
            source.&quot;ultimaMensagem&quot;::TIMESTAMP AS &quot;ultimaMensagem&quot;,
            source.&quot;infoExtra&quot;::TEXT AS &quot;infoExtra&quot;
          FROM
            input1 AS source
          WHERE
            source.&quot;ultimaMensagem&quot; (
              SELECT 
                COALESCE(MAX(&quot;ultimaMensagem&quot;), TIMESTAMP '1970-01-01')
              FROM
                input0
            )
      input:
        input0: otto-bq-sbx-bigquery-update-param
        input1: otto-bq-sbx-postgresql-source
    - name: otto-bq-sbx-sink
      type: WriteToBigQuery
      input: otto-bq-sbx-incremental-transform
      config:
        table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi'
        num_streams: 1
</code></pre>
<p>It's still a mystery to me why this is still struggling with the types, since they are all well-defined.</p>
",0,0,0,2025-08-01T17:42:21+00:00,1,121,False
79723451,14551577,,postgresql,How to implement permission prerequisites in a role-based access control system?,"<p>I'm implementing a role-based access control (RBAC) system and want to model <strong>permission prerequisites</strong>(<em>certain permissions require other permissions to function correctly</em>) in my database.</p>
<p>For example: If a role has the <code>DELETE_USER</code> permission, it must also have <code>READ_USER</code>, because deleting a user implies the need to read user data first.</p>
<p>This should be enforced:<br />
✅ When assigning permissions to a role (backend and UI)<br />
✅ When checking permissions at runtime (e.g. in API authorization logic)</p>
<hr />
<h3>Here's my schema so far:</h3>
<pre class=""lang-sql prettyprint-override""><code>-- Permissions table
CREATE TABLE permissions (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Roles table
CREATE TABLE roles (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Role-Permission mapping table
CREATE TABLE role_permissions (
    role_id INT NOT NULL REFERENCES roles(id) ON DELETE CASCADE,
    permission_id INT NOT NULL REFERENCES permissions(id) ON DELETE CASCADE,
    PRIMARY KEY (role_id, permission_id)
);

-- Permission Prerequisites table
-- e.g., DELETE_USER requires READ_USER
CREATE TABLE permission_prerequisites (
    permission_id INT NOT NULL REFERENCES permissions(id) ON DELETE CASCADE,
    required_permission_id INT NOT NULL REFERENCES permissions(id) ON DELETE CASCADE,
    PRIMARY KEY (permission_id, required_permission_id),
    CHECK (permission_id &lt;&gt; required_permission_id)  -- Avoid self-dependency
);
</code></pre>
<hr />
<p>Is this a good way to model permission prerequisites, and what’s the best approach to enforce them—auto-adding or erroring—using SQL recursion or backend logic, with validation also in the frontend?</p>
<p>Any best practices, feedback, or implementation advice would be greatly appreciated!</p>
",0,0,0,2025-08-02T15:08:03+00:00,1,93,True
79723686,14606310,,postgresql,Supabase: Getting &quot;role &#39;user&#39; does not exist&quot; error only when authenticated user queries product/category tables,"<h1>Supabase: Getting &quot;role 'user' does not exist&quot; error only when authenticated user queries product/category tables</h1>
<h2>Problem Summary</h2>
<p>I'm experiencing a strange issue with Supabase where:</p>
<ul>
<li><strong>When user is logged out</strong> (anonymous): GET requests to <code>/rest/v1/product</code> and <code>/rest/v1/category</code> return <strong>200 OK</strong></li>
<li><strong>When user is logged in</strong> (authenticated): Same requests return <strong>400 Bad Request</strong> with error:</li>
</ul>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;code&quot;: &quot;22023&quot;,
    &quot;details&quot;: null,
    &quot;hint&quot;: null,
    &quot;message&quot;: &quot;role \&quot;user\&quot; does not exist&quot;
}
</code></pre>
<h2>Database Schema</h2>
<p>I have the following tables in my Supabase project:</p>
<pre class=""lang-sql prettyprint-override""><code>-- profiles table (contains user role info)
CREATE TABLE public.profiles (
    id UUID REFERENCES auth.users(id) PRIMARY KEY,
    name TEXT,
    email TEXT,
    role TEXT DEFAULT 'user',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- product table
CREATE TABLE public.product (
    id INT4 PRIMARY KEY,
    name VARCHAR,
    price NUMERIC,
    stock INT4,
    category INT4,
    image_url TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- category table  
CREATE TABLE public.category (
    id INT4 PRIMARY KEY,
    name VARCHAR,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
</code></pre>
<h2>Custom Access Token Hook</h2>
<p>I have a custom access token hook that reads user role from profiles:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION public.custom_access_token_hook(event jsonb)
RETURNS jsonb
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  user_role text;
  user_name text;
  user_email text;
BEGIN
  SELECT role, name, email 
  INTO user_role, user_name, user_email
  FROM public.profiles
  WHERE id = (event-&gt;&gt;'user_id')::uuid;

  IF user_role IS NULL THEN
    user_role := 'user';
  END IF;

  RETURN jsonb_set(
    event,
    '{claims}',
    COALESCE(event-&gt;'claims', '{}') || jsonb_build_object(
      'role', user_role,
      'user_name', COALESCE(user_name, ''),
      'user_email', COALESCE(user_email, '')
    )
  );
END;
$$;
</code></pre>
<h2>Frontend Code</h2>
<pre class=""lang-js prettyprint-override""><code>// This works when user is logged out
const { data, error } = await supabase
  .from('product')
  .select('*');

// Same code returns error when user is logged in
</code></pre>
<h2>What I've Tried</h2>
<ol>
<li><p><strong>Disabled RLS</strong> on product and category tables:</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE public.product DISABLE ROW LEVEL SECURITY;
ALTER TABLE public.category DISABLE ROW LEVEL SECURITY;
</code></pre>
</li>
<li><p><strong>Removed all RLS policies</strong> from these tables</p>
</li>
<li><p><strong>Granted permissions</strong> to anon and authenticated roles:</p>
<pre class=""lang-sql prettyprint-override""><code>GRANT SELECT ON public.product TO anon;
GRANT SELECT ON public.product TO authenticated;
GRANT SELECT ON public.category TO anon;
GRANT SELECT ON public.category TO authenticated;
</code></pre>
</li>
<li><p><strong>Simplified the custom access token hook</strong> to just return the event without modifications</p>
</li>
<li><p><strong>Temporarily disabled the custom access token hook</strong> entirely</p>
</li>
</ol>
<p>None of these solutions worked. The error persists only when the user is authenticated.</p>
<h2>Additional Context</h2>
<ul>
<li>Supabase project is relatively new</li>
<li>The error started appearing after implementing the custom access token hook</li>
<li>User login/logout functionality works correctly</li>
<li>The error specifically mentions PostgreSQL role &quot;user&quot; which suggests it's trying to use the application role as a database role</li>
</ul>
<h2>Questions</h2>
<ol>
<li>Why does the same query work for anonymous users but fail for authenticated users?</li>
<li>How is the custom JWT claim <code>role: &quot;user&quot;</code> being interpreted as a PostgreSQL database role?</li>
<li>What's the proper way to handle user roles in Supabase without causing this conflict?</li>
</ol>
<h2>Environment</h2>
<ul>
<li>Supabase: Latest version</li>
<li>Frontend: Next.js with @supabase/supabase-js</li>
<li>Database: PostgreSQL (Supabase managed)</li>
</ul>
<p>Any help would be greatly appreciated!</p>
<p>i want data products and category can be access anyone</p>
",1,1,0,2025-08-02T22:19:58+00:00,1,228,True
79724928,1470327,Netherlands,postgresql,Volume mappings from Windows to WSL are causing problems when using /mnt/C/,"<p>We are developing an ASP.NET app on windows, which runs in docker. When using docker desktop, it &quot;all just works&quot;. But a few of us are used to develop in Linux and prefer to work with WSL instead (I understand that docker desktop also relies on WSL, but we just prefer to be able to do &quot;everything&quot; through CLI).</p>
<p>It all works fine, except the volume mapping are causing problems. When we have this (simplified) docker compose as example</p>
<pre><code>    timescale-db:
        container_name: timescale        
        image: timescale/timescaledb:latest-pg16
        environment:
          # some env vars
        volumes:
        - ./dbdata:/var/lib/postgresql/data
</code></pre>
<p>The folder &quot;dbdata&quot; will be created on the windows host in the current directory. docker compose is ran inside WSL, and it will execute in my case in /mnt/c/Users/bpr/dev/iai.productionmaster.central/</p>
<p>In WSL, the path /mnt/c/Users/bpr/dev/iai.productionmaster.central/dbdata is then mapped to the docker container in /var/lib/postgresql/data.</p>
<p>So far so good. But, when the postgress instance then starts, it wants to change permissions on its /var/lib/postgresql/data folder, which fails</p>
<blockquote>
<p>initdb: error: could not change permissions of directory &quot;/var/lib/postgresql/data&quot;: Operation not permitted
2025-08-04T12:29:08.635400362Z fixing permissions on existing directory /var/lib/postgresql/data ... 2025-08-04T12:29:21.768621211Z chmod: /var/lib/postgresql/data: Operation not permitted
2025-08-04T12:29:21.778706750Z</p>
</blockquote>
<p>According to several posts (e.g. <a href=""https://github.com/docker/for-win/issues/4824"" rel=""nofollow noreferrer"">this one</a>) this has everything to do with NTFS permissions getting in the way.</p>
<p>So when the &quot;host&quot; part of the volume mapping is inside the NTFS mount, I can run into this kind of problems. When I would change the volume mapping to something like &quot;/home/test:/var/lib/postgresql/data&quot;, it also works just fine. But then I would need to navigate to \wsl$\home\test to see the contents. Again, not a big issue, but we are trying to get as close as possible to what docker desktop does.</p>
<p>The obvious solution <code>sudo chmod -R 777 /mnt/c/Users</code> does not solve anything.</p>
<p>Does anybody recognize this problem and do you have a solution to this problem?</p>
",0,0,0,2025-08-04T12:53:18+00:00,1,106,True
79725198,21662604,,postgresql,Unable to Authenticate to Docker image via Golang,"<p>I am new to Docker and Golang. I am trying to run Golang server locally and connect it with my postgres docker image.Basically I want the DB to run as docker container with persitant volume</p>
<p><strong>Error</strong></p>
<pre><code>PS C:\Users\usero\Desktop\Operation_GatiShakti\goBackEnd&gt; go run .
mongodb://mongoadmin:password@localhost:27017/?tls=false
2025/08/04 22:08:16 POSTGRES_URI: postgres://myuser:mypassword@localhost:5432/mydatabase?sslmode=disable
2025/08/04 22:08:16 Failed to connect to PostgreSQL: failed to connect to `user=myuser database=mydatabase`: [::1]:5432 (localhost): failed SASL auth: FATAL: password authentication failed for user &quot;myuser&quot; (SQLSTATE 28P01)
exit status 1
</code></pre>
<p>I have created my docker-compose.yml file as -</p>
<pre><code>services:
  postgres:
    image: postgres:latest
    container_name: my-postgres
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - &quot;5432:5432&quot;
    restart: unless-stopped

volumes:
  postgres_data:
</code></pre>
<p>Inside my Golang code the .env file has</p>
<pre><code>POSTGRES_URI=postgres://myuser:mypassword@localhost:5432/mydatabase?sslmode=disable
</code></pre>
<p>The main.go file:</p>
<pre><code>func main() {

    var pgPool *pgxpool.Pool

    //======================================
    // ENVIRONMENT VARIABLE
    //======================================
    err := godotenv.Load(&quot;.env&quot;)
    if err != nil {
        log.Fatal(&quot;Error loading .env file&quot;)
    }

    //======================================
    // MongoDB connection
    //======================================
    client, err := mongoFunctions.ConnectMongoDB()
    if err != nil {
        log.Fatalf(&quot;Failed to connect to MongoDB: %v&quot;, err)
    }
    defer func() {
        if err = client.Disconnect(context.Background()); err != nil {
            log.Fatalf(&quot;Failed to disconnect from MongoDB: %v&quot;, err)
        }
    }()

    // Set global MongoDB collection handle
    MONGO_DB_UPSC := client.Database(&quot;UPSC&quot;)

    //======================================
    // PostgreSQL connection
    //======================================
    pgPool, err = connectPostgres()
    if err != nil {
        log.Fatalf(&quot;Failed to connect to PostgreSQL: %v&quot;, err)
    }
    defer pgPool.Close()

    smtpConfig := resetPassword.SMTPConfig{
        Host:     &quot;smtp.hostinger.com&quot;,
        Port:     &quot;587&quot;,
        Username: &quot;office@somemail.com&quot;,
        Password: os.Getenv(&quot;SMTP_PASSWORD&quot;),
        From:     &quot;office@somemail.com&quot;,
    }

    resetService := resetPassword.NewService(pgPool, smtpConfig)

    // Setup logger
    logger := slog.New(slog.NewJSONHandler(os.Stdout, nil))
    slog.SetDefault(logger)

    // Setup Gin router
    router := gin.Default()
    router.Use(corsMiddleware)

    //-----------------------------------
    // LOGIN
    //-----------------------------------
    router.POST(&quot;/login&quot;, ratelimit.LoginMiddleware(), func(c *gin.Context) {
        authentication.LoginUser(c, pgPool)
    })

    router.GET(&quot;/refresh&quot;, func(c *gin.Context) {
        authentication.RefreshHandler(c.Writer, c.Request)
    })

    router.GET(&quot;/me&quot;, func(c *gin.Context) {
        userID, err := authentication.ValidateAccessToken(c.Request)
        if err != nil {
            c.JSON(http.StatusUnauthorized, gin.H{&quot;error&quot;: &quot;Unauthorized&quot;})
            return
        }
        c.JSON(http.StatusOK, gin.H{&quot;id&quot;: userID})
    })

    //-----------------------------------
    // PASSWORD RESET
    //-----------------------------------
    router.POST(&quot;/resetpassword&quot;, resetService.ResetPasswordHandler)

    //-----------------------------------
    // MONGO
    //-----------------------------------
    router.GET(&quot;/UPSC_History&quot;, func(c *gin.Context) {
        mongoFunctions.GetAllDocument(c, MONGO_DB_UPSC, &quot;History&quot;)
    })

    //---------------------------------------
    //  POSTGRES
    //---------------------------------------
    router.GET(&quot;/pg-users&quot;, func(c *gin.Context) {
        registration.GetPostgresUsersHandler(c, pgPool)
    })
    router.POST(&quot;/pg-users&quot;, func(c *gin.Context) {
        registration.CreatePostgresUserHandler(c, pgPool)
    })

    router.POST(&quot;/pg-users/check&quot;, func(c *gin.Context) {
        registration.CheckUserExistsHandler(c, pgPool)
    })

    router.Run(&quot;:8080&quot;)
}

// ✅ CORS middleware
func corsMiddleware(c *gin.Context) {
    origin := c.Request.Header.Get(&quot;Origin&quot;)
    if origin != &quot;&quot; {
        c.Writer.Header().Set(&quot;Access-Control-Allow-Origin&quot;, origin)
        c.Writer.Header().Set(&quot;Access-Control-Allow-Credentials&quot;, &quot;true&quot;)
        c.Writer.Header().Set(&quot;Access-Control-Allow-Headers&quot;, &quot;Content-Type&quot;)
        c.Writer.Header().Set(&quot;Access-Control-Allow-Methods&quot;, &quot;POST, GET, OPTIONS&quot;)
    }
    if c.Request.Method == &quot;OPTIONS&quot; {
        c.AbortWithStatus(204)
        return
    }
    c.Next()
}

// ✅ PostgreSQL connection
func connectPostgres() (*pgxpool.Pool, error) {
    uri := os.Getenv(&quot;POSTGRES_URI&quot;)
    log.Println(&quot;POSTGRES_URI:&quot;, os.Getenv(&quot;POSTGRES_URI&quot;))
    if uri == &quot;&quot; {
        log.Fatal(&quot;POSTGRES_URI environment variable is required&quot;)

    }

    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    pool, err := pgxpool.New(ctx, uri)
    if err != nil {
        return nil, err
    }

    if err := pool.Ping(ctx); err != nil {
        return nil, err
    }

    return pool, nil
}
</code></pre>
<p>I have tried removing image and volumes and restarting container, creating a new user as well. Tried Localhost, 127.0.0.1 and 0.0.0.0 as Ip.Postgres Docker image logs show it runs on 0.0.0.0</p>
<p>This fails even on typing correct password</p>
<pre><code>psql -h localhost -p 5432 -U myuser
</code></pre>
<p>. But commands like - <code>docker exec -it postgres_demo psql -U postgres</code> work</p>
",0,1,1,2025-08-04T17:00:46+00:00,1,98,False
79725454,20720947,,postgresql,How to connect an external PostgreSQL database into my NextJs app to query data,"<p>I am building a software with NextJS Supabase, Prisma --- using typescript.</p>
<p>My goal is simple, connect one or multiple EXTERNAL PostgreSQL databases to my software's UI. Once connected, I can write queries from the database as if I am querying the data in the database, but I am doing it in my own software.</p>
<p>For example, in VSCode, the PostgreSQL extension already does this. I enter my own Username, Password, Host, Port into the extension and it connects and I have access to an external PostgreSQL Database within the extension. I want the same functionality for my software, but I am unsure how to make it work --- since it just brings lots of error when I attempt to connect. What is easiest/fastest way to connect and write the code to properly connect external PostgreSQL databases?</p>
",0,0,0,2025-08-04T22:53:06+00:00,0,244,False
79725806,15435022,,postgresql,PostgreSQL Github Database File Structure Template,"<p>I am creating a Github project for PostgreSQL database files (tables, stored procedures). Is there a Github template, and folder template I should follow?</p>
<p>What should be my gitignore template also? Didn't see anything in official documentation. Any guidance would help.</p>
<p><a href=""https://i.sstatic.net/Z4TSkcMm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Z4TSkcMm.png"" alt=""enter image description here"" /></a></p>
",0,0,0,2025-08-05T08:25:57+00:00,1,48,True
79726186,7664601,Indore,postgresql,How to enable pgaudit to log RPC function queries in PostgreSQL running inside Docker (Supabase local)?,"<p>I'm running a local PostgreSQL database inside a Docker container as part of a Supabase self-hosted setup.</p>
<p>I want to log all internal SQL statements (query mainly to see what generated after RPC function call) (e.g., SELECT, UPDATE,  etc.) executed from within an RPC function (PostgreSQL function), using the pgaudit extension.</p>
<p><strong>What I’ve done so far:</strong></p>
<ol>
<li><p>Confirmed that pgaudit is installed via:</p>
</li>
<li><p>Set audit options via ALTER ROLE:</p>
</li>
</ol>
<pre><code>   ALTER ROLE postgres SET pgaudit.log = 'function, read';
   ALTER ROLE postgres SET pgaudit.log_relation = 'on';
</code></pre>
<p>also i don't want to use of <code>'all'</code> for set pgaudit.log</p>
<pre><code>   ALTER ROLE postgres RESET pgaudit.log = 'all'
</code></pre>
<p>because it crashes docker.</p>
<ol start=""3"">
<li>Called the function using Postman and browser via Supabase RPC (which works).
ex: <code>http://127.0.0.1:54321/rest/v1/rpc/rpc_function_name</code></li>
<li>Ran docker logs -f supabase_db_name, but no AUDIT: logs appear related to function
only some read, select ..... like thing print.</li>
</ol>
<p>also i'm not raising anything from function to log, i just want to see query that generated by my RPC function.</p>
<p>also watch particular docker container to see log in it, but nothing there.</p>
<p><strong>What I need help with:</strong></p>
<ol>
<li><p>Is there a way to log the SQL executed inside a PostgreSQL function (RPC) using
just SQL-level config (e.g., ALTER ROLE, SET), without editing postgresql.conf?</p>
</li>
<li><p>Can I make pgaudit or any alternative log those queries?</p>
</li>
</ol>
<p>Any suggestions to make this work in a production-safe way would be really helpful.</p>
<p>i want to avoid changes in docker file, because when it come to production check there is no way to direct access docker file until self-hosting use.</p>
",1,1,0,2025-08-05T13:30:43+00:00,1,139,False
79726283,4627124,,postgresql,Drop or cast zero default date in schema in sqlite-to-postgres migration using pgloader,"<p>I use pgloader to upload 11 databases from BIRD-DEV dataset into Postgres.</p>
<pre><code>pgloader &quot;$sqlite_path&quot; &quot;postgresql:///$pg_db_name&quot;
</code></pre>
<p>9 databases are uploaded correctly, but 2 give fatal error because of a zero default date in the schema:</p>
<pre><code>2025-08-05T14:30:41.188003Z ERROR Database error 22008: date/time field value out of range: &quot;0000-00-00&quot;
QUERY: CREATE TABLE races
(
  raceid    bigserial,
  year      bigint default '0',
  round     bigint default '0',
  circuitid bigint default '0',
  name      text default '',
  date      date default '0000-00-00',
  time      text,
  url       text
);
2025-08-05T14:30:41.188003Z FATAL Failed to create the schema, see above.
</code></pre>
<p>Is it possible to solve this problem on the level of pgloader without editing the database file ?</p>
",0,0,0,2025-08-05T14:38:35+00:00,0,70,False
79726943,20866297,,postgresql,"Dockerised WASM, ASP.NET Core Web API &amp; Postgres - 405 Error","<p>I have a project with 3 docker containers/services: Blazor WASM client, ASP.NET Core Web API, and a Postgres database. I have set up SSL in Nginx and can reach the client without issues. All containers appear to run fine, and no errors are being thrown during docker-compose build.</p>
<p>Although I try not to reach out too often, I'm really stumped by issues connecting between API and db container, with previous 502 error resolved by removing proxy reference from <code>nginx.conf</code> but an ongoing 405 error as the only response coming back. All database migration attempts have failed using code in <code>Program.cs</code> and in API Dockerfile.</p>
<p>I can overcome this with SQL script, but still after this is done and the database &amp; table schema is built, the http 405 error persists. Any advice from those more experienced would be greatly appreciated.</p>
<p>Please note: the pgAdmin4 service/container was only added for troubleshooting, and is not part of the project itself - it will be removed, but I left it here in case it helps guide a potential response - no issues in connecting from pgAdmin4 to postgres container.</p>
<p>EDIT: adding the proxy config into the <code>Nginx.conf</code> caused a http 502 Bad gateway response, as previously experienced. This was initially masked it seems by cached data in the browser.</p>
<p>The relevant code segments are shown here - <code>Nginx.conf</code>:</p>
<pre><code>worker_processes 4;
events {
    worker_connections 1024;

http {
    sendfile on;
    upstream app_servers {
        server backend:7000;
        # server 127.0.0.1:5432;
    }

include /etc/nginx/mime.types; 
server {
    listen 443 ssl;
    server_name 192.168.20.144;
    ssl_certificate /etc/nginx/certs/cert.pem;
    ssl_certificate_key /etc/nginx/certs/key.pem;
    root /usr/share/nginx/html;
    location /api/ {
        proxy_pass http://backend:7000/api;
        proxy_http_version 1.1;
        proxy_set_header   Host $host;
        proxy_set_header   X-Real-IP $remote_addr;
        proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header   X-Forwarded-Host $server_name;
    }
}
</code></pre>
<p>This is the <code>Docker-compose.yml</code> file:</p>
<pre><code>services:

  frontend:
    build:
      context: .
      dockerfile: Dockerfile-app
    ports:
      - '80:80'
      - '443:443'
    tty: true
    networks:
      - frontend

  backend:
    build:
      context: .
      dockerfile: Dockerfile-api
    ports:
      - '7000:7000'
    depends_on:
      - database
    networks:
      - frontend
      - backend
  
  pgadmin:
    container_name: pgadmin
    image: dpage/pgadmin4:snapshot
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: password
      PGADMIN_LISTEN_PORT: 80
    ports:
      - &quot;8000:80&quot;
    networks:
      - backend
    depends_on:
      - database
    volumes:
      - data:/data/db

  database:
    build:
      context: .
      dockerfile: Dockerfile-db
    restart: always
    volumes:
      - data:/data/db
      - ./cert.pem://var/lib/postgresql/cert.pem
      - ./key.pem://var/lib/postgresql/key.pem
    ports:
      - '5432:5432'
    environment:
      POSTGRES_PASSWORD: pass
      POSTGRES_USERNAME: postgres
      POSTGRES_DB: mpau
    networks:
      - backend

volumes:
  data:

networks:
  frontend:
    name: custom_frontend
  backend:
    name: custom_backend
</code></pre>
<p><code>Dockerfile</code> for the Web API looks like this:</p>
<pre><code>FROM --platform=$BUILDPLATFORM mcr.microsoft.com/dotnet/sdk:9.0 AS build
ARG TARGETARCH
WORKDIR /source

# Copy project file and restore as distinct layers
COPY --link Server/*.csproj .
RUN dotnet restore -a $TARGETARCH
RUN dotnet tool restore
ENTRYPOINT dotnet ef database update --project ./*.csproj --connection &quot;host=database-1 port=5432 dbname=mpau user=postgres password=pass connect_timeout=10 sslmode=prefer sslcert=&lt;STORAGE_DIR&gt;/.postgresql/cert.pem sslkey=&lt;STORAGE_DIR&gt;/.postgresql/key.pem&quot;

# Copy source code and publish app
COPY --link Server/. .
RUN dotnet publish -a $TARGETARCH -o /app

# Runtime stage
FROM mcr.microsoft.com/dotnet/aspnet:9.0
EXPOSE 8080
WORKDIR /app
COPY --link --from=build /app .
USER $APP_UID
ENTRYPOINT [&quot;dotnet&quot;, &quot;wasm.Server.dll&quot;]
</code></pre>
",0,1,1,2025-08-06T07:12:58+00:00,1,109,True
79727094,23117925,,postgresql,Postgres container password authentication failed for user &quot;admin&quot;,"<p>I am building a microservices application using Spring Boot, and I am running two distinct postgres containers on docker for two of the microservices running on my local machine (Windows 10). The <code>docker-compose.yml</code> file is as follows:</p>
<pre><code>services:
  notification-postgres:
    container_name: notification-postgres
    image: postgres
    environment:
      POSTGRES_DB: notification_service_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      PGDATA: /var/lib/postgresql/data
    volumes:
      - notification-postgres:/var/lib/postgresql/data
    ports:
      - &quot;5432:5432&quot;
    restart: unless-stopped

  auth-postgres:
    container_name: auth-postgres
    image: postgres
    environment:
      POSTGRES_DB: auth_service_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      PGDATA: /var/lib/postgresql/data
    volumes:
      - auth-postgres:/var/lib/postgresql/data
    ports:
      - &quot;5431:5431&quot;
    expose:
      - 5431
    command: -p 5431
    restart: unless-stopped

volumes:
  notification-postgres:
  auth-postgres:
</code></pre>
<p>The application.yml for notification service is as follows:</p>
<pre><code>spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/notification_service_db
    username: admin
    password: admin123
    driver-class-name: org.postgresql.Driver
  jpa:
    hibernate:
      ddl-auto: update
</code></pre>
<p>The application.yml for atuh service is as follows:</p>
<pre><code>spring:
  datasource:
    url: jdbc:postgresql://localhost:5431/auth_service_db
    username: admin
    password: admin123
    driver-class-name: org.postgresql.Driver

  jpa:
    hibernate:
      ddl-auto: update
</code></pre>
<p>I was able to connect to notification-postgres from my IDE (IntelliJ IDEA) by stopping the <code>postgres.exe</code> service (as instructed <a href=""https://stackoverflow.com/questions/58562024/docker-password-authentication-failed-for-user-postgres"">here</a>), which is installed in my local machine.</p>
<p>But, as for the auth-postgres, which runs on ports &quot;5431:5431&quot;, I am getting the following error:
<code>org.postgresql.util.PSQLException: FATAL: password authentication failed for user &quot;admin&quot;</code></p>
<p>However, I am able to connect to it using psql, and it is working fine. Here are some logs:<br />
<code>docker logs auth-postgres:</code></p>
<pre><code>...
2025-08-04 12:12:36.349 UTC [1] LOG:  starting PostgreSQL 17.5 (Debian 17.5-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
2025-08-04 12:12:36.350 UTC [1] LOG:  listening on IPv4 address &quot;0.0.0.0&quot;, port 5431
2025-08-04 12:12:36.350 UTC [1] LOG:  listening on IPv6 address &quot;::&quot;, port 5431
2025-08-04 12:12:36.354 UTC [1] LOG:  listening on Unix socket
</code></pre>
<p>Overall, I cannot connect to multiple postgres containers with my IDE. If anyone knows how to fix the issue, your solutions are welcome.</p>
",2,2,0,2025-08-06T09:30:20+00:00,0,127,False
79727814,10663096,,postgresql,python+asyncpg+PostgreSQL returns jsonb column as a string instead of an object,"<p>I'm new to python and just came to a problem: i've set up a simple program to fetch some data from PostgreSQL database. They say that asyncpg library automatically converts JSONB data to Python objects (dicts/lists) by default. This doesn't happen in my case when I select data from a table. I've made synthetic example to demonstrate this:</p>
<pre><code>import asyncio
import asyncpg

async def main():
    conn = await asyncpg.connect(&quot;postgres://user:pass@localhost/database&quot;)
    res = await conn.fetch(&quot;select '[1, 2, 3]'::jsonb as column;&quot;)
    for row in res:
        for key, value in row.items():
            print(&quot;'&quot; + key + &quot;'&quot;)
            print(&quot;'&quot; + value + &quot;'&quot;)
# Call main
if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
</code></pre>
<p>and the output is</p>
<pre><code>'column'
'[1, 2, 3]'
</code></pre>
<p>Using debugger, I can see that value is a string as well.
How can I fix that?</p>
",1,1,0,2025-08-06T20:25:08+00:00,2,418,True
79727955,6739197,,postgresql,Upsert only columns that have a value that is not undefined or null using @supabase/ssr,"<p>I am trying to upsert a table when a web hook is triggered. This web hook only returns the updated fields, so sometimes <code>name</code> and <code>notify</code> fields are missing. The only field that is always coming from the web hook is the <code>phone</code> field.</p>
<p>Right now, when I perform an upsert and the <code>name</code> field is missing from the web hook, it is updating my row but removing and setting to <code>null</code> the <code>name</code> field.</p>
<p><strong>select()</strong> and <strong>update()</strong> afterwards is not possible strategy here because there are a lot of items coming from that web hook.</p>
<p>How can I fix it so that <strong>upsert</strong> does not set to null fields that are not present if they already have a value?</p>
<pre><code>  const contactsToUpsert = nextContacts.map((contact: {phone: string, name?: string, notify?: string}) =&gt; ({
    user_id: settings.user_id,
    phone,
    ...(contact.name &amp;&amp; { name: contact.name }),
    ...(contact.notify &amp;&amp; { notify: contact.notify })
}));

const { error } = await supabase
    .from('contacts')
    .upsert(contactsToUpsert, {
        onConflict: 'user_id,phone',
        defaultToNull: false
    });
</code></pre>
",0,0,0,2025-08-06T23:49:36+00:00,2,170,True
79728189,7836976,Ireland,postgresql,Unique constraint on partitioned table must include all partitioning columns,"<p>I'm trying to create this partitioned table in my PostgreSQL database:</p>
<pre class=""lang-sql prettyprint-override""><code>create table messages_table (
    id  bigserial not null, 
    class_name varchar(255),
    date_created timestamp, 
    service varchar(255), 
    message TEXT, 
    message_type varchar(255), 
    method_name varchar(255), 
    payment_type varchar(255), 
    call_type varchar(255), 
    quote_identification_id int8, 
    primary key (id)
)   PARTITION BY RANGE (date_created);
</code></pre>
<p>It gives me this error:</p>
<pre class=""lang-none prettyprint-override""><code>ERROR:  unique constraint on partitioned table must include all partitioning columns
DETAIL:  PRIMARY KEY constraint on table &quot;messages_table&quot; lacks column &quot;date_created&quot; which is part of the partition key.
SQL state: 0A000
</code></pre>
<p>I considered <code>date_created timestamp UNIQUE</code>, but date_created is a timestamp field and may not necessarily be unique, but I want to partition by this as I wish to have a delete job to delete any records older than 6 months and deleting a partition seems to be the best way to do this. Can anyone advise how I should resolve this?</p>
",-2,0,2,2025-08-07T07:20:00+00:00,2,163,True
79728912,16018798,,postgresql,What is the closest a role can be to a superuser in Google Cloud PostgreSQL?,"<p>I'm re-organizing the user structure in my Google Cloud PostgreSQL instance. Originally, we had a single user with broad access to everything. I'm now aiming for a more granular approach, where each user represents a specific service and has restricted access only to its own database (rwa).</p>
<p>The challenge I'm facing is that I would like to have a single &quot;superuser-like&quot; account with full access to all databases and tables, including automatic access to any newly created databases or tables. However, I understand that Cloud SQL doesn't allow the creation of actual superusers.</p>
<p>Given that limitation, what’s the closest I can get to a superuser in this environment? And more importantly, is this a reasonable and recommended approach?</p>
",0,2,2,2025-08-07T17:03:09+00:00,1,74,True
79729171,15435022,,postgresql,Generated column with data type timestamptz (timestamp with time zone),"<p>In PostgreSQL, I want to make a computed column, where end_datetime = start_datetime + minute_duration adding an interval to a timestamp.</p>
<p>I keep getting error, how can I fix?</p>
<blockquote>
<p>ERROR:  generation expression is not immutable
SQL state: 42P17</p>
</blockquote>
<p>Tried two options below:</p>
<pre><code>CREATE TABLE appt ( 
  appt_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  minute_duration INTEGER NOT NULL,
  start_datetime TIMESTAMPTZ NOT NULL,
  end_datetime TIMESTAMPTZ GENERATED ALWAYS AS (start_datetime + (minute_duration || ' minutes')::INTERVAL) STORED
 );


 CREATE TABLE appt ( 
  appt_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  minute_duration INTEGER NOT NULL,
  start_datetime TIMESTAMPTZ NOT NULL,
  end_datetime TIMESTAMPTZ GENERATED ALWAYS AS (start_datetime + make_interval(mins =&gt; minute_duration)) STORED
);
</code></pre>
<p>The only other option (I can think of) would be trigger, but trying to refrain trigger method for now.
If trigger is the only answer, feel free to give trigger solution.</p>
",2,2,0,2025-08-07T22:11:45+00:00,2,254,True
79729500,13392257,,postgresql,Sqlalchemy delete on table &quot;tasks&quot; violates foreign key constraint for cascade mode,"<p>I have the following postgres table</p>
<pre><code>dbname=# \d+ tasks
                                                   Table &quot;public.tasks&quot;
       Column       |            Type             | Collation | Nullable | Default | Storage  | Stats target | Description 
--------------------+-----------------------------+-----------+----------+---------+----------+--------------+-------------
 id                 | character varying           |           | not null |         | extended |              | 
 external_id        | character varying           |           | not null |         | extended |              | 
 rq_id              | character varying           |           | not null |         | extended |              | 
 priority           | integer                     |           | not null |         | plain    |              | 
 status             | taskstatus                  |           | not null |         | plain    |              | 
 creation_time      | timestamp without time zone |           | not null |         | plain    |              | 
 other fields ...             
Indexes:
    &quot;tasks_pkey&quot; PRIMARY KEY, btree (id)
Referenced by:
    TABLE &quot;solutions&quot; CONSTRAINT &quot;solutions_task_id_fkey&quot; FOREIGN KEY (task_id) REFERENCES tasks(id)
    TABLE &quot;tasks_files&quot; CONSTRAINT &quot;tasks_files_task_id_fkey&quot; FOREIGN KEY (task_id) REFERENCES tasks(id)
Access method: heap
</code></pre>
<p>The table created with help of sqlalchemy class</p>
<pre><code>class Task(Base):
    __tablename__ = 'tasks'

    id = Column(String, unique=True, nullable=False, primary_key=True)
    external_id = Column(String, nullable=False)
    rq_id = Column(String, nullable=False)
    priority = Column(Integer,  nullable=False)
    status = Column(Enum(TaskStatus),  nullable=False)
    creation_time = Column(DateTime, nullable=False)
    ...
    files: Mapped[List[&quot;TaskFile&quot;]] = relationship(cascade=&quot;all, delete&quot;)
    solution: Mapped[List[&quot;Solution&quot;]] = relationship(cascade=&quot;all, delete&quot;)
</code></pre>
<p>I have a python function, the function running SQL command and I have an error:</p>
<pre><code>
dbname=# DELETE FROM tasks WHERE tasks.status = 'SOLVED' AND tasks.creation_time &lt;= '2025-08-07T08:15:40.867870'::timestamp OR tasks.status = 'ERROR' AND tasks.creation_time &lt;= '2025-08-07T08:15:40.867877'::timestamp RETURNING tasks.id;
ERROR:  update or delete on table &quot;tasks&quot; violates foreign key constraint &quot;solutions_task_id_fkey&quot; on table &quot;solutions&quot;
DETAIL:  Key (id)=(7bff9459-2ef0-4650-8f3e-6774203e10fb) is still referenced from table &quot;solutions&quot;.
</code></pre>
<pre><code>def f(db, solved_task_window_days: int, error_task_window_days: int) -&gt; None:
    solved_days_ago = datetime.utcnow() - timedelta(days=solved_task_window_days)
    failed_days_ago = datetime.utcnow() - timedelta(days=error_task_window_days)
    print(&quot;XXX_&quot;, db.query(Task).filter(
        or_(
            and_(
                Task.status == TaskStatus.SOLVED,
                Task.creation_time &lt;= solved_days_ago
            ),
            and_(
                Task.status == TaskStatus.ERROR,
                Task.creation_time &lt;= failed_days_ago
            )
        )
    ).delete(synchronize_session='fetch')) # was trying different flags
</code></pre>
<p>I have the following error (from python function)</p>
<pre><code>(psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block\n\n[SQL: DELETE FROM tasks WHERE tasks.status = %(status_1)s AND tasks.creation_time &lt;= %(creation_time_1)s OR tasks.status = %(status_2)s AND tasks.creation_time &lt;= %(creation_time_2)s RETURNING tasks.id]\n[parameters: {'status_1': 'SOLVED', 'creation_time_1': datetime.datetime(2025, 8, 7, 8, 15, 40, 863536), 'status_2': 'ERROR', 'creation_time_2': datetime.datetime(2025, 8, 7, 8, 15, 40, 863541)}]\n(Background on this error at: https://sqlalche.me/e/20/2j85)&quot;

2025-08-08 08:15:40.869 UTC [26] ERROR:  current transaction is aborted, commands ignored until end of transaction block
2025-08-08 08:15:40.869 UTC [26] STATEMENT:  DELETE FROM tasks WHERE tasks.status = 'SOLVED' AND tasks.creation_time &lt;= '2025-08-07T08:15:40.867870'::timestamp OR tasks.status = 'ERROR' AND tasks.creation_time &lt;= '2025-08-07T08:15:40.867877'::timestamp RETURNING tasks.id
</code></pre>
<p>Solution table looks like this:</p>
<pre><code>dbname=# \d+ solutions
                                                 Table &quot;public.solutions&quot;
      Column       |            Type             | Collation | Nullable | Default | Storage  | Stats target | Description 
-------------------+-----------------------------+-----------+----------+---------+----------+--------------+-------------
 id                | character varying           |           | not null |         | extended |              | 
 task_id           | character varying           |           | not null |         | 
...
Indexes:
    &quot;solutions_pkey&quot; PRIMARY KEY, btree (id)
Foreign-key constraints:
    &quot;solutions_task_id_fkey&quot; FOREIGN KEY (task_id) REFERENCES tasks(id)
</code></pre>
",0,0,0,2025-08-08T08:24:50+00:00,0,87,False
79729636,287948,SP - Brasil,postgresql,Looking for the operator that extracts the key of one of JSONB objects,"<p>Is there an operator that extracts <em>xx</em> from <code>'{&quot;xx&quot;:123}'::jsonb</code>?</p>
<p>In my data I always have only one key, so something like <code>#&gt;&gt;'{$.*}'</code>  that meets this requirement, perhaps exists.</p>
<p>PS: the function <code>jsonb_each</code> is not the answer.</p>
<hr />
<h2>Note: why a table-function is not the answer!</h2>
<p>It is ugly, &quot;an elephant to kill a fly&quot;.  Let see the real-life use case.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE or replace FUNCTION jreduxseq_to_text(
  p_list jsonb, p_level int default 1, p_max_level int default 3
) RETURNS TABLE(
 level      integer,
 ordinality bigint,
 prefix     text,
 suffixes    jsonb,
 is_prefix  boolean
) AS $f$
 SELECT p_level, t.ordinality,
     CASE WHEN t.is_prefix THEN t2.key ELSE t.value #&gt;&gt; '{}' 
     END as prefix,
     CASE WHEN t.is_prefix THEN t2.value ELSE NULL
     END as suffixes,
     t.is_prefix
 FROM (
  SELECT ordinality, value,
         jsonb_typeof(value)='object' AS is_prefix 
  FROM jsonb_array_elements(p_list) WITH ORDINALITY
 ) t, LATERAL ( -- BELOW UGLY! all for get a key!!
     SELECT *
     FROM jsonb_each(CASE WHEN t.is_prefix THEN t.value ELSE '{}'::jsonb END)
     UNION 
     (SELECT '' as key, NULL::jsonb as value WHERE not(t.is_prefix))
 ) t2
$f$ LANGUAGE SQL IMMUTABLE;

SELECT * FROM jreduxseq_to_text(
  '[&quot;123&quot;,{&quot;df62&quot;:[&quot;6cV&quot;,&quot;6eM&quot;,{&quot;3&quot;:[&quot;6f&quot;,&quot;1&quot;]}]},{&quot;G&quot;:[&quot;1&quot;,&quot;2&quot;]},&quot;H&quot;,&quot;X&quot;]'
  ::jsonb);
</code></pre>
<ol>
<li>Very complex SQL (LATERAL and CASE) to achieve something so simple.</li>
<li>Side-effect of disappearing an entire row, requiring more artificial complexity with UNION.</li>
</ol>
",-2,0,2,2025-08-08T10:25:32+00:00,2,125,True
79729716,7836976,Ireland,postgresql,Use function on foreign key constraint,"<p>Can I have a foreign key constraint that takes a timestamp column but uses it as date only?</p>
<p>I'm trying something like this but it doesn't work:</p>
<pre class=""lang-sql prettyprint-override""><code>alter table 
  table1 
add 
  constraint my_constraint foreign key (
    date(created_date)
  ) references table2(
    date(transaction_timestamp)
  );

</code></pre>
",2,3,1,2025-08-08T11:42:22+00:00,2,153,True
79731194,17089291,,postgresql,TypeORM: importing entities via directory path fails with multiple database connections,"<p>In NestJS, I import entities via directory path:</p>
<pre><code>@Module({
  imports: [
    TypeOrmModule.forRoot({
      name: DatabaseName.USERS, 
      database: envVar.POSTGRES_USERS_DB, 
      retryAttempts: 3,
      entities: [__dirname + '/../models/users/**/*.entity{.ts,.js}'], 
      ...commonDBConfig,
    }),
    TypeOrmModule.forRoot({
      name: DatabaseName.BOOKS, 
      database: envVar.POSTGRES_BOOKS_DB,
      retryAttempts: 3,
      entities: [__dirname + '/../models/books/**/*.entity{.ts,.js}'], 
      ...commonDBConfig,
    }),
</code></pre>
<p>I try to inject a repository in a service and use it like this:</p>
<pre><code>private readonly UsersRepository: Repository&lt;User&gt;

...

const users = await this.UserRepository.find({});
</code></pre>
<p>But I get an error:</p>
<blockquote>
<p>No metadata for &quot;User&quot; was found.</p>
</blockquote>
<p>This is the absolute path of the entity:</p>
<pre><code>Root-Repo\src\models\users\data\entities\user.entity.ts
</code></pre>
<p>Importing entities like this does work:</p>
<pre><code>TypeOrmModule.forRoot({
  name: DatabaseName.USERS, 
  database: envVar.POSTGRES_USERS_DB,
  retryAttempts: 3,
  entities: [User, Profile], 
  ...commonDBConfig,
}),
</code></pre>
<p>Typeorm version:</p>
<pre><code>&quot;typeorm&quot;: &quot;^0.3.25&quot;
</code></pre>
<p>I don't want to give up on the import via directory path, is it fixable?</p>
",2,2,0,2025-08-10T11:50:18+00:00,1,111,False
79732194,4842667,,postgresql,Use trigger to log addition of child record only if parent not added in same transaction,"<p>I'm using triggers in Postgres to do audit logging in a system with parent/child tables. Think an invoice with line items.</p>
<p>The problem I'm trying to solve is that if an invoice is new, the invoice and line item records are inserted within a transaction. In that case, I want to log that the invoice was created, but I don't need to log the creation of each line item. However, if someone comes back after the fact and adds a new line item, I do want to log the creation of that line item.</p>
<p>I can think of some kludgy-feeling ways to do this involving extra flag fields or maintaining a table of session variables or suchlike, but I can't find a way to check for this condition inside the trigger function. I know I can pass args into a trigger function, but I don't see any way to check for &quot;just inserted&quot; or &quot;inserted within this transaction&quot; to supply the correct argument value without still having to add some sort of audit status field.</p>
<p>Is there a clean mechanism that addresses this or is the &quot;extra fields&quot; route the way to go?</p>
",2,2,0,2025-08-11T15:10:28+00:00,2,93,True
79732490,30455159,,postgresql,How to insert large amounts of data from CSV into Valentina Studio?,"<p>I want to import (relatively) large amounts of data from CSV files into a PostgreSQL database but ran into some problems.</p>
<p>I have learned the basics of SQL in PostgreSQL - if I create a table (I do it in PowerShell after a SQL bootcamp), the foreign key gets a &quot;UUID REFERENCES&quot; part. I wanted to import large amounts of data (350k records) from CSV files in Valentina Studio 15 into this DB but the (integer) foreign key column threw the error:</p>
<blockquote>
<p>invalid input syntax for type uuid &quot;92&quot;</p>
</blockquote>
<p>where the first foreign key value is 92.</p>
<p>Why is that and how do I insert/import my pre-existing data? If I do this with a simple integer, how will the link come into existence? (I did this in Valentina Studio 15 before but had to create the link manually).
I also have no idea how to import this amount of data (CSVs) from PowerShell.</p>
",0,0,0,2025-08-11T20:45:08+00:00,0,78,False
79735413,6180031,,postgresql,Passing parameter to JSON filter from EF Core to PostgreSQL 17,"<p>My ultimate goal is to pass the filter regular expression as a named parameter to the query, but I am stuck with the basic syntax.</p>
<p>This works:</p>
<pre><code>SELECT *
FROM runtime.&quot;Results&quot;
WHERE 
    json_exists(
        &quot;ListViewData&quot;,
        '$.** ? (@ like_regex &quot;wahtever&quot;)'
    )
</code></pre>
<p>This is the example from <a href=""https://www.postgresql.org/docs/current/functions-json.html#SQLJSON-QUERY-FUNCTIONS"" rel=""nofollow noreferrer"">here</a>, that obviously works:</p>
<pre><code>select JSON_EXISTS(jsonb '{&quot;key1&quot;: [1,2,3]}', 'strict $.key1[*] ? (@ &gt; $x)' PASSING 2 AS x)
</code></pre>
<p>I have tailored a string-based version from it that also works:</p>
<pre><code>select JSON_EXISTS(jsonb '{&quot;key1&quot;: [1,2,&quot;what&quot;]}', '$.** ? (@ == $x)' PASSING 'what' AS x)
</code></pre>
<p>However, my original one doesn't work with <code>PASSING</code>:</p>
<pre><code>SELECT *
FROM runtime.&quot;Results&quot;
WHERE 
    json_exists(
        &quot;ListViewData&quot;,
        '$.** ? (@ like_regex $x)'  PASSING 'what' AS x
    )
</code></pre>
<p>Fails with:</p>
<blockquote>
<p>SQL Error [42601]: ERROR: syntax error at or near &quot;$x&quot; of jsonpath input</p>
</blockquote>
<p>This also fails:</p>
<pre><code>select JSON_EXISTS(jsonb '{&quot;key1&quot;: [1,2,&quot;what&quot;]}', '$.** ? (@ like_regex $x)' PASSING 'what' AS x)
</code></pre>
<p>That means that <code>like_regex</code> doesn't support dynamic parameters. That isn't explicitly stated anywhere in the documentation, although there is a sentence that talks about the pattern as literal. Is this the case?</p>
<p>Can I somehow still pass the filter dynamically as a proper parameter?</p>
",2,2,0,2025-08-14T12:37:52+00:00,0,22,False
79736389,7740888,,postgresql,How does the Postgres daterange() function work?,"<p>This SQL:</p>
<pre class=""lang-sql prettyprint-override""><code>select daterange(current_date, current_date);
</code></pre>
<p>Results in:</p>
<pre class=""lang-none prettyprint-override""><code> daterange 
-----------
 empty
(1 row)
</code></pre>
<p>But the <code>daterange()</code> function does not seem to be documented. I've looked <a href=""https://www.postgresql.org/docs/current/rangetypes.html#RANGETYPES-IO"" rel=""nofollow noreferrer"">here</a> and <a href=""https://postgrespro.com/docs/postgresql/13/functions-range"" rel=""nofollow noreferrer"">there</a>. All I can see is that <code>daterange()</code> creates a <code>daterange</code> value with an exclusive end-date (hence the <code>empty</code> result).</p>
<p>Where can I find more about this function?</p>
<p>Is there a related function that can create a <code>daterange</code> value with an inclusive end-date?</p>
",1,1,0,2025-08-15T11:46:29+00:00,3,172,True
79736466,294657,,postgresql,Why does my query get slower and not faster when I constrain the data set?,"<p>I have a TimescaleDB table that records activities of various devices. This is a huge table and any interactions with it are normally expensive. Devices can be enabled as a group, and activation has a planned start and end time. I want to see which currently enabled group has recently stopped producing activities.</p>
<p>If I write a query like this:</p>
<pre class=""lang-sql prettyprint-override""><code>select *
from group g
where g.state = 'enabled'
    --there was activity within the past 2 days
    and exists(
               select 1
               from activity act
                 join device d on act.device_id = d.id
               where d.group_id = g.id
                 and act.date &gt;= current_timestamp - interval '2 day')
    --there was no activity in the past day
    and not exists(
                   select 1
                   from activity act
                     join device d on act.device_id = d.id
                   where d.group_id = g.id
                     and act.date &gt;= current_timestamp - interval '1 day');

</code></pre>
<p>it executes in 3s.</p>
<p>Yet if I further constrain the groups to only those that are still within their planned activation period, or out of it by less than a week:</p>
<pre class=""lang-sql prettyprint-override""><code>select *
from group g
where g.state = 'enabled'
    --adding this condition slows things down???
    and g.planned_end &gt;= current_timestamp - interval '7 days'
    and exists( 
               select 1
               from activity act
                 join device d on act.device_id = d.id
               where d.group_id = g.id
                 and act.date &gt;= current_timestamp - interval '2 day')
    and not exists( 
                   select 1
                   from activity act
                     join device d on act.device_id = d.id
                   where d.group_id = g.id
                     and act.date &gt;= current_timestamp - interval '1 day');

</code></pre>
<p>the execution time balloons to 8s!</p>
<p>Why would this happen? One would assume further filtering the groups prior to the expensive <code>exists</code> subqueries would speed it up, not slow it down. What gives?</p>
<p>Here are the execution plans for both cases:</p>
<pre><code>Nested Loop Anti Join  (cost=9604961.92..19571124.95 rows=3 width=661)
  Join Filter: (d_1.group_id = g.id)
  -&gt;  Merge Join  (cost=9603623.70..9603624.28 rows=17 width=661)
        Merge Cond: (g.id = d.group_id)
        -&gt;  Sort  (cost=3.32..3.37 rows=20 width=661)
              Sort Key: g.id
              -&gt;  Seq Scan on group r  (cost=0.00..2.89 rows=20 width=661)
                    Filter: (state = 'active'::group_state)
        -&gt;  Sort  (cost=9603620.38..9603620.54 rows=62 width=16)
              Sort Key: d.group_id
              -&gt;  HashAggregate  (cost=9603617.92..9603618.54 rows=62 width=16)
                    Group Key: d.group_id
                    -&gt;  Gather  (cost=1338.22..9570071.15 rows=13418705 width=16)
                          Workers Planned: 2
                          -&gt;  Hash Join  (cost=338.22..8227200.65 rows=5591127 width=16)
                                Hash Cond: (act.device_id = d.id)
                                -&gt;  Parallel Custom Scan (ChunkAppend) on activity act  (cost=0.00..8211590.04 rows=5814701 width=16)
                                      Chunks excluded during startup: 200
                                      -&gt;  Parallel Seq Scan on _hyper_1_263_chunk act_2  (cost=0.00..152549.80 rows=198561 width=16)
                                            Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
                                      -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_3  (cost=0.00..35876.12 rows=782444 width=16)
                                            Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
                                -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                                      -&gt;  Seq Scan on device a  (cost=0.00..246.43 rows=7343 width=32)
  -&gt;  Materialize  (cost=1338.22..9571273.34 rows=12234558 width=16)
        -&gt;  Gather  (cost=1338.22..9450360.55 rows=12234558 width=16)
              Workers Planned: 2
              -&gt;  Hash Join  (cost=338.22..8225904.75 rows=5097732 width=16)
                    Hash Cond: (act_1.device_id = d_1.id)
                    -&gt;  Parallel Custom Scan (ChunkAppend) on activity act_1  (cost=0.00..8211590.04 rows=5321306 width=16)
                          Chunks excluded during startup: 201
                          -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_4  (cost=0.00..35876.12 rows=487285 width=16)
                                Filter: (date &gt;= (CURRENT_TIMESTAMP - '1 day'::interval))
                    -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                          -&gt;  Seq Scan on device d_1  (cost=0.00..246.43 rows=7343 width=32)
</code></pre>
<p>and</p>
<pre><code>Nested Loop Semi Join  (cost=2676.58..19021891.33 rows=1 width=661)
  Join Filter: (g.id = d.group_id)
  -&gt;  Nested Loop Anti Join  (cost=1338.37..9451818.12 rows=1 width=661)
        Join Filter: (d_1.group_id = g.id)
        -&gt;  Index Scan using complex_key on group r  (cost=0.15..8.77 rows=2 width=661)
              Index Cond: (ended_at &gt;= (CURRENT_TIMESTAMP - '7 days'::interval))
              Filter: (state = 'active'::group_state)
        -&gt;  Gather  (cost=1338.22..9450372.27 rows=12234722 width=16)
              Workers Planned: 2
              -&gt;  Hash Join  (cost=338.22..8225900.07 rows=5097801 width=16)
                    Hash Cond: (act_1.device_id = d_1.id)
                    -&gt;  Parallel Custom Scan (ChunkAppend) on activity act_1  (cost=0.00..8211585.18 rows=5321374 width=16)
                          Chunks excluded during startup: 201
                          -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_4  (cost=0.00..35871.27 rows=487353 width=16)
                                Filter: (date &gt;= (CURRENT_TIMESTAMP - '1 day'::interval))
                    -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                          -&gt;  Seq Scan on device d_1  (cost=0.00..246.43 rows=7343 width=32)
  -&gt;  Gather  (cost=1338.22..9570071.66 rows=13418758 width=16)
        Workers Planned: 2
        -&gt;  Hash Join  (cost=338.22..8227195.86 rows=5591149 width=16)
              Hash Cond: (act.device_id = d.id)
              -&gt;  Parallel Custom Scan (ChunkAppend) on activity act  (cost=0.00..8211585.18 rows=5814723 width=16)
                    Chunks excluded during startup: 200
                    -&gt;  Parallel Seq Scan on _hyper_1_263_chunk act_2  (cost=0.00..152549.80 rows=198689 width=16)
                          Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
                    -&gt;  Parallel Seq Scan on _hyper_1_265_chunk act_3  (cost=0.00..35871.27 rows=782338 width=16)
                          Filter: (date &gt;= (CURRENT_TIMESTAMP - '2 days'::interval))
              -&gt;  Hash  (cost=246.43..246.43 rows=7343 width=32)
                    -&gt;  Seq Scan on device a  (cost=0.00..246.43 rows=7343 width=32)
</code></pre>
",3,3,0,2025-08-15T13:21:24+00:00,1,99,True
79736516,7836976,Ireland,postgresql,Syntax error when trying to create PostgreSQL function,"<p>I'm trying to create this PostgreSQL function:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION sales.fn_drop_old_partitions(
    )
    RETURNS boolean
    LANGUAGE 'plpgsql'
    COST 100
    VOLATILE PARALLEL UNSAFE
AS $BODY$   
DECLARE
    quote_table_partition character varying(255);
BEGIN

    quote_table_partition := select pt.relname as partition_name
    from pg_class base_tb 
    join pg_inherits i on i.inhparent = base_tb.oid 
    join pg_class pt on pt.oid = i.inhrelid
    where base_tb.oid = 'quote_table'::regclass
    ORDER BY pt.relname asc
    LIMIT 1;
    
    RAISE NOTICE 'Value of : %', quote_table_partition;
    
    ALTER TABLE sales.quote_table DETACH PARTITION quote_table_partition;
    DROP TABLE quote_table_partition;
    
    ANALYZE sales.quote_table;
    RETURN true;
END;
$BODY$
</code></pre>
<p>When I run the code I get this error:</p>
<pre class=""lang-none prettyprint-override""><code>ERROR:  syntax error at or near &quot;select&quot;
LINE 13:  quote_table_partition := select pt.relname a...
                                                  ^
SQL state: 42601
Character: 361
</code></pre>
<p>The <code>SELECT</code> works on its own and beyond ensuring semicolons are at the end of each statement, I've no idea what the issue is. Can anyone help?</p>
",-1,0,1,2025-08-15T14:30:43+00:00,1,79,False
79736536,4376,"Athens, Georgia, United States",postgresql,Is there a better way of doing &quot;natural sort&quot; in PostgreSQL?,"<p>I'm doing some work where I need &quot;natural sort&quot; - eg, &quot;Foo 2&quot; comes before &quot;Foo 10&quot;. In application code (Elixir), this is not hard: use a regex to split a string into numeric and text chunks, and create an array/list of strings and numbers. It sorts as expected.</p>
<pre class=""lang-none prettyprint-override""><code>  @doc &quot;&quot;&quot;
  Generates a natural sort key for a string so that (for example) strings sort
  as &quot;foo 1&quot;, &quot;foo 2&quot;, &quot;foo 10&quot;, instead of &quot;foo 1&quot;, &quot;foo 10&quot;, &quot;foo 2&quot;.

  It does this by splitting the string into a list of lowercase strings and
  numbers.

  Note that numbers sort before strings: in Elixir/Erlang: `number &lt; atom &lt;
  reference &lt; function &lt; port &lt; pid &lt; tuple &lt; map &lt; list &lt; bitstring`

  Examples:

    iex&gt; natural_sort_key(&quot;foo 1&quot;)
    [&quot;foo &quot;, 1]
    
    iex&gt; natural_sort_key(&quot;FOO 10&quot;)
    [&quot;foo &quot;, 10]

    iex&gt; natural_sort_key(&quot;foo 1.3&quot;)
    [&quot;foo &quot;, 1.3]

    iex&gt; natural_sort_key(&quot;🌟 Café 1.3&quot;)
    [&quot;🌟 café &quot;, 1.3]

    iex&gt; natural_sort_key(&quot;2 foo 1.3 bar 2&quot;)
    [2, &quot; foo &quot;, 1.3, &quot; bar &quot;, 2]

  Sorting examples:

    iex&gt; Enum.sort_by(
    ...&gt;   [&quot;1.1B&quot;, &quot;1.2A&quot;, &quot;1A&quot;, &quot;10A&quot;, &quot;1B&quot;, &quot;1.1A&quot;],
    ...&gt;   &amp;natural_sort_key/1
    ...&gt; )
    [&quot;1A&quot;, &quot;1B&quot;, &quot;1.1A&quot;, &quot;1.1B&quot;, &quot;1.2A&quot;, &quot;10A&quot;]

    iex&gt; Enum.sort_by(
    ...&gt;   [%{name: &quot;foo1&quot;}, %{name: &quot;foo10&quot;}, %{name: &quot;foo2&quot;}],
    ...&gt;   fn foo -&gt; natural_sort_key(foo.name) end
    ...&gt; )
    [%{name: &quot;foo1&quot;}, %{name: &quot;foo2&quot;}, %{name: &quot;foo10&quot;}]

    iex&gt; Enum.sort_by(
    ...&gt;   [&quot;1&quot;, &quot;A&quot;, &quot;2&quot;, &quot;B&quot;],
    ...&gt;   fn val -&gt; natural_sort_key(val) end
    ...&gt; )
    [&quot;1&quot;, &quot;2&quot;, &quot;A&quot;, &quot;B&quot;]
  &quot;&quot;&quot;
  @spec natural_sort_key(String.t()) :: [String.t() | integer() | float()]
  def natural_sort_key(string) when is_binary(string) do
    # Use named captures to identify floats, integers, and text parts
    ~r/(?&lt;float&gt;\d+\.\d+)|(?&lt;integer&gt;\d+)|(?&lt;text&gt;[^\d]+)/
    |&gt; Regex.scan(string, capture: :all_names)
    |&gt; Enum.map(fn
      [&quot;&quot;, integer, &quot;&quot;] when integer != &quot;&quot; -&gt; String.to_integer(integer)
      [float, &quot;&quot;, &quot;&quot;] when float != &quot;&quot; -&gt; String.to_float(float)
      [&quot;&quot;, &quot;&quot;, text] when text != &quot;&quot; -&gt; String.downcase(text)
    end)
  end
</code></pre>
<p>Getting PostgreSQL to sort the same way proved difficult. I tried &quot;numeric collation&quot; as shown in <a href=""https://stackoverflow.com/a/67975489/4376"">this answer</a>, but it didn't handle all the cases I expected.</p>
<p>Finally I (with the help of AI, a test suite, and a lot of patience) came up with this complex, expensive, but working function, which allows doing <code>ORDER BY natural_sort_key(any_textual_column)</code>.</p>
<pre class=""lang-sql prettyprint-override""><code>-- Allow doing `ORDER BY natural_sort_key(any_textual_column)`
-- such that it sorts numbers naturally (e.g., &quot;HVAC2&quot; before &quot;HVAC10&quot;).
-- Works on text, citext, varchar, char columns.
CREATE FUNCTION &quot;natural_sort_key&quot;(input_text text) RETURNS jsonb AS $$
BEGIN
  RETURN (
    WITH components AS (
      SELECT jsonb_agg(
        CASE
          -- Split the text into chunks of contiguous numbers and text.
          -- We need jsonb arrays, not normal arrays, to support a mix of numbers and text.
          --
          -- In each case, we wrap the value in an array and add a leading 0 or 1.
          -- This is because JSONB natively sorts numbers after strings, and we need
          -- to overrule that to be consistent with how sorting works elsewhere.
          --
          -- We also need to pad the json array because shorter arrays are
          -- sorted before longer ones.
          WHEN m[1] ~ '^\d+\.\d+$' THEN jsonb_build_array(0, m[1]::numeric)
          WHEN m[1] ~ '^\d+$' THEN jsonb_build_array(0, m[1]::numeric)
          ELSE jsonb_build_array(1, lower(m[1]))
        END
      ) as components_array
      FROM regexp_matches(input_text, '\d+\.\d+|\d+|[a-zA-Z]+', 'g') m
    )
    SELECT components_array || (
      -- To ensure that the number of &quot;chunks&quot; doesn't affect the sorting, ensure
      -- that we always have 10 of them.
      -- (This would break if the input had more than 10 chunks, like
      -- 1A3A5A7D9E11, but that's very unlikely.)
      -- Pad with [2, ''] so that these &quot;blank&quot; chunks sort after all actual numbers or text.
      SELECT jsonb_agg(jsonb_build_array(2, ''))
      FROM generate_series(1, GREATEST(0, 10 - jsonb_array_length(components_array)))
    )
    FROM components
  );
END;
$$ LANGUAGE plpgsql IMMUTABLE;
</code></pre>
<p>Here are the cases where I confirmed that it does the right thing:</p>
<pre class=""lang-sql prettyprint-override""><code>-- Test Suite: All these cases must sort in the exact order shown

-- Test 1: Purely numeric values
WITH test_purely_numeric AS (
  SELECT unnest(ARRAY['100', '10', '2', '1']) as name
)
SELECT 'Test 1: Purely Numeric' as test_name, 
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       '1, 2, 10, 100' as expected
FROM test_purely_numeric;

-- Test 2: Purely alphabetic values  
WITH test_purely_alpha AS (
  SELECT unnest(ARRAY['Gamma', 'Alpha', 'Beta']) as name
)
SELECT 'Test 2: Purely Alphabetic' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       'Alpha, Beta, Gamma' as expected
FROM test_purely_alpha;

-- Test 3: Mixed alphanumeric values
WITH test_mixed_alpha AS (
  SELECT unnest(ARRAY['HVAC B1', 'HVAC A10', 'HVAC 1', 'HVAC A1', 'HVAC 2', 'HVAC A2']) as name
)
SELECT 'Test 3a: Mixed Alphanumeric' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       'HVAC 1, HVAC 2, HVAC A1, HVAC A2, HVAC A10, HVAC B1' as expected
FROM test_mixed_alpha;

WITH test_mixed_alpha2 AS (
  SELECT unnest(ARRAY['5', '4', '1.1A', '1A']) as name
)
SELECT 'Test 3b: Mixed Alphanumeric' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       '1A, 1.1A, 4, 5' as expected
FROM test_mixed_alpha2;

-- Test 4: Numbers before letters
WITH test_numbers_first AS (
  SELECT unnest(ARRAY['B', 'A', '2', '1']) as name
)
SELECT 'Test 4: Numbers Before Letters' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       '1, 2, A, B' as expected
FROM test_numbers_first;

-- Test 5: Multiple numbers in string
WITH test_multiple_numbers AS (
  SELECT unnest(ARRAY['Room 10 Unit 1', 'Room 2 Unit 1', 'Room 1 Unit 10', 'Room 1 Unit 2']) as name
)
SELECT 'Test 5: Multiple Numbers' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       'Room 1 Unit 2, Room 1 Unit 10, Room 2 Unit 1, Room 10 Unit 1' as expected
FROM test_multiple_numbers;

-- Test 6: Leading zeros
WITH test_leading_zeros AS (
  SELECT unnest(ARRAY['Unit 10', 'Unit 02', 'Unit 01']) as name
)
SELECT 'Test 6: Leading Zeros' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       'Unit 01, Unit 02, Unit 10' as expected
FROM test_leading_zeros;

-- Test 7: Case insensitive
WITH test_case_insensitive AS (
  SELECT unnest(ARRAY['unit 11', 'UNIT 10', 'Unit 3', 'unit 2', 'Unit 1']) as name
)
SELECT 'Test 7: Case Insensitive' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       'Unit 1, unit 2, Unit 3, UNIT 10, unit 11' as expected
FROM test_case_insensitive;

-- Test 8: Decimal handling (the complex one!)
WITH test_decimals AS (
  SELECT unnest(ARRAY[
    'B1.1', 'B1', 'A10', 'A1.2', 'A1.1', 'A1', '10A', '1.2A', 
    '1.1B', '1.1A', '1.01A', '1B', '1A'
  ]) as name
)
SELECT 'Test 8: Decimals' as test_name,
       string_agg(name, ', ' ORDER BY natural_sort_key(name)) as sorted_result,
       '1A, 1B, 1.01A, 1.1A, 1.1B, 1.2A, 10A, A1, A1.1, A1.2, A10, B1, B1.1' as expected
FROM test_decimals;
</code></pre>
<p>I made a fiddle where you can play with the function and the test cases here: <a href=""https://www.db-fiddle.com/f/pEYUN1z2ickWZAq1iDM3Y/6"" rel=""nofollow noreferrer"">https://www.db-fiddle.com/f/pEYUN1z2ickWZAq1iDM3Y/6</a></p>
<p>There has to be a simpler, cheaper way, right?</p>
",4,4,0,2025-08-15T14:50:02+00:00,1,119,True
79736595,3971553,"Brindisi, BR, Italia",postgresql,POSTGRES equivalent of mysqli_select_db,"<p>How can I switch to a different database in POSTGRES via PHP?</p>
<p>I mean a way similar to</p>
<p><code>mysqli_select_db( $db_conn, $db_name )</code></p>
<p>on any other workaround, but using the same connection.</p>
",0,1,1,2025-08-15T15:52:40+00:00,3,123,True
79736957,23896719,,postgresql,Odoo 18 on Windows: UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe7 on server startup,"<p>I'm setting up a development environment for Odoo 18 on a Windows machine. I've cloned the repository, created a virtual environment (venv), and successfully installed all dependencies from the requirements.txt file.</p>
<p>However, when I try to start the Odoo server for the first time using the command below, the process fails with a UnicodeDecodeError.</p>
<p>Command executed:</p>
<p>PowerShell</p>
<pre><code>(venv) PS D:\tarefas\trarefa_kaue_1\teste\odoo&gt; python odoo-bin -r odoo -w 123
Full Error (Traceback):


2025-08-16 04:07:47,493 14676 INFO ? odoo: Odoo version 18.0
2025-08-16 04:07:47,493 14676 INFO ? odoo: addons paths: ['D:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\addons', 'c:\\users\\kauen\\appdata\\local\\openerp s.a\\odoo\\addons\\18.0', 'd:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\odoo\\addons', 'd:\\tarefas\\trarefa_kaue_1\\teste\\odoo\\addons']
2025-08-16 04:07:47,493 14676 INFO ? odoo: database: odoo@default:default
2025-08-16 04:07:50,148 14676 INFO ? odoo.addons.base.models.ir_actions_report: You need Wkhtmltopdf to print a pdf version of the reports.
2025-08-16 04:07:50,152 14676 INFO ? odoo.addons.base.models.ir_actions_report: You need Wkhtmltoimage to generate images from html.
2025-08-16 04:07:52,396 14676 INFO ? odoo.service.server: HTTP service (werkzeug) running on kaue_martins.intelbras.local:8069
Exception in thread odoo.service.cron.cron0:
Traceback (most recent call last):
  File &quot;C:\Python312\Lib\threading.py&quot;, line 1075, in _bootstrap_inner
    self.run()
  File &quot;C:\Python312\Lib\threading.py&quot;, line 1012, in run
    self._target(*self._args, **self._kwargs)
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\odoo\service\server.py&quot;, line 541, in target
    self.cron_thread(i)
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\odoo\service\server.py&quot;, line 522, in cron_thread
    with contextlib.closing(conn.cursor()) as cr:
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\odoo\sql_db.py&quot;, line 799, in cursor
    return Cursor(self.__pool, self.__dbname, self.__dsn)
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\odoo\sql_db.py&quot;, line 288, in __init__
    self._cnx = pool.borrow(dsn)
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\venv\Lib\site-packages\decorator.py&quot;, line 232, in fun
    return caller(func, *(extras + args), **kw)
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\odoo\tools\func.py&quot;, line 97, in locked
    return func(inst, *args, **kwargs)
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\odoo\sql_db.py&quot;, line 726, in borrow
    result = psycopg2.connect(
  File &quot;D:\tarefas\trarefa_kaue_1\teste\odoo\venv\Lib\site-packages\psycopg2\__init__.py&quot;, line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 78: invalid continuation byte
</code></pre>
<p>My Environment Details:</p>
<p>OS: Windows 11</p>
<p>Odoo: Version 18.0</p>
<p>Python: Version 3.12 (installed at C:\Python312)</p>
<p>PostgreSQL: Version 15</p>
<p>What I've investigated so far:</p>
<p>The traceback indicates the error occurs during the psycopg2.connect call, which is the library used to connect to PostgreSQL.</p>
<p>The UnicodeDecodeError with byte 0xe7 suggests there's a special character (likely ç in a latin-1 or windows-1252 encoding) somewhere in the connection string that is not UTF-8 encoded.</p>
<p>I have checked the parameters I passed on the command line (-r odoo and -w 123), and they do not contain any special characters.</p>
<p>The PostgreSQL database was initialized with UTF-8 encoding.</p>
<p>My suspicion is that the problem might be in a Windows environment variable or in the Odoo configuration file (odoo.conf), which might be read with an incorrect encoding (like windows-1252) instead of UTF-8.</p>
<p>My questions are:</p>
<p>Where do Odoo or psycopg2 on Windows look for connection information besides the command-line parameters?</p>
<p>What is the most likely cause for this UnicodeDecodeError in this scenario, and how can I fix it?</p>
<p>Thanks for any help!</p>
",3,3,0,2025-08-16T04:34:51+00:00,1,204,True
79737525,13968392,,postgresql,Connection string to read and write database with different engines,"<p>In polars, there are the engines <code>connextorx</code> and <code>adbc</code> for <code>pl.read_database_uri</code> and <code>sqlalchemy</code> and <code>adbc</code> for <code>pl.write_database</code>. In the case of a postgresql database: Is it possible to use the same connection string for these engines when the connection string looks as follows?</p>
<pre><code>&quot;postgresql://scott:tiger@localhost:5432/mydatabase?options=-c%20default_transaction_read_only%3DTrue&quot;
</code></pre>
<p>The essential part is that read_only should be True.
I just know that <code>adbc</code> accepts the part <code>options=-c%20default_transaction_read_only%3DTrue</code>. If it is not possible to use such a connection string for these engines, what would be the closest alternative?</p>
",0,0,0,2025-08-16T22:30:05+00:00,3,149,True
79737680,29738716,,postgresql,Count relations in Typeorm and then use the result column,"<p>I have an entities <code>User</code> and <code>Post</code> with <code>many-to-many</code> relation (table with <code>postId</code> and <code>userId</code>).</p>
<p>I need to find all users without given <code>postId</code>.</p>
<p><code>leftjoin</code> with exclude id doesn't work, because if user have multiple posts, then it always returns user.</p>
<p>So far, <code>loadRelationCountAndMap</code> is a best result (it returns counts)</p>
<pre><code>const builder = this.userRepository
    .createQueryBuilder('user')
    .loadRelationCountAndMap(
        'user.userPosts',
        'user.posts',
        'userPosts',
        (qb) =&gt; {
            return qb.andWhere(
                'userPosts.userId != :excludePostId',
                { excludePostId },
            );
        },
    );
</code></pre>
<p>But i can't do something like this</p>
<pre class=""lang-js prettyprint-override""><code>builder.andWhere('userPosts &gt; 0');
</code></pre>
<p>It cause en error, <code>Column not found.</code></p>
<p>Also try this</p>
<pre class=""lang-js prettyprint-override""><code>builder.innerJoin('user.posts', 'userPosts')
    .addSelect('COUNT(userPosts.userId)', 'userPostsCount')
    .groupBy('user.id')
    .addGroupBy('userPosts.userId')
    .addGroupBy('userPosts.postId')
</code></pre>
<p>Column exist in <code>rawQuery</code> but still, when i try</p>
<pre class=""lang-js prettyprint-override""><code>builder.andWhere('userPostsCount &gt; 0');
</code></pre>
<p>Same error <code>Column not found</code></p>
",1,1,0,2025-08-17T06:58:11+00:00,1,41,True
79737831,952740,,postgresql,What is the right column type to use for storing currency amounts?,"<p>I have this fintech application (written in Ruby with some JS micro-services) which stores transactions in a database table, amounts are stored as integers in cents, eg $123.45 is stored as 12345, -Є56.78 is stored as -5678. The transactions table currently has about 400 million entries across all partitions. Amounts are typically under 4 digits with under 1% being over 4 digits and under 0.1% over 6 digits.</p>
<p>This has worked pretty well for me for a number of years but now I am facing two issues:</p>
<ol>
<li>some amounts in a few specific currencies can't be stored eg Rials since $1 would be about 45000 Rials, so far I just didn't offer support for these currencies and it caused some loss of business.</li>
<li>for the same reason can't store amounts in a large number of cryptocurrencies/tokens including Bitcoins</li>
</ol>
<p>The time has come to sort this out...hopefully without breaking any of the Ruby or JS applications as I know both languages and also the database will have some issues with floating point numbers.</p>
<p>What data type should I use to store these amounts?</p>
",1,1,0,2025-08-17T12:42:24+00:00,2,155,True
79738755,8655052,,postgresql,PostgreSQL stored procedure call from Spring Boot gives &quot;Procedure does not exist&quot; and &quot;might need to add explicit type casts&quot; error,"<p>I am trying to learn and implement PostgreSQL Stored procedure call from Spring Boot repository using <code>@Procedure</code> annotated method. While calling stored procedure I am getting error saying that:</p>
<p><strong>Error</strong></p>
<pre><code>org.postgresql.util.PSQLException: ERROR: procedure spgetusers1(integer) does not exist
Hint: No procedure matches the given name and argument types. You might need to add explicit type casts.
</code></pre>
<p><strong>Stored Procedure</strong></p>
<pre><code>CREATE OR REPLACE PROCEDURE spgetusers1(nuseridparam  integer
                                   ,out nuserid   integer
                                   ,out susername text
                                   )                                    
LANGUAGE 'plpgsql'
AS $BODY$
BEGIN   
  SELECT
    u.nuser_id,
    u.nuser_name      
  into nuserid, susername
  FROM
    users u
  WHERE 
   u.nuser_id = nuseridparam;   
 END; 
$BODY$;
</code></pre>
<p><strong>Called Repository Method</strong></p>
<pre><code>@Procedure(name = &quot;spgetusers1&quot;)
List&lt;Users&gt; spgetusers1(@Param(&quot;nuserid&quot;) Integer nuserid);
</code></pre>
<p>I am able to get result using stored function way. Here I am trying to learn Spring Boot with stored procedure which contains <code>SELECT</code> statement and trying to return query result</p>
<p>Is there any wrong implementation I followed here to use <code>@Procedure</code>? How do I need to change my implementation?</p>
",-2,1,3,2025-08-18T13:00:37+00:00,4,411,True
79739025,583271,"Belfast, UK",postgresql,Query to set a value to null if it contains non-numeric characters,"<p>I currently have this query, where <code>response</code> a column defined as  <code>varchar(255)</code>:</p>
<pre><code>SELECT 
    CAST((NULLIF(REGEXP_REPLACE(response, '[^0-9]+', '', 'g'), ''), '0') AS INTEGER) 
FROM
    my_table;
</code></pre>
<p>The idea is that we only want to pull out those rows where the value of response is numeric. So this strips out non-numeric values, sets to null if the result is 0, and casts to an integer.</p>
<p>This is working fine for most columns, except when the column is a date - this removes all the ':' and '+' etc. and ends up with a number which it tries and fails to parse (because it's too long for an integer).</p>
<p>I'm not sure that this is the best way of handling this case anyway - is there a way to return a number if the column contains a string representing a number, and null otherwise? E.g. try to cast to an integer and return null if it fails or something like that?</p>
",3,4,1,2025-08-18T16:32:35+00:00,7,303,True
79740005,13390413,,postgresql,How can I make Telegraf create TimescaleDB columns as double precision[] when ingesting array data from MQTT?,"<p>I am trying to configure Telegraf to ingest array data from an MQTT topic into TimescaleDB.
My goal is for array-like values to end up as <code>double precision[]</code> instead of <code>text</code>.</p>
<p>For example, if I prepare the TimescaleDB schema manually:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE sensor_data.accelerometer_list (
    time timestamptz NOT NULL,
    sensor_id text NOT NULL,
    values double precision[] NOT NULL
);

INSERT INTO sensor_data.accelerometer_list (time, sensor_id, values)
VALUES (now(), 'accel_1', ARRAY[3.141598745, 2.718281828459045, -9.123456789]);
</code></pre>
<p>…then start my datalogger (publishing JSON over MQTT):</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;TableName&quot;:&quot;accelerometer_list&quot;,
  &quot;Timestamp&quot;:&quot;2025-08-19T09:30:26.4734288Z&quot;,
  &quot;Data&quot;:{&quot;accel_1&quot;:&quot;{0.952368,-1.25896,10.582698}&quot;},
  &quot;Tags&quot;:{&quot;TelegramName&quot;:&quot;My favorite accelerometer&quot;,&quot;SensorID&quot;:&quot;3d8e00a3-65e3-4b62-9060-26ec55196c48&quot;}
}
</code></pre>
<p>Telegraf ingests successfully and the <code>accel_1</code> column has type <code>double precision[]</code>.</p>
<hr />
<p><strong>Problem:</strong>
If I don’t create the table ahead of time, Telegraf auto-creates it, but <code>accel_1</code> ends up as <code>text</code>.
This happens because Telegraf’s internal data model only supports <code>int</code>, <code>uint</code>, <code>float</code>, <code>string</code>, and <code>bool</code> — so arrays default to text.</p>
<hr />
<p><strong>Relevant config (shortened):</strong></p>
<pre class=""lang-ini prettyprint-override""><code>[[outputs.postgresql]]
  tagpass = { source = [&quot;mqtt_sensordata_timescaledb&quot;] }
  tagexclude = [&quot;source&quot;, &quot;topic&quot;]
  connection = &quot;host=…&quot;
  schema = &quot;${TIMESCALEDB_SCHEMA_NAME}&quot;
  timestamp_column_type = &quot;timestamp with time zone&quot;
  timestamp_column_name = &quot;time&quot;

  create_templates = [
    '''CREATE TABLE {{ .table }} ({{ .columns }})''',
    '''SELECT create_hypertable({{ .table|quoteLiteral }}, 'time', chunk_time_interval =&gt; INTERVAL '7d')''',
  ]
</code></pre>
<hr />
<p><strong>What I want to achieve:</strong>
I’d like Telegraf to detect that a field is an array and create the column with <code>double precision[]</code> instead of <code>text</code>.
Something like this pseudo-logic:</p>
<pre><code>If (table does not exist &amp;&amp; field value is a list):
    CREATE TABLE … key double precision[] …
Else if value is uint → uint
Else if value is float → float
Else if value is string → text
…
</code></pre>
<hr />
<p><strong>Question:</strong>
Is there a way to configure Telegraf’s <code>create_templates</code> (or another mechanism) so that it auto-creates <code>double precision[]</code> columns when ingesting array data from MQTT, instead of falling back to <code>text</code>?</p>
",2,2,0,2025-08-19T13:53:10+00:00,1,235,False
79740056,12473843,,postgresql,JPA repository method with native query containing &#39;&amp;&amp;&#39; works with Long[] but not Collection&lt;Long&gt;,"<p>In one of my Spring Data JPA repositories I have this native query (PostgreSQL):</p>
<pre class=""lang-java prettyprint-override""><code>@Query(value = &quot;SELECT * FROM ballots b WHERE b.companies &amp;&amp; :companyIds and b.meeting_id in :meetingIds&quot;, nativeQuery = true)
List&lt;Ballot&gt; findByIds(Long[] companyIds, Long[] meetingIds);
</code></pre>
<p>Here's my entity:</p>
<pre class=""lang-java prettyprint-override""><code>@Table(name=&quot;ballots&quot;)
public class Ballot {
    @Id
    private long id;

    @Column
    private long meetingId;

    @Type(ListArrayType.class)
    @Column(name = &quot;companies&quot;,columnDefinition = &quot;bigint[]&quot;)
    private List&lt;Long&gt; companies = new ArrayList&lt;&gt;();
}
</code></pre>
<p>Here's how I call the method:</p>
<pre class=""lang-java prettyprint-override""><code>private final BallotRepository ballotRepo;

  public void findBallots(Set&lt;Long&gt; companyIds, Set&lt;Long&gt; companyMeetingIds) {
    List&lt;Ballot&gt; ballots = ballotRepository.findByIds(companyIds, companyMeetingIds);
    // the rest of the code
  }
}
</code></pre>
<p>Method signature with</p>
<ul>
<li><code>Long[] companyIds, Long[] meetingIds</code> works fine</li>
<li><code>Long[] companyIds, Collection&lt;Long&gt; meetingIds</code> works fine</li>
<li><code>Collection&lt;Long&gt; companyIds, Collection&lt;Long&gt; meetingIds</code> fails:</li>
</ul>
<pre class=""lang-none prettyprint-override""><code>org.springframework.dao.InvalidDataAccessResourceUsageException: JDBC exception executing SQL [SELECT * FROM ballots b WHERE b.companies &amp;&amp; (?) and b.meeting_id in (?,?)] [ERROR: operator does not exist: bigint[] &amp;&amp; bigint
  Hint: No operator matches the given name and argument types. You might need to add explicit type casts.
  Position: 42] [n/a]; SQL [n/a]
</code></pre>
<p>How should I modify the query to make it work?</p>
",2,2,0,2025-08-19T14:29:05+00:00,1,156,True
79740817,6351559,,postgresql,Why does the sort method used by Postgres change from &quot;top-N&quot; to &quot;external merge&quot; when FOR UPDATE is added?,"<p>I have a table in Postgres (14) that is used like a queue with <code>FOR UPDATE SKIP LOCKED</code> queries. It was missing an index, but since any rows are usually processed quickly that never became a problem. Until it suddenly did, after a surge of new work, which slowed the entire database down. I now added the index and all is good, but I'd like to understand what really happened.</p>
<p>A simplified version of the table looks like this:</p>
<pre><code>CREATE TABLE outgoing(
    id int PRIMARY KEY,
    created_at timestamptz NOT NULL,
    some_data text NOT NULL
);
</code></pre>
<p>It is periodically queried like this:</p>
<pre><code>SELECT * 
FROM outgoing
ORDER BY created_at
LIMIT 100
FOR UPDATE SKIP LOCKED;
</code></pre>
<p>When I suddenly got around 500,000 rows in the table, the above query started to take about 30 seconds to execute. After some debugging, I've found that it was the sorting that was the culprit. With the <code>FOR UPDATE</code> clause, it would choose <code>Sort Method: external merge</code>. However, if I removed <code>FOR UPDATE</code> from the query, it changed to the much faster and less resource hungry (and also expected) <code>Sort Method: top-N heapsort</code>. Why does the presence of <code>FOR UPDATE</code> change the sort method?</p>
<p>The behaviour can be reproduced with the above table definition and query, plus the following block to add some rows:</p>
<pre><code>do $$
begin
    for i in 1..500000 loop
        insert into outgoing values (i, now() - make_interval(0, 0, 0, 0, 0, i), i::text);
    end loop;
end; $$;
</code></pre>
<p>Plan without <code>FOR UPDATE</code>:</p>
<pre><code>EXPLAIN (ANALYZE, BUFFERS) SELECT * 
FROM outgoing
ORDER BY created_at
LIMIT 100;

Limit  (cost=14230.69..14242.36 rows=100 width=18) (actual time=80.188..85.939 rows=100 loops=1)
  Buffers: shared hit=3305 dirtied=1
  -&gt;  Gather Merge  (cost=14230.69..62845.12 rows=416666 width=18) (actual time=80.186..85.929 rows=100 loops=1)
        Workers Planned: 2
        Workers Launched: 2
        Buffers: shared hit=3305 dirtied=1
        -&gt;  Sort  (cost=13230.67..13751.50 rows=208333 width=18) (actual time=25.455..25.459 rows=67 loops=3)
              Sort Key: created_at
              Sort Method: top-N heapsort  Memory: 40kB
              Buffers: shared hit=3305 dirtied=1
              Worker 0:  Sort Method: top-N heapsort  Memory: 40kB
              Worker 1:  Sort Method: quicksort  Memory: 25kB
              -&gt;  Parallel Seq Scan on outgoing  (cost=0.00..5268.33 rows=208333 width=18) (actual time=0.007..7.269 rows=166667 loops=3)
                    Buffers: shared hit=3185 dirtied=1
Planning Time: 0.097 ms
Execution Time: 85.972 ms
</code></pre>
<p>Plan with <code>FOR UPDATE</code>:</p>
<pre><code>EXPLAIN (ANALYZE, BUFFERS) SELECT * 
FROM outgoing
ORDER BY created_at
LIMIT 100
FOR UPDATE SKIP LOCKED;

Limit  (cost=27294.64..27295.89 rows=100 width=24) (actual time=163.076..163.107 rows=100 loops=1)
  Buffers: shared hit=3285, temp read=494 written=2441
  -&gt;  LockRows  (cost=27294.64..33544.64 rows=500000 width=24) (actual time=163.075..163.101 rows=100 loops=1)
        Buffers: shared hit=3285, temp read=494 written=2441
        -&gt;  Sort  (cost=27294.64..28544.64 rows=500000 width=24) (actual time=163.053..163.058 rows=100 loops=1)
              Sort Key: created_at
              Sort Method: external merge  Disk: 19432kB
              Buffers: shared hit=3185, temp read=494 written=2441
              -&gt;  Seq Scan on outgoing  (cost=0.00..8185.00 rows=500000 width=24) (actual time=0.010..29.042 rows=500000 loops=1)
                    Buffers: shared hit=3185
Planning Time: 0.073 ms
Execution Time: 166.365 ms
</code></pre>
",4,4,0,2025-08-20T08:28:29+00:00,2,163,True
79741226,1838816,"St Petersburg, Russia",postgresql,Select jsonb dictionary key using db functions / operators,"<p>I have structure jsonb value in database like that:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;orgs&quot;: {
        &quot;780162681473&quot;: {
            &quot;leg&quot;: &quot;50102&quot;,
            &quot;oid&quot;: 1180829008,
            &quot;eTag&quot;: &quot;DAC0261357CBD02A832DECA9C0EB92FADAF2DC69&quot;,
            &quot;ogrn&quot;: &quot;318470400085793&quot;,
            &quot;type&quot;: &quot;BUSINESS&quot;,
            &quot;fullName&quot;: &quot;Индивидуальный предприниматель АНТОНОВА ЕКАТЕРИНА ВЛАДИМИРОВНА&quot;,
            &quot;shortName&quot;: &quot;ИП АНТОНОВА Е. В.&quot;,
            &quot;isLiquidated&quot;: false
        }
    }
}
</code></pre>
<p>I want to select the &quot;780162681473&quot; key as field into one var, then process its array content in other vars. But this is not an array, so I can't select it by -&gt;0 or similar methods like #&gt;'{orgs, 0}'. At least it says about error.</p>
<p>How do I do that?</p>
",1,1,0,2025-08-20T14:43:14+00:00,3,96,True
79742113,9400754,,postgresql,Why a spatial JOIN query is faster with a WHERE condition,"<p>I was tring out to optimizing my query as much as possible, and I found out that adding a WHERE with my join condition in it, which in the result doesn't add anything, was 50 times less costly.
I saw that it added a &quot;UNIQUE&quot; element, but I'm realy not sure about the reason of it.</p>
<p>The slower query could be resumed as :</p>
<pre><code>EXPLAIN
SELECT DISTINCT b.gid, b.geom, l.other_id
FROM link_table as l
  JOIN geom_table_1 as a 
    ON a.gid = l.gid
  JOIN geom_table_2 AS b
    ON ST_Intersects(a.geom, b.geom)
</code></pre>
<p>And for the faster query:</p>
<pre><code>EXPLAIN
SELECT DISTINCT b.gid, b.geom, l.other_id
FROM link_table as l
  JOIN geom_table_1 as a 
    ON a.gid = l.gid
  JOIN geom_table_2 AS b
    ON ST_Intersects(a.geom, b.geom)
WHERE ST_Intersects(a.geom, b.geom)
</code></pre>
<p>Below the DDL of the 3 tables involved :</p>
<p>l table:</p>
<pre><code>CREATE TABLE IF NOT EXISTS staski.passage_bati_uf
(
    site_id smallint,
    dist double precision,
    bat_isole boolean,
    gid character varying COLLATE pg_catalog.&quot;default&quot;
)

-- DROP INDEX IF EXISTS staski.passage_bati_uf_dist_idx;

CREATE INDEX IF NOT EXISTS passage_bati_uf_dist_idx
    ON staski.passage_bati_uf USING btree
    (dist ASC NULLS LAST)
    TABLESPACE pg_default;

-- Index: passage_bati_uf_gid_idx

-- DROP INDEX IF EXISTS staski.passage_bati_uf_gid_idx;

CREATE INDEX IF NOT EXISTS passage_bati_uf_gid_idx
    ON staski.passage_bati_uf USING btree
    (gid COLLATE pg_catalog.&quot;default&quot; ASC NULLS LAST)
    TABLESPACE pg_default;

-- Index: passage_bati_uf_gid_site_id_midx

-- DROP INDEX IF EXISTS staski.passage_bati_uf_gid_site_id_midx;

CREATE INDEX IF NOT EXISTS passage_bati_uf_gid_site_id_midx
    ON staski.passage_bati_uf USING btree
    (gid COLLATE pg_catalog.&quot;default&quot; ASC NULLS LAST, site_id ASC NULLS LAST)
    TABLESPACE pg_default;
</code></pre>
<p>Both of the a and b table are <strong>parent tables</strong> of region children tables representing, in both case, the same values but only on one french region (around 50 of it). Eg : a_74 is for the 74th region.</p>
<p>a table:</p>
<pre><code>CREATE TABLE IF NOT EXISTS bd_topo.bati_indifferencie
(
    gid character varying COLLATE pg_catalog.&quot;default&quot; NOT NULL,
    id character varying(24) COLLATE pg_catalog.&quot;default&quot; NOT NULL,
    prec_plani numeric(6,1) NOT NULL,
    prec_alti numeric(7,1) NOT NULL,
    origin_bat character varying(8) COLLATE pg_catalog.&quot;default&quot; DEFAULT 'NR'::character varying,
    hauteur integer NOT NULL,
    z_min double precision,
    z_max double precision,
    the_geom geometry,
    CONSTRAINT bati_indifferencie_pkey PRIMARY KEY (gid),
    CONSTRAINT enforce_dims_the_geom CHECK (st_ndims(the_geom) = 3),
    CONSTRAINT enforce_geotype_the_geom CHECK (geometrytype(the_geom) = 'MULTIPOLYGON'::text OR the_geom IS NULL),
    CONSTRAINT enforce_srid_the_geom CHECK (st_srid(the_geom) = 2154)
)
</code></pre>
<p>b table:</p>
<pre><code>CREATE TABLE IF NOT EXISTS bd_parcellaire.geo_parcelle
(
    gid integer NOT NULL DEFAULT nextval('bd_parcellaire.geo_parcelle_gid_seq'::regclass),
    numero character varying(4) COLLATE pg_catalog.&quot;default&quot;,
    feuille smallint,
    section character varying(2) COLLATE pg_catalog.&quot;default&quot;,
    code_dep character varying(2) COLLATE pg_catalog.&quot;default&quot;,
    nom_com character varying(45) COLLATE pg_catalog.&quot;default&quot;,
    code_com character varying(3) COLLATE pg_catalog.&quot;default&quot;,
    com_abs character varying(3) COLLATE pg_catalog.&quot;default&quot;,
    code_arr character varying(3) COLLATE pg_catalog.&quot;default&quot;,
    the_geom geometry(MultiPolygon,2154),
    CONSTRAINT geo_parcelle_pkey PRIMARY KEY (gid)
)

-- DROP INDEX IF EXISTS bd_parcellaire.geo_parcelle_the_geom_idx;

CREATE INDEX IF NOT EXISTS geo_parcelle_the_geom_idx
    ON bd_parcellaire.geo_parcelle USING gist
    (the_geom)
    TABLESPACE pg_default;
</code></pre>
<p>Below the detailed explain of the query without WHERE:</p>
<pre><code>Unique  (cost=315279149407.35..315281444157.89 rows=9240000 width=291) (actual time=487573.943..488067.948 rows=184743 loops=1)
  Output: dp.gid, dp.the_geom, p.site_id
  Buffers: shared hit=515186515 read=388231, temp read=18219 written=34371
  -&gt;  Gather Merge  (cost=315279149407.35..315281305557.89 rows=18480000 width=291) (actual time=487573.941..487982.308 rows=184900 loops=1)
        Output: dp.gid, dp.the_geom, p.site_id
        Workers Planned: 2
        Workers Launched: 2
        Buffers: shared hit=515186366 read=388231, temp read=18219 written=34371
        -&gt;  Sort  (cost=315279148407.33..315279171507.33 rows=9240000 width=291) (actual time=487455.200..487473.956 rows=61633 loops=3)
              Output: dp.gid, dp.the_geom, p.site_id
              Sort Key: dp.gid, dp.the_geom, p.site_id
              Sort Method: external merge  Disk: 27952kB
              Buffers: shared hit=515186366 read=388231, temp read=18219 written=34371
              Worker 0:  actual time=487484.393..487510.658 rows=79737 loops=1
                Sort Method: external merge  Disk: 26536kB
                JIT:
                  Functions: 432
                  Options: Inlining true, Optimization true, Expressions true, Deforming true
                  Timing: Generation 32.410 ms (Deform 23.525 ms), Inlining 94.116 ms, Optimization 4892.988 ms, Emission 3141.103 ms, Total 8160.616 ms
                Buffers: shared hit=165994321 read=90495, temp read=8195 written=14450
              Worker 1:  actual time=487346.270..487352.166 rows=13700 loops=1
                Sort Method: external merge  Disk: 4088kB
                JIT:
                  Functions: 432
                  Options: Inlining true, Optimization true, Expressions true, Deforming true
                  Timing: Generation 31.737 ms (Deform 22.946 ms), Inlining 93.475 ms, Optimization 4822.790 ms, Emission 3117.632 ms, Total 8065.634 ms
                Buffers: shared hit=174913943 read=156825, temp read=763 written=2741
              -&gt;  HashAggregate  (cost=291316570107.71..315274289515.21 rows=9240000 width=291) (actual time=487345.659..487409.590 rows=61633 loops=3)
                    Output: dp.gid, dp.the_geom, p.site_id
                    Group Key: dp.gid, dp.the_geom, p.site_id
                    Planned Partitions: 256  Batches: 257  Memory Usage: 8337kB  Disk Usage: 64008kB
                    Buffers: shared hit=515186320 read=388231, temp read=10897 written=27033
                    Worker 0:  actual time=487347.478..487422.934 rows=79737 loops=1
                      Batches: 257  Memory Usage: 8337kB  Disk Usage: 50704kB
                      Buffers: shared hit=165994298 read=90495, temp read=4878 written=11125
                    Worker 1:  actual time=487311.137..487330.418 rows=13700 loops=1
                      Batches: 250  Memory Usage: 8337kB  Disk Usage: 15880kB
                      Buffers: shared hit=174913920 read=156825, temp read=252 written=2229
                    -&gt;  Parallel Hash Join  (cost=5295.22..188202943467.43 rows=153328812848 width=291) (actual time=8318.751..487084.930 rows=96626 loops=3)
                          Output: dp.gid, dp.the_geom, p.site_id
                          Hash Cond: ((bati.gid)::text = (p.gid)::text)
                          Buffers: shared hit=515185882 read=388231
                          Worker 0:  actual time=8342.057..487031.869 rows=120929 loops=1
                            Buffers: shared hit=165994132 read=90495
                          Worker 1:  actual time=8374.485..487238.999 rows=22621 loops=1
                            Buffers: shared hit=174913844 read=156825
                          -&gt;  Nested Loop  (cost=0.00..186742601495.27 rows=148907963637 width=297) (actual time=1.644..476726.546 rows=4413959 loops=3)
                                Output: bati.gid, dp.gid, dp.the_geom
                                Buffers: shared hit=515183078 read=388231
                                Worker 0:  actual time=2.369..476628.491 rows=4344343 loops=1
                                  Buffers: shared hit=165994116 read=90495
                                Worker 1:  actual time=1.367..476922.181 rows=4488989 loops=1
                                  Buffers: shared hit=174911058 read=156825
                                -&gt;  Parallel Append  (cost=0.00..478721.37 rows=3925166 width=258) (actual time=0.101..3673.684 rows=3140130 loops=3)
                                      Buffers: shared hit=101166 read=318586
                                      Worker 0:  actual time=0.073..3062.621 rows=3014315 loops=1
                                        Buffers: shared hit=64912 read=69450
                                      Worker 1:  actual time=0.162..4098.456 rows=3219801 loops=1
                                        Buffers: shared hit=8980 read=136244
                                      -&gt;  Parallel Seq Scan on bd_topo.bati_indifferencie_31 bati_17  (cost=0.00..29207.25 rows=240725 width=272) (actual time=0.160..517.489 rows=577739 loops=1)
                                            Output: bati_17.gid, bati_17.the_geom
                                            Buffers: shared hit=7275 read=19525
                                            Worker 1:  actual time=0.160..517.489 rows=577739 loops=1
                                              Buffers: shared hit=7275 read=19525

                                      ...

                                      -&gt;  Parallel Seq Scan on bd_topo.bati_indifferencie bati_1  (cost=0.00..0.00 rows=1 width=64) (actual time=0.002..0.002 rows=0 loops=1)
                                            Output: bati_1.gid, bati_1.the_geom
                                -&gt;  Append  (cost=0.00..47537.64 rows=3796 width=289) (actual time=0.099..0.149 rows=1 loops=9420391)
                                      Buffers: shared hit=515081912 read=69645
                                      Worker 0:  actual time=0.111..0.155 rows=1 loops=3014315
                                        Buffers: shared hit=165929204 read=21045
                                      Worker 1:  actual time=0.091..0.145 rows=1 loops=3219801
                                        Buffers: shared hit=174902078 read=20581
                                      -&gt;  Seq Scan on bd_parcellaire.geo_parcelle dp_1  (cost=0.00..0.00 rows=1 width=36) (actual time=0.000..0.000 rows=0 loops=9420391)
                                            Output: dp_1.gid, dp_1.the_geom
                                            Filter: st_intersects(bati.the_geom, dp_1.the_geom)
                                            Worker 0:  actual time=0.000..0.000 rows=0 loops=3014315
                                            Worker 1:  actual time=0.000..0.000 rows=0 loops=3219801
                                      -&gt;  Index Scan using geo_parcelle_01_2013_the_geom_idx on bd_parcellaire.geo_parcelle_01_2013 dp_2  (cost=0.29..1640.11 rows=131 width=301) (actual time=0.004..0.005 rows=0 loops=9420391)
                                            Output: dp_2.gid, dp_2.the_geom
                                            Index Cond: (dp_2.the_geom &amp;&amp; bati.the_geom)
                                            Filter: st_intersects(bati.the_geom, dp_2.the_geom)
                                            Rows Removed by Filter: 0
                                            Buffers: shared hit=11462884 read=6111
                                            Worker 0:  actual time=0.002..0.002 rows=0 loops=3014315
                                              Buffers: shared hit=3022698 read=9
                                            Worker 1:  actual time=0.008..0.010 rows=0 loops=3219801
                                              Buffers: shared hit=5237123 read=6101
                                              
                                              ...

                                      -&gt;  Index Scan using geo_parcelle_95_2014_the_geom_idx on bd_parcellaire.geo_parcelle_95_2014 dp_50  (cost=0.28..375.81 rows=30 width=247) (actual time=0.001..0.001 rows=0 loops=9420391)
                                            Output: dp_50.gid, dp_50.the_geom
                                            Index Cond: (dp_50.the_geom &amp;&amp; bati.the_geom)
                                            Filter: st_intersects(bati.the_geom, dp_50.the_geom)
                                            Buffers: shared hit=9420391
                                            Worker 0:  actual time=0.001..0.001 rows=0 loops=3014315
                                              Buffers: shared hit=3014315
                                            Worker 1:  actual time=0.001..0.001 rows=0 loops=3219801
                                              Buffers: shared hit=3219801
                          -&gt;  Parallel Hash  (cost=3895.65..3895.65 rows=111965 width=10) (actual time=8149.881..8149.882 rows=63447 loops=3)
                                Output: p.site_id, p.gid
                                Buckets: 262144  Batches: 1  Memory Usage: 11040kB
                                Buffers: shared hit=2776
                                Worker 0:  actual time=8122.063..8122.064 rows=313 loops=1
                                  Buffers: shared hit=2
                                Worker 1:  actual time=8122.115..8122.117 rows=189722 loops=1
                                  Buffers: shared hit=2772
                                -&gt;  Parallel Seq Scan on staski.passage_bati_uf p  (cost=0.00..3895.65 rows=111965 width=10) (actual time=8116.050..8125.031 rows=63447 loops=3)
                                      Output: p.site_id, p.gid
                                      Buffers: shared hit=2776
                                      Worker 0:  actual time=8118.382..8118.419 rows=313 loops=1
                                        Buffers: shared hit=2
                                      Worker 1:  actual time=8024.591..8051.455 rows=189722 loops=1
                                        Buffers: shared hit=2772
Settings: effective_cache_size = '48GB'
Planning:
  Buffers: shared hit=5791
Planning Time: 98.697 ms
JIT:
  Functions: 1297
  Options: Inlining true, Optimization true, Expressions true, Deforming true
  Timing: Generation 96.448 ms (Deform 69.897 ms), Inlining 285.899 ms, Optimization 14680.547 ms, Emission 9410.789 ms, Total 24473.684 ms
Execution Time: 488251.077 ms
</code></pre>
<p>And the explain with the WHERE:</p>
<pre><code>Unique  (cost=4979644174.85..4983449113.52 rows=9240000 width=291) (actual time=557893.909..558497.582 rows=184743 loops=1)
  Output: dp.gid, dp.the_geom, p.site_id
  Buffers: shared hit=515564336 read=12330, temp read=19693 written=19722
  -&gt;  Gather Merge  (cost=4979644174.85..4983310513.52 rows=18480000 width=291) (actual time=557893.906..558420.382 rows=184952 loops=1)
        Output: dp.gid, dp.the_geom, p.site_id
        Workers Planned: 2
        Workers Launched: 2
        Buffers: shared hit=515564187 read=12330, temp read=19693 written=19722
        -&gt;  Unique  (cost=4979643174.83..4981176462.96 rows=9240000 width=291) (actual time=557781.431..557866.829 rows=61651 loops=3)
              Output: dp.gid, dp.the_geom, p.site_id
              Buffers: shared hit=515564187 read=12330, temp read=19693 written=19722
              Worker 0:  actual time=557726.301..557758.958 rows=13700 loops=1
                JIT:
                  Functions: 476
                  Options: Inlining true, Optimization true, Expressions true, Deforming true
                  Timing: Generation 34.182 ms (Deform 22.559 ms), Inlining 95.702 ms, Optimization 4916.877 ms, Emission 3374.155 ms, Total 8420.916 ms
                Buffers: shared hit=175379687 read=7197, temp read=1025 written=1027
              Worker 1:  actual time=557726.763..557846.095 rows=79786 loops=1
                JIT:
                  Functions: 476
                  Options: Inlining true, Optimization true, Expressions true, Deforming true
                  Timing: Generation 34.420 ms (Deform 22.693 ms), Inlining 94.475 ms, Optimization 4924.061 ms, Emission 3338.399 ms, Total 8391.355 ms
                Buffers: shared hit=166731351 read=136, temp read=5716 written=5728
              -&gt;  Sort  (cost=4979643174.83..4980026496.86 rows=153328813 width=291) (actual time=557781.429..557816.364 rows=96626 loops=3)
                    Output: dp.gid, dp.the_geom, p.site_id
                    Sort Key: dp.gid, dp.the_geom, p.site_id
                    Sort Method: external merge  Disk: 51832kB
                    Buffers: shared hit=515563838 read=12330, temp read=19693 written=19722
                    Worker 0:  actual time=557726.299..557737.689 rows=22621 loops=1
                      Sort Method: external merge  Disk: 8200kB
                      Buffers: shared hit=175379631 read=7197, temp read=1025 written=1027
                    Worker 1:  actual time=557726.761..557783.772 rows=120951 loops=1
                      Sort Method: external merge  Disk: 45728kB
                      Buffers: shared hit=166731219 read=136, temp read=5716 written=5728
                    -&gt;  Parallel Hash Join  (cost=5295.22..4874944850.33 rows=153328813 width=291) (actual time=8633.423..557630.030 rows=96626 loops=3)
                          Output: dp.gid, dp.the_geom, p.site_id
                          Hash Cond: ((bati.gid)::text = (p.gid)::text)
                          Buffers: shared hit=515563386 read=12330
                          Worker 0:  actual time=8714.727..557688.142 rows=22621 loops=1
                            Buffers: shared hit=175379552 read=7197
                          Worker 1:  actual time=8660.065..557584.496 rows=120951 loops=1
                            Buffers: shared hit=166730980 read=136
                          -&gt;  Nested Loop  (cost=0.00..4873479218.43 rows=148907964 width=297) (actual time=9.060..546985.354 rows=4413959 loops=3)
                                Output: bati.gid, dp.gid, dp.the_geom
                                Buffers: shared hit=515560582 read=12330
                                Worker 0:  actual time=23.158..547182.678 rows=4499396 loops=1
                                  Buffers: shared hit=175379303 read=7197
                                Worker 1:  actual time=2.385..547002.615 rows=4363038 loops=1
                                  Buffers: shared hit=166728906 read=136
                                -&gt;  Parallel Append  (cost=0.00..478721.37 rows=3925166 width=258) (actual time=7.257..2236.253 rows=3140130 loops=3)
                                      Buffers: shared hit=407866 read=11886
                                      Worker 0:  actual time=21.473..2243.610 rows=3225611 loops=1
                                        Buffers: shared hit=138262 read=7183
                                      Worker 1:  actual time=0.163..2080.099 rows=3025964 loops=1
                                        Buffers: shared hit=134925
                                      -&gt;  Parallel Seq Scan on bd_topo.bati_indifferencie_31 bati_17  (cost=0.00..29207.25 rows=240725 width=272) (actual time=21.471..421.406 rows=577739 loops=1)
                                            Output: bati_17.gid, bati_17.the_geom
                                            Buffers: shared hit=19617 read=7183
                                            Worker 0:  actual time=21.471..421.406 rows=577739 loops=1
                                              Buffers: shared hit=19617 read=7183

                                      ...
                                      
                                      -&gt;  Parallel Seq Scan on bd_topo.bati_indifferencie bati_1  (cost=0.00..0.00 rows=1 width=64) (actual time=0.001..0.001 rows=0 loops=1)
                                            Output: bati_1.gid, bati_1.the_geom
                                -&gt;  Append  (cost=0.00..1240.98 rows=50 width=289) (actual time=0.117..0.171 rows=1 loops=9420391)
                                      Buffers: shared hit=515152716 read=444
                                      Worker 0:  actual time=0.107..0.167 rows=1 loops=3225611
                                        Buffers: shared hit=175241041 read=14
                                      Worker 1:  actual time=0.130..0.178 rows=1 loops=3025964
                                        Buffers: shared hit=166593981 read=136
                                      -&gt;  Seq Scan on bd_parcellaire.geo_parcelle dp_1  (cost=0.00..0.00 rows=1 width=36) (actual time=0.000..0.000 rows=0 loops=9420391)
                                            Output: dp_1.gid, dp_1.the_geom
                                            Filter: (st_intersects(bati.the_geom, dp_1.the_geom) AND st_intersects(bati.the_geom, dp_1.the_geom))
                                            Worker 0:  actual time=0.000..0.000 rows=0 loops=3225611
                                            Worker 1:  actual time=0.000..0.000 rows=0 loops=3025964
                                      -&gt;  Index Scan using geo_parcelle_01_2013_the_geom_idx on bd_parcellaire.geo_parcelle_01_2013 dp_2  (cost=0.29..25.34 rows=1 width=301) (actual time=0.005..0.006 rows=0 loops=9420391)
                                            Output: dp_2.gid, dp_2.the_geom
                                            Index Cond: ((dp_2.the_geom &amp;&amp; bati.the_geom) AND (dp_2.the_geom &amp;&amp; bati.the_geom))
                                            Filter: (st_intersects(bati.the_geom, dp_2.the_geom) AND st_intersects(bati.the_geom, dp_2.the_geom))
                                            Rows Removed by Filter: 0
                                            Buffers: shared hit=11469023
                                            Worker 0:  actual time=0.010..0.012 rows=0 loops=3225611
                                              Buffers: shared hit=5249062
                                            Worker 1:  actual time=0.002..0.002 rows=0 loops=3025964
                                              Buffers: shared hit=3034356

                                      ...

                                      -&gt;  Index Scan using geo_parcelle_95_2014_the_geom_idx on bd_parcellaire.geo_parcelle_95_2014 dp_50  (cost=0.28..25.31 rows=1 width=247) (actual time=0.001..0.001 rows=0 loops=9420391)
                                            Output: dp_50.gid, dp_50.the_geom
                                            Index Cond: ((dp_50.the_geom &amp;&amp; bati.the_geom) AND (dp_50.the_geom &amp;&amp; bati.the_geom))
                                            Filter: (st_intersects(bati.the_geom, dp_50.the_geom) AND st_intersects(bati.the_geom, dp_50.the_geom))
                                            Buffers: shared hit=9420391
                                            Worker 0:  actual time=0.001..0.001 rows=0 loops=3225611
                                              Buffers: shared hit=3225611
                                            Worker 1:  actual time=0.001..0.001 rows=0 loops=3025964
                                              Buffers: shared hit=3025964
                          -&gt;  Parallel Hash  (cost=3895.65..3895.65 rows=111965 width=10) (actual time=8428.435..8428.436 rows=63447 loops=3)
                                Output: p.site_id, p.gid
                                Buckets: 262144  Batches: 1  Memory Usage: 11008kB
                                Buffers: shared hit=2776
                                Worker 0:  actual time=8400.262..8400.263 rows=31946 loops=1
                                  Buffers: shared hit=235
                                Worker 1:  actual time=8400.069..8400.070 rows=92715 loops=1
                                  Buffers: shared hit=2060
                                -&gt;  Parallel Seq Scan on staski.passage_bati_uf p  (cost=0.00..3895.65 rows=111965 width=10) (actual time=8399.283..8409.611 rows=63447 loops=3)
                                      Output: p.site_id, p.gid
                                      Buffers: shared hit=2776
                                      Worker 0:  actual time=8385.255..8390.430 rows=31946 loops=1
                                        Buffers: shared hit=235
                                      Worker 1:  actual time=8355.818..8372.740 rows=92715 loops=1
                                        Buffers: shared hit=2060
Settings: effective_cache_size = '48GB'
Planning:
  Buffers: shared hit=5791
Planning Time: 120.319 ms
JIT:
  Functions: 1429
  Options: Inlining true, Optimization true, Expressions true, Deforming true
  Timing: Generation 118.057 ms (Deform 78.086 ms), Inlining 294.695 ms, Optimization 14842.825 ms, Emission 10064.856 ms, Total 25320.433 ms
Execution Time: 558781.509 ms
</code></pre>
<p><strong>EDIT:</strong> I replaced the inital LEFT JOINS by JOIN for an easier understanding of the Query Plan.</p>
<p><strong>EDIT 2:</strong> Added the DDL and the explain(analyze, verbose, buffers, settings) for both queries.</p>
",2,2,0,2025-08-21T09:46:34+00:00,2,167,True
79742185,13656845,,postgresql,Why does adding Nmap port scanning inside a Celery task slow down my FastAPI + SQLAlchemy + PostgreSQL app?,"<p>I am building a scanning service using FastAPI + Celery + PostgreSQL + SQLAlchemy + Nmap.</p>
<p>I have a Celery task that performs WHOIS, DNS lookups, IP lookups, and then port scanning using python-nmap. The issue is that when I add the port scanning part inside my Celery task, my entire API becomes very slow (requests take 10–20 seconds).</p>
<p>If I restart the application without triggering the task, the API works fine and responds in milliseconds.</p>
<p>Here’s a simplified version of my code:</p>
<pre><code>
@tasks_router.post(
    &quot;/create&quot;,
    response_model=schemas.TaskRead,
    summary=&quot;Create a new scan task&quot;,
    description=(
            &quot;Create a task for scanning an entity (domain or IP). The task is queued to the background worker.\n\n&quot;
            &quot;- For domains: performs WHOIS and DNS lookups.\n&quot;
            &quot;- For IPs: performs geolocation lookup via ipinfo.io.&quot;
    ),
)
def create_task(payload: schemas.TaskCreate, db: Session = Depends(get_db)):
    entity = payload.entity
    is_domain = not entity.replace(&quot;.&quot;, &quot;&quot;).isdigit()  # crude check (schema regex already validated)

    task = models.IpDomainScanTask(
        user_id=payload.user_id,
        entity=entity,
        is_domain=is_domain,
        status=models.TaskStatus.pending,
        progress_message=&quot;Task created&quot;
    )
    db.add(task)
    db.commit()
    db.refresh(task)

    # Send to Celery worker
    try:
        scan_entity.delay(str(task.id))
    except Exception as e:
        task.progress_message = f&quot;Queueing failed: {e}&quot;
        task.status = models.TaskStatus.failed
        db.commit()
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=&quot;Task queue unavailable&quot;)

    return task

</code></pre>
<pre><code>import nmap

def scan_ip(ip: str, mode: str = &quot;fast&quot;) -&gt; dict:
    scanner = nmap.PortScanner(nmap_search_path=[r&quot;C:\Program Files (x86)\Nmap\nmap.exe&quot;])
    arguments = &quot;--top-ports 100 -sV --version-light -T4&quot; if mode == &quot;fast&quot; else &quot;-A&quot;
    scanner.scan(ip, arguments=arguments)
    results = {&quot;host&quot;: ip, &quot;ports&quot;: []}
    for host in scanner.all_hosts():
        for proto in scanner[host].all_protocols():
            for port in scanner[host][proto]:
                port_info = scanner[host][proto][port]
                results[&quot;ports&quot;].append({
                    &quot;port&quot;: port,
                    &quot;service&quot;: port_info.get(&quot;name&quot;, &quot;&quot;),
                    &quot;state&quot;: port_info.get(&quot;state&quot;, &quot;&quot;)
                })
    return results

</code></pre>
<p>And the Celery task:</p>
<pre><code>

@celery.task(name=&quot;tasks.scan_entity&quot;)
def scan_entity(task_id: str):
    db = CelerySessionLocal()
    try:
        try:
            task_uuid = uuid.UUID(task_id)
        except Exception:
            return {&quot;error&quot;: &quot;Invalid task id&quot;}
        task = (
            db.query(models.IpDomainScanTask)
            .filter(models.IpDomainScanTask.id == task_id)
            .first()
        )
        #,....other scaning

        # when ever i add this block my all API's get very slow if remove it my api call is fast
        update_progress(db, task, &quot;Starting port scan&quot;)
        db.close()
        os_records = []
        port_records = []
        for idx, ip in enumerate(['1.1.1.1.1'], start=1):

            # IP lookup

            # Port scanning
            scan_results = scan_ip(ip)

            # Store OS detection results
            for os_info in scan_results.get(&quot;os&quot;, []):
                os_record = models.OperatingSystem(
                    task_id=task.id,
                    ip=ip,
                    name=os_info[&quot;name&quot;],
                    accuracy=os_info[&quot;accuracy&quot;]
                )
                os_records.append(os_record)

            # Store port scan results
            for port_info in scan_results.get(&quot;ports&quot;, []):
                port_record = models.PortScan(
                    task_id=task.id,
                    ip=ip,
                    port=port_info[&quot;port&quot;],
                    protocol=port_info[&quot;protocol&quot;],
                    state=port_info[&quot;state&quot;],
                    service=port_info[&quot;service&quot;],
                    product=port_info[&quot;product&quot;],
                    version=port_info[&quot;version&quot;],
                    extra_info=port_info[&quot;extra_info&quot;]
                )
                port_records.append(port_record)
            # update_progress(db, task, f&quot;Port scan {idx}/{len(checkip)} completed&quot;)
        db = CelerySessionLocal()  # Reconnect to the database after closing it
        db.add_all(os_records)
        db.add_all(port_records)
        db.commit()

        update_progress(db, task, &quot;Scan completed successfully&quot;, models.TaskStatus.completed)
        db.close()
        return {&quot;task_id&quot;: str(task.id), &quot;status&quot;: &quot;completed&quot;}
    except Exception as e:
        task = (
            db.query(models.IpDomainScanTask)
            .filter(models.IpDomainScanTask.id == task_id)
            .first()
        )
        update_progress(db, task, f&quot;Scan failed: {str(e)}&quot;, models.TaskStatus.failed)
        return {&quot;task_id&quot;: str(task.id), &quot;status&quot;: &quot;failed&quot;, &quot;error&quot;: str(e)}
    finally:
        db.close()

</code></pre>
<p><strong>Problem:</strong></p>
<ul>
<li>Without the port scanning block, everything works fast.</li>
<li>With port scanning inside Celery, my FastAPI endpoints become very slow (10–20s).</li>
<li>Even requests unrelated to the task are affected.</li>
</ul>
<p><strong>My setup:</strong></p>
<ul>
<li>FastAPI (running with Uvicorn)</li>
<li>Celery (with Redis as broker)</li>
<li>PostgreSQL + SQLAlchemy</li>
<li>python-nmap</li>
</ul>
<p><strong>Question:</strong></p>
<ul>
<li>Why does running Nmap scans inside a Celery worker cause my API to slow down?</li>
<li>Is this a DB session/connection pool issue (since I close/reopen SessionLocal)?</li>
<li>Or is it because Nmap blocks the event loop or consumes too many resources?</li>
<li>What’s the best practice to run heavy I/O tasks (like port scanning) in Celery without affecting FastAPI performance?</li>
</ul>
",0,0,0,2025-08-21T10:55:14+00:00,1,81,False
79742402,1001236,,postgresql,Orphaned pg_temp_# and pg_toast_temp_# schemas in postgresql 17.5 slow down ORM model update,"<p>I am running PG 17.5 Windows using the EDB community distribution.  I am using Entity Framework ORM with Devart's dotConnect for PostgreSQL.  The issue I am about to describe did not occur under PG 13.18 with the same ORM and DotConnect..</p>
<p>I observed that the large number of <code>pg_temp_###</code> and <code>pg_toast_temp_###</code> schemas is interfering with my ORM's ability to complete the &quot;Update Model From Database&quot; operation due to the fact that the large query that it sends to the database to determine all of the tables, columns, views, etc never completes.  I confirmed that manually removing all of the orphaned schemas with commands like</p>
<pre><code>DROP SCHEMA IF EXISTS pg_toast_temp_497;
DROP SCHEMA IF EXISTS pg_temp_497;
</code></pre>
<p>fixes the problem, but these schemas gradually come back over time.  Under PG 13.18 I also see a large number of orphaned schemas but the ORM &quot;Update Model From Database&quot; query runs ok.</p>
<p>Now I am trying to find the source of the orphaned schemas.</p>
<p>I have reviewed all of @LaurenzAlbe suggestions in the answer to a <a href=""https://stackoverflow.com/questions/79693816/why-do-the-pg-temp-and-pg-toast-temp-schemas-keep-growing-and-why-are-t"">related SO question</a>:</p>
<ul>
<li>make sure that database sessions don't take forever</li>
<li>tell your users not to use temporary tables</li>
<li>tell your users to <code>DROP</code> or <code>TRUNCATE</code> temporary tables when they are
done with them</li>
<li>tell your users to create their temporary tables with <code>ON COMMIT DROP</code>
or <code>ON COMMIT DELETE ROWS</code></li>
</ul>
<p>Where <em>our code</em> creates temporary tables, it always has <code>ON COMMIT DROP</code>.  Perhaps the <em>ORM Code</em> is creating temp tables without <code>ON COMMIT DROP</code>?  We don't have long-running queries.</p>
<p>Here are a couple more clues:</p>
<ul>
<li>When I query for objects inside the orphaned schemas using
<pre class=""lang-sql prettyprint-override""><code>SELECT c.relname, c.relkind 
FROM pg_catalog.pg_class c 
  JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace 
  WHERE nspname like '%pg_temp_%'
     OR nspname like '%pg_toast_temp_%';
</code></pre>
I don't see any objects so the schemas are empty</li>
<li>There is always the same number of <code>pg_temp_###</code> and <code>pg_toast_temp_###</code> schemas - does that indicate that the offending temp table query uses toast tables or are <code>pg_temp_###</code> and <code>pg_toast_temp_###</code> schemas always created in pairs?  This could be a clue.</li>
<li>The max number ### in <code>pg_temp_###</code> and <code>pg_toast_temp_###</code> goes from <code>1</code> to the <code>max_connections</code> setting.</li>
</ul>
<p>Is there anything else that can caused orphaned schemas?  Is there anything I can try to find the origin of the orphaned schemas short of logging every postgresql command and checking for &quot;on commit drop&quot; when every temp table is created?</p>
<p>I've run pieces of the query which appears below and it appears to me that the problem is focused in the forming of UnionAll2 from Extent7 and Extent8.</p>
<p>The execution plans were too large to include in the body of the question and they are available at the links below:</p>
<ul>
<li><a href=""https://explain.depesz.com/s/1n08"" rel=""nofollow noreferrer"">Query Plan for 13.18 with 404 namespaces</a> - completes in a few
minutes</li>
<li><a href=""https://explain.depesz.com/s/EFp8"" rel=""nofollow noreferrer"">Query Plan for 17.5 with 4 namespaces</a> - completes in 4 minutes</li>
<li><a href=""https://explain.depesz.com/s/aXet"" rel=""nofollow noreferrer"">Query Plan for 17.5 with 1004 namespaces</a> - does not complete</li>
</ul>
<p>Query:</p>
<pre><code>SELECT 
&quot;UnionAll1&quot;.&quot;Ordinal&quot; AS &quot;C1&quot;,
&quot;Extent1&quot;.&quot;CatalogName&quot;,
&quot;Extent1&quot;.&quot;SchemaName&quot;,
&quot;Extent1&quot;.&quot;Name&quot;,
&quot;UnionAll1&quot;.&quot;Name&quot; AS &quot;C2&quot;,
&quot;UnionAll1&quot;.&quot;IsNullable&quot; AS &quot;C3&quot;,
&quot;UnionAll1&quot;.&quot;TypeName&quot; AS &quot;C4&quot;,
&quot;UnionAll1&quot;.&quot;MaxLength&quot; AS &quot;C5&quot;,
&quot;UnionAll1&quot;.&quot;Precision&quot; AS &quot;C6&quot;,
&quot;UnionAll1&quot;.&quot;DateTimePrecision&quot; AS &quot;C7&quot;,
&quot;UnionAll1&quot;.&quot;Scale&quot; AS &quot;C8&quot;,
&quot;UnionAll1&quot;.&quot;IsIdentity&quot; AS &quot;C9&quot;,
&quot;UnionAll1&quot;.&quot;IsStoreGenerated&quot; AS &quot;C10&quot;,
CASE WHEN &quot;Project5&quot;.&quot;C2&quot; IS NULL THEN false ELSE &quot;Project5&quot;.&quot;C2&quot; END AS &quot;C11&quot;
FROM   (
            /*STables {C996CB0C-E37A-4f55-88AD-181F1989094D} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select table_schema || '.' || table_name as &quot;Id&quot;,
       table_catalog  as &quot;CatalogName&quot;,
       table_schema as &quot;SchemaName&quot;,
       table_name as &quot;Name&quot;
  from information_schema.tables
 where table_schema = ANY (current_schemas(false))
   and table_type = 'BASE TABLE'            
          ) AS &quot;Extent1&quot;
INNER JOIN  (SELECT 
    &quot;Extent2&quot;.&quot;Id&quot;,
    &quot;Extent2&quot;.&quot;Name&quot;,
    &quot;Extent2&quot;.&quot;Ordinal&quot;,
    &quot;Extent2&quot;.&quot;IsNullable&quot;,
    &quot;Extent2&quot;.&quot;TypeName&quot;,
    &quot;Extent2&quot;.&quot;MaxLength&quot;,
    &quot;Extent2&quot;.&quot;Precision&quot;,
    &quot;Extent2&quot;.&quot;DateTimePrecision&quot;,
    &quot;Extent2&quot;.&quot;Scale&quot;,
    &quot;Extent2&quot;.&quot;IsIdentity&quot;,
    &quot;Extent2&quot;.&quot;IsStoreGenerated&quot;,
    0 AS &quot;C1&quot;,
    &quot;Extent2&quot;.&quot;ParentId&quot;
    FROM (
            /*STableColumns {4D35D67E-B0EE-45e6-BA55-3238B93342FB} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select table_schema || '.' || table_name || '.' || column_name as &quot;Id&quot;,
       table_schema || '.' || table_name as &quot;ParentId&quot;,
       column_name as &quot;Name&quot;,
       ordinal_position as &quot;Ordinal&quot;,
       case is_nullable
         when 'YES' then true
         else false
       end as &quot;IsNullable&quot;,
       case data_type
         when 'integer' then 'int'
         when '&quot;char&quot;' then 'char'
         when 'character' then 'char'
         when 'character varying' then 'varchar'
         when 'bit varying' then 'varbit'
         when 'time with time zone' then 'timetz'
         when 'time without time zone' then 'time'
         when 'timestamp with time zone' then 'timestamptz'
         when 'timestamp without time zone' then 'timestamp'
         when 'USER-DEFINED' then
              case
                when (udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;) and &quot;PgCatalogTypeInputProc&quot; = 'enum_in' then 'enum'
                when udt_name = 'citext' then 'citext'
                when udt_name = 'geometry' then 'spatial_geometry'
                when udt_name = 'geography' then 'spatial_geography'
                else udt_catalog || '.' || udt_schema || '.' || udt_name
              end
         else data_type
       end as &quot;TypeName&quot;,
       character_maximum_length as &quot;MaxLength&quot;,
       case
         when numeric_precision is null then null
         when numeric_precision between 1 and 29 then numeric_precision
         else 29
       end as &quot;Precision&quot;,
       datetime_precision AS &quot;DateTimePrecision&quot;, 
       numeric_scale AS &quot;Scale&quot;,
       cast('' as varchar) AS &quot;CollationCatalog&quot;,
       cast('' as varchar) AS &quot;CollationSchema&quot;,
       cast('' as varchar) AS &quot;CollationName&quot;,
       cast('' as varchar) AS &quot;CharacterSetCatalog&quot;,
       cast('' as varchar) AS &quot;CharacterSetSchema&quot;,
       cast('' as varchar) AS &quot;CharacterSetName&quot;,
       false as &quot;IsMultiSet&quot;,
       case
         when is_nullable = 'NO' and (column_default like 'nextval(%' or is_identity = 'YES') then true
         else false
       end AS &quot;IsIdentity&quot;,
       case
         when is_nullable = 'NO' and (column_default like 'nextval(%' or is_identity = 'YES') then true
         else false
       end as &quot;IsStoreGenerated&quot;,
       cast(null as varchar) as &quot;Default&quot;
  from information_schema.columns left outer join
       (select sch.nspname || '.' || t.typname as &quot;PgCatalogTypeName&quot;,
               proc.proname as &quot;PgCatalogTypeInputProc&quot;
          from pg_catalog.pg_type t inner join pg_catalog.pg_namespace sch
            on (t.typnamespace = sch.oid) inner join pg_user cat
            on (sch.nspowner = cat.usesysid) inner join pg_proc proc
            on (t.typinput = proc.oid)
       ) as &quot;PgCatalogTypeInfo&quot;
    on (udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;)
 where table_schema = ANY (current_schemas(false))
          ) AS &quot;Extent2&quot;
UNION ALL
    SELECT 
    &quot;Extent3&quot;.&quot;Id&quot;,
    &quot;Extent3&quot;.&quot;Name&quot;,
    &quot;Extent3&quot;.&quot;Ordinal&quot;,
    &quot;Extent3&quot;.&quot;IsNullable&quot;,
    &quot;Extent3&quot;.&quot;TypeName&quot;,
    &quot;Extent3&quot;.&quot;MaxLength&quot;,
    &quot;Extent3&quot;.&quot;Precision&quot;,
    &quot;Extent3&quot;.&quot;DateTimePrecision&quot;,
    &quot;Extent3&quot;.&quot;Scale&quot;,
    &quot;Extent3&quot;.&quot;IsIdentity&quot;,
    &quot;Extent3&quot;.&quot;IsStoreGenerated&quot;,
    6 AS &quot;C1&quot;,
    &quot;Extent3&quot;.&quot;ParentId&quot;
    FROM (
            /*SViewColumns {02C89419-DB97-4abb-9F1A-6B313FAE7866} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select table_schema || '.' || table_name || '.' || column_name as &quot;Id&quot;,
       table_schema || '.' || table_name as &quot;ParentId&quot;,
       column_name as &quot;Name&quot;,
       ordinal_position as &quot;Ordinal&quot;,
       case is_nullable
         when 'YES' then (select false
                            from information_schema.view_column_usage vcu inner join information_schema.columns cs
                              on (vcu.view_catalog = columns.table_catalog 
                             and vcu.view_schema = columns.table_schema
                             and vcu.view_name = columns.table_name
                             and vcu.column_name = columns.column_name
                             and cs.table_catalog = vcu.table_catalog 
                             and cs.table_schema = vcu.table_schema
                             and cs.table_name = vcu.table_name
                             and cs.column_name = vcu.column_name
                             and cs.is_nullable != 'YES')
                          union all
                          select true
                          limit 1
                          )
         else false
       end as &quot;IsNullable&quot;,
       case data_type
         when 'integer' then 'int'
         when '&quot;char&quot;' then 'char'
         when 'character' then 'char'
         when 'character varying' then 'varchar'
         when 'bit varying' then 'varbit'
         when 'time with time zone' then 'timetz'
         when 'time without time zone' then 'time'
         when 'timestamp with time zone' then 'timestamptz'
         when 'timestamp without time zone' then 'timestamp'
         when 'USER-DEFINED' then
              case 
                when (udt_catalog || '.' || udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;) and &quot;PgCatalogTypeInputProc&quot; = 'enum_in' then 'enum'
                        when udt_name = 'citext' then 'citext'
                when udt_name = 'geometry' then 'spatial_geometry'
                when udt_name = 'geography' then 'spatial_geography'
                else udt_catalog || '.' || udt_schema || '.' || udt_name
              end
         else data_type
       end as &quot;TypeName&quot;,
       character_maximum_length as &quot;MaxLength&quot;,
       case
         when numeric_precision is null then null
         when numeric_precision between 1 and 29 then numeric_precision
         else 29
       end as &quot;Precision&quot;,
       0 AS &quot;DateTimePrecision&quot;, 
       numeric_scale AS &quot;Scale&quot;,
       cast('' as varchar) AS &quot;CollationCatalog&quot;,
       cast('' as varchar) AS &quot;CollationSchema&quot;,
       cast('' as varchar) AS &quot;CollationName&quot;,
       cast('' as varchar) AS &quot;CharacterSetCatalog&quot;,
       cast('' as varchar) AS &quot;CharacterSetSchema&quot;,
       cast('' as varchar) AS &quot;CharacterSetName&quot;,
       false AS &quot;IsMultiSet&quot;,
       case 
         when is_nullable = 'NO' and column_default like 'nextval(%' then true
         else false
       end as &quot;IsIdentity&quot;,
       case 
         when is_nullable = 'NO' and column_default like 'nextval(%' then true
         else false
       end as &quot;IsStoreGenerated&quot;,
       null as &quot;Default&quot;
  from information_schema.columns left outer join
       (select cat.usename || '.' ||  sch.nspname || '.' || t.typname as &quot;PgCatalogTypeName&quot;,
               proc.proname as &quot;PgCatalogTypeInputProc&quot;
          from pg_catalog.pg_type t inner join pg_catalog.pg_namespace sch
            on (t.typnamespace = sch.oid) inner join pg_user cat
            on (sch.nspowner = cat.usesysid) inner join pg_proc proc
            on (t.typinput = proc.oid)
       ) as &quot;PgCatalogTypeInfo&quot;
    on (udt_catalog || '.' || udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;)
 where table_schema = ANY (current_schemas(false))            
          ) AS &quot;Extent3&quot;) AS &quot;UnionAll1&quot; ON (&quot;UnionAll1&quot;.&quot;C1&quot; = 0) AND (&quot;Extent1&quot;.&quot;Id&quot; = &quot;UnionAll1&quot;.&quot;ParentId&quot;)
LEFT OUTER JOIN  (SELECT 
    &quot;UnionAll2&quot;.&quot;Id&quot; AS &quot;C1&quot;,
    true AS &quot;C2&quot;
    FROM  (
            /*SConstraints {B3F70AEA-8CC8-4da8-8008-2FFF4B9C1461} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select constraint_schema || '.' || constraint_name as &quot;Id&quot;,
       table_schema || '.' || table_name as &quot;ParentId&quot;,
       constraint_name as &quot;Name&quot;,
       constraint_type as &quot;ConstraintType&quot;,
       false as &quot;IsDeferrable&quot;,
       false as &quot;IsInitiallyDeferred&quot;
  from information_schema.table_constraints
 where constraint_type in ('PRIMARY KEY', 'UNIQUE', 'FOREIGN KEY')
   and table_schema = ANY (current_schemas(false))
          ) AS &quot;Extent4&quot;
    INNER JOIN  (SELECT 
        7 AS &quot;C1&quot;,
        &quot;Extent5&quot;.&quot;ConstraintId&quot;,
        &quot;Extent6&quot;.&quot;Id&quot;
        FROM  (
            /*SConstraintColumns {F2DCFD5C-8B38-4e6c-92A5-34493F5A9237} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select constraint_schema || '.' || constraint_name as &quot;ConstraintId&quot;,
       table_schema || '.' || table_name || '.' || column_name as &quot;ColumnId&quot;
  from information_schema.key_column_usage
 where table_schema = ANY (current_schemas(false))
          ) AS &quot;Extent5&quot;
        INNER JOIN (
            /*STableColumns {4D35D67E-B0EE-45e6-BA55-3238B93342FB} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select table_schema || '.' || table_name || '.' || column_name as &quot;Id&quot;,
       table_schema || '.' || table_name as &quot;ParentId&quot;,
       column_name as &quot;Name&quot;,
       ordinal_position as &quot;Ordinal&quot;,
       case is_nullable
         when 'YES' then true
         else false
       end as &quot;IsNullable&quot;,
       case data_type
         when 'integer' then 'int'
         when '&quot;char&quot;' then 'char'
         when 'character' then 'char'
         when 'character varying' then 'varchar'
         when 'bit varying' then 'varbit'
         when 'time with time zone' then 'timetz'
         when 'time without time zone' then 'time'
         when 'timestamp with time zone' then 'timestamptz'
         when 'timestamp without time zone' then 'timestamp'
         when 'USER-DEFINED' then
              case
                when (udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;) and &quot;PgCatalogTypeInputProc&quot; = 'enum_in' then 'enum'
                when udt_name = 'citext' then 'citext'
                when udt_name = 'geometry' then 'spatial_geometry'
                when udt_name = 'geography' then 'spatial_geography'
                else udt_catalog || '.' || udt_schema || '.' || udt_name
              end
         else data_type
       end as &quot;TypeName&quot;,
       character_maximum_length as &quot;MaxLength&quot;,
       case
         when numeric_precision is null then null
         when numeric_precision between 1 and 29 then numeric_precision
         else 29
       end as &quot;Precision&quot;,
       datetime_precision AS &quot;DateTimePrecision&quot;, 
       numeric_scale AS &quot;Scale&quot;,
       cast('' as varchar) AS &quot;CollationCatalog&quot;,
       cast('' as varchar) AS &quot;CollationSchema&quot;,
       cast('' as varchar) AS &quot;CollationName&quot;,
       cast('' as varchar) AS &quot;CharacterSetCatalog&quot;,
       cast('' as varchar) AS &quot;CharacterSetSchema&quot;,
       cast('' as varchar) AS &quot;CharacterSetName&quot;,
       false as &quot;IsMultiSet&quot;,
       case
         when is_nullable = 'NO' and (column_default like 'nextval(%' or is_identity = 'YES') then true
         else false
       end AS &quot;IsIdentity&quot;,
       case
         when is_nullable = 'NO' and (column_default like 'nextval(%' or is_identity = 'YES') then true
         else false
       end as &quot;IsStoreGenerated&quot;,
       cast(null as varchar) as &quot;Default&quot;
  from information_schema.columns left outer join
       (select sch.nspname || '.' || t.typname as &quot;PgCatalogTypeName&quot;,
               proc.proname as &quot;PgCatalogTypeInputProc&quot;
          from pg_catalog.pg_type t inner join pg_catalog.pg_namespace sch
            on (t.typnamespace = sch.oid) inner join pg_user cat
            on (sch.nspowner = cat.usesysid) inner join pg_proc proc
            on (t.typinput = proc.oid)
       ) as &quot;PgCatalogTypeInfo&quot;
    on (udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;)
 where table_schema = ANY (current_schemas(false))
          ) AS &quot;Extent6&quot; ON &quot;Extent6&quot;.&quot;Id&quot; = &quot;Extent5&quot;.&quot;ColumnId&quot;
    UNION ALL
        SELECT 
        11 AS &quot;C1&quot;,
        &quot;Extent7&quot;.&quot;ConstraintId&quot;,
        &quot;Extent8&quot;.&quot;Id&quot;
        FROM  (
            /*SViewConstraintColumns {5AA2F6A5-AA81-4724-8593-55D34067FFEC} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
SELECT cast(null as varchar) as &quot;ConstraintId&quot;,
       cast(null as varchar) as &quot;ColumnId&quot;
  from information_schema.routines     
 where false
          ) AS &quot;Extent7&quot;
        INNER JOIN (
            /*SViewColumns {02C89419-DB97-4abb-9F1A-6B313FAE7866} {BFD32F32-EEE4-4826-9725-C489B0EE1ACC}*/
select table_schema || '.' || table_name || '.' || column_name as &quot;Id&quot;,
       table_schema || '.' || table_name as &quot;ParentId&quot;,
       column_name as &quot;Name&quot;,
       ordinal_position as &quot;Ordinal&quot;,
       case is_nullable
         when 'YES' then (select false
                            from information_schema.view_column_usage vcu inner join information_schema.columns cs
                              on (vcu.view_catalog = columns.table_catalog 
                             and vcu.view_schema = columns.table_schema
                             and vcu.view_name = columns.table_name
                             and vcu.column_name = columns.column_name
                             and cs.table_catalog = vcu.table_catalog 
                             and cs.table_schema = vcu.table_schema
                             and cs.table_name = vcu.table_name
                             and cs.column_name = vcu.column_name
                             and cs.is_nullable != 'YES')
                          union all
                          select true
                          limit 1
                          )
         else false
       end as &quot;IsNullable&quot;,
       case data_type
         when 'integer' then 'int'
         when '&quot;char&quot;' then 'char'
         when 'character' then 'char'
         when 'character varying' then 'varchar'
         when 'bit varying' then 'varbit'
         when 'time with time zone' then 'timetz'
         when 'time without time zone' then 'time'
         when 'timestamp with time zone' then 'timestamptz'
         when 'timestamp without time zone' then 'timestamp'
         when 'USER-DEFINED' then
              case 
                when (udt_catalog || '.' || udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;) and &quot;PgCatalogTypeInputProc&quot; = 'enum_in' then 'enum'
                        when udt_name = 'citext' then 'citext'
                when udt_name = 'geometry' then 'spatial_geometry'
                when udt_name = 'geography' then 'spatial_geography'
                else udt_catalog || '.' || udt_schema || '.' || udt_name
              end
         else data_type
       end as &quot;TypeName&quot;,
       character_maximum_length as &quot;MaxLength&quot;,
       case
         when numeric_precision is null then null
         when numeric_precision between 1 and 29 then numeric_precision
         else 29
       end as &quot;Precision&quot;,
       0 AS &quot;DateTimePrecision&quot;, 
       numeric_scale AS &quot;Scale&quot;,
       cast('' as varchar) AS &quot;CollationCatalog&quot;,
       cast('' as varchar) AS &quot;CollationSchema&quot;,
       cast('' as varchar) AS &quot;CollationName&quot;,
       cast('' as varchar) AS &quot;CharacterSetCatalog&quot;,
       cast('' as varchar) AS &quot;CharacterSetSchema&quot;,
       cast('' as varchar) AS &quot;CharacterSetName&quot;,
       false AS &quot;IsMultiSet&quot;,
       case 
         when is_nullable = 'NO' and column_default like 'nextval(%' then true
         else false
       end as &quot;IsIdentity&quot;,
       case 
         when is_nullable = 'NO' and column_default like 'nextval(%' then true
         else false
       end as &quot;IsStoreGenerated&quot;,
       null as &quot;Default&quot;
  from information_schema.columns left outer join
       (select cat.usename || '.' ||  sch.nspname || '.' || t.typname as &quot;PgCatalogTypeName&quot;,
               proc.proname as &quot;PgCatalogTypeInputProc&quot;
          from pg_catalog.pg_type t inner join pg_catalog.pg_namespace sch
            on (t.typnamespace = sch.oid) inner join pg_user cat
            on (sch.nspowner = cat.usesysid) inner join pg_proc proc
            on (t.typinput = proc.oid)
       ) as &quot;PgCatalogTypeInfo&quot;
    on (udt_catalog || '.' || udt_schema || '.' || udt_name = &quot;PgCatalogTypeName&quot;)
 where table_schema = ANY (current_schemas(false))            
          ) AS &quot;Extent8&quot; ON &quot;Extent8&quot;.&quot;Id&quot; = &quot;Extent7&quot;.&quot;ColumnId&quot;) AS &quot;UnionAll2&quot; ON (&quot;UnionAll2&quot;.&quot;C1&quot; = 7) AND (&quot;Extent4&quot;.&quot;Id&quot; = &quot;UnionAll2&quot;.&quot;ConstraintId&quot;)
    WHERE &quot;Extent4&quot;.&quot;ConstraintType&quot; = 'PRIMARY KEY' ) AS &quot;Project5&quot; ON &quot;UnionAll1&quot;.&quot;Id&quot; = &quot;Project5&quot;.&quot;C1&quot;
WHERE &quot;Extent1&quot;.&quot;Name&quot; LIKE E'%'
</code></pre>
",1,1,0,2025-08-21T13:48:12+00:00,0,168,False
79742774,22638257,,postgresql,How do I enforce uniqueness across parent and child tables without denormalizing my data?,"<p>I have two tables in Postgres with a parent/child relationship:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE table_a (
    id SERIAL PRIMARY KEY,
    type TEXT NOT NULL,
    rundate DATE NOT NULL
);

CREATE TABLE table_b (
    id SERIAL PRIMARY KEY,
    a_id INT NOT NULL REFERENCES table_a(id),
    number INT NOT NULL
);
</code></pre>
<p>Constraints:</p>
<p>In <code>table_a</code>, it is fine to have duplicate (<code>type</code>, <code>rundate</code>) combinations.</p>
<p>In <code>table_b</code>, <code>number</code> can repeat for different parents.</p>
<p>But across both tables together: for each unique (<code>type</code>, <code>rundate</code>) pair in <code>table_a</code>, I must not allow the same number to appear more than once in table_b.</p>
<p>In other words, I need to enforce uniqueness of (table_a.type, table_a.rundate, table_b.number).</p>
<p>Since Postgres doesn’t allow a unique index that spans two tables, not does it allow constraints to use multiple tables. What’s the best way to enforce this constraint?</p>
<p>I also considered denormalizing type and rundate into table_b so I could add a unique constraint there, but I’d rather avoid redundant data if possible.</p>
<p>What I expect:</p>
<p>I want the database to reject an insert/update into table_b if it would create a duplicate (type, rundate, number) across the parent/child relationship.</p>
",3,4,1,2025-08-21T21:14:19+00:00,1,145,True
79743290,10300113,Austria,postgresql,postgres - unique index on master linked to unique constraints on partitions,"<p>I have a unique index on the master. On the partitions, I have a unique constraint linked to the unique index.</p>
<p>I would like to promote the unique index on the master to a unique constraint so that new partitions also get the unique constraint and not only the unique index.</p>
<p>I would like to do said promotion without recreating all indexes across all partitions across all partitioned tables across all systems - we are talking about a lot of data here and millions of tables.</p>
<p>So far I failed. How can this be done?</p>
<p>Reproduce the scenario:</p>
<pre><code>---- (1) Master table ---- 
create table ttab (id int4,partition_num int4) partition by list(partition_num);

---- (2) Unique Index on Master table ----
create unique index unq_ttab_1 on ttab (partition_num, id);

---- (3) First Partition Table ----
create table ttab_p4 (like ttab including defaults including constraints);

---- (4) Unique Index on First Partition Table ----
create unique index unq_ttab_p4_1 on ttab_p4 (partition_num, id) ;

---- (5) Constraint based on Unique Index on First Partition Table ----
alter table ttab_p4 add constraint unq_ttab_p4_1 unique using index unq_ttab_p4_1;

---- (6) Attach First Partition Table to Master Table ----
alter table ttab attach partition ttab_p4 for values in (4);
</code></pre>
<p>The above creates the scenarion that I have. And from here I am stuck in my attempts to make the unique index on the master table a unique constraint:</p>
<pre><code>alter table ttab add constraint unq_ttab unique using index unq_ttab_1;
-- not supported on partitioned tables    

drop index unq_ttab_1;
-- SQL Error [2BP01]: ERROR: cannot drop index unq_ttab_1 because other objects depend on it
--   Detail: constraint unq_ttab_p4_1 on table ttab_p4 depends on index unq_ttab_1

alter table ttab add constraint unq_ttab_1 unique (partition_num, id);
-- relation &quot;unq_ttab_1&quot; already exists
</code></pre>
<p>And because of that, new partitions do not get the constraint automatically, that I want them to get automatically:</p>
<pre><code>create table ttab_p5 (like ttab including defaults including constraints);

alter table ttab attach partition ttab_p5 for values in (5);

    \d+ ttab_p5
                                            Table &quot;public.ttab_p5&quot;
    Column     |  Type   | Collation | Nullable | Default | Storage | Compression | Stats target | Description
---------------+---------+-----------+----------+---------+---------+-------------+--------------+-------------
 id            | integer |           |          |         | plain   |             |              |
 partition_num | integer |           |          |         | plain   |             |              |
Partition of: ttab FOR VALUES IN (5)
Partition constraint: ((partition_num IS NOT NULL) AND (partition_num = 5))
Indexes:
    &quot;ttab_p5_partition_num_id_idx&quot; UNIQUE, btree (partition_num, id)
Access method: heap
</code></pre>
",2,2,0,2025-08-22T10:36:16+00:00,3,95,True
79743762,8908951,,postgresql,Check if a JdbcClient transaction ran successfully in Java Spring,"<p>I need to insert one row of data in two different tables. Since the tables are related, i grouped the insert in a <code>TRANSACTION</code> in order to make them behave as an atomic operation.</p>
<p>My code follows:</p>
<pre><code>    String query = &quot;&quot;&quot;
        BEGIN TRANSACTION;

        INSERT INTO usuario (
            email,
            senha,
            nome,
            status
        )
        VALUES (
            :email,
            :senha,
            :nome,
            'Ativo'
        );

        INSERT INTO organizador (
            id,
            cpf_cnpj
        )
        VALUES (
            (
                SELECT us.id 
                FROM usuario AS us 
                WHERE us.email = :email AND us.senha = :senha
            ),
            :cpf
        );

        COMMIT;
    &quot;&quot;&quot;;

    int res = jdbcClient
        .sql(query)
        .param(&quot;email&quot;, email)
        .param(&quot;senha&quot;, senha)
        .param(&quot;nome&quot;, nome)
        .param(&quot;cpf&quot;, cpf)
        .update();

        
    System.out.println(&quot;result: &quot; + res);
    
    return res == 2;
</code></pre>
<p>After running, I can check pgAdmin and verify that the rows were correctly inserted.</p>
<p>However, when checking the variable <code>res</code> in the code, to see how many rows were affected, it's value is always 0. i believe this is because its not a plain insert, but a transaction.</p>
<p>How would I be able to check wether my transaction ran successfully or not?</p>
<p>Note that I'm using the <strong>Spring</strong> framework and a <strong>PostgreSQL</strong> database, accessed through the <strong>JdbcClient</strong></p>
",-1,0,1,2025-08-22T18:54:19+00:00,1,135,True
79744077,23659551,,postgresql,Why is this Deferred Constraint Trigger Raising an Exception on the Not Final State of the Record,"<p>Why is my deferred constraint checking an intermediate state and not the final state of the record?</p>
<p>We use PostgreSQL 16.9 for our database and we use Python3 and Psycopg2 to manage our data pipeline.</p>
<p>We have a table in our database, <code>member</code> that contains the member data (primary key: <code>member_id</code>), along with its <code>parent_id</code> that is a foreign key reference to another <code>member</code>.</p>
<p>This table is populated through three means:</p>
<ul>
<li>the application</li>
<li>an external data drop</li>
<li>direct inserts against the database through the deployment process</li>
</ul>
<p>The source of the record is identified through the <code>source_id</code> field on the table.</p>
<p>There are two fields in the <code>member</code> table that are managed through our application. These fields are required for all application or external data drop source records.</p>
<p>I implemented this constraint using a constraint trigger rather than a <code>check(function)</code> due to the foreign key reference existing on the source field. I also set it to be <code>deferrable initially deferred</code> due to the stored procedure for the external data drops.</p>
<p>For the external data drops, we use a stored procedure to load the data from our staging table <code>stage</code> to our working table <code>member</code>. This stored procedure inserts new members or updates existing members and then sets the <code>parent_id</code> and then sets the child's inherited values using the <code>parent_id</code>.</p>
<p>Each record's parent references either a record that existed BEFORE the new drop, a record that was inserted DURING the new drop, or is null.</p>
<p>The stored procedure does not set the <code>parent_id</code> at the same time as the inherited values because if the <code>parent_id</code> record is a new record with a null <code>parent_id</code> then the inherited values should be set to the default values.</p>
<p>The expected behavior is that the exception is only thrown if the END state of the record violates the constraint due to both the trigger being defined as <code>deferrable initially deferred</code> and that <code>set constraints all deferred;</code> is the first line in the procedure.</p>
<p>I have added <code>raise notice</code> statements throughout the stored procedure that log the output of various select statements that get the values from the <code>member</code> table at various times in the procedure.
The last raise has the final values as being not null which satisfies the constraint, but when the procedure ends, it is throwing an exception and the exception is outputting the values of a previous state.</p>
<p>I have added the <code>CTID</code> value to be logged during the procedure and returned in the exception from the trigger in order to confirm the assumption that it was not validating the final version of the record being inserted and the final CTID value does not match the CTID value returned in the exception.</p>
<pre class=""lang-sql prettyprint-override""><code>-- create tables:
drop table if exists member;
drop table if exists first_inherited_field;
drop table if exists second_inherited_field;
drop table if exists source;
drop table if exists stage;

CREATE TABLE source
(
    source_id integer primary key generated always as identity ,
    name varchar UNIQUE not null
);

insert into source (name)
values ('APP'),
       ('PROC');

CREATE TABLE first_inherited_field
(
    first_inherited_field_id integer primary key generated always as identity,
    name varchar not null unique
);

insert into first_inherited_field (name)
values ('DEFAULT'),
       ('TEST');

CREATE TABLE second_inherited_field
(
    second_inherited_field_id integer primary key generated always as identity,
    name varchar not null unique
);

insert into second_inherited_field (name)
values ('DEFAULT'),
       ('TEST');
drop table if exists member;
CREATE TABLE member
(
    member_id integer primary key generated always as identity,
    name varchar NOT NULL,
    first_inheritable_field_id integer references first_inherited_field (first_inherited_field_id),
    second_inheritable_field_id integer references second_inherited_field (second_inherited_field_id),
    source_id integer NOT NULL references source (source_id),
    parent_member_id integer references member (member_id)
    -- &lt;other fields&gt;
);

CREATE UNIQUE INDEX family_name_source_unq ON member (name, source_id);

CREATE TABLE stage
(
    name varchar,
    parent_name varchar
    -- &lt;other fields&gt;
);

INSERT INTO stage (name, parent_name)
values ('TEST', 'PARENT'),
       ('PARENT', null);


-- function and trigger definition:
CREATE OR REPLACE FUNCTION required_fields() RETURNS TRIGGER AS
$$
DECLARE
    app_source_id integer = (select source_id from source where name = 'APP'); -- the application has specific restrictions that do not apply to the data pipeline data
    proc_source_id integer = (select source_id from source where name = 'PROC'); -- this is the source handling the stored proc data called by python
BEGIN
    IF new.source_id = app_source_id
    THEN
        RETURN NEW;
        --&lt;do some checks based on app constraints&gt;
    ELSEIF new.source_id = proc_source_id
    THEN
        IF (new.first_inheritable_field_id is not null
            AND new.second_inheritable_field_id is not null)
            AND tg_op ILIKE 'UPDATE'
        THEN
            RETURN NEW;
        ELSE
            RAISE EXCEPTION 'Required fields are null. Member_Id: %, MEMBER NAME: %, first_inheritable_trait_id: %, second_inheritable_trait_id: %, parent_member_id: %, CTID: %', new.member_id, new.name, new.first_inheritable_field_id, new.second_inheritable_field_id, new.parent_member_id, new.ctid;
        END IF;
    ELSE
        RETURN NEW;
    END IF;
END;
$$ LANGUAGE PLPGSQL;

CREATE CONSTRAINT TRIGGER check_required_fields
    AFTER INSERT OR UPDATE
    ON member
    DEFERRABLE INITIALLY DEFERRED
    FOR EACH ROW
EXECUTE FUNCTION required_fields();

-- stored proc:
CREATE OR REPLACE PROCEDURE load_proc_data() AS
$$
DECLARE
    proc_source_id integer = (select source_id from source where name = 'PROC');
    insert_count integer;
    default_first_trait integer = (select first_inherited_field_id from first_inherited_field where name = 'DEFAULT');
    default_second_trait integer = (select second_inherited_field_id from second_inherited_field where name = 'DEFAULT');
BEGIN
    SET CONSTRAINTS ALL DEFERRED;

    INSERT INTO member(name, source_id)
    SELECT name,
           proc_source_id
    FROM stage
    WHERE not exists (SELECT 1
                      FROM member
                      WHERE member.name = stage.name
                        AND member.source_id = proc_source_id);
    get diagnostics insert_count = row_count;

    with parents as (select stage.name,
                            stage.parent_name,
                            member.member_id as child_id,
                            member.name,
                            parent.member_id as parent_id,
                            parent.name as parent_name
                     from stage
                              left join member
                                        on member.name = stage.name
                              left join member parent
                                        on parent.name = stage.parent_name)
    UPDATE member
    SET parent_member_id = parents.parent_id
    from parents
    where parents.child_id = member.member_id
      and member.parent_member_id is distinct from parents.parent_id;

    if insert_count &gt; 0 THEN
        update member
        set first_inheritable_field_id = default_first_trait,
            second_inheritable_field_id = default_second_trait
        where member.parent_member_id is null
        and member.source_id = proc_source_id;

        update member
        set first_inheritable_field_id = coalesce(parent.first_inheritable_field_id, default_first_trait),
            second_inheritable_field_id = coalesce(parent.second_inheritable_field_id, default_second_trait)
        from member parent
        where parent.member_id = member.parent_member_id
          and member.first_inheritable_field_id is null
          and member.second_inheritable_field_id is null
          and member.source_id = proc_source_id;
        raise notice '(LOCATION: In IF Statement) Member Name: TEST - first_inherited_field_id: %)', (select first_inheritable_field_id from member where name = 'TEST');
        raise notice 'second_inherited_field_id:%', (select second_inheritable_field_id from member where name = 'TEST');
        raise notice 'CTID: %', (select CTID from member where name = 'TEST');
    end if;

    truncate stage;

    raise notice '(LOCATION: AT END) Member Name: TEST - first_inherited_field_id: %)', (select first_inheritable_field_id from member where name = 'TEST');
    raise notice 'second_inherited_field_id:%', (select second_inheritable_field_id from member where name = 'TEST');
    raise notice 'CTID: %', (select CTID from member where name = 'TEST');
end;
$$ LANGUAGE PLPGSQL;

call load_proc_data();
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>public&gt; call load_proc_data()
[2025-08-22 11:13:24] [P0001] ERROR: Required fields are null. Member_Id: 1, MEMBER NAME: TEST, first_inheritable_trait_id: &lt;NULL&gt;, second_inheritable_trait_id: &lt;NULL&gt;, parent_member_id: &lt;NULL&gt;
[2025-08-22 11:13:24] Where: PL/pgSQL function required_fields() line 18 at RAISE
(LOCATION: In IF Statement) Member Name: TEST - first_inherited_field_id: 1)
second_inherited_field_id:1
(LOCATION: AT END) Member Name: TEST - first_inherited_field_id: 1)
second_inherited_field_id:1
</code></pre>
<p><a href=""https://dbfiddle.uk/fLWtO_tP"" rel=""nofollow noreferrer"">https://dbfiddle.uk/fLWtO_tP</a></p>
",1,1,0,2025-08-23T07:28:44+00:00,1,121,True
79744777,275221,Austria,postgresql,How to execute non-transactional statements like CREATE DATABASE in libpqxx?,"<p>I’m using libpqxx to run SQL scripts. For most DDL and DML this works fine, but I’ve run into a problem with statements that cannot be executed inside a transaction block, e.g.</p>
<pre><code>CREATE DATABASE mydb;
</code></pre>
<p>Every transaction type in libpqxx (work, nontransaction, etc.) seems to wrap statements in a <code>BEGIN … COMMIT</code>. That makes PostgreSQL reject the query with:</p>
<pre><code>ERROR:  CREATE DATABASE cannot run inside a transaction block
</code></pre>
<p>I was expecting some way to send raw, non-transactional statements through the public API, but I haven’t found one. The closest thing in the docs is</p>
<pre><code>pqxx::result pqxx::transaction_base::direct_exec();
</code></pre>
<p>but that’s not part of the public interface.</p>
<p>So:</p>
<p>Is there a supported way in libpqxx to run non-transactional commands like <code>CREATE DATABASE</code>, <code>DROP DATABASE</code>, etc.?</p>
<p>Or is the intended approach to bypass libpqxx and call <code>PQexec</code> on the underlying <code>PGconn*</code> directly?</p>
<p><strong>Update:</strong></p>
<p>I already tested <code>pqxx::notransaction</code> class, for some reason the query still seems to be inside a transaction. (Also, my actual query statement does not have <code>BEGIN / END</code>)</p>
<pre><code>// Connect to the database. 
pqxx::connection conn(std::format(&quot;host={} port={} dbname={} user={} password={}&quot;, db.host, db.port, db.dbname, db.user, db.password));
if(!conn.is_open()) 
{
    m_logger-&gt;error(&quot;Failed to open database connection to {}:{}&quot;, db.host, db.port); 
    return false; 
}

try
{
  pqxx::connection conn(std::format(
            &quot;host={} port={} dbname={} user={} password={}&quot;,
            db.host, db.port, db.dbname, db.user, db.password));

  pqxx::nontransaction ntx(conn);
            ntx.exec(script);
            ntx.commit(); // no-op for nontransaction
  conn.close();

} 
catch(const std::exception&amp; e) 
{ 
  m_logger-&gt;error(&quot;Exception during script execution: {}&quot;, e.what()); return false; 
}
</code></pre>
<p>I still get the error from db:</p>
<pre><code>{
   &quot;level&quot;:&quot;error&quot;,
   &quot;message&quot;:&quot;Exception during script execution: ERROR: CREATE DATABASE cannot run inside a transaction block\n&quot;,
   &quot;timestamp&quot;:&quot;2025-08-24T09:38:05Z&quot;
}
</code></pre>
<p><strong>Update 2:</strong></p>
<p>Postgreql logs:</p>
<pre><code>2025-08-24 22:04:13.903 CEST [11287] postgres@postgres LOG:  duration: 8.838 ms
2025-08-24 22:04:13.903 CEST [11287] postgres@postgres LOG:  statement: COMMIT
2025-08-24 22:04:13.904 CEST [11287] postgres@postgres LOG:  duration: 0.491 ms
2025-08-24 22:04:14.467 CEST [11293] postgres@postgres LOG:  statement: --
        -- Create database
        -- Database Name: &quot;my_app_db&quot;;
        -- Type: DATABASE; 
        --

        CREATE DATABASE &quot;my_app_db&quot;
            WITH
            ENCODING = 'UTF8'
            LOCALE_PROVIDER = 'libc'
            CONNECTION LIMIT = -1
            IS_TEMPLATE = False
        ;

        ALTER DATABASE &quot;my_app_db&quot; OWNER TO &quot;my_app&quot;;
2025-08-24 22:04:14.467 CEST [11293] postgres@postgres ERROR:  CREATE DATABASE cannot run inside a transaction block
</code></pre>
",3,3,0,2025-08-24T09:59:46+00:00,2,142,True
79745378,972647,,postgresql,Self-join on table to find missing entries with conditions,"<p>I have a table in which I want to find missing rows in a specific column but with conditions.</p>
<p>I want to see if rows that have an id starting with &quot;A&quot; have a corresponding row (= same value in join column) that has an id starting with &quot;B&quot;.</p>
<p>I then want to see matches and such that do not have an entry in B. So all rows starting with &quot;A&quot; should be in the results set.</p>
<p>This is my current SQl statement:</p>
<pre><code>SELECT
    a.id,
    b.id
FROM
    abc a
LEFT JOIN
    abc b on a.join_column = b.join_column
WHERE
    a.id LIKE 'A%'
    AND (b.id LIKE 'B%' OR b.id is null)
</code></pre>
<p>I need to specific that the id can start with &quot;B&quot; or is null (does not exists). The issue is I'm only getting matches like if I did an inner join and I'm not getting why. somehow I seem to misunderstand the boolean logic.</p>
<p>How can I get the desired result, all entires of A and in case there is a entry with B with same join column also the id of B?</p>
",1,2,1,2025-08-25T05:32:13+00:00,2,151,True
79745618,13255966,,postgresql,Parameter 0 of method entityManagerFactory in PrimaryDataSourceConfig required a bean of type EntityManagerFactoryBuilder that could not be found,"<p>I am trying to configure 2 databases in a single spring boot application. So that I have configured 2 separate Config files for Postgres and Postgres vector DB.</p>
<pre><code>@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(
        basePackages = &quot;com.example.sse_chat_bot.repository.primary&quot;,
        entityManagerFactoryRef = &quot;primaryEntityManagerFactory&quot;,
        transactionManagerRef = &quot;primaryTransactionManager&quot;
)
public class PrimaryDataSourceConfig {



    @Bean(name = &quot;primaryDataSourceProperties&quot;)
    @ConfigurationProperties(&quot;spring.datasource.primary&quot;)
    public DataSourceProperties dataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean(name = &quot;primaryDataSource&quot;)
    public DataSource dataSource() {
        return dataSourceProperties().initializeDataSourceBuilder().build();
    }

    // The streamlined, modern approach - no separate builder bean needed
    @Bean(name = &quot;primaryEntityManagerFactory&quot;)
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder,

            @Qualifier(&quot;primaryDataSource&quot;) DataSource dataSource
    ) {
        return builder
                .dataSource(dataSource) // Set the DataSource
                .packages(&quot;com.example.sse_chat_bot.entity.primary&quot;)
                .build();
    }

    @Bean(name = &quot;primaryTransactionManager&quot;)
    public PlatformTransactionManager transactionManager(
            @Qualifier(&quot;primaryEntityManagerFactory&quot;) LocalContainerEntityManagerFactoryBean entityManagerFactory
    ) {
        return new JpaTransactionManager(entityManagerFactory.getObject());
    }
}
</code></pre>
<p>but the error</p>
<pre><code>***************************
APPLICATION FAILED TO START
***************************

Description:

Parameter 0 of method entityManagerFactory in com.example.sse_chat_bot.config.PrimaryDataSourceConfig required a bean of type 'org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder' that could not be found.


Action:

Consider defining a bean of type 'org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder' in your configuration.


Spring is unable to autowire the EntityManagerFactoryBuilder. 
</code></pre>
<p>How can i solve this problem??</p>
",1,1,0,2025-08-25T10:32:25+00:00,0,121,False
79745661,1103606,,postgresql,Date field value out of range,"<p>I'm trying to migrate this Oracle SQL query to PostgreSQL:</p>
<pre><code>CASE 
    WHEN (:dateOfBirthYear IS NULL 
          OR :dateOfBirthMonth IS NULL 
          OR :dateOfBirthDay IS NULL) 
        THEN NULL 
        ELSE to_date(to_char(:dateOfBirthYear) || 
                     to_char(:dateOfBirthMonth, '00') || 
                     to_char(:dateOfBirthDay, '00'), 'YYYY mm DD')                                  
END, 
</code></pre>
<p>I tried this one:</p>
<pre><code>CASE 
    WHEN (:dateOfBirthYear IS NULL 
          OR :dateOfBirthMonth IS NULL 
          OR :dateOfBirthDay IS NULL) 
        THEN NULL 
        ELSE make_date(:dateOfBirthYear, :dateOfBirthMonth, :dateOfBirthDay) 
END, 
</code></pre>
<p>But I get this error because I send empty date value.</p>
<blockquote>
<p>org.postgresql.util.PSQLException: ERROR: date field value out of range: 0-00-00</p>
<p>at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.5.jar:42.7.5]</p>
</blockquote>
<p>Is there any solution to modify the SQL code to work with empty value?</p>
",0,1,1,2025-08-25T11:06:24+00:00,2,203,True
79745744,20476625,,postgresql,Cloud Function -&gt; Python connect to PostgreSQL via IAM user,"<p>So im only just starting with GCP/Python. and am still in the learning phase.</p>
<p>I'm using Terraform/Github to deploy my packages to GCP.</p>
<p>I'm trying to get a pub/sub to trigger a Cloud Run Function to run a Python script to connect to a PostgreSQL DB and do a simple count on a table and print it to the log. But it looks like it doesn't want to connect.</p>
<p>I have a few TF files to keep things a bit tidy, but put them all in the block below to show together</p>
<p>I don't want to go down the private IP via connector. its an extra $36 a month which i don't want to pay for something i'm just playing around with.</p>
<pre><code>resource &quot;google_cloudfunctions2_function&quot; &quot;monster_count_fn&quot; {
  name     = var.function_name
  project  = var.project              # &lt;— was var.project_id
  location = var.region

  build_config {
    runtime     = &quot;python311&quot;
    entry_point = &quot;count_tmonsters&quot;
    source {
      storage_source {
        bucket = var.code_bucket
        object = var.code_object
      }
    }
  }

  service_config {
    available_memory      = &quot;256M&quot;
    timeout_seconds       = 60
    max_instance_count    = 1
    service_account_email = google_service_account.game_engine.email
    environment_variables = {
      INSTANCE_CONNECTION_NAME = var.instance_connection_name
      DB_USER                  = var.db_user
      DB_NAME                  = var.db_name
    }
    ingress_settings = &quot;ALLOW_ALL&quot;
  }

  event_trigger {
    trigger_region = var.region
    event_type     = &quot;google.cloud.pubsub.topic.v1.messagePublished&quot;
    pubsub_topic   = google_pubsub_topic.monster_count.id
    retry_policy   = &quot;RETRY_POLICY_RETRY&quot;
  }

  # Make this function wait for your API enables
  depends_on = [
    google_project_service.cloudresourcemanager,
    google_project_service.project_apis
  ]
}

variable &quot;db_user&quot;    { 
  type = string 
  default = &quot;game-engine@rpg-dev.iam&quot;
}

# Ensure the necessary APIs are enabled for the game_engine service account.
locals {
  game_engine_roles = {
    &quot;roles/cloudsql.client&quot;        = &quot;sqladmin.googleapis.com&quot;
    &quot;roles/cloudsql.instanceUser&quot;  = &quot;sqladmin.googleapis.com&quot;
    &quot;roles/logging.logWriter&quot;      = &quot;logging.googleapis.com&quot;
    &quot;roles/eventarc.eventReceiver&quot; = &quot;eventarc.googleapis.com&quot; # Required for Pub/Sub triggered Gen 2 functions
    # Add more if needed later (e.g. storage, secrets)
  }
}

# Grant the game engine service account the necessary roles for its operations.
# These roles are required for the game engine to interact with Cloud SQL, Pub/Sub, and other services. 
resource &quot;google_project_iam_member&quot; &quot;game_engine_access&quot; {
  for_each = local.game_engine_roles

  project  = var.project
  role     = each.key
  member   = &quot;serviceAccount:${google_service_account.game_engine.email}&quot;

  depends_on = [google_project_service.project_apis]
}
</code></pre>
<p>And below is the Python script i'm trying to run via the cloud function:</p>
<pre><code>import base64
import json
import logging, sys, os
from google.cloud.sql.connector import Connector, IPTypes
import pg8000

logging.basicConfig(
    level=logging.DEBUG,
    format=&quot;%(levelname)s %(asctime)s %(filename)s:%(lineno)d - %(message)s&quot;,
    stream=sys.stdout,
    force=True,
)

connector = Connector()

def _get_conn():
    inst = os.environ[&quot;INSTANCE_CONNECTION_NAME&quot;]
    db   = os.environ[&quot;DB_NAME&quot;]
    user = os.environ[&quot;DB_USER&quot;]
    logging.debug(&quot;Using instance=%s user=%s db=%s&quot;, inst, user, db)
    # Public IP path; IAM DB Auth
    return connector.connect(
        inst,
        &quot;pg8000&quot;,
        user=user,
        db=db,
        enable_iam_auth=True,
        ip_type=IPTypes.PUBLIC,
    )

def _decode_event(event):
    try:
        if isinstance(event, dict) and &quot;data&quot; in event:
            return base64.b64decode(event[&quot;data&quot;]).decode(&quot;utf-8&quot;, &quot;ignore&quot;)
    except Exception as e:
        logging.warning(&quot;event decode failed: %s&quot;, e, exc_info=True)
    return None

def count_tmonsters(event, context):
    logging.debug(&quot;Handler start. eventId=%s, event=%s&quot;, getattr(context, &quot;event_id&quot;, None), event)

    # Helpful runtime identity / env diagnostics
    logging.debug(&quot;ENV PROJECT_ID=%s, SERVICE_ACCOUNT=%s&quot;,
                  os.getenv(&quot;GCP_PROJECT&quot;) or os.getenv(&quot;GOOGLE_CLOUD_PROJECT&quot;),
                  os.getenv(&quot;K_SERVICE_ACCOUNT&quot;) or os.getenv(&quot;GOOGLE_SERVICE_ACCOUNT&quot;) or &quot;unknown&quot;)
    logging.debug(&quot;INSTANCE_CONNECTION_NAME=%s, DB_USER=%s, DB_NAME=%s&quot;,
                  os.getenv(&quot;INSTANCE_CONNECTION_NAME&quot;), os.getenv(&quot;DB_USER&quot;), os.getenv(&quot;DB_NAME&quot;))

    msg = _decode_event(event)
    if msg:
        logging.info(&quot;Pub/Sub message: %s&quot;, msg)

    conn = None
    cur = None
    try:
        logging.debug(&quot;Attempting DB connection with IAM auth...&quot;)
        conn = _get_conn()
        logging.info(&quot;DB connection established&quot;)
        cur = conn.cursor()
        logging.debug(&quot;Running query…&quot;)
        cur.execute(&quot;SELECT COUNT(*) FROM tmonsters;&quot;)
        row = cur.fetchone()
        logging.info(&quot;monster_count=%s&quot;, row[0] if row else None)
    except Exception as e:
        logging.error(&quot;DB operation failed: %s&quot;, e, exc_info=True)
        # Surface a short line as well (some platforms trim long traces)
        logging.error(&quot;FAIL_REASON: %r&quot;, e)
    finally:
        try:
            if cur:
                cur.close()
        except Exception as e:
            logging.warning(&quot;cursor close error: %s&quot;, e, exc_info=True)
        try:
            if conn:
                conn.close()
        except Exception as e:
            logging.warning(&quot;conn close error: %s&quot;, e, exc_info=True)

    logging.debug(&quot;Handler end&quot;)

</code></pre>
<p>And when i check the log, all it shows is:</p>
<pre><code>         monster-count-fn                2025-08-25 11:57:47.324  DEBUG 2025-08-25 11:57:47,325 main.py:25 - Using instance=cso-rpg-dev:europe-west2:cso-rpg-sql user=game-engine@cso-rpg-dev.iam db=rpg
         monster-count-fn                2025-08-25 11:57:47.323  DEBUG 2025-08-25 11:57:47,325 main.py:19 - Attempting DB connection with IAM auth...
         monster-count-fn                2025-08-25 11:57:47.323  DEBUG 2025-08-25 11:57:47,324 main.py:45 - Handler start. eventId=15973659715120867, event={'data': 'dGVzdA==', 'message_id': '15973659715120867', 'publish_time': '2025-08-25T11:19:58.156Z'}
E        monster-count-fn                2025-08-25 11:57:47.304
         monster-count-fn                2025-08-25 11:56:40.205  DEBUG 2025-08-25 11:56:40,206 main.py:25 - Using instance=cso-rpg-dev:europe-west2:cso-rpg-sql user=game-engine@cso-rpg-dev.iam db=rpg
         monster-count-fn                2025-08-25 11:56:40.205  DEBUG 2025-08-25 11:56:40,206 main.py:19 - Attempting DB connection with IAM auth...
         monster-count-fn                2025-08-25 11:56:40.205  DEBUG 2025-08-25 11:56:40,206 main.py:45 - Handler start. eventId=15975546303899728, event={'data': 'dGVzdA==', 'message_id': '15975546303899728', 'publish_time': '2025-08-25T11:55:50.993Z'}
E        monster-count-fn                2025-08-25 11:56:38.272
         monster-count-fn                2025-08-25 11:55:40.211  DEBUG 2025-08-25 11:55:40,212 main.py:25 - Using instance=cso-rpg-dev:europe-west2:cso-rpg-sql user=game-engine@cso-rpg-dev.iam db=rpg
         monster-count-fn                2025-08-25 11:55:40.211  DEBUG 2025-08-25 11:55:40,212 main.py:19 - Attempting DB connection with IAM auth...
         monster-count-fn                2025-08-25 11:55:40.211  DEBUG 2025-08-25 11:55:40,212 main.py:45 - Handler start. eventId=15973204489076835, event={'data': 'dGVzdA==', 'message_id': '15973204489076835', 'publish_time': '2025-08-25T11:22:56.173Z'}
E        monster-count-fn                2025-08-25 11:55:40.175
I        monster-count-fn                2025-08-25 11:52:32.048  Default STARTUP TCP probe succeeded after 1 attempt for container &quot;worker&quot; on port 8080.
         monster-count-fn                2025-08-25 11:52:31.933  DEBUG 2025-08-25 11:52:31,934 connectionpool.py:544 - http://metadata.google.internal:80 &quot;GET /computeMetadata/v1/universe/universe-domain HTTP/1.1&quot; 200 14
         monster-count-fn                2025-08-25 11:52:31.932  DEBUG 2025-08-25 11:52:31,932 connectionpool.py:241 - Starting new HTTP connection (1): metadata.google.internal:80
         monster-count-fn                2025-08-25 11:52:31.883  DEBUG 2025-08-25 11:52:31,884 _default.py:256 - Cloud SDK credentials not found on disk; not using them
         monster-count-fn                2025-08-25 11:52:31.883  DEBUG 2025-08-25 11:52:31,884 _default.py:250 - Checking Cloud SDK credentials as part of auth process...
         monster-count-fn                2025-08-25 11:52:31.883  DEBUG 2025-08-25 11:52:31,883 _default.py:278 - Checking None for explicit credentials as part of auth process...
         monster-count-fn                2025-08-25 11:52:31.797  DEBUG 2025-08-25 11:52:31,797 selector_events.py:54 - Using selector: EpollSelector
I        monster-count-fn                2025-08-25 11:52:28.304  Starting new instance. Reason: DEPLOYMENT_ROLLOUT - Instance started due to traffic shifting between revisions due to deployment, traffic split adjustment, or deployment health check.
</code></pre>
",-1,0,1,2025-08-25T12:22:49+00:00,0,63,False
79745982,26379245,,postgresql,How to find the table name and when it is last used in PostgreSQL?,"<p>I am using PostgreSQL (on GCP Cloud SQL) and I want to find out when each table in the database was last used (read or write).</p>
<p>So far, I have checked:</p>
<p><code>pg_stat_user_tables</code> → this shows row counts, sequential scans, index scans, etc., but not the last access time.</p>
<p><code>pg_stat_statements</code> → this stores query statistics, but I would need to parse queries to map them back to table names.</p>
<p><code>pgAudit</code> → this can log all queries, but it may add overhead in terms of CPU/storage.</p>
<p>What I actually need is something like:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>table_name</th>
<th>last_accessed_at</th>
</tr>
</thead>
<tbody>
<tr>
<td>customers</td>
<td>2025-08-20 10:15</td>
</tr>
<tr>
<td>orders</td>
<td>2025-08-18 17:42</td>
</tr>
<tr>
<td>products</td>
<td>2025-08-21 09:33</td>
</tr>
</tbody>
</table></div>
<p>Is there any native PostgreSQL feature, extension, or best practice to achieve this?
If not directly, what’s the recommended way to track when a table was last queried, preferably with minimal performance impact?</p>
<p>I want this for all tables, not just a single query.</p>
",1,1,0,2025-08-25T16:08:17+00:00,2,191,True
79746804,10155537,,postgresql,Dockerized .NET Project - handling DB migrations,"<p>I have dockerized API &amp; PostgreSQL database, when I locally execute</p>
<pre><code>dotnet ef migrations add InitialCreate -p MyProject1 -s MyProject2
</code></pre>
<p>It successfully creates initial migration.</p>
<p>When I try to update DB via</p>
<pre><code>dotnet ef database update -p MyProject1 -s MyProject2
</code></pre>
<p>I get error, because it cannot communicate with Docker and it tries to update it locally (I assume) - error message:</p>
<pre><code>...The ConnectionString property has not been initialized.
</code></pre>
<p>However, in both of my <code>appsettings.json</code> I have <code>ConnectionStrings: &quot;&quot;</code> just empty, and use environment variables:</p>
<pre class=""lang-yaml prettyprint-override""><code>services:
  
  api:
    build:
      context: ../../                
      dockerfile: environments/dev/Dockerfile
    ports:
      - &quot;5000:5000&quot;
    environment:
      - ConnectionStrings__DefaultConnection=User ID=${POSTGRES_USER};Password=${POSTGRES_PASSWORD};Server=${POSTGRES_SERVER};Port=${POSTGRES_PORT};Database=${POSTGRES_DB};
      - ASPNETCORE_URLS=http://+:5000
      - ASPNETCORE_ENVIRONMENT=Development
      - DOTNET_USE_POLLING_FILE_WATCHER=1
    volumes:
      - ../../:/src
    working_dir: /src/API
    env_file:
      - ../../.env
    depends_on:
      - db
    restart: always
    command: dotnet watch --no-hot-reload --project API.csproj run --urls=http://0.0.0.0:5000

  db:
    image: postgres:latest
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - &quot;5432:5432&quot;
    restart: always
    env_file:
      - ../../.env
    volumes:
      - app_data:/var/lib/postgresql/data
        
volumes:
  app_data:
</code></pre>
<p>How to handle migrations when using dockerized PostgreSQL?</p>
",-1,0,1,2025-08-26T11:58:15+00:00,1,77,False
79747129,19524898,,postgresql,pg_dump fails while taking backup from Dbeaver,"<p>I take backup from Dbeaver for my Postgres production database on daily basis. there are three schemas in it: two client and one workflow schema. the size of client databases are larger than that of workflow schema.</p>
<p>While taking backup, the client schema backups get completed very quickly (~ 60-90 seconds). but workflow schema backup fails after running for 40-50 seconds.</p>
<p>I am using Dbeaver 25.1.1 and pg17 binaries for the backup. I keep monitoring the backups using below query:</p>
<pre><code>select query,now()-query_start
from pg_stat_activity
where application_name='pg_dump';
</code></pre>
<p>but it stops giving error after few seconds and database gets invalidated.</p>
<p>What am I missing? How can I check the DBeaver settings and fix the issue?</p>
",2,2,0,2025-08-26T17:12:49+00:00,0,119,False
79747791,7740888,,postgresql,PostgreSQL alter column type from tstzrange to daterange,"<p>Similar to <a href=""https://stackoverflow.com/questions/60404546/postgresql-alter-column-type-from-daterange-to-tstzrange-and-migrate-the-data"">this</a> SO question, how to convert a <code>tstzrange</code> value to <code>daterange</code> (ignoring the <code>time</code> part) ?</p>
<pre class=""lang-sql prettyprint-override""><code>alter table x alter column y type daterange using y::daterange;
</code></pre>
<p>Ouput:</p>
<pre class=""lang-none prettyprint-override""><code>ERROR:  cannot cast type tstzrange to daterange
</code></pre>
",0,0,0,2025-08-27T09:43:27+00:00,1,67,True
79747980,12980093,,postgresql,Requesting an example where query with BRIN index runs much faster than with BTREE,"<p>It's very easy to see that BRIN indexes require orders of magnitude less space than BTREE indexes.</p>
<p>However, I'm struggling to come up with a query where elapsed time with BRIN index vs BTREE index is orders of magnitude faster. Frankly speaking, it's quite challenging to find a query which runs even twice faster.</p>
<p>The only requirement is that all necessary data for query result is in the index - i.e. no table look-ups are required.</p>
<p>In below test case I read 1M rows from index on column which has perfectly correlated data with physical storage (pg_stats.correlation = 1).</p>
<pre><code>drop table if exists ttt;
create table ttt(a int, b int);
insert into ttt select i, i from generate_series(1, 1e7) i;

create index brin_ttt on ttt using brin(b) with (pages_per_range = 128);
create index btree_ttt on ttt(b);

analyze ttt;
set max_parallel_workers_per_gather = 0;

select relname, pg_size_pretty(pg_relation_size(oid)) size
from pg_class c
where c.relname in ('brin_ttt', 'btree_ttt', 'ttt');

-- run below a few times to cache required blocks

set enable_indexscan = off; 
explain (analyze, buffers) select count(*) from ttt where b &gt; 1e7::int - 1e6::int;

reset enable_indexscan;
explain (analyze, buffers) select count(*) from ttt where b &gt; 1e7::int - 1e6::int;
</code></pre>
<p>Still btree gives result faster - 167ms vs 190ms.</p>
<pre><code>set enable_indexscan = off; 
explain (analyze, buffers) select count(*) from ttt where b &gt; 1e7::int - 1e6::int;

reset enable_indexscan;
explain (analyze, buffers) select count(*) from ttt where b &gt; 1e7::int - 1e6::int;
SET
                                                           QUERY PLAN                                                            
---------------------------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=59199.58..59199.59 rows=1 width=8) (actual time=190.156..190.158 rows=1 loops=1)
   Buffers: shared hit=4442
   -&gt;  Bitmap Heap Scan on ttt  (cost=254.47..56785.71 rows=965548 width=0) (actual time=0.518..132.440 rows=1000000 loops=1)
         Recheck Cond: (b &gt; 9000000)
         Rows Removed by Index Recheck: 3392
         Heap Blocks: lossy=4440
         Buffers: shared hit=4442
         -&gt;  Bitmap Index Scan on brin_ttt  (cost=0.00..13.09 rows=982659 width=0) (actual time=0.108..0.109 rows=44400 loops=1)
               Index Cond: (b &gt; 9000000)
               Buffers: shared hit=2
 Planning:
   Buffers: shared hit=11
 Planning Time: 0.138 ms
 Execution Time: 190.184 ms
(14 rows)

RESET
                                                                QUERY PLAN                                                                 
-------------------------------------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=29903.40..29903.40 rows=1 width=8) (actual time=167.632..167.633 rows=1 loops=1)
   Buffers: shared hit=2736
   -&gt;  Index Only Scan using btree_ttt on ttt  (cost=0.43..27489.53 rows=965548 width=0) (actual time=0.045..111.415 rows=1000000 loops=1)
         Index Cond: (b &gt; 9000000)
         Heap Fetches: 0
         Buffers: shared hit=2736
 Planning:
   Buffers: shared hit=1
 Planning Time: 0.107 ms
 Execution Time: 167.657 ms
(10 rows)
</code></pre>
<p>I played with <code>pages_per_range</code> parameter and with different ranges in <code>where clause</code> but BRIN still does not outperform BTREE.</p>
",5,5,0,2025-08-27T12:17:35+00:00,3,113,True
79747999,2444661,"Mumbai, India",postgresql,Command START_REPLICATION failing while trying to conusme logical replication,"<p>I am trying to consume logical replication data from postgres.</p>
<p>I follow <a href=""https://www.postgresql.org/docs/current/protocol-replication.html"" rel=""nofollow noreferrer"">this</a>.</p>
<p>First I establish pg connection as:</p>
<pre><code>pub async fn logical_replication_connection() -&gt; Result&lt;tokio_postgres::Client, Box&lt;dyn std::error::Error&gt;&gt; {
    println!(&quot;Consuming replication...&quot;);

    let conn_str = &quot;host=localhost user=postgres password=postgres dbname=mydb&quot;;
    let (client, connection) = tokio_postgres::connect(conn_str, NoTls).await?;

    tokio::spawn(async move {
        if let Err(e) = connection.await {
            eprintln!(&quot;connection error: {}&quot;, e);
        }
    });

    Ok(client)
}
</code></pre>
<p>This works fine. Then I use it as:</p>
<pre><code>fn main() {
    let tokio_rt = tokio::runtime::Runtime::new().unwrap();
    tokio_rt.block_on(async {
        match db::logical_replication_connection().await {
            Ok(client) =&gt; {
                println!(&quot;Connected client: {:?}&quot;, client);
                process_replication(client).await;
            }
            Err(e) =&gt; eprintln!(&quot;Error connecting to database: {}&quot;, e),
        }
    });
}

async fn process_replication(client: tokio_postgres::Client) -&gt; () {
    println!(&quot;Processing replication....&quot;);

    let query = &quot;START_REPLICATION SLOT my_slot LOGICAL 0/0 (proto_version '1', publication_names 'my_pub')&quot;;
    match client.simple_query(query).await {
        Ok(messages) =&gt; {
            for message in messages {
                match message {
                    tokio_postgres::SimpleQueryMessage::Row(row) =&gt; {
                        println!(&quot;Replication message: {:?}&quot;, row);
                    }
                    _ =&gt; {}
                }
            }
        }
        Err(e) =&gt; eprintln!(&quot;Failed to start replication: {}&quot;, e),
    }
}
</code></pre>
<p>Running this program throws error:</p>
<blockquote>
<p>Failed to start replication: db error: ERROR: syntax error at or near
&quot;START_REPLICATION&quot;</p>
</blockquote>
<p>Seems 'START_REPLICATION' command is not understood by postgres.</p>
<p>What mistake am I making here?</p>
",1,1,0,2025-08-27T12:37:26+00:00,1,101,True
79748162,38666,United States,postgresql,Using SQLAlchemy 2.0 to scope database transactions to pytest modules via Postgres,"<p>I am working to migrate an older codebase from SQLAlchemy 1.3 to 2.0. I successfully migrated to 1.4, but I am running into trouble with the testing suite. The tests are using Postgres SAVEPOINTs and monkeypatching <code>session.commit</code> =&gt; <code>session.flush</code> to add two nested layers of isolation:</p>
<ul>
<li>Root level tests just get standard &quot;each test method is wrapped in a separate transaction, rolled back at the end of the test&quot; <a href=""https://docs.sqlalchemy.org/en/14/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites"" rel=""nofollow noreferrer"">per the official recommendations</a></li>
<li>Tests in modules, however, are in nested transactions with some monkeypatching that prevents the outer transaction from getting closed (FWIW the <code>package_session</code> logic is basically identical to the root-level <code>session</code> fixture, except with the monkeypatch):
<pre><code>@pytest.fixture(scope=&quot;package&quot;)
def package_session(test_engine: Engine, monkeypatch_package) -&gt; Session:
    # Create a nested transaction that includes standard module data
    connection = test_engine.connect()
    transaction = connection.begin()
    session = Session(bind=connection)
    # Overwrite commits with flushes so that we can query stuff, but it's in the same transaction
    monkeypatch_package.setattr(session, &quot;commit&quot;, session.flush)
    # Create our data that is shared by the tests in this module
    _create_data_for_test(session)

    try:
        yield session
    finally:
        transaction.rollback()
        connection.close()


@pytest.fixture(scope=&quot;function&quot;)
def session(package_session):
    &quot;&quot;&quot;Return a nested transaction on the outer session, to prevent rolling back shared data&quot;&quot;&quot;
    package_session.begin_nested()
    try:
        yield package_session
    finally:
        package_session.rollback()
</code></pre>
</li>
</ul>
<p>Under SQLAlchemy 1.4 this works, but upon upgrading to 2.0 the test suite is littered with errors about missing shared data (no low-level SQLAlchemy errors, though, unlike when I updated this pattern from 1.3 to 1.4).</p>
<p>I've been fighting this for a couple days now, so I figured I'd ask: is this pattern just not possible in 2.0? I could populate shared data in the &quot;function&quot; scoped session if I had to (just will slow the test suite down), but I figured I'd ask if anyone can spot something obvious that I'm doing wrong, since I liked the pattern under 1.3/1.4.</p>
",2,2,0,2025-08-27T15:24:36+00:00,2,189,True
79748303,17846979,,postgresql,Why won&#39;t JPA let me save a ByteArray into a column of type bytea?,"<p>When I try to save Product to this table (with refImg set to bytes from File.readBytes()), the following error is thrown by Spring,</p>
<pre><code>InvalidDataAccessResourceUsageException: could not execute statement [ERROR: column &quot;ref_img&quot; is of type bytea but expression is of type bigint
</code></pre>
<p>I have read <a href=""https://stackoverflow.com/questions/3677380/proper-hibernate-annotation-for-byte"">This other StackOverflow Question</a> and this <a href=""https://gist.github.com/vegaasen/7ffb86fe380f33655ba3c59fc28459e5"" rel=""nofollow noreferrer"">Gist</a>, however both suggest variations of annotating the ByteArray field with @Type(type=&quot;org.hibernate.type.BinaryType&quot;), however the @Type annotation only accepts a class, and no class org.hibernate.type.BinaryType exists.  How would I solve this?</p>
<p>The JPA entity is as follows (SUUID is a typealias for java.util.UUID):</p>
<pre class=""lang-kotlin prettyprint-override""><code>@Entity
@Table(name = &quot;stock&quot;)
@Serializable
class Product {
    @Id
    @Column(columnDefinition = &quot;uuid&quot;)
    var id: SUUID? = null

    @Column(name=&quot;time&quot;)
    @Temporal(TemporalType.DATE)
    @SerialTransient
    var time: Date? = null

    @PreUpdate fun onUpdate() { time = Date() }

    @PrePersist fun onPersist() {
        if(id == null) id = UUID.randomUUID()
    }

    @Column var name: String? = null

    @Convert(converter = ProductInfoConverter::class)
    @Type(JsonBinaryType::class)
    @Column(columnDefinition = &quot;jsonb&quot;) var info: ProductInfo? = null

    @Lob
    @SerialTransient
    @Column(columnDefinition = &quot;BYTEA&quot;)
    var refImg: ByteArray = byteArrayOf()

    @Serializable
    data class ProductInfo(
        val description: String,
        val totalStage1: Long,
        val totalStage2: Long,
        val totalStage3: Long,
        val pricePerBoardFoot: Double,
        val color: String
    )
}

@Converter class ProductInfoConverter : AttributeConverter&lt;Product.ProductInfo?, String&gt; {
    override fun convertToDatabaseColumn(attribute: Product.ProductInfo?) =
        Json.encodeToString(attribute)

    override fun convertToEntityAttribute(dbData: String?) =
        if(dbData != null) Json.decodeFromString&lt;Product.ProductInfo?&gt;(dbData)
        else null
}
</code></pre>
",0,1,1,2025-08-27T17:33:09+00:00,1,105,True
79748794,29465844,,postgresql,Operational Error - password authentication failed using SQLAlchemy in python,"<p>Initially, I executed these commands to set up a docker container:</p>
<pre class=""lang-bash prettyprint-override""><code>docker pull postgres:latest

docker run --name postgres -e POSTGRES_USER=myusername -e POSTGRES_PASSWORD=mypassword -p 5432:5432 -v /data:/var/lib/postgresql/data -d postgres:latest

docker exec -it postgres bash
</code></pre>
<p>Things have been operating as expected. Then, I tried to create and connect an SQLAlchemy engine in Python:</p>
<pre class=""lang-py prettyprint-override""><code>from sqlalchemy import create_engine

DATABASE_URL = &quot;postgresql://myusername:mypassword@localhost/postgres&quot;

engine = create_engine(DATABASE_URL)

try:
    engine.connect()
except Exception as e: print(str(e))
</code></pre>
<p>however, the interpreter always complains:</p>
<pre><code>OperationalError: (psycopg2.OperationalError) connection to server at &quot;localhost&quot; (::1), port 5432 failed: FATAL:  password authentication failed for user &quot;myusername&quot;
</code></pre>
<p>I tried to directly connect it from <code>local</code> using</p>
<pre class=""lang-bash prettyprint-override""><code>psql -h localhost -U myusername

\c postgres
</code></pre>
<p>and have succeeded. So I thought it should be a configuration problem. Thus I modified my <code>pg_hba.conf</code> to be like the following:</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD

host    all             all             all                     trust
# &quot;local&quot; is for Unix domain socket connections only
local   all             all                                     trust
# IPv4 local connections:
host    all             all             127.0.0.1/32            trust
# IPv6 local connections:
host    all             all             ::1/128                 trust
# Allow replication connections from localhost, by a user with the
# replication privilege.
local   replication     all                                     trust
host    replication     all             127.0.0.1/32            trust
host    replication     all             ::1/128                 trust
</code></pre>
<p>but that did not work too.</p>
<p>Why can't I connect from running the above python code, provided that I could connect from local without difficulty?</p>
",2,2,0,2025-08-28T08:00:25+00:00,3,266,True
79748803,2516892,Austria,postgresql,Postgres Docker won&#39;t setup properly,"<p>I'm not really sure what's the issue here. I wrote a rather &quot;generic&quot; docker-compose.yml for Postgres, but neither won't it do its own setup properly, nor can any of the other containers (Django,..) access the database.</p>
<p>According to .env these are the credentials:</p>
<pre><code># ---- Postgres ----
POSTGRES_USER=app
POSTGRES_PASSWORD=app
POSTGRES_DB=appdb
</code></pre>
<p>Here's the docker-compose.yml:</p>
<pre><code>  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: [&quot;CMD-SHELL&quot;, &quot;pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}&quot;]
      &lt;&lt;: *health
    networks: [backend]
    logging: *json-logging
</code></pre>
<p>I'm getting this from the postgres-container logs:</p>
<pre><code>$ docker logs techstack2025-postgres-1

PostgreSQL Database directory appears to contain a database; Skipping initialization

2025-08-22 14:32:51.771 UTC [1] LOG:  starting PostgreSQL 16.10 on x86_64-pc-linux-musl, compiled by gcc (Alpine 14.2.0) 14.2.0, 64-bit
2025-08-22 14:32:51.773 UTC [1] LOG:  listening on IPv4 address &quot;0.0.0.0&quot;, port 5432
2025-08-22 14:32:51.773 UTC [1] LOG:  listening on IPv6 address &quot;::&quot;, port 5432
2025-08-22 14:32:51.795 UTC [1] LOG:  listening on Unix socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
2025-08-22 14:32:51.825 UTC [30] LOG:  database system was shut down at 2025-08-22 14:31:34 UTC
2025-08-22 14:32:51.864 UTC [1] LOG:  database system is ready to accept connections
2025-08-22 14:32:56.819 UTC [40] FATAL:  role &quot;app&quot; does not exist
2025-08-22 14:33:14.255 UTC [49] FATAL:  role &quot;app&quot; does not exist
2025-08-22 14:33:29.609 UTC [55] FATAL:  role &quot;app&quot; does not exist
2025-08-22 14:33:32.230 UTC [57] FATAL:  password authentication failed for user &quot;app&quot;
2025-08-22 14:33:32.230 UTC [57] DETAIL:  Role &quot;app&quot; does not exist.
        Connection matched file &quot;/var/lib/postgresql/data/pg_hba.conf&quot; line 128: &quot;host all all all scram-sha-256&quot;
</code></pre>
<p>Django can't connect and neither of the other services can. I'm not using a Dockerfile for Postgres as by default it's supposed to set up the database by default?</p>
",-1,0,1,2025-08-28T08:07:52+00:00,0,56,False
79749620,31374417,,postgresql,Python in a Docker Container refused connection to Postgres in a networked Docker Container,"<p>I am trying to connect from a python script to a postgres database. Here's my docker-compose:</p>
<pre><code>networks:
  backend:
    name: value_tracker_backend
    external: true
services:
  db:
    build:
      context: ./sql
      dockerfile: db.dockerfile
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: EntitiesAndValues
    ports:
      - &quot;5431:5432&quot;
    healthcheck:
      test: [&quot;CMD-SHELL&quot;, &quot;pg_isready -U postgres -d EntitiesAndValues&quot;]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - backend
  dataseeder:
    build:
      context: .
      dockerfile: dataseeder/dataseeder.dockerfile
    depends_on:
      db:
        condition: service_healthy
        restart: true
    networks:
      - backend   
</code></pre>
<p>And here's a snippet of the python code that's attempting to connect via dockers internal network:</p>
<pre><code>import psycopg2
from enum import Enum

class DatabaseConnector:
    def __init__(self, db_name, db_user):
        self.db_name = db_name
        self.db_user = db_user
    
    def __enter__(self):
        self.conn = psycopg2.connect(user = self.db_user,
                                     database = self.db_name,
                                     password = &quot;data&quot;,
                                     host = &quot;db&quot;,
                                     port = 5432)
        return self.conn

    def __exit__(self, type, value, tb):
        self.conn.close()
</code></pre>
<p>I keep getting the following error:</p>
<pre><code>0.394 dsn = 'user=data_seeder password=data host=db port=5432 dbname=EntitiesAndValues'
0.394 connection_factory = None, cursor_factory = None
0.394 kwargs = {'database': 'EntitiesAndValues', 'host': 'db', 'password': 'data', 'port': 5432, ...}
0.394 kwasync = {}
0.394 
0.394     def connect(dsn=None, connection_factory=None, cursor_factory=None, **kwargs):
0.394         &quot;&quot;&quot;
0.394         Create a new database connection.
0.394     
0.394         The connection parameters can be specified as a string:
0.394     
0.394             conn = psycopg2.connect(&quot;dbname=test user=postgres password=secret&quot;)
0.394     
0.394         or using a set of keyword arguments:
0.394     
0.394             conn = psycopg2.connect(database=&quot;test&quot;, user=&quot;postgres&quot;, password=&quot;secret&quot;)
0.394     
0.394         Or as a mix of both. The basic connection parameters are:
0.394     
0.394         - *dbname*: the database name
0.394         - *database*: the database name (only as keyword argument)
0.394         - *user*: user name used to authenticate
0.394         - *password*: password used to authenticate
0.394         - *host*: database host address (defaults to UNIX socket if not provided)
0.394         - *port*: connection port number (defaults to 5432 if not provided)
0.394     
0.394         Using the *connection_factory* parameter a different class or connections
0.394         factory can be specified. It should be a callable object taking a dsn
0.394         argument.
0.394     
0.394         Using the *cursor_factory* parameter, a new default cursor factory will be
0.394         used by cursor().
0.394     
0.394         Using *async*=True an asynchronous connection will be created. *async_* is
0.394         a valid alias (for Python versions where ``async`` is a keyword).
0.394     
0.394         Any other keyword parameter will be passed to the underlying client
0.394         library: the list of supported parameters depends on the library version.
0.394     
0.394         &quot;&quot;&quot;
0.394         kwasync = {}
0.394         if 'async' in kwargs:
0.394             kwasync['async'] = kwargs.pop('async')
0.394         if 'async_' in kwargs:
0.394             kwasync['async_'] = kwargs.pop('async_')
0.394     
0.394         dsn = _ext.make_dsn(dsn, **kwargs)
0.394 &gt;       conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
0.394                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
0.394 E       psycopg2.OperationalError: could not translate host name &quot;db&quot; to address: Name or service not known
0.394 
0.394 usr/local/lib/python3.13/site-packages/psycopg2/__init__.py:122: OperationalError
</code></pre>
<p>I've tried a number of things to get this to work, leading up the the pre-created external network. This seems to be useful because i'm starting the database first with docker compose up -d db. Then once i know its running i start the dataseeder image as well. Its running some tests (RUN pytest ...) before building and starting the 'release' version.</p>
<p>The run pytest is started in the dataseeder.dockerfile:</p>
<pre><code>FROM python:latest
# Create integration testing environment
RUN apt-get update
COPY dataseeder/ dataseeder/
COPY packages/ packages/
COPY tests/ tests/
COPY localtest.txt .
RUN pip install -r localtest.txt
RUN pytest -v tests/test_EntitiesValues.py
</code></pre>
<p>pinging from dataseeder to db:</p>
<pre><code># ping db
PING db (172.18.0.2) 56(84) bytes of data.
64 bytes from valuetrackersampleproject-db-1.value_tracker_backend (172.18.0.2): icmp_seq=1 ttl=64 time=0.166 ms
64 bytes from valuetrackersampleproject-db-1.value_tracker_backend (172.18.0.2): icmp_seq=2 ttl=64 time=0.093 ms
64 bytes from valuetrackersampleproject-db-1.value_tracker_backend (172.18.0.2): icmp_seq=3 ttl=64 time=0.095 ms
64 bytes from valuetrackersampleproject-db-1.value_tracker_backend (172.18.0.2): icmp_seq=4 ttl=64 time=0.095 ms
64 bytes from valuetrackersampleproject-db-1.value_tracker_backend (172.18.0.2): icmp_seq=5 ttl=64 time=0.079 ms
64 bytes from valuetrackersampleproject-db-1.value_tracker_backend (172.18.0.2): icmp_seq=6 ttl=64 time=0.094 ms
</code></pre>
<p>What am I missing here?</p>
",-2,0,2,2025-08-28T21:24:32+00:00,1,101,True
79749626,3022254,"Cleveland, OH",postgresql,substring(enumColumn::TEXT from &#39;pattern&#39;) not IMMUTABLE in PostgreSQL,"<p>As I understand it, the <code>substring</code> function should be immutable. However, when I try to generate a column from the expression <code>substring(enumColumn::TEXT from '-(.+)')</code> I get a <code>generation expression is not immutable</code> error.</p>
<p><code>enumColumn</code> is constrained to not be NULL, and it is not a generated column. Its type is an enum whose values are strings that all match the pattern <code>'-(.+)'</code>.</p>
<p>Is the issue related to the possibility that the pattern won't match and return NULL? Or is PostgreSQL complaining about something else?</p>
<hr />
<p>UPDATE: this works fine when the column type from which the substring is computed is already TEXT. I think the issue is that enumType::TEXT is only STABLE, not IMMUTABLE?</p>
",1,1,0,2025-08-28T21:34:01+00:00,1,75,True
79749985,22080682,,postgresql,Django-Tenant-Users: IntegrityError on permissions_usertenantpermissions_profile_id_key when creating a tenant,"<p>I’m using Django with django-tenants and django-tenant-users to manage multi-tenant accounts.
I’m having an issue when creating a new tenant:</p>
<p>When I try to create a new user + its tenant through my view, I get the following error:</p>
<pre><code>django.db.utils.IntegrityError: ERROR: Duplicate key value breaks unique constraint 'permissions_usertenantpermissions_profile_id_key'
DETAIL: Key '(profile_id)=(8)' already exists.
</code></pre>
<p>I even cleared the table and tried but with all that the error is there</p>
<p>Here is the code for my school_login view:</p>
<pre class=""lang-py prettyprint-override""><code>def school_login(request):
    if request.method == 'POST':
        form = CreateSchoolForm(request.POST)
        if form.is_valid():
            sigle = form.cleaned_data['sigle']
            slug = form.cleaned_data['sigle'].lower()
            email = form.cleaned_data['email']
            password = form.cleaned_data['password']

            # Create the user
            user = SchoolUser.objects.create_user(
                email=email,
                password=password,
            )
            user.role = 'director'
            user.is_verified = True
            user.save()

            # Create the tenant
            tenant, domain = provision_tenant(
                tenant_name=sigle,
                tenant_slug=slug,
                owner=user,
                is_superuser=True,
                is_staff=True,
            )

            # Authenticate and login
            authenticated_user = authenticate(request, username=email, password=password)
            if authenticated_user:
                login(request, authenticated_user)
                tenant_domain = get_tenant_domain_model().objects.get(tenant=tenant).domain
                return HttpResponseRedirect(f&quot;http://{tenant_domain}/&quot;)

    else:
        form = CreateSchoolForm()
    return render(request, 'school_login.html', {'form': form})

</code></pre>
<p>Here is the code of models.py in the shared app:</p>
<pre class=""lang-py prettyprint-override""><code>class SchoolUser(UserProfile):
    ROLE_CHOICES = (
        ('director', 'Directeur'),
        ('accountant', 'Comptable'),
    )
    role = models.CharField(max_length=20, choices=ROLE_CHOICES, default='accountant')

    def is_director(self):
        return self.role == 'director'

    def is_accountant(self):
        return self.role == 'accountant'

class School(TenantBase):
    name = models.CharField(max_length=100)  # Ajout du champ name
    created_on = models.DateField(auto_now_add=True)

    def __str__(self):
        return self.name

class Domain(DomainMixin):
    pass
</code></pre>
<p>What I’ve tried:</p>
<ul>
<li>Clearing all data in the database → same error.</li>
<li>Checking if I’m calling tenant.add_user() twice → only called once.</li>
<li>Checking for post_save signals → nothing special in my project.</li>
<li>Configuring the settings.py</li>
</ul>
<p>Why does provision_tenant try to create the same user profile twice in permissions_usertenantpermissions, even though I only call it once?</p>
<p>How can I fix this IntegrityError and properly create a tenant with its owner?</p>
",0,0,0,2025-08-29T08:09:32+00:00,0,50,False
79750124,853462,"Groningen, Netherlands",postgresql,"Django doesn&#39;t release database connections for re-use, long after request has finished","<p>My django app loads several pages (images) at once, e.g. to show a document, and queries the database mainly to check permissions. Django keeps the connections alive, but doesn't re-use them. At some point the maximum number of connections of postgresql is reached, and will never be 'unreached' because of this behavior. I've added <code>CONN_MAX_AGE</code> of 15 seconds and intermediate <code>pgbouncer</code>, but the behavior is the same, the connections stay open, even after 15&quot;.</p>
<pre class=""lang-json prettyprint-override""><code>{
   &quot;default&quot;: {
       &quot;ENGINE&quot;: &quot;django.db.backends.postgresql&quot;,
       &quot;HOST&quot;: &quot;pgbouncer&quot;,&quot;PORT&quot;: 6432,&quot;NAME&quot;: &quot;...&quot;, &quot;USER&quot;: &quot;...&quot;,
       &quot;PASSWORD&quot;: &quot;...&quot;,
       &quot;CONN_MAX_AGE&quot;: 15, &quot;CONN_HEALTH_CHECKS&quot;: true
   }
}
</code></pre>
<p>The settings point to a pgbouncer proxy in this case, but I also have this issue without pgbouncer. Here's what I see. I've loaded a couple of pages on the website, and they fill up <code>pg_stat_activity</code>.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
  split_part(query, 'WHERE', 2) AS sql,
  COUNT(*) AS count
FROM pg_stat_activity
WHERE datname = '...'
GROUP BY query
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>sql</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>... &quot;page&quot;.&quot;page_id&quot; = 12 ...</td>
<td>1</td>
</tr>
<tr>
<td>... &quot;page&quot;.&quot;page_id&quot; = 65 ...</td>
<td>1</td>
</tr>
<tr>
<td>... &quot;page&quot;.&quot;page_id&quot; = 23 ...</td>
<td>1</td>
</tr>
<tr>
<td>... &quot;page&quot;.&quot;page_id&quot; = 78 ...</td>
<td>1</td>
</tr>
<tr>
<td>... &quot;page&quot;.&quot;page_id&quot; = 32 ...</td>
<td>1</td>
</tr>
<tr>
<td>... many more rows ...</td>
<td>...</td>
</tr>
</tbody>
</table></div>
<p>These entries stay, with exactly the same <code>page_id</code>, for hours, eventhough the development console of the website has no traffic and also when the webpage is closed in the browser. Nor is there traffic in the nginx logs. These are only released by killing django. Otherwise, these connections stay open indefinitely, and aren't re-used as the sql stays the same.</p>
<p>I also tried <code>&quot;CONN_MAX_AGE&quot;: None</code> and <code>&quot;CONN_MAX_AGE&quot;: 0</code>, the connections stay open.</p>
<p>FYI, the pgbouncer config:</p>
<pre class=""lang-ini prettyprint-override""><code>[databases]
...= dbname=printedregistries_... host=... port=5432 user=... password=...

[pgbouncer]
auth_type=scram-sha-256
auth_file=/etc/pgbouncer/users.txt
pool_mode=session
listen_addr=*
listen_port=6432
verbose=3

# Connection limits
max_client_conn=100
default_pool_size=20
min_pool_size=5
reserve_pool_size=5
max_db_connections=50
max_user_connections=50

# Timeouts to prevent connection accumulation
server_idle_timeout=300
server_lifetime=3600
client_idle_timeout=300
client_login_timeout=60

# This is the SSL of the postgres server, not of whats served by pgbouncer.
server_tls_sslmode=require
</code></pre>
<p>When flooded, there are two outcomes. Either <code>query_wait_timeout</code> with the settings above or <code>connection failed: connection to server at &quot;...&quot;, port 6432 failed: ERROR:  no more connections allowed (max_client_conn)</code> when only <code>max_client_conn</code> is set. That is, pgbouncer tries to fix the lack of connections by waiting, but of course that won't matter if they never come free.</p>
<p>How can I make django use the database connections in a 'sane' way, e.g. how can I avoid the connections from staying open AND 'locked' or 'used' or 'occupied' even though the request has long finished?</p>
<p>This I tried:</p>
<ul>
<li>client-side loading pool, load max 5 pages at once. Didn't help, limit is eventually still reached.</li>
<li>pgbouncer intermediate, didn't help, pgbouncer also doesn't re-use connections if they are kept 'used' by django.</li>
<li>django <code>CONN_MAX_AGE</code>, seems to do nothing.</li>
</ul>
<h1>Update 2025-09-01</h1>
<p>One of the views that seems to clutter the database server with un-reusable connections is this one:</p>
<pre class=""lang-py prettyprint-override""><code>class DocumentAPIView(viewsets.ModelViewSet):
    serializer_class = DocumentSerializer
    permission_classes = [permissions.IsAuthenticated]
    lookup_field = 'id'

    @action(detail=True, methods=['get'], name='image', url_path='image/(?P&lt;pagenum&gt;\d+)')
    @method_decorator(cache_control(max_age=60 * 60 * 24 * 7, private=True))
    def image(self, request, id, pagenum):
        page = Page.objects.filter(
            document__corpus__owner=request.user,
            document_id=int(id),
            seq_within_document=int(pagenum),
        ).first()

        return HttpResponse(page.img, content_type=&quot;image/png&quot;)
</code></pre>
<p>Nonetheless, all the connections list as <code>idle</code> in <code>pg_stat_activity</code>.</p>
",0,0,0,2025-08-29T10:11:50+00:00,1,223,False
79751216,7799308,,postgresql,PostgreSQL JSONB fields being encoded as hex (\x...) instead of JSON text when using pgtype.JSONB with sqlx,"<p>I'm using Go with sqlx and <code>pgtype.JSONB</code> to store JSON data in PostgreSQL, but the data is being transmitted as hex-encoded bytes instead of regular JSON text, causing &quot;invalid input syntax for type json&quot; errors.</p>
<pre><code>type Platform struct {
    Logo               pgtype.JSONB `db:&quot;logo&quot;`
    Motivation  pgtype.JSONB `db:&quot;motivation&quot;`
    Rules       pgtype.JSONB `db:&quot;rules&quot;`
}

func convertToJSONB(dest *pgtype.JSONB, src interface{}) error {
    data, err := json.Marshal(src)
    if err != nil {
        return err
    }
    return dest.Set(data) // or direct assignment: dest.Bytes = data; dest.Status = pgtype.Present
}
</code></pre>
<p>The SQL query shows hex-encoded data instead of JSON: <code>logo = '\x7b2264656661756c74223a22687474707...'</code></p>
<p>Which decodes to valid JSON: {&quot;default&quot;:&quot;https://...&quot;}</p>
<pre><code>ERROR: invalid input syntax for type json at character 435
DETAIL: Token &quot;\&quot; is invalid
</code></pre>
<p><strong>What I've tried:</strong></p>
<pre><code>Using dest.Set(data)
Direct assignment: dest.Bytes = data; 
dest.Status = pgtype.Present
</code></pre>
<p>Both approaches result in the same hex encoding</p>
<p><strong>Questions:</strong></p>
<p>Why is <code>pgtype.JSONB</code> being transmitted as hex instead of JSON text?
Is this a driver issue with how sqlx handles <code>pgtype.JSONB</code>?
Should I use *string fields instead for JSON columns?
What's the correct way to store JSON data in PostgreSQL JSONB columns using Go?</p>
<p><strong>Environment:</strong></p>
<p>Go with sqlx and github.com/jackc/pgtype
PostgreSQL with JSONB columns
Data is valid JSON when decoded from hex</p>
",0,1,1,2025-08-30T14:35:23+00:00,0,82,False
79751843,2853583,,postgresql,postgresql complex grouping within json_build_object + sum,"<p>Please help me with a request. I have two tables. The first table stores data on the user's use of the application for a certain period of time.<br />
And I have a table of granules, which I compiled using a query on the first table.<br />
Screen my tables:<br />
<a href=""https://i.sstatic.net/f5KRpdW6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f5KRpdW6.png"" alt=""enter image description here"" /></a><br />
<a href=""https://i.sstatic.net/85H0BWTK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/85H0BWTK.png"" alt=""enter image description here"" /></a><br />
Example value for field app_info:<br />
<a href=""https://i.sstatic.net/xV0oF2Ri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xV0oF2Ri.png"" alt=""enter image description here"" /></a><br />
But I need to get the resulting query, which for each granule will create an array of json for the programs used during the granule of time. I also need to group the programs by name or domain_site field and create an array of headers. For example, a user used the browser for 10 minutes, opening different tabs. We then need to get the following json:</p>
<pre class=""lang-json prettyprint-override""><code>            [
          {
            &quot;app_name&quot;: &quot;googlechrome&quot;,
            &quot;path&quot;: &quot;C:\\ProgramFiles\\google\\googlechrome.exe&quot;,
            &quot;domain_site&quot;: &quot;stackoverfloww.com&quot;,
            &quot;count_seconds&quot;: 540,
            &quot;titles&quot;: [
              {
    &quot;count_seconds&quot;: 320,
                &quot;url&quot;: &quot;https://stackoverflow.com/questions/40978290/construct-json-object-from-query-with-group-by-sum&quot;,
                &quot;title&quot;: &quot;Construct json object from query with group by / sum&quot;
              },
              {
    &quot;count_seconds&quot;: 220,
                &quot;url&quot;: &quot;https://stackoverflow.com/questions/43117033/aggregate-function-calls-cannot-be-nested-postgresql&quot;,
                &quot;title&quot;: &quot;aggregate function calls cannot be nested postgresql&quot;
              }
            ]
          }
        ]
</code></pre>
<p>And if the user used several applications in 10 minutes, then for each application such statistics. For each application, you need to count the number of seconds of its use.<br />
I expect to get such a table:<br />
<a href=""https://i.sstatic.net/eu9JjyvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eu9JjyvI.png"" alt=""enter image description here"" /></a></p>
<p>I can't cope with such a request. Here is the request I wrote:</p>
<pre class=""lang-sql prettyprint-override""><code>        select 
    employee_id,
    date,
    granula_start,
    granula_end,
    
    (select 
        array_agg(json_build_object('seconds', SUM( case 
                                          when end_time &gt; granula_end 
                                          then ((EXTRACT(MINUTE FROM granula_end) - EXTRACT(MINUTE FROM start_time))*60) 
                                          else (case  when EXTRACT(MINUTE FROM start_time) = EXTRACT(MINUTE FROM end_time) 
                                                      then 
                                                       EXTRACT(SECOND FROM end_time) - EXTRACT(SECOND FROM start_time)
                                                      else
                                                       (EXTRACT(MINUTE FROM end_time) - EXTRACT(MINUTE FROM start_time)) * 60 
                                               end )
                                          end ),
                          'domain_site', m.app_info::jsonb-&gt;'domain_site',
                          'app_name', m.app_info::jsonb-&gt;'app_name',
                          'app_type', m.app_info::jsonb-&gt;'app_type' )) as app_info
     from pps.my_temp m
     where start_time &gt;= granula_start and start_time &lt;= granula_end
     group by m.app_info::jsonb-&gt;'domain_site', m.app_info::jsonb-&gt;'app_name'
     )
     
    from granules
</code></pre>
<p>But I get an error...<br />
<a href=""https://i.sstatic.net/JXPmiI2C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JXPmiI2C.png"" alt=""enter image description here"" /></a></p>
",-3,0,3,2025-08-31T15:21:51+00:00,2,113,True
79752209,5272283,"New Delhi, Delhi, India",postgresql,node js + postgres service returning vastly different SQL row count,"<p>I've created a backend service using node 18 for a frontend dashboard which requires execution of various types of filtering, grouping, export data SQLs to be executed on the postgres DB. For the export functionality, my backend first gets the count of total rows to be exported (for sending the export progress update) and then executes the actual SQL whose data is streamed to the front end. I tried 2 consecutive exports with same variables. The count returned by the SQLs in both cases is almost similar but the actual data returned by the main SQLs is vastly different.</p>
<p>Queries are being executed to the same DB and using the same DB connection pool.</p>
<p>Count SQLs in both cases :</p>
<pre><code>SELECT COUNT(*) as count 
FROM table_a a JOIN table_b b ON a.id = b.id
WHERE a.log_date_time &gt;= NOW() - CAST('15 days' AS INTERVAL)
</code></pre>
<p>Row Count 1 : 679045</p>
<p>Row Count 2 : 679270 (2 minutes later)</p>
<p>Main Data SQLs in both cases :</p>
<pre><code>SELECT TO_CHAR(a.log_date_time, 'YYYY-MM-DD HH24:MI:SS.MS') as log_date_time, ...
FROM table_a a JOIN table_b b ON a.id = b.id
WHERE a.log_date_time &gt;= NOW() - CAST('15 days' AS INTERVAL) 
ORDER BY a.log_date_time DESC
</code></pre>
<p>Row Count 1 : 677414</p>
<p>Row Count 2 : 501708 (2 minutes later)</p>
",-3,0,3,2025-09-01T06:15:40+00:00,1,95,False
79752353,31394875,,postgresql,Prepared statements not working with Psycopg3 + PgBouncer (transaction pooling),"<p>I'm running into unexpected behavior when trying to use prepared statements with TimescaleDB through PgBouncer in transaction pooling mode.</p>
<p>According to the Psycopg3 docs on <a href=""https://www.psycopg.org/psycopg3/docs/advanced/prepare.html#prepared-statements"" rel=""nofollow noreferrer"">prepared statements</a>
and resources from TimescaleDB like this <a href=""https://www.tigerdata.com/blog/boosting-postgres-performance-with-prepared-statements-and-pgbouncers-transaction-mode"" rel=""nofollow noreferrer"">article</a>, this should work.</p>
<p>However, in practice:</p>
<ul>
<li>With a direct connection to PgBouncer's port: prepared statements work as expected.</li>
<li>With a NullConnectionPool required for pooling with PgBouncer: prepared statements don’t seem to be created or reused at all, even when explicitly setting prepare=True.</li>
</ul>
<p>Here are two minimal examples that demonstrate the issue.</p>
<h3>Example 1: Direct Connection + PgBouncer (works)</h3>
<pre class=""lang-py prettyprint-override""><code># client.py
import psycopg


class DBClient:
    def __init__(self, server, name, username, password, port, *args, **kwargs):
        self.server = server
        self.name = name
        self.username = username
        self.password = password
        self.port = port
        self._connection: psycopg.Connection | None = None

    @property
    def connection(self) -&gt; psycopg.Connection:
        if not self._connection or self._connection.closed:
            self._connection = psycopg.connect(
                host=self.server,
                dbname=self.name,
                user=self.username,
                password=self.password,
                port=self.port,
                prepare_threshold=0,
            )
        return self._connection

    def close_connection(self):
        if self._connection and not self._connection.closed:
            self._connection.close()
            self._connection = None
</code></pre>
<pre class=""lang-py prettyprint-override""><code># data.py
import pandas as pd
from client import DBClient

def get_data(start_date):
    db = DBClient(&quot;host&quot;, &quot;mydb&quot;, &quot;user&quot;, &quot;pass&quot;, 5432)
    query = &quot;SELECT date, value FROM my_schema.my_table WHERE date &gt;= %s&quot;
    params = [start_date]

    with db.connection.cursor() as cursor:
        result = cursor.execute(query, params, prepare=True)
        rows = result.fetchall()
        columns = [desc[0] for desc in result.description]

    df = pd.DataFrame(rows, columns=columns)
    return df
</code></pre>
<hr />
<h3>Example 2: NullConnectionPool + PgBouncer (does not work)</h3>
<pre class=""lang-py prettyprint-override""><code># client.py
from contextlib import contextmanager
from psycopg_pool import NullConnectionPool


class DBClient:
    def __init__(self, server, name, username, password, port, *args, **kwargs):
        self.pool = NullConnectionPool(
            f&quot;host={server} dbname={name} user={username} password={password} port={port}&quot;,
            min_size=0,
            max_size=20,
        )

    @contextmanager
    def get_connection(self):
        if not self.pool:
            raise RuntimeError(&quot;Connection pool not initialized&quot;)

        conn = self.pool.getconn()
        try:
            yield conn
        finally:
            self.pool.putconn(conn)

    @contextmanager
    def get_cursor(self, **cursor_kwargs):
        with self.get_connection() as conn:
            with conn.cursor(**cursor_kwargs) as cursor:
                yield cursor

    def close_pool(self):
        if self.pool:
            self.pool.close()
            self.pool = None
</code></pre>
<pre class=""lang-py prettyprint-override""><code># data.py
import pandas as pd
from client import DBClient

def get_data(start_date):
    db = DBClient(&quot;host&quot;, &quot;mydb&quot;, &quot;user&quot;, &quot;pass&quot;, 5432)
    query = &quot;SELECT date, value FROM my_schema.my_table WHERE date &gt;= %s&quot;
    params = [start_date]

    with db.get_cursor() as cursor:
        result = cursor.execute(query, params, prepare=True)
        rows = result.fetchall()
        columns = [desc[0] for desc in result.description]

    df = pd.DataFrame(rows, columns=columns)
    return df
</code></pre>
<hr />
<p>In Example 1, the prepared statement is created and used correctly.
In Example 2, despite passing <code>prepare=True</code>, no prepared statements are created or reused.</p>
<p>I'm using <code>psycopg==3.2.9</code> and <code>pgBouncer 1.24</code>.</p>
<p>Since using a single persistent connection in a backend service is not a viable option, I’d like to know:</p>
<ul>
<li>Is this expected behavior with TimescaleDB, PgBouncer, or Psycopg3?</li>
<li>Can prepared statements be reliably used when connecting through PgBouncer in transaction pooling mode?</li>
</ul>
<p>Does anyone know how to solve this issue and properly use prepared statements with TimescaleDB and PgBouncer?</p>
",0,0,0,2025-09-01T09:22:09+00:00,0,106,False
79752827,31398660,,postgresql,OpenSSL::Cipher::CipherError Rails 7 does not decrypt manually,"<p>I'm having some trouble to decrypt ciphertext that was added to my logs using Rails 7.1.5.1</p>
<p>I recently added the Rails Encryption and made the upgrade to Rails 7 and now I need to decrypt logs that I use alongside paper trail gem</p>
<p>Using <code>ActiveRecord::Encryption::Encryptor.new.decrypt(cipher)</code> to decrypt data, not sure if this is the actual one expected to be used to do it manually like I'm trying</p>
<pre><code>irb(main):001&gt; cipher = &quot;{\&quot;p\&quot;:\&quot;mNFHAtQCJ1eDF2qlSMBnkblVxz6wbQ==\&quot;,\&quot;h\&quot;:{\&quot;iv\&quot;:\&quot;kFniqm95y2jn4+ts\&quot;,\&quot;at\&quot;:\&quot;MgoR67m8PjB9Meut6V181A==\&quot;}}&quot;
=&gt; &quot;{\&quot;p\&quot;:\&quot;mNFHAtQCJ1eDF2qlSMBnkblVxz6wbQ==\&quot;,\&quot;h\&quot;:{\&quot;iv\&quot;:\&quot;kFniqm95y2jn4+ts\&quot;,\&quot;at\&quot;:\&quot;MgoR67m8PjB9Meut6V181A==\&quot;}}&quot;
irb(main):002&gt; ActiveRecord::Encryption::Encryptor.new.decrypt(cipher)
(irb):2:in `&lt;main&gt;': ActiveRecord::Encryption::Errors::Decryption (ActiveRecord::Encryption::Errors::Decryption)
/Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:79:in `rescue in decrypt': ActiveRecord::Encryption::Errors::Decryption (ActiveRecord::Encryption::Errors::Decryption)
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:55:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:42:in `block in try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `with_index'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:26:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/encryptor.rb:56:in `decrypt'
        from (irb):2:in `&lt;main&gt;'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `eval'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:591:in `evaluate_expression'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:557:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:201:in `block (2 levels) in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:512:in `signal_status'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:193:in `block in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:272:in `block in each_top_level_statement'
        ... 23 levels...
/Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:75:in `final': OpenSSL::Cipher::CipherError
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:75:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:42:in `block in try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `with_index'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:26:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/encryptor.rb:56:in `decrypt'
        from (irb):2:in `&lt;main&gt;'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `eval'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:591:in `evaluate_expression'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:557:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:201:in `block (2 levels) in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:512:in `signal_status'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:193:in `block in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:272:in `block in each_top_level_statement'
        ... 23 levels...

</code></pre>
<p>Even if I do this directly with the cyphertext generated by Rails method it does not work, but the whole system works to encrypt and decrypt in the database level, so I wonder if this method is the same that Rails is using to decrypt data from database</p>
<pre><code>irb(main):003&gt; Shop.last.ciphertext_for(:email)
  Shop Load (4.2ms)  SELECT &quot;shops&quot;.* FROM &quot;shops&quot; ORDER BY &quot;shops&quot;.&quot;id&quot; DESC LIMIT $1  [[&quot;LIMIT&quot;, 1]]
=&gt; &quot;{\&quot;p\&quot;:\&quot;mNFHAtQCJ1eDF2qlSMBnkblVxz6wbQ==\&quot;,\&quot;h\&quot;:{\&quot;iv\&quot;:\&quot;kFniqm95y2jn4+ts\&quot;,\&quot;at\&quot;:\&quot;MgoR67m8PjB9Meut6V181A==\&quot;}}&quot;
irb(main):004&gt; cipher = _
=&gt; &quot;{\&quot;p\&quot;:\&quot;mNFHAtQCJ1eDF2qlSMBnkblVxz6wbQ==\&quot;,\&quot;h\&quot;:{\&quot;iv\&quot;:\&quot;kFniqm95y2jn4+ts\&quot;,\&quot;at\&quot;:\&quot;MgoR67m8PjB9Meut6V181A==\&quot;}}&quot;
irb(main):005&gt; ActiveRecord::Encryption::Encryptor.new.decrypt(cipher)
(irb):5:in `&lt;main&gt;': ActiveRecord::Encryption::Errors::Decryption (ActiveRecord::Encryption::Errors::Decryption)
/Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:79:in `rescue in decrypt': ActiveRecord::Encryption::Errors::Decryption (ActiveRecord::Encryption::Errors::Decryption)
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:55:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:42:in `block in try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `with_index'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:26:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/encryptor.rb:56:in `decrypt'
        from (irb):5:in `&lt;main&gt;'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `eval'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:591:in `evaluate_expression'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:557:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:201:in `block (2 levels) in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:512:in `signal_status'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:193:in `block in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:272:in `block in each_top_level_statement'
        ... 23 levels...
/Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:75:in `final': OpenSSL::Cipher::CipherError
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher/aes256_gcm.rb:75:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:42:in `block in try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `with_index'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:41:in `try_to_decrypt_with_each'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/cipher.rb:26:in `decrypt'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/activerecord-7.1.5.1/lib/active_record/encryption/encryptor.rb:56:in `decrypt'
        from (irb):5:in `&lt;main&gt;'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `eval'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/workspace.rb:101:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:591:in `evaluate_expression'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb/context.rb:557:in `evaluate'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:201:in `block (2 levels) in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:512:in `signal_status'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:193:in `block in eval_input'
        from /Users/caio.motta/.rbenv/versions/3.2.3/lib/ruby/gems/3.2.0/gems/irb-1.15.2/lib/irb.rb:272:in `block in each_top_level_statement'
        ... 23 levels...
</code></pre>
<p>My credentials are set in the application.rb with env vars, I saw that others had problem with credentials that was stored in the credentials.yml.enc, but it seems not my case</p>
<pre><code>    config.active_record.encryption.primary_key = ENV.fetch('ACTIVE_RECORD_PRIMARY_KEY', nil)
    config.active_record.encryption.deterministic_key = ENV.fetch('ACTIVE_RECORD_DETERMINISTIC_KEY', nil)
    config.active_record.encryption.key_derivation_salt = ENV.fetch('ACTIVE_RECORD_KEY_DERIVATION_SALT', nil)
</code></pre>
<p>Was expecting the same behavior in the <a href=""https://i.sstatic.net/Knfe956G.png"" rel=""nofollow noreferrer"">screenshot</a></p>
",-1,0,1,2025-09-01T18:53:23+00:00,1,108,True
79753045,2699523,"Hyderabad, India",postgresql,Inserting a binary image string (of a pdf) into Postgres,"<p>I have trying to insert a binary image string (of a PDF) into Postgres table from my Spring Boot Application. I'm using JPA for DB operations.</p>
<p>The corresponding field in my Entity class is declared as below</p>
<pre><code>@Column(name = &quot;doc_image&quot;)
@Lob
private byte[] docImage;
</code></pre>
<p>Similarly, the column in the table is created as follows (in DDL)</p>
<pre><code>doc_image bytea NULL
</code></pre>
<p>When I try to insert my string, it is failing with the message &quot;column doc_image is of type bytea but expression is of type bigint. Hint: you will need to rewrite or cast the expression.&quot;</p>
<p>I understood that there is a conversion problem from byte[] to bytea but i'm not able to resolve it.</p>
<p>before inserting my binary-image-string to the byte[] field, I'm doing the following conversion. Please check whether this is causing any problem.</p>
<pre><code>entityObj.setDocImage(Base64.getDecoder.decode(binaryImageString));
</code></pre>
<p>The String variable binaryImageString contains the binary image of my pdf in the String format.</p>
<p>Please let me know where am i missing or doing wrong and give me a solution.</p>
<p>When I declared the doc_image as text (like, doc_image text NULL) in the table creation DDL, it is working fine but storing a number (eg: 23923) instead of binary content.</p>
<p>So, I understood, everything else is fine but the conversion has some issues. But, please guide me on this.</p>
",0,0,0,2025-09-02T03:33:03+00:00,1,79,False
79753268,30849885,,postgresql,PostgreSQL C Extension: popen crashes backend on external process failure,"<p>I am developing a PostgreSQL C extension to run an external data transfer tool (<code>FastTransfer.exe</code>) using <code>popen</code>. The extension works well when the external process succeeds, but it causes the entire PostgreSQL server to crash and restart when the external process exits with a fatal error.</p>
<p><strong>Problem Description</strong></p>
<p>My C extension, which is a wrapper for a custom C# executable, is designed to perform data transfers. The core functionality is to call the executable with various command-line arguments and capture its output to return to the user. I'm using <code>popen</code> for this purpose.</p>
<p>The issue occurs when the external executable fails to start or encounters a fatal, unhandled error (for instance, if a required license file is missing). Instead of simply returning an error code or an error message to my extension, the external process's ungraceful termination causes the PostgreSQL backend process that called it to crash as well. This forces PostgreSQL to enter recovery mode on the next restart.</p>
<p><strong>Code Snippet</strong></p>
<p>Here is the relevant portion of my C code showing the popen call and the error handling:</p>
<pre><code>ereport(LOG, (errmsg(&quot;pg_fasttransfer: Final command to be executed: %s&quot;, command-&gt;data)));
    
    PG_TRY();
    {
       fp = popen(command-&gt;data, &quot;r&quot;);
        if (!fp) {
            ereport(FATAL, (errmsg(&quot;pg_fasttransfer: unable to execute FastTransfer. Check the binary path, permissions, and environment variables.&quot;)));
        } else {
            result_output = makeStringInfo();
            while (fgets(buffer, sizeof(buffer), fp) != NULL) {
                appendStringInfoString(result_output, buffer);
            }
            
            // Copier le contenu du StringInfo dans le tampon statique pour l'analyse
            strlcpy(result_buffer, result_output-&gt;data, sizeof(result_buffer));
            pfree(result_output-&gt;data); // Libère la mémoire allouée par StringInfo
            pfree(result_output);
            
            status = pclose(fp);

#ifdef _WIN32
            exit_code = status;
#else
            if (WIFEXITED(status)) {
                exit_code = WEXITSTATUS(status);
            } else {
                exit_code = -2;
                strncat(result_buffer, &quot;\nUnknown error of FastTransfer\n&quot;, sizeof(result_buffer) - strlen(result_buffer) - 1);
            }
#endif
    
            if (strstr(result_buffer, &quot;Licence file not found&quot;) != NULL) {
                exit_code = -4; // Code d'erreur pour la licence
                strlcpy(result_buffer, &quot;Error: FastTransfer.exe could not find the license file. Please ensure 'FastTransfer.lic' is in the same directory as the executable.&quot;, sizeof(result_buffer));
            }
    
            ereport(LOG, (errmsg(&quot;pg_fasttransfer: Process exited with status code: %d&quot;, exit_code)));
            
            if (result_buffer[0] != '\0') {
                token = strstr(result_buffer, &quot;Total rows : &quot;);
                if (token != NULL) {
                    total_rows = strtol(token + strlen(&quot;Total rows : &quot;), NULL, 10);
                }
                
                token = strstr(result_buffer, &quot;Total columns : &quot;);
                if (token != NULL) {
                    total_columns = strtol(token + strlen(&quot;Total columns : &quot;), NULL, 10);
                }
                
                token = strstr(result_buffer, &quot;Transfert time : Elapsed&quot;);
                if (token != NULL) {
                    transfer_time = strtol(token + strlen(&quot;Transfert time : Elapsed=&quot;), NULL, 10);
                }
                
                token = strstr(result_buffer, &quot;Total time : Elapsed=&quot;);
                if (token != NULL) {
                    total_time = strtol(token + strlen(&quot;Total time : Elapsed=&quot;), NULL, 10);
                }
            }
        }
    }
    PG_CATCH();
    {
        exit_code = -3;
        ereport(LOG, (errmsg(&quot;pg_fasttransfer: An exception occurred during execution of FastTransfer. The process likely crashed.&quot;)));
        ereport(LOG, (errmsg(&quot;pg_fasttransfer: Process exited with status code: %d&quot;, exit_code)));
        strlcpy(result_buffer, &quot;An internal error occurred during data transfer. Check PostgreSQL logs for details.\n&quot;, sizeof(result_buffer));
    }
    PG_END_TRY();
    
    pfree(command-&gt;data);
    pfree(command);

    values[0] = Int32GetDatum(exit_code);
    values[1] = CStringGetTextDatum(result_buffer);
    values[2] = Int64GetDatum(total_rows);
    values[3] = Int32GetDatum(total_columns);
    values[4] = Int64GetDatum(transfer_time);
    values[5] = Int64GetDatum(total_time);
    
    tuple = heap_form_tuple(tupdesc, values, nulls);
    PG_RETURN_DATUM(HeapTupleGetDatum(tuple));
}
</code></pre>
<pre><code>SQL Error [57P03]: FATAL: the database system is in recovery mode
</code></pre>
<p><strong>What I've tried</strong></p>
<ol>
<li><p>File Permissions: I have confirmed that the PostgreSQL service account has Read and Execute permissions on the executable and its directory.</p>
</li>
<li><p>Path and Existence Check: I added a check using fopen to verify the executable's path and existence before calling popen. This prevents a crash when the file is not found.</p>
</li>
<li><p>Error Redirection: I'm using 2&gt;&amp;1 to capture standard error output, but the process crashes before my code can capture the message.</p>
</li>
<li><p>Graceful Exit: I've implemented a PG_TRY/PG_CATCH block, which successfully catches some errors, but not the fatal crash of the child process.</p>
</li>
</ol>
<p>When I run the same command directly from a system shell, the executable exits gracefully with an error message and does not cause a crash.</p>
<pre><code>PS C:\&gt; D:\pacollet\FastTransfer\FastTransfer.exe --sourceconnectiontype &quot;mssql&quot; --sourceserver &quot;localhost&quot; --sourceuser &quot;FastLogin&quot; --sourcepassword &quot;FastPassword&quot; --sourcedatabase &quot;tpch10&quot; --sourceschema &quot;dbo&quot; --sourcetable &quot;orders&quot; --targetconnectiontype &quot;pgcopy&quot; --targetserver &quot;localhost:15433&quot; --targetuser &quot;postgres&quot; --targetpassword &quot;postgres&quot; --targetdatabase &quot;postgres&quot; --targetschema &quot;public&quot; --targettable &quot;orders&quot; --degree &quot;12&quot; --method &quot;Ntile&quot; --distributekeycolumn &quot;o_orderkey&quot; --loadmode &quot;Truncate&quot; --batchsize &quot;1048576&quot; --mapmethod &quot;Position&quot;

2025-09-02T09:51:29.849 +02:00 -|-  -|- b063c52b-291e-4e27-b861-4aba8787057d -|- INFORMATION -|- postgres.public.orders -|- The FastTransfer_Settings.json file does not exist. Using default settings. Console Only
2025-09-02T09:51:29.866 +02:00 -|-  -|- b063c52b-291e-4e27-b861-4aba8787057d -|- INFORMATION -|- postgres.public.orders -|- FastTransfer - running in normal mode.
Licence file not found: FastTransfer.lic
</code></pre>
<p>My core question is: <strong>Why does the premature and unhandled termination of the child process cause a fatal crash of the PostgreSQL parent process, and how can I prevent this behavior?</strong> It seems like <code>popen</code> or the underlying system libraries are failing in a way that is not caught by <code>PG_TRY/PG_CATCH</code>. Is there a more robust way to execute an external process from a PostgreSQL C extension?</p>
<p>EDIT :</p>
<p><strong>Postgres log</strong></p>
<pre><code>2025-09-02 10:02:31 CEST LOG:  pg_fasttransfer: Final command to be executed: D:\pacollet\FastTransfer\FastTransfer.exe --sourceconnectiontype &quot;mssql&quot; --sourceserver &quot;localhost&quot; --sourceuser &quot;FastLogin&quot; --sourcepassword &quot;FastPassword&quot; --sourcedatabase &quot;tpch10&quot; --sourceschema &quot;dbo&quot; --sourcetable &quot;orders&quot; --targetconnectiontype &quot;pgcopy&quot; --targetserver &quot;localhost:15433&quot; --targetuser &quot;postgres&quot; --targetpassword &quot;postgres&quot; --targetdatabase &quot;postgres&quot; --targetschema &quot;public&quot; --targettable &quot;orders&quot; --degree &quot;12&quot; --method &quot;Ntile&quot; --distributekeycolumn &quot;o_orderkey&quot; --loadmode &quot;Truncate&quot; --batchsize &quot;1048576&quot; --mapmethod &quot;Position&quot; 2&gt;&amp;1
2025-09-02 10:02:31 CEST STATEMENT:  SELECT * from xp_RunFastTransfer_secure(
      sourceconnectiontype := 'mssql',
      sourceserver := 'localhost',
      sourceuser := 'FastLogin',
      method := 'Ntile',
      sourcedatabase := 'tpch10',
      sourceschema := 'dbo',
      sourcetable := 'orders',
      targetserver := 'localhost:15433',
      targetconnectiontype := 'pgcopy',
      sourcepassword := 'ww0EBwMC6ZfGQWy756Z10j0Bftt65IDBo78h6sJZpduXEI7ziV4VBXaTHbAAqusOCkzgr8WfALi1
    0dckj30U9zyFSYuEuJTOQPpvVhha',
      targetuser := 'postgres',
      targetdatabase := 'postgres',
      targetschema := 'public',
      targettable := 'orders',
      degree := 12,
      distributekeycolumn := 'o_orderkey',
      loadmode := 'Truncate',
      targetpassword := 'ww0EBwMCoGexPfZuuKt80jkBtCbQKgPgxHFko0bA+afdEvMgeCURBVfnSZi3sBVmn0GjmrpWdapl
    7gY5urqUYiAn1hODJEzxVdQ=',
      batchsize := 1048576,
      mapmethod := 'Position',
      fasttransfer_path := 'D:\pacollet\FastTransfer'
    )
2025-09-02 10:02:47 CEST FATAL:  the database system is in recovery mode
2025-09-02 10:02:47 CEST LOG:  database system was interrupted; last known up at 2025-09-02 10:02:19 CEST
2025-09-02 10:02:47 CEST FATAL:  the database system is in recovery mode
2025-09-02 10:02:47 CEST FATAL:  the database system is in recovery mode
2025-09-02 10:02:47 CEST FATAL:  the database system is in recovery mode
2025-09-02 10:02:48 CEST LOG:  database system was not properly shut down; automatic recovery in progress
2025-09-02 10:02:48 CEST LOG:  redo starts at 11/23BF6210
2025-09-02 10:02:48 CEST LOG:  invalid record length at 11/23C07C10: expected at least 24, got 0
2025-09-02 10:02:48 CEST LOG:  redo done at 11/23C07BD8 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
2025-09-02 10:02:48 CEST LOG:  checkpoint starting: end-of-recovery immediate wait
2025-09-02 10:02:48 CEST LOG:  checkpoint complete: wrote 18 buffers (0.1%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.009 s, sync=0.015 s, total=0.037 s; sync files=14, longest=0.002 s, average=0.002 s; distance=70 kB, estimate=70 kB; lsn=11/23C07C10, redo lsn=11/23C07C10

</code></pre>
<p><strong>After pg_ctl :</strong></p>
<pre><code>2025-09-02 11:25:29 CEST LOG:  starting PostgreSQL 17.5 on x86_64-windows, compiled by msvc-19.43.34808, 64-bit
2025-09-02 11:25:29 CEST LOG:  listening on IPv6 address &quot;::&quot;, port 15433
2025-09-02 11:25:29 CEST LOG:  listening on IPv4 address &quot;0.0.0.0&quot;, port 15433
2025-09-02 11:25:29 CEST LOG:  database system was interrupted; last known up at 2025-09-02 10:36:20 CEST
2025-09-02 11:25:35 CEST LOG:  could not open file &quot;./Log.log&quot;: sharing violation
2025-09-02 11:25:35 CEST DETAIL:  Continuing to retry for 30 seconds.
2025-09-02 11:25:35 CEST HINT:  You might have antivirus, backup, or similar software interfering with the database system.
2025-09-02 11:26:02 CEST LOG:  syncing data directory (fsync), elapsed time: 32.84 s, current path: ./pg_commit_ts
2025-09-02 11:26:02 CEST LOG:  database system was not properly shut down; automatic recovery in progress
2025-09-02 11:26:02 CEST LOG:  redo starts at 11/9B002220
2025-09-02 11:26:02 CEST LOG:  invalid record length at 11/9B002258: expected at least 24, got 0
2025-09-02 11:26:02 CEST LOG:  redo done at 11/9B002220 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
2025-09-02 11:26:02 CEST LOG:  checkpoint starting: end-of-recovery immediate wait
2025-09-02 11:26:02 CEST LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.004 s, sync=0.003 s, total=0.020 s; sync files=2, longest=0.002 s, average=0.002 s; distance=0 kB, estimate=0 kB; lsn=11/9B002258, redo lsn=11/9B002258
2025-09-02 11:26:02 CEST LOG:  database system is ready to accept connections
2025-09-02 11:26:16 CEST LOG:  pg_fasttransfer: Final command to be executed: D:\pacollet\FastTransfer\FastTransfer.exe
2025-09-02 11:26:16 CEST STATEMENT:  SELECT * from xp_RunFastTransfer_secure(
      sourceconnectiontype := 'mssql',
      sourceserver := 'localhost',
      sourceuser := 'FastLogin',
      method := 'Ntile',
      sourcedatabase := 'tpch10',
      sourceschema := 'dbo',
      sourcetable := 'orders',
      targetserver := 'localhost:15433',
      targetconnectiontype := 'pgcopy',
      sourcepassword := 'ww0EBwMC6ZfGQWy756Z10j0Bftt65IDBo78h6sJZpduXEI7ziV4VBXaTHbAAqusOCkzgr8WfALi1
    0dckj30U9zyFSYuEuJTOQPpvVhha',
      targetuser := 'postgres',
      targetdatabase := 'postgres',
      targetschema := 'public',
      targettable := 'orders',
      degree := 12,
      distributekeycolumn := 'o_orderkey',
      loadmode := 'Truncate',
      targetpassword := 'ww0EBwMCoGexPfZuuKt80jkBtCbQKgPgxHFko0bA+afdEvMgeCURBVfnSZi3sBVmn0GjmrpWdapl
    7gY5urqUYiAn1hODJEzxVdQ=',
      batchsize := 1048576,
      mapmethod := 'Position',
      fasttransfer_path := 'D:\pacollet\FastTransfer'
    )
    
2025-09-02 11:26:16 CEST LOG:  pg_fasttransfer: Final command to be executed: D:\pacollet\FastTransfer\FastTransfer.exe --sourceconnectiontype &quot;mssql&quot; --sourceserver &quot;localhost&quot; --sourceuser &quot;FastLogin&quot; --sourcepassword &quot;FastPassword&quot; --sourcedatabase &quot;tpch10&quot; --sourceschema &quot;dbo&quot; --sourcetable &quot;orders&quot; --targetconnectiontype &quot;pgcopy&quot; --targetserver &quot;localhost:15433&quot; --targetuser &quot;postgres&quot; --targetpassword &quot;postgres&quot; --targetdatabase &quot;postgres&quot; --targetschema &quot;public&quot; --targettable &quot;orders&quot; --degree &quot;12&quot; --method &quot;Ntile&quot; --distributekeycolumn &quot;o_orderkey&quot; --loadmode &quot;Truncate&quot; --batchsize &quot;1048576&quot; --mapmethod &quot;Position&quot;
2025-09-02 11:26:16 CEST STATEMENT:  SELECT * from xp_RunFastTransfer_secure(
      sourceconnectiontype := 'mssql',
      sourceserver := 'localhost',
      sourceuser := 'FastLogin',
      method := 'Ntile',
      sourcedatabase := 'tpch10',
      sourceschema := 'dbo',
      sourcetable := 'orders',
      targetserver := 'localhost:15433',
      targetconnectiontype := 'pgcopy',
      sourcepassword := 'ww0EBwMC6ZfGQWy756Z10j0Bftt65IDBo78h6sJZpduXEI7ziV4VBXaTHbAAqusOCkzgr8WfALi1
    0dckj30U9zyFSYuEuJTOQPpvVhha',
      targetuser := 'postgres',
      targetdatabase := 'postgres',
      targetschema := 'public',
      targettable := 'orders',
      degree := 12,
      distributekeycolumn := 'o_orderkey',
      loadmode := 'Truncate',
      targetpassword := 'ww0EBwMCoGexPfZuuKt80jkBtCbQKgPgxHFko0bA+afdEvMgeCURBVfnSZi3sBVmn0GjmrpWdapl
    7gY5urqUYiAn1hODJEzxVdQ=',
      batchsize := 1048576,
      mapmethod := 'Position',
      fasttransfer_path := 'D:\pacollet\FastTransfer'
    )
    
2025-09-02 11:26:24 CEST LOG:  server process (PID 53140) was terminated by exception 0xC0000409
2025-09-02 11:26:24 CEST DETAIL:  Failed process was running: SELECT * from xp_RunFastTransfer_secure(
      sourceconnectiontype := 'mssql',
      sourceserver := 'localhost',
      sourceuser := 'FastLogin',
      method := 'Ntile',
      sourcedatabase := 'tpch10',
      sourceschema := 'dbo',
      sourcetable := 'orders',
      targetserver := 'localhost:15433',
      targetconnectiontype := 'pgcopy',
      sourcepassword := 'ww0EBwMC6ZfGQWy756Z10j0Bftt65IDBo78h6sJZpduXEI7ziV4VBXaTHbAAqusOCkzgr8WfALi1
    0dckj30U9zyFSYuEuJTOQPpvVhha',
      targetuser := 'postgres',
      targetdatabase := 'postgres',
      targetschema := 'public',
      targettable := 'orders',
      degree := 12,
      distributekeycolumn := 'o_orderkey',
      loadmode := 'Truncate',
      targetpassword := 'ww0EBwMCoGexPfZuuKt80jkBtCbQKgPgxHFko0bA+afdEvMgeCURBVfnSZi3sBVmn0GjmrpWdapl
    7gY5urqUYiAn1hODJEzxVdQ=',
      batchsize := 1048576,
      mapmethod := 'Position',
      fasttransfer_path := 'D:\pacollet\FastTransfer'
    )
    
2025-09-02 11:26:24 CEST HINT:  See C include file &quot;ntstatus.h&quot; for a description of the hexadecimal value.
2025-09-02 11:26:24 CEST LOG:  terminating any other active server processes
2025-09-02 11:26:24 CEST LOG:  all server processes terminated; reinitializing
2025-09-02 11:26:34 CEST FATAL:  pre-existing shared memory block is still in use
2025-09-02 11:26:34 CEST HINT:  Check if there are any old server processes still running, and terminate them.
2025-09-02 11:26:34 CEST LOG:  database system is shut down
Licence file not found: FastTransfer.lic

</code></pre>
",1,1,0,2025-09-02T08:49:23+00:00,0,91,False
79753589,8655052,,postgresql,Make sure the consumer code uses @Transactional or any other way of declaring a (read-only) transaction] with root cause,"<p>I am trying to learn how to call stored procedure from springboot data jpa repository and returns a cursor from stored procedure. While I am implementing I am getting error like <code>org.springframework.dao.InvalidDataAccessApiUsageException: You're trying to execute a @Procedure method without a surrounding transaction that keeps the connection open so that the ResultSet can actually be consumed; Make sure the consumer code uses @Transactional or any other way of declaring a (read-only) transaction] with root cause</code></p>
<p>My Stored Procedure</p>
<pre><code>CREATE OR REPLACE PROCEDURE public.get_user_cursor(OUT user_cursor refcursor)
LANGUAGE 'plpgsql'
AS $BODY$
DECLARE
    user_cursor refcursor;
BEGIN
  OPEN user_cursor FOR SELECT nuser_id FROM users;
END;
 $BODY$;
</code></pre>
<p>Repository Code</p>
<pre><code>@Transactional(readOnly = true)
@Procedure(name = &quot;Users.get_user_cursor&quot;)
List&lt;Users&gt; get_user_cursor();
</code></pre>
<p>Users.class</p>
<pre><code>@NamedStoredProcedureQuery(
    name = &quot;Users.get_user_cursor&quot;,
    procedureName = &quot;get_user_cursor&quot;,
    resultClasses = Users.class,
    parameters = {
        @StoredProcedureParameter(mode = ParameterMode.REF_CURSOR, name = &quot;user_cursor&quot;, type = void.class)
    }
    
)
 @Table(name = &quot;users&quot;)
 public class Users implements Serializable {
     // Model Codes
  }
</code></pre>
<p>Even I tried both library <code>import org.springframework.transaction.annotation.Transactional;</code> and <code>import jakarta.transaction.Transactional;</code> and for just @transactional and with readOnly=true. While running application is getting starting without error. But when I calls the API end point, error propogating.</p>
<p>Can anyone suggest where I went wrong in implementation  and how can I resolve this error please?</p>
",2,2,0,2025-09-02T14:13:47+00:00,1,273,True
79753780,24888000,,postgresql,postgres jsonb array aggregate functions,"<p>I have a table with data on user applications. It contains several fields: <code>employee_id text</code>, <code>date date</code>, <code>app_info jsonb</code>.</p>
<pre><code>+-------------+------------+------------+
| employee_id |   date     |  app_info  |
+-------------+------------+------------+
| 2223eb0f0d0x| 01/07/2025 |   jsonb    |
| 2223eb0f0d0x| 01/08/2025 |   jsonb    |
| 2223eb0f0d0x| 31/07/2025 |   jsonb    |
| 2223eb0f0d0x| 31/08/2025 |   jsonb    |
+-------------+------------+------------+
</code></pre>
<p>The info <code>field app_info</code> contains the following code:</p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;apps&quot;: [
      {
        &quot;grade&quot;: &quot;UNPRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Google chrome&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;http://vk.com&quot;,
        &quot;count_seconds&quot;: 57.731
      }
    ],
    &quot;granula_end&quot;: &quot;17:55:00&quot;,
    &quot;granula_start&quot;: &quot;17:50:00&quot;
  },
  {
    &quot;apps&quot;: [
      {
        &quot;grade&quot;: &quot;UNPRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Google chrome&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;http://vk.com&quot;,
        &quot;count_seconds&quot;: 217.879
      },
      {
        &quot;grade&quot;: &quot;PRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Windows Explorer&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;&quot;,
        &quot;count_seconds&quot;: 3.174
      }
    ],
    &quot;granula_end&quot;: &quot;19:25:00&quot;,
    &quot;granula_start&quot;: &quot;19:20:00&quot;
  },
  {
    &quot;apps&quot;: [
      {
        &quot;grade&quot;: &quot;NEUTRAL&quot;,
        &quot;app_name&quot;: &quot;Time Doctor 2&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;&quot;,
        &quot;count_seconds&quot;: 118.299
      }
    ],
    &quot;granula_end&quot;: &quot;19:30:00&quot;,
    &quot;granula_start&quot;: &quot;19:25:00&quot;
  },
  {
    &quot;apps&quot;: [
      {
        &quot;grade&quot;: &quot;NEUTRAL&quot;,
        &quot;app_name&quot;: &quot;Time Doctor 2&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;&quot;,
        &quot;count_seconds&quot;: 29.992
      },
      {
        &quot;grade&quot;: &quot;PRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Windows Explorer&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;&quot;,
        &quot;count_seconds&quot;: 3.002
      }
    ],
    &quot;granula_end&quot;: &quot;19:55:00&quot;,
    &quot;granula_start&quot;: &quot;19:50:00&quot;
  }
]
</code></pre>
<p>I have the following task:</p>
<blockquote>
<p>For a given date range, calculate the number of minutes in each application with an interval of 15 minutes. That is, I need to combine the existing 5-minute granules into 15-minute granules and calculate the amount of time for all applications in this granule for each granule. Group the list of applications itself so that there are no duplicates.</p>
</blockquote>
<p>I tried to do this, but I can’t figure out how to write the query correctly.</p>
<p>And I need a second query that finds the largest granule by application runtime. That is, where the greatest amount of work is in different applications. I don’t understand how to write this query at all...</p>
<p>Here is a link to my work - <a href=""https://dbfiddle.uk/q88VfV3l"" rel=""nofollow noreferrer"">https://dbfiddle.uk/q88VfV3l</a></p>
<p>P.S. I looked through different questions on the forum, but none of them helped me solve the problem, so I posted my question.</p>
<p>P.S. I used your answers and am trying to create a new query based on the table provided in the question.</p>
<p>I need to calculate the total activity time for all applications in the granule for each 5-minute granule (these granules are already in the table).</p>
<blockquote>
<p>Find the application that has the largest number of seconds, and I need to display the application name and productivity rating. Sort all applications in the granule by time and productivity rating.</p>
<p>Merge identical granules. Granules in which the most used application is the same are considered identical. After merging, get an extended granule with a count of the number of seconds. For example, we have granules:</p>
</blockquote>
<pre><code>[
  {
    &quot;apps&quot;: [
      {
        &quot;grade&quot;: &quot;UNPRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Google chrome&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;http://vk.com&quot;,
        &quot;count_seconds&quot;: 57.731
      }
    ],
    &quot;granula_end&quot;: &quot;16:55:00&quot;,
    &quot;granula_start&quot;: &quot;16:50:00&quot;
  },
  {
    &quot;apps&quot;: [
      {
        &quot;grade&quot;: &quot;UNPRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Google chrome&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;http://vk.com&quot;,
        &quot;count_seconds&quot;: 217.879
      },
      {
        &quot;grade&quot;: &quot;PRODUCTIVE&quot;,
        &quot;app_name&quot;: &quot;Windows Explorer&quot;,
        &quot;type_name&quot;: &quot;Native&quot;,
        &quot;domain_site&quot;: &quot;&quot;,
        &quot;count_seconds&quot;: 3.174
      }
    ],
    &quot;granula_end&quot;: &quot;16:55:00&quot;,
    &quot;granula_start&quot;: &quot;17:00:00&quot;
  }]
</code></pre>
<p>You can see that these two granules have the same most used application. And the granules are adjacent to each other in time! These are the granules that need to be merged into one:</p>
<pre><code>{
        &quot;apps&quot;: [
          {
            &quot;grade&quot;: &quot;UNPRODUCTIVE&quot;,
            &quot;app_name&quot;: &quot;Google chrome&quot;,
            &quot;type_name&quot;: &quot;Native&quot;,
            &quot;domain_site&quot;: &quot;http://vk.com&quot;,
            &quot;count_seconds&quot;: 275.61
          },
          {
            &quot;grade&quot;: &quot;PRODUCTIVE&quot;,
            &quot;app_name&quot;: &quot;Windows Explorer&quot;,
            &quot;type_name&quot;: &quot;Native&quot;,
            &quot;domain_site&quot;: &quot;&quot;,
            &quot;count_seconds&quot;: 3.174
          }
        ],
        &quot;granula_end&quot;: &quot;16:50:00&quot;,
        &quot;granula_start&quot;: &quot;17:00:00&quot;
 }
</code></pre>
<p>Only adjacent granules need to be merged. And the granula_start and granula_end fields are changed into one granule per 10 minutes. This merging rule applies to all granules, both lower and higher in time. I understand that I will need to use recursion here, but I can't write such a query.</p>
<p>I also couldn't select not just the maximum number of seconds in the array of applications, but immediately an object that contains the maximum number of seconds with information about the application name and productivity rating.</p>
<p>There is also a rule for sorting by productivity ratings: unproductive, productive, no rating, neutral, inaction. And you need to sort all the applications in each granule according to this order. But first we sort by time, and then by rating. As a result, I should get a query like this:</p>
<pre><code>{
            &quot;userInfo&quot;: {
                &quot;employeeId&quot;: &quot;2223eb0f0d0c4941a16e83dc7274771b&quot;,
            },
            &quot;granules&quot;: [
                {
                    &quot;dateTimeStart&quot;: &quot;2025-07-08T12:30&quot;,
                    &quot;dateTimeEnd&quot;:   &quot;2025-07-08T12:35&quot;,
                    &quot;highestTypeActivity&quot;: &quot;PRODUCTIVE&quot;,
                    &quot;activeTime&quot;:    210,
                    &quot;apps:&quot; [
                        {
                            &quot;name&quot;:       &quot;telegram.exe&quot;,
                            &quot;activeTime&quot;: 140,
                            &quot;grade&quot;:      &quot;PRODUCTIVE&quot;
                        },
                        {
                            &quot;name&quot;:       &quot;notepad.exe&quot;,
                            &quot;activeTime&quot;: 70,
                            &quot;grade&quot;:      &quot;PRODUCTIVE&quot;
                        }
                    ]
                },
                {
                    &quot;dateTimeStart&quot;: &quot;2025-07-08T12:35&quot;,
                    &quot;dateTimeEnd&quot;:   &quot;2025-07-08T12:40&quot;,
                    &quot;activeTime&quot;:    210,
                    &quot;apps:&quot; [
                        {
                            &quot;name&quot;:       &quot;google chrome&quot;,
                            &quot;details&quot;:    &quot;http://vk.com&quot;,
                            &quot;activeTime&quot;: 140,
                            &quot;grade&quot;:      &quot;UNPRODUCTIVE&quot;
                        },
                        {
                            &quot;name&quot;:       &quot;google chrome&quot;,
                            &quot;details&quot;:    &quot;http://mail.ru&quot;,
                            &quot;activeTime&quot;: 70,
                            &quot;grade&quot;:      &quot;PRODUCTIVE&quot;
                        }
                    ]
                },
            ]
            
        }
</code></pre>
<p>I am update table in <a href=""https://dbfiddle.uk/K5MnXSjx"" rel=""nofollow noreferrer"">https://dbfiddle.uk/K5MnXSjx</a></p>
",2,3,1,2025-09-02T17:30:22+00:00,2,113,True
79754111,23177910,,postgresql,How to use liquibase-hibernate plugin in gradle,"<p>I’m trying to generate a Liquibase changelog from my Hibernate entities, but I’m running into issues. No changesets are being written to my desired changelog file, and I’m stuck here for like forever!!</p>
<p>Here’s my setup:</p>
<pre class=""lang-none prettyprint-override""><code>plugins {
    id 'java'
    id 'org.springframework.boot' version '3.5.5'
    id 'io.spring.dependency-management' version '1.1.7'
    id 'org.liquibase.gradle' version '2.2.0'
}

group = 'com.example'
version = '0.0.1-SNAPSHOT'
description = 'Demo project for Spring Boot'

java {
    toolchain {
        languageVersion = JavaLanguageVersion.of(24)
    }
}

configurations {
    compileOnly {
        extendsFrom annotationProcessor
    }
}

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
    implementation 'org.springframework.boot:spring-boot-starter-security'
    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation(&quot;org.springframework.boot:spring-boot-starter-validation&quot;)
    implementation 'org.springdoc:springdoc-openapi-starter-webmvc-ui:2.8.12'
    implementation 'com.auth0:java-jwt:4.4.0'
    compileOnly 'org.projectlombok:lombok'
    developmentOnly 'org.springframework.boot:spring-boot-devtools'
    implementation 'org.postgresql:postgresql'
    annotationProcessor 'org.springframework.boot:spring-boot-configuration-processor'
    annotationProcessor 'org.projectlombok:lombok'
    testImplementation 'org.springframework.boot:spring-boot-starter-test'
    testImplementation 'org.springframework.security:spring-security-test'
    testRuntimeOnly 'org.junit.platform:junit-platform-launcher'

    // Liquibase core
    liquibaseRuntime 'org.liquibase:liquibase-core:4.28.0'
    liquibaseRuntime 'org.liquibase.ext:liquibase-hibernate6:4.28.0'
    liquibaseRuntime 'info.picocli:picocli:4.7.5'
    // 👇 Add your app classes so Liquibase can see entities
    liquibaseRuntime sourceSets.main.output
    liquibaseRuntime sourceSets.main.runtimeClasspath
}

tasks.named(&quot;diffChangelog&quot;) {
    dependsOn(&quot;classes&quot;)
}
liquibase {
    activities {
        main {
            url &quot;jdbc:postgresql://localhost:5433/tech_tijarat_db&quot;
            username &quot;postgres&quot;
            password &quot;postgres&quot;
            driver &quot;org.postgresql.Driver&quot;

            // ✅ Hibernate entities as reference model
            referenceDriver &quot;liquibase.ext.hibernate.database.connection.HibernateDriver&quot;
            referenceUrl &quot;hibernate:spring:app?dialect=org.hibernate.dialect.PostgreSQLDialect&quot;
            changelogFile &quot;src/main/resources/db/changelog/generated-changelog.yaml&quot;
        }
    }
    runList = 'main'
}


tasks.named('test') {
    useJUnitPlatform()
}
</code></pre>
<p>My db is running on docker and its <code>conn url = postgresql://localhost:5433/tech_tijarat_db</code> is correct</p>
<p>here is <code>application.properties</code></p>
<pre class=""lang-none prettyprint-override""><code>spring.datasource.url=jdbc:postgresql://localhost:5433/tech_tijarat_db
spring.datasource.username=postgres
spring.datasource.password=postgres
spring.datasource.driver-class-name=org.postgresql.Driver
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.show-sql=true
</code></pre>
<p>when i run <code>./gradlew diffChangelog</code></p>
<p>there are the logs that printed out</p>
<pre class=""lang-none prettyprint-override""><code>####################################################
Starting Liquibase at 08:51:39 (version 4.28.0 #2272 built at 2024-05-16 19:00+0000)
Liquibase Version: 4.28.0
[2025-09-03 08:51:39] INFO [liquibase.ui] Liquibase Version: 4.28.0
WARNING: License service not loaded, cannot determine Liquibase Pro license status. Please consider re-installing Liquibase to include all dependencies. Continuing operation without Pro license.
[2025-09-03 08:51:39] INFO [liquibase.ui] WARNING: License service not loaded, cannot determine Liquibase Pro license status. Please consider re-installing Liquibase to include all dependencies. Continuing operation without Pro license.
[2025-09-03 08:51:39] INFO [liquibase.integration] Starting command execution.
[2025-09-03 08:51:39] INFO [liquibase.ext] Reading hibernate configuration hibernate:spring:app?dialect=org.hibernate.dialect.PostgreSQLDialect     
[2025-09-03 08:51:39] INFO [liquibase.ext] Found package app
08:51:39.663 [main] INFO org.hibernate.jpa.internal.util.LogHelper -- HHH000204: Processing PersistenceUnitInfo [name: default]
08:51:39.709 [main] INFO org.hibernate.Version -- HHH000412: Hibernate ORM core version 6.6.26.Final
08:51:39.739 [main] INFO org.hibernate.cache.internal.RegionFactoryInitiator -- HHH000026: Second-level cache disabled
08:51:39.820 [main] INFO org.hibernate.envers.boot.internal.EnversServiceImpl -- Envers integration enabled? : true
08:51:40.108 [main] INFO org.springframework.orm.jpa.persistenceunit.SpringPersistenceUnitInfo -- No LoadTimeWeaver setup: ignoring JPA class transformer
08:51:40.120 [main] WARN org.hibernate.orm.deprecation -- HHH90000021: Encountered deprecated setting [hibernate.temp.use_jdbc_metadata_defaults], use [hibernate.boot.allow_jdbc_metadata_access] instead
08:51:40.139 [main] WARN org.hibernate.orm.deprecation -- HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)


08:51:40.150 [main] INFO org.hibernate.orm.connections.pooling -- HHH10001005: Database info:
        Database JDBC URL [undefined/unknown]
        Database driver: undefined/unknown
        Database version: 12.0
        Autocommit mode: undefined/unknown
        Isolation level: &lt;unknown&gt;
        Minimum pool size: undefined/unknown
        Maximum pool size: undefined/unknown


08:51:40.616 [main] INFO org.hibernate.validator.internal.util.Version -- HV000001: Hibernate Validator 8.0.3.Final
08:51:40.966 [main] WARN org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator -- HHH000181: No appropriate connection provider encountered, assuming application will be supplying connections
08:51:41.528 [main] INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator -- HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)


[2025-09-03 08:51:41] INFO [liquibase.ext] Using dialect org.hibernate.dialect.PostgreSQLDialect
[2025-09-03 08:51:41] WARNING [liquibase.command] Ignoring the global liquibase.driver value in favor of the command value.
[2025-09-03 08:51:41] INFO [liquibase.snapshot] Creating snapshot
[2025-09-03 08:51:41] INFO [liquibase.ext] Found table order
...
[2025-09-03 08:51:41] INFO [liquibase.ext] Found column updatedAt timestamp(6)
[2025-09-03 08:51:41] INFO [liquibase.ext] Found column title varchar(255)
[2025-09-03 08:51:41] INFO [liquibase.database] Set default schema name to public


[2025-09-03 08:51:41] INFO [liquibase.snapshot] Creating snapshot
[2025-09-03 08:51:41] INFO [liquibase.command] Diff command completed
BEST PRACTICE: The changelog generated by diffChangeLog/generateChangeLog should be inspected for correctness and completeness before being deployed. Some database objects and their dependencies cannot be represented automatically, and they may need to be manually updated before being deployed.
[2025-09-03 08:51:41] INFO [liquibase.ui] BEST PRACTICE: The changelog generated by diffChangeLog/generateChangeLog should be inspected for correctn
ess and completeness before being deployed. Some database objects and their dependencies cannot be represented automatically, and they may need to be manually updated before being deployed.
[2025-09-03 08:51:42] INFO [liquibase.diff] file:///D:/code/Tech%20Tijarat/Tech-Tijarat-ecommerce/src/main/resources/db/changelog/generated-changelog.yaml exists, appending
[2025-09-03 08:51:42] INFO [liquibase.command] Diff changelog command succeeded
[2025-09-03 08:51:42] INFO [liquibase.command] Command execution complete
Liquibase command 'diff-changelog' was executed successfully.
[2025-09-03 08:51:42] INFO [liquibase.ui] Liquibase command 'diff-changelog' was executed successfully.

[Incubating] Problems report is available at: file:///D:/code/Tech%20Tijarat/Tech-Tijarat-ecommerce/build/reports/problems/problems-report.html

Deprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.

You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.

For more on this, please refer to https://docs.gradle.org/8.14.3/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.

BUILD SUCCESSFUL in 4s
3 actionable tasks: 2 executed, 1 up-to-date
</code></pre>
<p>but my file is not written
seeing these weed change logs</p>
<pre><code>   databaseChangeLog: []
   databaseChangeLog: []
</code></pre>
<p>Please if someone can help, Not my db is completely empty</p>
<p>I am excepting have a change logs generated in my file for db migrations</p>
",1,1,0,2025-09-03T04:02:19+00:00,0,63,False
79754299,19740963,,postgresql,how to pipe pasword string to another cmd get started with golang,"<p>i want exactly to run postgres backup using pgdump_all, but when starting it using cmd package in golang it shows in terminal it need postgres password to run the job , i searched previously and find out some way to use stdin pip of cmd get run , but even with a lot effort on it , still can not make any result and still show me in terminal it need password ... and more crazy to me ... even when i personally type the password in terminal still shows me wrong password ... i even added PGPASSWORD as env variable before run command but still not working</p>
<pre><code>os.Setenv(&quot;PGPASSWORD&quot;, &quot;secret&quot;)
cmd := exec.Command(&quot;C:\Program Files\PostgreSQL\17\bin\pg_dumpall.exe&quot;, --username=&quot;postgres&quot;, -p 5432, -w, &gt; &quot;.\backup\backup.dump&quot;)

stdin, err := cmd.StdinPipe()
if err != nil {
    fmt.Println(&quot;not get in ... \n &quot; + err.Error())
}
defer stdin.Close()

cmd.Stdout = os.Stdout
cmd.Stderr = os.Stderr

err = cmd.Start()
if err != nil {
    fmt.Println(&quot;not started ... \n &quot; + err.Error())
}

_, err = io.WriteString(stdin, &quot;secret&quot;+&quot;\r\n&quot;)
if err != nil {
    fmt.Println(&quot;not input... \n &quot; + err.Error())
}

err = cmd.Wait()
if err != nil {
    fmt.Println(&quot;not wait ... \n &quot; + err.Error())
}
</code></pre>
<p>i need a way to pass password string to pgdump_all get run</p>
",-1,1,2,2025-09-03T08:00:27+00:00,1,114,True
79755445,2663853,,postgresql,Update where current of cursor in a PL/pgSQL procedure. ERROR: cursor does not exist,"<p>I'm using 17.0.6 version (and 13 yet, in another test) of ODBC driver against a PostGreSQL 17 database, I got an unexplicable error when, from a PowerBuilder 12 application using the ODBC, I call a stored procedure; but the same procedure, when I execute it on PGAdmin works fine!</p>
<p>The error is:</p>
<blockquote>
<p>SQLSTATE = 34000<br />
ERROR: cursor &quot;doppie_ricette&quot; does not exists; Error while executing the query</p>
</blockquote>
<p>The stored is:</p>
<pre class=""lang-none prettyprint-override""><code>LANGUAGE 'plpgsql'
AS $BODY$
    declare ultima_riga     integer;
            ultima_nr         char(16);
            ultima_id         char(8);
            nR100             integer;
            ultima_prov     char(1);
            doppie_ricette    record;
begin
   
    ultima_nr     := '' ;
    ultima_id     := '';
    ultima_riga := 0;
    ultima_prov    := '';
    
    for doppie_ricette in
        select     numero_ricetta as nR,
                id_univoco as id,
                numero_riga_2 as _riga,
                anno as _anno,
                ospedale as osp,
                id_riga as idR,
                provenienza as _prov,
                  controllo as _controllo
        from AMB_LOMB
        where numero_ricetta is not null
                and Length(Trim(numero_ricetta))&gt;0
                and pronto_soccorso&lt;&gt;'P'
                and regime&lt;&gt;'7'
                and provenienza&lt;&gt;'S'
        order by 1 asc,7 asc,5 asc,4 asc,2 asc,6 asc
        FOR UPDATE    
    loop
        if     doppie_ricette.nr = ultima_nr
            and doppie_ricette._prov = ultima_prov
            and doppie_ricette.id &lt;&gt; ultima_id then
           
            -- Marcare la seconda (E TUTTE LE COLLEGATE) con codice di errore
            update amb_lomb
            set errore='10C',
                errore_grave=1,
                riferimento_errore=Trim(to_char(ultima_riga, '99999'))
            where current of doppie_ricette ;
        else
            -- TEST SU NOS_100 !
            select Count(1) into nR100
            from nos_100
            where     ris7 = doppie_ricette.nr
                    and ris8 = doppie_ricette._prov
                    and(ris15&lt;&gt;doppie_ricette.id or ris18&lt;&gt;doppie_ricette._anno or ris1&lt;&gt;doppie_ricette.osp);
           
            if nR100&gt;0 then
                if doppie_ricette._controllo in('A','C') then
                    update amb_lomb
                    set    errore='10E',
                        errore_grave=1,
                        riferimento_errore=''
                    where current of doppie_ricette ;
                else
                    update amb_lomb
                    set errore='10C',
                        errore_grave=1,
                        riferimento_errore='archivio'
                    where current of doppie_ricette ;
                end if ;
            end if ;

        end if;

        ultima_nr     := doppie_ricette.nr;
        ultima_id    := doppie_ricette.id;
        ultima_riga    := doppie_ricette._riga;
        ultima_prov    := doppie_ricette._PROV ;
       
    end loop ;
    
    call LOMB_AMB45_ERR_GEN('10C',3);
    call LOMB_AMB45_ERR_GEN('10E',3);

end;
$BODY$;
</code></pre>
<p>The mentioned table (amb_lomb) are without blobs or similar; there are many others stored like this, working fine.</p>
<p>Additional:I'm using only one db, only one session, only one connection, so by PgAdmin and ODBC.</p>
<p>The function has the variable &quot;doppie_ricette&quot; declared locally, and this name is never used elsewhere.</p>
<p>The very problems are:</p>
<pre><code>In the test cases, the query used in the cursor &quot;doppie_ricette&quot; never returns any row.

If I execute - like all the processing data before this procedure - in the PgAdmin environment, everything goes well, without errors; but if I do this in a PowerBuilder (12.6) program, doing exactly the same steps, when I call this procedure I obtain the error. The sequence of operations (INSERTs, CALLs) is obviously the same, and it's on a unique connection on the same PG 17 DB.
</code></pre>
<p>I have done only one direct setting of the ODBC connection: &quot;SET CLIENT_ENCODING TO 'UTF8'&quot;, for the rest the configuration of the ODBC is the original one.</p>
<p>After all:</p>
<p>In the process, after this procedure, I call another control procedure very similar to this: lomb_amb3_verifica_4(), in the exactly same way ... but this one doesn't generate any error in the ODBC env. (also in the PB one, of course); the procedure:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE PROCEDURE public.lomb_amb3_verifica_4()
LANGUAGE 'plpgsql'
AS $BODY$
    declare ultima_riga     integer;
            ultima_nr       char(16);
            ultima_id       char(8);
            ultima_osp      char(6);
            ultima_idr      char(2);
            doppi_nr        record;
begin
    /*
        creata  15.3.01
        
        5.3: Altra condizione di errore: come in DETTAGLIO ma sul file importato,
        non può mai essere che a parità di id univoco ci siano 2 id_riga eguali     
    */
    
    ultima_nr   := '' ;
    ultima_id   := '';
    ultima_riga := 0;
    ultima_idr  := '';
    ultima_osp  := '';
    
    for doppi_nr in 
        select  ospedale as osp,
                ANNO as _anno,
                id_univoco as id,
                id_riga as idR,
                numero_riga as _riga 
        from AMB_LOMB 
        order by 1 asc,2 asc,3 asc,4 asc 
        FOR UPDATE  
    loop 
        if  doppi_nr.osp = ultima_osp
            and doppi_nr.id = ultima_id
            and doppi_nr.idr = ultima_idr then
            
            -- Marcare la seconda con codice di errore
            update amb_lomb 
            set errore='22B',
                riferimento_errore='AMB1: '||TRIM(to_char(ultima_riga, '999999')),
                errore_grave=2 
            where current of doppi_nr ;

        end if;

        ultima_osp  := doppi_nr.osp;
        ultima_id   := doppi_nr.id;
        ultima_riga := doppi_nr._riga;
        ultima_idr  := doppi_nr.idr ;
        
    end loop ;
    
    call LOMB_AMB45_ERR_GEN('22B',3);

end;
$BODY$;
</code></pre>
<p>The cursor is almost the same, but the name 'doppi_nr' is different</p>
",3,3,0,2025-09-04T08:20:55+00:00,1,120,True
79755530,21518887,,postgresql,PgLoader Migrate data from SQL Server to PostgreSQL server got error with character &quot;&#241;/&#237;/...&quot;,"<p>I'm using pgloader to migrate data from SQL Server to PostgreSQL server, the database is huge and I can't export to CSVv due to special characters. So I migrate db to db directly, almost done but it gets stuck at columns that have <code>ñ/í/...</code> data, and everything breaks. If I remove all above characters, then everything is good again, below is my setup .load script:</p>
<pre><code>-- Single Table Migration Template: Data Only, No Drop Table
-- Replace: HOST_IP, username, password, database names, and TABLE_NAME as needed

LOAD DATABASE
     FROM mssql://sa:sa@172.22.0.1/ADB
     INTO postgresql://postgres:sa@172.22.0.1/ADB

-- Performance and migration settings
WITH
     data only,                -- Only copy data, don't drop or create tables
     disable triggers,
     no foreign keys,
     quote identifiers,
     batch rows = 50000,
     prefetch rows = 10000

CAST
    type varchar TO text ,
    type nvarchar TO text,
    type text TO text 

SET 
    client_encoding = 'WIN1252',
    work_mem to '256MB',
    maintenance_work_mem to '1GB'

ALTER SCHEMA 'dbo' RENAME TO 'public'

INCLUDING ONLY TABLE NAMES LIKE 'DRecords' IN SCHEMA 'dbo'

BEFORE LOAD DO
  $$ SET session_replication_role = replica; $$
AFTER LOAD DO
  $$ SET session_replication_role = DEFAULT; $$;
</code></pre>
<p>It used to work well when running inside docker desktop for windows, but windows server does not support docker desktop. The error message contains:</p>
<blockquote>
<p>7: ((FLET &quot;H1&quot; :IN PGLOADER.SOURCES:MAP-ROWS) #&lt;BABEL-ENCODINGS:INVALID-UTF8-STARTER-BYTE {10082879B3}&gt;)
8: (SB-KERNEL::%SIGNAL #&lt;BABEL-ENCODINGS:INVALID-UTF8-STARTER-BYTE {10082879B3}&gt;)
9: (ERROR BABEL-ENCODINGS:INVALID-UTF8-STARTER-BYTE :OCTETS #(176) :ENCODING :UTF-8 :BUFFER #.(SB-SYS:INT-SAP #X72DEDC01FB60) :POSITION 252)
10: ((LABELS BABEL-ENCODINGS::UTF-8-DECODER :IN</p>
</blockquote>
<p>Can someone please advise.</p>
",0,0,0,2025-09-04T09:37:13+00:00,1,280,False
79755868,31420696,,postgresql,How to dynamically update only provided fields in PostgreSQL using Neon + TypeScript + Zod schema?,"<p>I’m working on a Node.js/Express backend using PostgreSQL (Neon serverless), TypeScript, and Zod for validation.
I have a Students table with columns:</p>
<pre><code>id UUID PRIMARY KEY,
firstName TEXT NOT NULL,
lastName TEXT NOT NULL,
email TEXT UNIQUE NOT NULL,
courseId UUID NOT NULL
</code></pre>
<p>I defined the following Zod schema for partial updates:</p>
<pre><code>const updateSchema = z.object({
  firstName: z.string(),
  lastName: z.string(),
  email: z.string(),
  courseId: z.uuid(),
}).partial();
</code></pre>
<p>In my controller, I want to update only the fields provided in the request body.
For example:</p>
<ul>
<li><code>PATCH /students/10</code> with body <code>{ firstName: &quot;John&quot; }</code> → only updates <code>firstName</code>.</li>
<li><code>PATCH /students/10</code> with body <code>{ email: &quot;john@example.com&quot;, courseId: &quot;uuid-123&quot; }</code> → updates <code>email</code> and <code>courseId</code>.</li>
</ul>
<p>Currently, my update query looks like this:</p>
<pre><code>await sql`
  UPDATE Students 
  SET firstName = ${fields.firstName},
      lastName = ${fields.lastName},
      email = ${fields.email},
      courseId = ${fields.courseId}
  WHERE id = ${id}
  RETURNING *;
`;
</code></pre>
<p>But this always tries to update all fields, even if they’re null or not provided in the request body.</p>
<p>the sql is coming from</p>
<pre><code>import { neon } from &quot;@neondatabase/serverless&quot;;

const sql=neon(
    `postgresql://${PGUSER}:${PGPASSWORD}@${PGHOST}/${PGDATABASE}?sslmode=require&amp;channel_binding=require`
);
</code></pre>
<p>How can I use Neon to dynamically construct the SET clause in PostgreSQL so that just the supplied (non-null/non-undefined) fields are updated?</p>
",2,2,0,2025-09-04T14:46:19+00:00,1,149,True
79755894,1742777,"Gaithersburg, MD. USA",postgresql,Can I find out if an instance of a SQL Alchemy Model meets the filter requirements without scanning the entire table?,"<p>Suppose I have the SQLAlchemy code shown below. It retrieves all the users who are older than 28 years old.</p>
<p>Now suppose I create a new User <code>u2 = User(name='Bart', age=66)</code>.</p>
<p>I want to find out if <code>u2</code> matches the filter of <code>my_query</code> without going back to the DB and scanning the entire table. Can I do it?</p>
<pre><code>from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, declarative_base

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    age = Column(Integer)

engine = create_engine('sqlite:///:memory:')
Base.metadata.create_all(engine)

Session = sessionmaker(bind=engine)
session = Session()

# Add some data
session.add_all([
    User(name='Alice', age=30),
    User(name='Bob', age=25),
    User(name='Charlie', age=35)
])
session.commit()

# Filter users older than 28
my_query = session.query(User).filter(User.age &gt; 28)
users = my_query.all()
for user in users:
    print(f&quot;Name: {user.name}, Age: {user.age}&quot;)
</code></pre>
",1,1,0,2025-09-04T15:11:27+00:00,1,69,False
79755983,7636640,,postgresql,Connection Error To postGres from Tcl/Tk UI script,"<p>Kinda of new to Tcl/Tk, more familiar with CLI scripting. I've got a script (below) to establish a connection with a postGres DB. Once this works, I will continue efforts to create a table editor, as the one to which we have access, doesn't work as desired.</p>
<p><strong>DISCLAIMERS:</strong></p>
<ul>
<li><strong>NOT</strong> a DBA</li>
<li><strong>NOT</strong> a system admin</li>
<li><strong>NOT</strong> a privileged user</li>
<li><strong>NOT</strong> an expert in psql/postgres</li>
<li><strong>NO PERMISSION</strong> to install any software/patches</li>
<li><strong>NO PERMISSION</strong> to pgadmin</li>
<li><strong>NO PERMISSION</strong> to ping</li>
<li><strong>NO PERMISSION</strong> to edit system files like pg_hba.conf, /etc/nsswitch.conf, /etc/hosts, etc</li>
</ul>
<p>Sorry for all the disclaimers, trying to minimize some questions.</p>
<ul>
<li>Working on a remote system via RDC (Windows 10 RDC to RHEL9)</li>
<li>CLI access only on the remote system</li>
<li>Yes I know putting a password in a script is not secure, this is for testing purposes.</li>
</ul>
<p>Using:</p>
<p>PostgreSQL 16.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17),64-bit</p>
<p>Here's my script:</p>
<pre><code>package require Tcl
package require Tk

# Create Main Window
set mainWindow [tk::frame .main]
pack $mainWindow

# Variables
set dbHost &quot;&quot;
set dbName &quot;&quot;
set dbUser &quot;&quot;
set dbPass &quot;&quot;
set tableName &quot;&quot;

# Create UI Elements
label $mainWindow.hostLabel -text &quot;Host:&quot;
entry $mainWindow.hostEntry -textvariable dbHost

label $mainWindow.dbNameLabel -text &quot;DataBase Name:&quot;
entry $mainWindow.dbNameEntry -textvariable dbName

label $mainWindow.userLabel -text &quot;User:&quot;
entry $mainWindow.userEntry -textvariable dbUser

label $mainWindow.passLabel -text &quot;Password:&quot;
entry $mainWindow.passEntry -textvariable dbPass -show &quot;*&quot;

label $mainWindow.tableLabel -text &quot;Table Name:&quot;
entry $mainWindow.tableEntry -textvariable tableName

button $mainWindow.loginButton -text &quot;Login&quot; -command {loginToDatabase} -bg mediumseagreen

# Pack Layout
pack $mainWindow.hostLabel
pack $mainWindow.hostEntry
pack $mainWindow.dbName
pack $mainWindow.dbName
pack $mainWindow.userLabel
pack $mainWindow.userEntry
pack $mainWindow.passLabel
pack $mainWindow.passEntry
pack $mainWindow.tableLabel
pack $mainWindow.tableEntry
pack $mainWindow.loginButton

# Login Function
proc databaseLogin {} {
    global dbHost dbName dbUser dbPass tableName

# Command to execute psql for given table
    set command &quot;psql -h $dbHost -d $dbName -U $dbUser -c 'SELECT * FROM $tableName LIMIT 1;'&quot;

# Execute / Capature Output
    set env(PGPASSWORD) $dbPass ;
    set result [catch {exec $command} output]

# Check Result
    if {$result == 0} {
    tk_messageBox -message &quot;Connection Success&quot; -icon info
    } else {
    tk_messageBox -message &quot;Connection Failed: $output&quot; icon error
    }

    unset env(PGPASSWORD)
}
vwait forever
</code></pre>
<p>The error message is:</p>
<p><em>Connection Failed! Reason: couldn't execute &quot;psql -h  -d postgres -U  -c 'SELECT * FROM &lt;table_name&gt; LIMIT 1' &quot;: no such file or directory</em></p>
<p>For the host, I've used both the IP and the fully qualified domain name, both getting the same error. From CLI, no problems getting what I want. I do have a .pgpass in $HOME. Additionally, running the <em>psql</em> statement from CLI works.</p>
<p>My feeling is I'm missing something very basic, and haven't been able to find the cause.</p>
",1,1,0,2025-09-04T16:31:24+00:00,1,77,True
79756342,11658801,,postgresql,What to do if PostgreSQL and GStreamer dlls depend on different versions of libiconv-2.dll and libwinpthread-1.dll?,"<p>I'm updating the project, in which Qt, PostgreSQL and GStreamer are used. I need to update PostgreSQL to the latest version. PostgreSQL <code>libpq.dll</code> depends on several dlls and one of them is <code>libintl-9.dll</code>. <code>libintl-9.dll</code> depends on <code>libiconv-2.dll</code> and <code>libwinpthread-1.dll</code>.</p>
<p>The problem is that GStreamer also provides <code>libiconv-2.dll</code> and <code>libwinpthread-1.dll</code>. I compared <code>libiconv-2.dll</code> and <code>libwinpthread-1.dll</code> from GStreamer and PostgreSQL  and they are different. I'm not going to update GStreamer.</p>
<p>The dlls aren't loaded in my program in the code by <code>LoadLibrary</code>. <code>libpq.dll</code> is required for Qt to work with DB, so <code>libpq.dll</code> is loaded by Qt. Import libraries of GStreamer are linked to my project, so GStreamer is loaded via import libs.</p>
<p>I have the following ideas to solve the problem:</p>
<ol>
<li>Edit import table of <code>libintl-9.dll</code> to rename <code>libiconv-2.dll</code> and <code>libwinpthread-1.dll</code>, for example, to <code>libiconv2I.dll</code> and <code>libwinpthread1I.dll</code> (the same number of characters).</li>
<li>Pick only one version of <code>libiconv-2.dll</code> and <code>libwinpthread-1.dll</code> and use it for both. What version should be picked?</li>
</ol>
<p><strong>What would you recommend in such situation?</strong></p>
<p>Thank you in advance for your help!</p>
",0,0,0,2025-09-05T01:06:10+00:00,1,65,False
79756514,27550553,,postgresql,Hypercore Timescaledb ORDER BY not working as expected,"<p>We have a hypertable :
transact_time (BIGINT) : time column for partition,
id (BIGINT) : unique key</p>
<p>All the queries that hit this table are of the pattern :</p>
<pre><code>SELECT [columns]
FROM hypertable
WHERE transact_time &gt;= (start_range_int) AND transact_time &lt;= (end_range_int)
AND account_id IN (a1, a2, ..)
AND column1 IN (..)
AND column2 IN (..)
ORDER BY transact_time DESC, id DESC
LIMIT 1000;
</code></pre>
<p>We convert the hypertable into hypercore using :</p>
<pre><code>ALTER TABLE hypertable
SET(
    timescaledb.enable_columnstore = true,
    timescaledb.orderby = 'transact_time DESC, id DESC',
    timescaledb.segmentby = 'account_id'
);

CALL add_columnstore_policy('hypertable', BIGINT '86400000000');
</code></pre>
<p>From my understanding, this will convert the hypertable into columnstore, batching the data together by account_id and maintaining the data in the order transact_time DESC, id DESC.</p>
<p>That means, all account_id data in a chunk should be compressed together in batches ordered by transact_time DESC, id DESC.</p>
<p>But when I run the query :</p>
<pre><code>EXPLAIN
SELECT account_account_id, transact_time, id
FROM hypertable
WHERE account_id = 2507
AND transact_time &gt; 1738195200000000 AND transact_time &lt; 1738281600000000
ORDER BY transact_time DESC, id DESC
LIMIT 1000;
</code></pre>
<p>I get the result :</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>QUERY PLAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>Limit  (cost=2.51..12.90 rows=1000 width=24)</td>
</tr>
<tr>
<td>-&gt;  Custom Scan (ColumnarScan) on _hyper_9_1396_chunk e  (cost=2.51..12.90 rows=1000 width=24)</td>
</tr>
<tr>
<td>Filter: ((transact_time &gt; '1738195200000000'::bigint) AND (transact_time &lt; '1738281600000000'::bigint))</td>
</tr>
<tr>
<td>-&gt;  Sort  (cost=2.50..2.51 rows=1 width=108)</td>
</tr>
<tr>
<td>Sort Key: compress_hyper_25_1901_chunk._ts_meta_max_1 DESC, compress_hyper_25_1901_chunk._ts_meta_max_2 DESC</td>
</tr>
<tr>
<td>-&gt;  Index Scan using compress_hyper_25_1901_chunk_account_account_id__ts_meta_mi_idx on compress_hyper_25_1901_chunk  (cost=0.27..2.49 rows=1 width=108)</td>
</tr>
<tr>
<td>Index Cond: ((account_id = 2507) AND (_ts_meta_min_1 &lt; '1738281600000000'::bigint) AND (_ts_meta_max_1 &gt; '1738195200000000'::bigint))</td>
</tr>
</tbody>
</table></div>
<p>You can see the Sort operation is happening even though the data is already sorted by the order provided. I have tried to update the table statistics, but the issue still persists.</p>
<p>I want to understand why the postgres engine is not able to identify that the data is already sorted.</p>
<p>This is only happening with hypercore. For normal hypertables with B-tree indexes, I am getting the correct plan.</p>
",0,0,0,2025-09-05T07:38:26+00:00,0,61,False
79756732,4543065,,postgresql,Query to information_schema.columns runs for hours when column_default is added,"<p>Our application needs to check the existing Postgres tables and calculate necessary modifications for adding new columns / making changes.</p>
<p>It executes this query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT column_name, data_type, column_default, is_nullable, character_maximum_length, numeric_precision, datetime_precision, numeric_scale, ordinal_position
FROM information_schema.columns
WHERE table_schema = 'our_schema' AND table_name = 'table1' ORDER BY ordinal_position;
</code></pre>
<p>This query runs in less than 1 second for most tables, but for one of the tables, it runs over 1 hour.
In addition, removing the <code>column_default</code> field also reduces the query time to &lt; 1 second.</p>
<p>How can this be explained? How can we ensure this does not happen?</p>
",0,0,0,2025-09-05T11:33:27+00:00,0,47,False
79756740,2418347,,postgresql,postgres: parameterized query with pattern matching,"<p>in node, i'm creating the following query:</p>
<pre><code>let sql_query = &quot;\
   SELECT \
      ... \
   WHERE \
      FACILITY_ID = $1 \
      AND (EVENT_START_TIME_UNIX_MS &lt; $2) \
      AND (EVENT_END_TIME_UNIX_MS &gt; $3) \
      AND (LOWER(DESCRIPTION_TEXT) ~* '\yden\y')&quot;
</code></pre>
<p>that fourth line is doing a word boundary regex for the term &quot;den&quot; but i need it to be parameterized like the preceding lines. i tried the below but it doesn't work.</p>
<pre><code>      AND (LOWER(DESCRIPTION_TEXT) ~* '\y$4\y')
</code></pre>
<p>i also tried this:</p>
<pre><code>      AND (LOWER(DESCRIPTION_TEXT) ~* $4)
</code></pre>
<p>where <code>$4</code> is set to <code>'\yden\y'</code>. but that didn't work either.</p>
<p>thanks for any tips.</p>
",2,3,1,2025-09-05T11:45:46+00:00,1,153,True
79756911,14365967,,postgresql,Docker-in-Docker - Unable to connect payara server to postgres docker instance on GitLab CI/CD Pipeline,"<p>In order to improve the testing process for my Java application I want to introduce postgres as the database for integration tests.</p>
<p>In my current approach I am utilizing the <code>maven-docker-plugin</code> to startup and initialize a <code>postgres:16</code>  container. The integration tests run on a arquillian payara embedded server and connect via JDBC to the postgres instance.</p>
<p>This setup works effortlessly with local executions on Linux and WSL environments with docker, but executions on the GitLab pipelines fail due to connection issues, while I still see the logs of postgres, waiting for connections.</p>
<p>My build runs on an alpine based maven 3.8.8-jdk21 image with docker accessibility.</p>
<p>Actual error:
<code>org.postgresql.util.PSQLException: Connection to localhost:5455 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.</code></p>
<p>Configuration:</p>
<p>pom.xml (docker-maven-plugin config)</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;plugin&gt;
        &lt;groupId&gt;io.fabric8&lt;/groupId&gt;
        &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;
        &lt;version&gt;0.38.1&lt;/version&gt;
        &lt;configuration&gt;
          &lt;images&gt;
            &lt;image&gt;
              &lt;name&gt;postgres:16&lt;/name&gt;
              &lt;alias&gt;postgres&lt;/alias&gt;
              &lt;run&gt;
                &lt;env&gt;
                  &lt;POSTGRES_DB&gt;postgres&lt;/POSTGRES_DB&gt;
                  &lt;POSTGRES_USER&gt;postgres&lt;/POSTGRES_USER&gt;
                  &lt;POSTGRES_PASSWORD&gt;integPass&lt;/POSTGRES_PASSWORD&gt;
                &lt;/env&gt;
                &lt;ports&gt;
                  &lt;port&gt;0.0.0.0:5455:5432&lt;/port&gt;
                &lt;/ports&gt;
                &lt;volumes&gt;
                  &lt;bind&gt;
                    &lt;volume&gt;${project.basedir}/src/test/resources/sqlImports/default/:/docker-entrypoint-initdb.d/&lt;/volume&gt;
                  &lt;/bind&gt;
                &lt;/volumes&gt;
                &lt;wait&gt;
                  &lt;log&gt;ready to accept connections&lt;/log&gt;
                  &lt;time&gt;5000&lt;/time&gt;
                &lt;/wait&gt;
              &lt;/run&gt;
            &lt;/image&gt;
          &lt;/images&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
</code></pre>
<p>payara-resource.xml</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;resources&gt;

  &lt;!-- addition resources --&gt;
  &lt;jdbc-resource pool-name=&quot;DefaultPool&quot; jndi-name=&quot;DefaultDS&quot; /&gt;
  &lt;jdbc-connection-pool name=&quot;DefaultPool&quot; res-type=&quot;javax.sql.XADataSource&quot;
    datasource-classname=&quot;org.postgresql.xa.PGXADataSource&quot;&gt;
    &lt;property name=&quot;URL&quot; value=&quot;jdbc:postgresql://127.0.0.1:5455/postgres&quot; /&gt;
    &lt;property name=&quot;User&quot; value=&quot;postgres&quot; /&gt;
    &lt;property name=&quot;Password&quot; value=&quot;integPass&quot; /&gt;
  &lt;/jdbc-connection-pool&gt;

&lt;/resources&gt;
</code></pre>
<p>Note: Based on further investiations I also tried to utilize the docker alias instead of 127.0.0.1 in the JDBC url (jdbc:postgresql://postgres:5455/postgres), but ended up with <code>java.net.UnknownHostException: postgres</code></p>
<p>I expected docker to either create a subcontainer in my build container, making it available via localhost:5455 or to utilize the docker deamon of the host (GitLab runner) and create the container &quot;next to&quot; the build container with a network bridge, making it available via it's alias.</p>
",0,0,0,2025-09-05T14:42:02+00:00,0,44,False
79756998,23332429,,postgresql,Filling Winforms datagrid using Postgres stored procedure,"<p>I am trying to get data from Postgres table with a help of stored procedure and fill a Winforms DataGrid with it.</p>
<p>This is the stored procedure:</p>
<pre><code>CREATE OR REPLACE PROCEDURE public.get_list()
 LANGUAGE sql
BEGIN ATOMIC
 SELECT field1,
        field2,
        field3
    FROM some_table;
END;
</code></pre>
<p>Here's my C# code:</p>
<pre><code>DataTable dtt = new DataTable();
string ConString = &quot;Server=x.xxx.xx.xxx;Port=5432;User Id=xxx;Password=xxx;Database=xxx;&quot;;

NpgsqlConnection connection = new NpgsqlConnection(ConString);
connection.Open();
NpgsqlCommand cmd = new NpgsqlCommand(&quot;CALL public.get_list()&quot;);            

try
{
    cmd.Connection = connection;
    cmd.CommandType = System.Data.CommandType.StoredProcedure;

    using (var dataReader = cmd.ExecuteReader())
    {
        if (dataReader.HasRows)
        {
            GridView.Visible = true;                        
            dtt.Load(dataReader);                        

            GridView.DataSource = dtt;
            GridView.Update();    
        }
    }
}
catch (Exception ex)
{
    MessageBox.Show(ex.ToString());
}
</code></pre>
<p>After executing this code, the <code>DataReader</code> returns 0 rows.
The stored procedure returns nothing: <a href=""https://dbfiddle.uk/dT1i5M_N"" rel=""nofollow noreferrer"">fiddle</a></p>
<pre><code>CALL public.get_list();
</code></pre>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>CALL
</code></pre>
</blockquote>
<p>What am I doing wrong?</p>
<p>Maybe the analog of a stored procedure in other RDBMSs (for example in SQL Server) in Postgres is a function?</p>
",0,2,2,2025-09-05T16:21:02+00:00,1,136,True
79757699,24888000,,postgresql,Exclude from the selection the values ​that are in the array,"<p>I have two tables:</p>
<p>my_temp:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employee_id</th>
<th>date</th>
<th>start_granula</th>
<th>end_granula</th>
<th>count_seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-22</td>
<td>07:40:00</td>
<td>07:50:00</td>
<td>625</td>
</tr>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-22</td>
<td>08:10:00</td>
<td>08:20:00</td>
<td>513</td>
</tr>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-22</td>
<td>12:35:00</td>
<td>12:41:00</td>
<td>128</td>
</tr>
<tr>
<td>2223eb0f0d0x</td>
<td>2025-08-24</td>
<td>15:10:00</td>
<td>15:25:00</td>
<td>3206</td>
</tr>
</tbody>
</table></div>
<p>schedules:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employee_id</th>
<th>start_time</th>
<th>end_time</th>
<th>breaks</th>
<th>workdays</th>
</tr>
</thead>
<tbody>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>08:00:00</td>
<td>17:00:00</td>
<td><code>[{&quot;to&quot;: &quot;13:00&quot;, &quot;from&quot;: &quot;12:30&quot;, &quot;nextDay&quot;: false}, {&quot;to&quot;: &quot;15:00&quot;, &quot;from&quot;: &quot;15:30&quot;, &quot;nextDay&quot;: false}]</code></td>
<td><code>[1, 2, 3, 4, 5]</code></td>
</tr>
</tbody>
</table></div>
<p>I need to select only those time ranges from <code>my_temp</code> that correspond to the employee's schedule. Breaks must be excluded.</p>
<p>It is also necessary to check whether this day was a working day for the employee according to the schedule. If it is not, then leave only one entry in the table for this date and in column <code>day_type</code> write one of the following values: <code>['working_day', 'not_working_day']</code>. In the <code>start_time</code> and <code>end_time</code> columns, write the time according to the work schedule.
I'm having trouble excluding all entries that fall during the employee's break, entries that go beyond working hours are not excluded.</p>
<p>How to leave only one entry in the query for a date that is not a working day for the employee?</p>
<p>My query:</p>
<pre><code>    select 
                e.employee_id, 
                e.date,
                case when 
                    EXTRACT(DOW FROM e.date) = ANY(SELECT jsonb_array_elements(s.workDays::jsonb)) 
                then granula_start else s.start_time end as granula_start,
                case when 
                    EXTRACT(DOW FROM e.date) = ANY(SELECT jsonb_array_elements(s.workDays::jsonb)) 
                then granula_end else s.end_time end as granula_end,
                count_seconds,
                case when 
                    EXTRACT(DOW FROM e.date) = ANY(SELECT jsonb_array_elements(s.workDays::jsonb)) 
                then 'working_day' else 'not_working_day' end as day_type
            FROM my_temp e
            inner join schedules s on s.employee_id = e.employee_id
      where
     
            granula_start
            &gt;= 
            s.start_time
            and not exists (
                            select 1 from jsonb_array_elements(s.breaks::jsonb) br
                            where granula_start &gt; (br-&gt;&gt;'from')::time
                            and   granula_end &lt; (br-&gt;&gt;'to')::time
                          )
</code></pre>
<p>which returns:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>employee_id</th>
<th>date</th>
<th>granula_start</th>
<th>granula_end</th>
<th>count_seconds</th>
<th>day_type</th>
</tr>
</thead>
<tbody>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>08:10:00</td>
<td>08:20:00</td>
<td>513</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>13:10:00</td>
<td>13:25:00</td>
<td>1205</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>15:10:00</td>
<td>15:25:00</td>
<td>3206</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-22</td>
<td>17:10:00</td>
<td>17:25:00</td>
<td>1205</td>
<td>working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-24</td>
<td>08:00:00</td>
<td>17:00:00</td>
<td>6504</td>
<td>not_working_day</td>
</tr>
<tr>
<td>3dd280f2-e4d3-4568-9d97-1cc3a9dff1e9</td>
<td>2025-08-24</td>
<td>08:00:00</td>
<td>17:00:00</td>
<td>7203</td>
<td>not_working_day</td>
</tr>
</tbody>
</table></div>
<p>Please help me. Here is my example: <a href=""https://dbfiddle.uk/G542L9Wh"" rel=""nofollow noreferrer"">https://dbfiddle.uk/G542L9Wh</a></p>
",0,0,0,2025-09-06T17:35:07+00:00,1,86,True
79758979,6212999,,postgresql,When does restart_lsn move forward in logical replication?,"<p>My understanding is that PostgreSQL retains WAL from <code>restart_lsn</code> because the logical decoder might need data before <code>confirmed_flush_lsn</code> to produce output for transactions that have not yet been committed.</p>
<p>But how can I identify those transactions?</p>
<p>In the following example, <code>restart_lsn</code> got stuck on <code>3931E/5E81A318</code> (with <code>catalog_xmin</code> at <code>637374494</code>):</p>
<pre><code>db=# SELECT * from pg_replication_slots;
-[ RECORD 1 ]-------+-------------------------
slot_name           | peerflow_slot_cdc_mirror
plugin              | pgoutput
slot_type           | logical
datoid              | 16385
database            | db
temporary           | f
active              | t
active_pid          | 3391213
xmin                |
catalog_xmin        | 637374494
restart_lsn         | 3931E/5E81A318
confirmed_flush_lsn | 3931F/515955A8
wal_status          | extended
safe_wal_size       |
two_phase           | f
inactive_since      |
conflicting         | f
invalidation_reason |
failover            | f
synced              | f
</code></pre>
<p>However, the transaction <code>637374494</code> is no longer active:</p>
<pre><code>db=# SELECT min((backend_xmin::text)::bigint) AS value
          FROM pg_stat_activity
         WHERE state &lt;&gt; 'idle';
-[ RECORD 1 ]----
value | 637390955
</code></pre>
<p><code>restart_lsn</code> points to CHECKPOINT entry and 637390955 is greater than any transactions running during that checkpoint.</p>
<pre><code>db=# SELECT * FROM pg_get_wal_record_info('3931E/5E81A318');
-[ RECORD 1 ]----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
start_lsn        | 3931E/5E81A318
end_lsn          | 3931E/5E81A3B0
prev_lsn         | 3931E/5E81A2E0
xid              | 0
resource_manager | Standby
record_type      | RUNNING_XACTS
record_length    | 150
main_data_length | 124
fpi_length       | 0
description      | nextXid 637374800 latestCompletedXid 637374799 oldestRunningXid 637374494; 25 xacts: 637374773 637374771 637374683 637374630 637374625 637374494 637374722 637374774 637374677 637374775 637374768 637374769 637374739 637374679 637374770 637374680 637374684 637374628 637374676 637374738 637374626 637374682 637374627 637374794 637374678
block_ref        |
</code></pre>
<p>When will <code>restart_lsn</code> be updated?</p>
",0,0,0,2025-09-08T13:51:31+00:00,2,89,False
79759088,21371705,,postgresql,BetterAuth api endpoints not working in Nuxt 3 app when deployed to cloudflare pages,"<p>So I am making Nuxt 3 application which I am deploying to Cloudflare pages.</p>
<p>My techstack is:</p>
<ul>
<li>Nuxt 3</li>
<li>Postgres.js</li>
<li>Zod</li>
<li>DrizzleORM</li>
<li>BetterAuth</li>
</ul>
<p>First I had problems with deploy and <code>pg</code> package so I followed the steps <a href=""https://stackoverflow.com/questions/78498288/deploying-nuxt-app-with-postgres-to-cloudflare-fails"">here</a> and added the following to my <code>nuxt.config.ts</code>:</p>
<pre class=""lang-ts prettyprint-override""><code> nitro: {
    externals: {
      external: [&quot;pg-native&quot;],
    },
  }
</code></pre>
<p>This didn't help so I switched from <code>pg</code> to <code>postgres.js</code>. So now when I deploy I don't get any errors but when I call any of the BetterAuth api endpoints I get <code>500</code>, but no good error message to share here.</p>
<p>So I created a separate api ednpoint which queries the DB and returns the data and it worked.</p>
<p>So this seems to be something specific to BetterAuth. Locally everything is working fine.</p>
<p>Specific endpoints which are failing:</p>
<p>`/api/auth/sign-in/email`, `/api/auth/sign-in/get-session`</p>
<p>In cloudflare pages logs for both of these endpoints I get the following:</p>
<pre><code> {
      &quot;message&quot;: [
        &quot;[request error] [unhandled] [POST] https://page.com/api/auth/sign-in/email\n&quot;,
        {
          &quot;message&quot;: &quot;Class extends value [object Module] is not a constructor or null&quot;,
          &quot;statusCode&quot;: 500
        }
      ],
      &quot;level&quot;: &quot;error&quot;,
      &quot;timestamp&quot;: 1757344987396
    }
</code></pre>
<p>My db client <code>server/db/index.ts</code> :</p>
<pre class=""lang-ts prettyprint-override""><code>import { drizzle } from &quot;drizzle-orm/postgres-js&quot;;

export const useDb = () =&gt;
  drizzle({
    connection: {
      url: process.env.DATABASE_URL,
    },
  });
</code></pre>
<p><code>/lib/auth-client.ts</code>:</p>
<pre class=""lang-ts prettyprint-override""><code>import { createAuthClient } from &quot;better-auth/vue&quot;;

export const authClient = createAuthClient({});
</code></pre>
<p><code>/utils/auth.ts</code>:</p>
<pre class=""lang-ts prettyprint-override""><code>import { betterAuth } from &quot;better-auth&quot;;
import { drizzleAdapter } from &quot;better-auth/adapters/drizzle&quot;;

import { emailClient } from &quot;./mailClient&quot;;
import { useDb } from &quot;../server/db/index&quot;;
import * as schema from &quot;../server/db/schemas/auth-schema&quot;;

export const auth = betterAuth({
  database: drizzleAdapter(useDb(), {
    provider: &quot;pg&quot;,
    schema,
  }),
  emailAndPassword: {
    enabled: true,
    autoSignIn: false,
  },
  emailVerification: {
    sendOnSignUp: true,
    requireEmailVerification: true,
    sendVerificationEmail: async ({ user, url }) =&gt; {
      await emailClient.sendMail({
        to: user.email,
        subject: &quot;Verify your email address&quot;,
        text: `Click the link to verify your email: ${url}`,
      });
    },
  },
});
</code></pre>
<p>When i deployed to Netlify BetterAuth endpoints are working there, but I would like to stay on cloudflare.</p>
<p>Thank you in advance for any suggestions.</p>
",0,0,0,2025-09-08T15:26:49+00:00,0,98,False
79759530,6180721,,postgresql,PostgreSQL 17.5: &quot;canceling statement due to conflict with recovery or timeout&quot; when creating logical replication slot on replica,"<p>I'm trying to create a logical replication slot on a GCP managed PostgreSQL 17 read replica but getting intermittent failures with two different errors. The replica was recently upgraded from PostgreSQL 14 to 17.</p>
<p><strong>Errors encountered:</strong></p>
<p>Error 1 - Recovery Conflict:</p>
<blockquote>
<p>SQL Error [40001]: ERROR: canceling statement due to conflict with
recovery Detail: User query might have needed to see row versions that
must be removed.</p>
</blockquote>
<p>Error 2 - Statement Timeout:</p>
<blockquote>
<p>SQL Error [57014]: ERROR: canceling statement due to statement timeout</p>
</blockquote>
<p><strong>Command that fails:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>SELECT pg_create_logical_replication_slot('my_slot', 'pgoutput');
</code></pre>
<p><strong>Configuration for prod replica[slot creation fails]:</strong></p>
<ul>
<li><strong>Database flags:</strong>
<ul>
<li><code>idle_in_transaction_session_timeout: 900000</code></li>
<li><code>max_replication_slots: 10</code></li>
<li><code>wal_sender_timeout: 0</code></li>
<li><code>log_lock_waits: on</code></li>
<li><code>max_standby_streaming_delay: 180000</code></li>
<li><code>max_standby_archive_delay: 180000</code></li>
<li><code>cloudsql.enable_pglogical: on</code></li>
<li><code>cloudsql.logical_decoding: on</code></li>
<li><code>max_wal_senders: 10</code></li>
</ul>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>-- Replica:
SHOW wal_level;                    -- Result: logical
SHOW max_replication_slots;        -- Result: 10  
SHOW hot_standby_feedback;         -- Result: off
SELECT COUNT(*) FROM pg_replication_slots WHERE active = true;  -- Result: 0
show statement_timeout; -- 150s
</code></pre>
<p><strong>Configuration for dev replica[Where slot creation is success]:</strong></p>
<ul>
<li><strong>Database flags:</strong>
<ul>
<li><code>max_replication_slots: 10</code></li>
<li><code>wal_sender_timeout: 0</code></li>
<li><code>cloudsql.enable_pglogical: on</code></li>
<li><code>cloudsql.logical_decoding: on</code></li>
<li><code>max_wal_senders: 10</code></li>
<li><code>max_locks_per_transaction: 105</code></li>
</ul>
</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>-- Replica:
SHOW wal_level;                    -- Result: logical
SHOW max_replication_slots;        -- Result: 10  
SHOW hot_standby_feedback;         -- Result: off
SELECT COUNT(*) FROM pg_replication_slots WHERE active = true;  -- Result: 2[able to create slots here]
show statement_timeout; -- 150s
</code></pre>
<p><strong>What works:</strong></p>
<ul>
<li>Same command succeeds on development replica (lower activity)</li>
<li>Primary database slot creation also works fine for main db on prod.</li>
</ul>
<p>Please help with what am I missing here.</p>
",0,0,0,2025-09-09T05:35:21+00:00,2,206,False
79760320,1152962,,postgresql,How do I configure an existing tomcat 7 web application to use a postgres database pool,"<p>I have legacy (Java 8) web application running with tomcat 7 that uses postgres database and castor-0.9.6.jar. As we have been hitting postgres connection limits I want to switch to using a database connection pool, which I understand I should be able to do.</p>
<p>Doing my own research and using GrokAI I have made the following configuration changes:</p>
<p>Added to tomcat7/conf/server.xml inside </p>
<pre><code>&lt;Resource name=&quot;jdbc/abode&quot; auth=&quot;Container&quot;
              type=&quot;javax.sql.DataSource&quot;
              factory=&quot;org.apache.tomcat.jdbc.pool.DataSourceFactory&quot;
              driverClassName=&quot;org.postgresql.Driver&quot;
              url=&quot;jdbc:postgresql://localhost:5432/abode&quot;
              username=&quot;username&quot; password=&quot;password&quot;
              maxActive=&quot;100&quot; maxIdle=&quot;30&quot; maxWait=&quot;10000&quot;
              validationQuery=&quot;SELECT 1&quot; testOnBorrow=&quot;true&quot;
              removeAbandoned=&quot;true&quot; removeAbandonedTimeout=&quot;60&quot; logAbandoned=&quot;true&quot; /&gt;
</code></pre>
<p>Added to WEB-INF/web.xml</p>
<pre><code>&lt;resource-ref&gt;
        &lt;description&gt;DB Connection Pool&lt;/description&gt;
        &lt;res-ref-name&gt;jdbc/abode&lt;/res-ref-name&gt;
        &lt;res-type&gt;javax.sql.DataSource&lt;/res-type&gt;
        &lt;res-auth&gt;Container&lt;/res-auth&gt;
    &lt;/resource-ref&gt;
</code></pre>
<p>Added to META-INF/context.xml</p>
<pre><code>&lt;Context&gt;
    &lt;ResourceLink name=&quot;jdbc/abode&quot; global=&quot;jdbc/abode&quot; type=&quot;javax.sql.DataSource&quot; /&gt;
&lt;/Context&gt;
</code></pre>
<p>Modified castor database.xml</p>
<pre><code>&lt;jdo-conf&gt;
    &lt;database name=&quot;abode&quot; engine=&quot;postgresql&quot;&gt;
        &lt;data-source class-name=&quot;org.exolab.castor.persist.spi.JNDIFactory&quot; jndi-name=&quot;java:comp/env/jdbc/abode&quot; /&gt;
        &lt;mapping href=&quot;mapping.xml&quot;/&gt;
    &lt;/database&gt;
    &lt;transaction-demarcation mode=&quot;local&quot;/&gt;
&lt;/jdo-conf&gt;
</code></pre>
<p>I have checked the /usr/share/tomcat7/lib for commons-dbcp.jar, commons-pool.jar and postgresql-42.7.7.jar</p>
<p>When I start tomcat I get the following exceptions:</p>
<pre><code>org.apache.tomcat.jdbc.pool.ConnectionPool init
SEVERE: Unable to create initial connections of pool.
java.sql.SQLException
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:254)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connect(PooledConnection.java:182)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.createConnection(ConnectionPool.java:710)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:644)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.init(ConnectionPool.java:466)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.&lt;init&gt;(ConnectionPool.java:143)
    at org.apache.tomcat.jdbc.pool.DataSourceProxy.pCreatePool(DataSourceProxy.java:116)
    at org.apache.tomcat.jdbc.pool.DataSourceProxy.createPool(DataSourceProxy.java:103)
    at org.apache.tomcat.jdbc.pool.DataSourceFactory.createDataSource(DataSourceFactory.java:554)
    at org.apache.tomcat.jdbc.pool.DataSourceFactory.getObjectInstance(DataSourceFactory.java:242)
    at org.apache.naming.factory.ResourceFactory.getObjectInstance(ResourceFactory.java:142)
    at javax.naming.spi.NamingManager.getObjectInstance(NamingManager.java:332)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:843)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:153)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:830)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:167)
    at org.apache.catalina.core.NamingContextListener.addResource(NamingContextListener.java:1110)
    at org.apache.catalina.core.NamingContextListener.createNamingContext(NamingContextListener.java:689)
    at org.apache.catalina.core.NamingContextListener.lifecycleEvent(NamingContextListener.java:273)
    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
    at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90)
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5472)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:147)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1572)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1562)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.NullPointerException
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:246)
    ... 28 more

Sep 10, 2025 9:31:52 AM org.apache.naming.NamingContext lookup
WARNING: Unexpected exception resolving reference
java.sql.SQLException
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:254)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connect(PooledConnection.java:182)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.createConnection(ConnectionPool.java:710)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:644)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.init(ConnectionPool.java:466)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.&lt;init&gt;(ConnectionPool.java:143)
    at org.apache.tomcat.jdbc.pool.DataSourceProxy.pCreatePool(DataSourceProxy.java:116)
    at org.apache.tomcat.jdbc.pool.DataSourceProxy.createPool(DataSourceProxy.java:103)
    at org.apache.tomcat.jdbc.pool.DataSourceFactory.createDataSource(DataSourceFactory.java:554)
    at org.apache.tomcat.jdbc.pool.DataSourceFactory.getObjectInstance(DataSourceFactory.java:242)
    at org.apache.naming.factory.ResourceFactory.getObjectInstance(ResourceFactory.java:142)
    at javax.naming.spi.NamingManager.getObjectInstance(NamingManager.java:332)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:843)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:153)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:830)
    at org.apache.naming.NamingContext.lookup(NamingContext.java:167)
    at org.apache.catalina.core.NamingContextListener.addResource(NamingContextListener.java:1110)
    at org.apache.catalina.core.NamingContextListener.createNamingContext(NamingContextListener.java:689)
    at org.apache.catalina.core.NamingContextListener.lifecycleEvent(NamingContextListener.java:273)
    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
    at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90)
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5472)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:147)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1572)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1562)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.NullPointerException
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:246)
    ... 28 more

Sep 10, 2025 9:31:52 AM org.apache.catalina.core.NamingContextListener addResource
WARNING: Failed to register in JMX: javax.naming.NamingException

</code></pre>
<p>Any suggestions on where I am going wrong. Much appreciated.</p>
",0,1,1,2025-09-09T22:00:19+00:00,2,105,True
79760645,15983262,,postgresql,Do explicit locks in postgresql CTEs lock every row even with a cursor?,"<p>Let's say I have a table</p>
<pre><code>CREATE TABLE mytable (
  user_id bigint,
  someval text
)
</code></pre>
<p>In my application, I want to lock and fetch every row for some given <code>user_id</code> e.g. 123 to do some additional processing.</p>
<p>But I don't want to fetch everything at once because of the volumetry.</p>
<p>In Jooq in my java application, I'd use something like</p>
<pre><code>dslContext.select(MYTABLE.SOMEVAL)
          .from(MYTABLE)
          .where(MYTABLE.USER_ID.eq(123))
          .orderBy(MY_TABLE.SOMEVAL)
          .forUpdate()
          .fetchSize(...)
          .fetchLazy()
          ...
</code></pre>
<p>But the <a href=""https://www.postgresql.org/docs/current/sql-select.html"" rel=""nofollow noreferrer"">postgresql documentation</a> states that</p>
<blockquote>
<p>if a locking clause is used in a cursor's query, only rows actually fetched or stepped past by the cursor will be locked</p>
</blockquote>
<p>So I instead might do something like</p>
<pre><code>var cte = name(&quot;cte&quot;).as(
    select(MYTABLE.SOMEVAL)
    .from(MYTABLE)
    .where(MYTABLE.USER_ID.eq(123))
    .orderBy(MY_TABLE.SOMEVAL)
    .forUpdate()
);
dslContext.with(cte)
          .select(cte.field(MYTABLE.SOMEVAL))
          .orderBy(cte.field(MY_TABLE.SOMEVAL))
          .fetchSize(...)
          .fetchLazy()
          ...
</code></pre>
<p>But I don't know if this actually locks all the rows from the CTE before the main query, or if it might do some optimization and still only lock the rows actually fetched</p>
",2,2,0,2025-09-10T08:49:12+00:00,1,69,True
79760943,1387438,,postgresql,How to get sqlc to generate a slice of structs from more complex aggregated SQL query,"<h2>Background</h2>
<p>I'm working on a Go project that uses <strong>sqlc</strong> (v1.29.0), and it's been working great so far. There is a lot of code around it, so abandoning sqlc is not an option.<br />
However, I’ve now encountered a more complex case and I’m unsure how to solve it.</p>
<h2>Working code example</h2>
<p>Here’s a minimal, complete, and verifiable example (MCVE) where everything works as expected.</p>
<p><strong>Schema definition:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE item (
    name TEXT PRIMARY KEY,
    description TEXT NOT NULL
);

CREATE TABLE &quot;group&quot; (
    uuid UUID PRIMARY KEY,
    name TEXT NOT NULL
);

CREATE TABLE group_item (
    group_uuid UUID NOT NULL REFERENCES &quot;group&quot;(uuid) ON DELETE CASCADE,
    item_name TEXT NOT NULL REFERENCES item(name) ON DELETE CASCADE,
    PRIMARY KEY (group_uuid, item_name)
);
</code></pre>
<p>Query that works as expected:</p>
<pre class=""lang-sql prettyprint-override""><code>-- name: ListGroupsWithItems :many
SELECT g.uuid,
       g.name,
       COALESCE(array_agg(gi.item_name ORDER BY gi.item_name), '{}')::TEXT[] AS items
FROM &quot;group&quot; g
LEFT JOIN group_item gi ON g.uuid = gi.group_uuid
GROUP BY g.uuid
ORDER BY g.name;
</code></pre>
<p>This generates a Go type with a very convenient field <code>Items []string</code>:</p>
<pre class=""lang-golang prettyprint-override""><code>type ListGroupsWithItemsRow struct {
    Uuid  uuid.UUID
    Name  string
    Items []string
}
</code></pre>
<hr />
<h2>Problematic case</h2>
<p>Now I want to write a similar query, but this time the <code>items</code> should include both the name and description. Essentially, I need to aggregate more complex data into a slice.</p>
<p>I’ve tried several approaches, but sqlc always generates a struct with an <code>interface{}</code> type for the <code>items</code> field:</p>
<pre class=""lang-golang prettyprint-override""><code>type DetailedListGroupsWithItemsRow struct {
    Uuid  uuid.UUID
    Name  string
    Items interface{}
}
</code></pre>
<p>One of my attempts:</p>
<pre class=""lang-sql prettyprint-override""><code>-- name: DetailedListGroupsWithItems :many
SELECT g.uuid,
       g.name,
       COALESCE(
         json_agg(
           json_build_object(
             'name', i.name,
             'description', i.description
           ) ORDER BY i.name
         ) FILTER (WHERE i.name IS NOT NULL),
         '[]'
       ) AS items
FROM &quot;group&quot; g
LEFT JOIN group_item gi ON g.uuid = gi.group_uuid
LEFT JOIN item i ON i.name = gi.item_name
GROUP BY g.uuid
ORDER BY g.name;
</code></pre>
<hr />
<h2>Questions</h2>
<ol>
<li>Is there a way to define a type that sqlc can use to generate code like this (or something similar)?</li>
</ol>
<pre class=""lang-golang prettyprint-override""><code>type Item struct {
    Name        string
    Description string
}

type DetailedListGroupsWithItemsRow struct {
    Uuid  uuid.UUID
    Name  string
    Items []Item
}
</code></pre>
<ol start=""2"">
<li><p>If this isn’t possible directly, can I write some manual code that works with the generated output to achieve the same result?</p>
</li>
<li><p>Or do I need to drop the SQL-side aggregation and instead perform it in Go, converting the types generated by sqlc into my domain types?</p>
</li>
</ol>
",2,2,0,2025-09-10T13:50:58+00:00,1,163,True
79760989,31461593,,postgresql,Postgres not using index with varchar_pattern_ops for pattern matching query,"<p>I have a query in PostgresSQL accessing a big table using a LIKE clause for pattern matching:</p>
<pre><code>                                                    Table &quot;rmx_service_schema.document&quot;
        Column         |            Type             | Collation | Nullable | Default | Storage  | Compression | Stats target | Description 
-----------------------+-----------------------------+-----------+----------+---------+----------+-------------+--------------+-------------
 id                    | character varying(36)       |           | not null |         | extended |             |              | 
 file_name             | character varying(512)      |           | not null |         | extended |             |              | 
...
</code></pre>
<p>The query has very good selectivity:</p>
<pre><code>select count(*) from RMX_SERVICE_SCHEMA.DOCUMENT d1_0;
 count  
--------
 630015


select count(*) from RMX_SERVICE_SCHEMA.DOCUMENT d1_0 where d1_0.FILE_NAME LIKE 'sunet_attachments/20240207.xml';
 count 
-------
     1
</code></pre>
<p>The application somtimes uses <code>%</code> at the end of the pattern, so replacing the <code>LIKE</code> by <code>=</code> is not always possible</p>
<p>I have created an index on that column with the matching operator definition:</p>
<pre><code>CREATE INDEX rse_tmp_doc_file_name ON RMX_SERVICE_SCHEMA.DOCUMENT (file_name varchar_pattern_ops);
</code></pre>
<p>But still, the pattern matching query does a Seq Scan:</p>
<pre><code>EXPLAIN ANALYZE select id from RMX_SERVICE_SCHEMA.DOCUMENT d1_0 where d1_0.FILE_NAME LIKE 'sunet_attachments/2024020.xml';

                                                          QUERY PLAN                                                           
-------------------------------------------------------------------------------------------------------------------------------
 Gather  (cost=1000.00..129562.16 rows=63 width=37) (actual time=81.075..90.793 rows=1 loops=1)
   Workers Planned: 4
   Workers Launched: 4
   -&gt;  Parallel Seq Scan on document d1_0  (cost=0.00..128555.86 rows=16 width=37) (actual time=72.099..77.022 rows=0 loops=5)
         Filter: ((file_name)::text ~~ 'sunet_attachments/20240207.xml'::text)
         Rows Removed by Filter: 126007
 Planning Time: 0.285 ms
 Execution Time: 90.814 ms
(8 rows)
</code></pre>
<p>If I replace the <code>LIKE</code> by <code>=</code>, it uses the index:</p>
<pre><code>EXPLAIN ANALYZE select id from RMX_SERVICE_SCHEMA.DOCUMENT d1_0 where d1_0.FILE_NAME ='sunet_attachments/20240207.xml';
                                                              QUERY PLAN                                                              
--------------------------------------------------------------------------------------------------------------------------------------
 Index Scan using rse_tmp_doc_file_name on document d1_0  (cost=0.55..8.57 rows=1 width=37) (actual time=0.025..0.026 rows=1 loops=1)
   Index Cond: ((file_name)::text = 'sunet_attachments/20240207.xml'::text)
 Planning Time: 0.053 ms
 Execution Time: 0.034 ms
(4 rows)
</code></pre>
<p>Did I miss some stpes required to make this btree index usable for pattern matching query?</p>
<pre><code>Indexes:
    &quot;pk_document&quot; PRIMARY KEY, btree (id)
     ....
    &quot;rse_tmp_doc_file_name&quot; btree (file_name varchar_pattern_ops)
</code></pre>
<p>I was expecting the index I created is used for pattern matching, too, as long as selectivity is good and the pattern doesn't start by wildcards.</p>
<p>I have tried <code>SET enable_seqscan=off</code>, as suggested. The plan changed, but is still very slow:</p>
<pre><code>EXPLAIN ANALYZE select id from RMX_SERVICE_SCHEMA.DOCUMENT d1_0 where d1_0.FILE_NAME LIKE 'sunet_attachments/20240207.xml';
                                                                      QUERY PLAN                                                                      
------------------------------------------------------------------------------------------------------------------------------------------------------
 Gather  (cost=31133.85..158837.55 rows=63 width=37) (actual time=300.945..314.717 rows=1 loops=1)
   Workers Planned: 4
   Workers Launched: 4
   -&gt;  Parallel Bitmap Heap Scan on document d1_0  (cost=30133.85..157831.25 rows=16 width=37) (actual time=290.728..297.328 rows=0 loops=5)
         Filter: ((file_name)::text ~~ 'sunet_attachments/20240207.xml'::text)
         Rows Removed by Filter: 71233
         Heap Blocks: exact=19555
         -&gt;  Bitmap Index Scan on rse_tmp_doc_file_name  (cost=0.00..30133.83 rows=355328 width=0) (actual time=149.426..149.426 rows=356167 loops=1)
               Index Cond: (((file_name)::text ~&gt;=~ 'sunet'::text) AND ((file_name)::text ~&lt;~ 'suneu'::text))
 Planning Time: 0.176 ms
 Execution Time: 314.747 ms
(11 rows)
</code></pre>
<p>But this plan gave me the right hint. The problem is the <code>_</code> after the string <code>sunet</code>. This has to be escaped, otherwise it isn't selective, since about 50% of the <code>file_name</code> values in the table start with <code>sunet</code>. With correct escaping in the SQL, the index works:</p>
<pre><code>EXPLAIN ANALYZE select id from RMX_SERVICE_SCHEMA.DOCUMENT d1_0 where d1_0.FILE_NAME LIKE 'sunet\_attachments/20240207.xml' escape '\';

                                                                             QUERY PLAN                                                                             
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Index Scan using rse_tmp_doc_file_name on document d1_0  (cost=0.55..8.57 rows=63 width=37) (actual time=0.014..0.015 rows=1 loops=1)
   Index Cond: (((file_name)::text ~&gt;=~ 'sunet_attachments/20240207_10111647337'::text) AND ((file_name)::text ~&lt;~ 'sunet_attachments/20240207'::text))
   Filter: ((file_name)::text ~~ 'sunet\_attachments/20240207.xml'::text)
 Planning Time: 0.152 ms
 Execution Time: 0.024 ms
(5 rows)
</code></pre>
",2,2,0,2025-09-10T14:49:08+00:00,1,74,True
79761286,13968392,,postgresql,"write_database(..., engine=&quot;adbc&quot;) with autocommit=False","<p>In polars, I would like to use <code>pl.write_database</code> multiple times with <code>engine=&quot;adbc&quot;</code> in the same session and then commit all at the end with <code>conn.commit()</code>, i.e. do a manual commit.</p>
<pre class=""lang-py prettyprint-override""><code>import adbc_driver_postgresql.dbapi as pg_dbapi
import polars as pl

conn = pg_dbapi.connect(&quot;postgresql://username:password@host:port/database&quot;)

df = pl.DataFrame({&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]})

df.write_database(
    &quot;public.table1&quot;,
    connection=conn,
    engine=&quot;adbc&quot;,
)

df.transpose().write_database(
    &quot;public.table2&quot;,
    connection=conn,
    engine=&quot;adbc&quot;,
)

conn.commit()
</code></pre>
<p>The reason behind this is to ensure that either both dfs are written to the database or none are. However, the dfs are written immediately into the database one after the other. In the <a href=""https://arrow.apache.org/adbc/current/format/specification.html#autocommit"" rel=""nofollow noreferrer"">adbc docs</a>, it is said:</p>
<blockquote>
<p>By default, connections are expected to operate in autocommit mode; that is, queries take effect immediately upon execution. This can be disabled in favor of manual commit/rollback calls, but not all implementations will support this.</p>
</blockquote>
<p>Is it supported to disable autocommit somehow in python? Maybe this can be done in <a href=""https://arrow.apache.org/adbc/current/python/api/adbc_driver_postgresql.html#adbc_driver_postgresql.dbapi.connect"" rel=""nofollow noreferrer""><code>adbc_driver_postgresql.dbapi.connect</code></a>, maybe with the <code>conn_kwargs</code> parameter? <code>conn_kwargs={&quot;autocommit&quot;: False}</code> didn't work.</p>
",3,3,0,2025-09-10T21:04:12+00:00,1,161,True
79761506,20492635,,postgresql,Having trouble with asyc_sessiomaker in FastAPI,"<p>I'm buiding endpoints with <code>FastAPI</code>, <code>PostgreSQL</code> as database, and the driver is <code>asyncpg</code> associated with <code>SQLAlchemy</code> for asynchronous. As mentioned in the title, I'm having trouble with <a href=""https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html#sqlalchemy.ext.asyncio.async_sessionmaker"" rel=""nofollow noreferrer""><code>async_sessionmaker</code></a>, it keeps showing:</p>
<pre class=""lang-none prettyprint-override""><code>'async_sessionmaker' object does not support the asynchronous context manager protocol.
</code></pre>
<p>Here the part of code in repository:</p>
<pre class=""lang-py prettyprint-override""><code>    
class GenreRepositoryImpl(GenreRepository):

    def __init__(self, sessionmaker: async_sessionmaker[AsyncSession]):
        self._sessionmaker = sessionmaker
    
    async def create(self, genre: Genre) -&gt; Genre:
        genre_entity = GenreEntityMappers.from_domain(genre)

        async with self._sessionmaker() as session:
            session.add(genre_entity) 
            await session.commit()
            await session.refresh(genre_entity)

        return GenreEntityMappers.to_domain(genre_entity)
</code></pre>
<p>Somehow it works when I use it as transaction with <a href=""https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html#sqlalchemy.ext.asyncio.async_sessionmaker.begin"" rel=""nofollow noreferrer""><code>begin()</code></a>, I don't understand what's wrong.</p>
",1,1,0,2025-09-11T05:40:40+00:00,0,121,False
79762084,4717783,,postgresql,How to get Postgres&#39; pg_dump to exclude certain &quot;types&quot; of exports?,"<p>When viewing the output of a <code>pg_dump</code> command, it seems like it organizes the output into &quot;types&quot; by way of the comment that it adds above each export line. Here's an example:</p>
<pre class=""lang-sql prettyprint-override""><code>--
-- Name: some_table_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.some_table_id_seq', 8, true);
</code></pre>
<p>How do I get it to ignore certain types of exports? Specifically, I want it to ignore the above type of exports (setting sequence values to their current values), since I want to use the output to help other team members get their DB schema set up.</p>
<p>I tried looking through the manpages for an option like this, to no avail.</p>
",1,1,0,2025-09-11T15:00:51+00:00,1,39,True
79762247,31470961,,postgresql,Purview SHIR scan to Posgresql hosted on premise,"<p>Purview scan using SHIR to on prem RHL hosted postgresql fails with the error</p>
<p>&quot;The version of the linked service not supported&quot;</p>
<p>I tried replicating by deploying postgres1ql on windows and new version of RHL linux with newer postgres and shows same result.</p>
<p>Microsoft ticket is pending with product team for three weeks. Suggesting API fix and no response. Any recommendation and any workaround for Postgres. the official MS document supports Postgres scan</p>
<p>Deployed SHIR performed SQL scan which was successful but PostgreSQL does not even pass the scan, therefore, not much of log in SHIR.
Replicated on test deployment with different version of OS and Postgres and same issue.
Microsoft support acknowledge issue with API but they have not provided many details and it still parked with product team,
Has anyone tried older version of SHIR or API using Apache Atlan to work around this issue?</p>
",0,0,0,2025-09-11T17:57:33+00:00,0,77,False
79762784,7355239,,postgresql,I am unable to generate drizzle migrations,"<p>I run the generate command, I get the error below. My neon migrations folder is still empty and <code>_journal.json</code> doesn't exist.</p>
<pre><code>
&gt; dotenv drizzle-kit generate

No config path provided, using default 'drizzle.config.ts'
Reading config file '/User/this/a/test/directory/drizzle.config.ts'
Error: ENOENT: no such file or directory, open 'neon/migrations/meta/_journal.json'
    at Object.openSync (node:fs:573:18)
    at readFileSync (node:fs:452:35)
    at prepareOutFolder (/User/this/a/test/directory/node_modules/.pnpm/drizzle-kit@0.30.1/node_modules/drizzle-kit/bin.cjs:7654:61)
    at prepareMigrationFolder (/User/this/a/test/directory/node_modules/.pnpm/drizzle-kit@0.30.1/node_modules/drizzle-kit/bin.cjs:7716:38)
    at prepareAndMigratePg (/User/this/a/test/directory/node_modules/.pnpm/drizzle-kit@0.30.1/node_modules/drizzle-kit/bin.cjs:51667:40)
    at Object.handler (/User/this/a/test/directory/node_modules/.pnpm/drizzle-kit@0.30.1/node_modules/drizzle-kit/bin.cjs:91970:13)
    at async run (/User/this/a/test/directory/node_modules/.pnpm/drizzle-kit@0.30.1/node_modules/drizzle-kit/bin.cjs:90501:7) {
  errno: -2,
  code: 'ENOENT',
  syscall: 'open',
  path: 'neon/migrations/meta/_journal.json'
}
</code></pre>
",1,1,0,2025-09-12T09:49:09+00:00,1,198,True
79763056,23400318,,postgresql,Next.js - TypeError: Cannot read properties of undefined (reading &#39;searchParams&#39;),"<p>I am trying to get my database connection to run querys but it refuses and runs the error I provided in the title. I didn't change anything and even reverted back a few commits to see what happened but couldn't find the source of my issue. I created a db connection in my lib/db.ts file and tried to run a query in other server only files but nothing would run and it would throw the same error. I narrowed it down to the connection by making it run a query in the same lib/db.ts set up file to confirm everything was set up properly and it still threw the same error. I have never called a searchParams type, or whatever its saying, in my project. I am using AWS RDS to host my database and my url in the env variable is set up correctly after double checking it a 1000 times. I did make sure to run npm i to update my dependences and checked to make sure I had them installed. I have gone through every chatgpt step and nothing helped. I am at a complete loss.</p>
<p>lib/db.ts file</p>
<pre><code>import 'server-only'
import { Pool } from 'pg'

declare global { var __pool: Pool | undefined }

const url = process.env.DATABASE_URL;
if (!url) {
  throw new Error('Missing DATABASE_URL');
}

const pool = global.__pool ?? new Pool({
    connectionString: url,
    ssl: { rejectUnauthorized: false }
})

if(pool){
  console.log(await pool.query('SELECT * FROM users'))
}

if (!global.__pool) global.__pool = pool

export default pool;
</code></pre>
<p>Any help is greatly appreciated.</p>
<p>Full stack trace:</p>
<pre><code> ⨯ TypeError: Cannot read properties of undefined (reading 'searchParams')
    at eval (src/lib/db.ts:17:25)
  15 |
  16 | if(pool){
&gt; 17 |   console.log(await pool.query('SELECT * FROM users'))
     |                         ^
  18 | }
  19 |
  20 | if (!global.__pool) global.__pool = pool {
  page: '/auth/login'
}
</code></pre>
",0,0,0,2025-09-12T14:22:38+00:00,0,519,False
79763104,11718232,,postgresql,Spring Boot 4 + Postgres EXTENSION vector not working correct,"<p>I'm trying to implement the use of vectors in my application.</p>
<p>I raised the version of Spring boot to 4.0.0-M1. This also gives me hibernate 7.</p>
<p>Added EXTENSION vector to Postgres</p>
<p>Added dependency:</p>
<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;org.hibernate.orm&lt;/groupId&gt;
      &lt;artifactId&gt;hibernate-vector&lt;/artifactId&gt;
      &lt;version&gt;${hibernate.version}&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<p>My entity:</p>
<pre><code>@Getter
@Setter
@Builder
@NoArgsConstructor
@AllArgsConstructor
@Entity
@Table(name = &quot;embedding&quot;)
public class Embedding {

    @Id
    @Column(name = &quot;id&quot;)
    private UUID id;

    @JdbcTypeCode(SqlTypes.VECTOR)
    @Array(length = 1536)
    @Column(name = &quot;vector&quot;, columnDefinition = &quot;vector(1536)&quot;)
    private double[] vector;

}
</code></pre>
<p>This setting allows saving to the database from the box but not selection.
Spring JPA named queries don't work.
SQL doesn't work too.</p>
<p>For example native query not working too:</p>
<pre><code>    @Query(value = &quot;&quot;&quot;
        SELECT * FROM embedding ORDER BY vector &lt;=&gt; CAST(:vector AS vector) LIMIT :limit
        &quot;&quot;&quot;, nativeQuery = true)
    List&lt;Embedding&gt; findNearest(@Param(&quot;vector&quot;) double[] vector, @Param(&quot;limit&quot;) int limit);
</code></pre>
<p>This query gives me:
Caused by: org.springframework.orm.jpa.JpaSystemException: org.postgresql.util.PSQLException: No results were returned by the query.
Caused by: org.hibernate.HibernateException: org.postgresql.util.PSQLException: No results were returned by the query.
Caused by: org.postgresql.util.PSQLException: No results were returned by the query.</p>
<p>Simple findById(id) gives the same error.</p>
<p>The only way that allows me to use findById(id) and query from example to get record from db is using custom AttributeConverter  for vector field that i got from GPT
now i use @Convert instead of @JdbcTypeCode and @Array:</p>
<pre><code>    @Convert(converter = VectorConverter.class)
    private double[] vector;
</code></pre>
<p>And converteer:</p>
<pre><code>@Converter(autoApply = true)
public class VectorConverter implements AttributeConverter&lt;double[], String&gt; {

    @Override
    public String convertToDatabaseColumn(double[] attribute) {
        if (attribute == null) return null;
        return Arrays.stream(attribute)
            .mapToObj(Double::toString)
            .collect(Collectors.joining(&quot;,&quot;, &quot;[&quot;, &quot;]&quot;));
    }

    @Override
    public double[] convertToEntityAttribute(String dbData) {
        if (dbData == null) return null;
        String clean = dbData.replaceAll(&quot;[\\[\\]\\s]&quot;, &quot;&quot;);
        if (clean.isEmpty()) return new double[0];
        String[] parts = clean.split(&quot;,&quot;);
        return Arrays.stream(parts).mapToDouble(Double::parseDouble).toArray();
    }
}
</code></pre>
<p>But with this converter i can't save data with other error:
Caused by: org.postgresql.util.PSQLException: ERROR: column &quot;vector&quot; is of type vector but expression is of type character varying</p>
<p>Also i was trying to use PGobject instead of String -&gt; AttributeConverter&lt;double[], PGobject&gt; and got similar error:
Caused by: org.postgresql.util.PSQLException: ERROR: column &quot;vector&quot; is of type vector but expression is of type bytea</p>
<p>Different types didn't bring changes:</p>
<pre><code>    @JdbcTypeCode(SqlTypes.VECTOR)
    @JdbcTypeCode(SqlTypes.VECTOR_FLOAT64)
    @JdbcTypeCode(SqlTypes.ARRAY)
</code></pre>
<p>Please help me set it up.
Thank you.</p>
",0,0,0,2025-09-12T15:22:14+00:00,1,130,False
79763477,4632019,Ukraine,postgresql,Why the same IMMUTABLE function takes more time than STABLE?,"<p>I have changed <code>STABLE</code> to <code>IMMUTABLE</code> and expecting it to work faster, but it works more slower. Did I miss something?</p>
<p><code>IMMUTABLE</code> function:</p>
<pre><code>test=&gt; \sf+ rls_guard 
        CREATE OR REPLACE FUNCTION public.rls_guard()
         RETURNS TABLE(org_id integer, unit_id integer, user_id integer)
         LANGUAGE sql
         IMMUTABLE
1       AS $function$
2       SELECT
3           app_config( 'org_id'  )::int,
4           app_config( 'unit_id' )::int,
5           app_config( 'user_id' )::int;
6       $function$
test=&gt; begin;
BEGIN
Time: 0,306 ms
test=*&gt; SELECT app_config( '{&quot;user_id&quot;:3, &quot;org_id&quot;:1, &quot;unit_id&quot;:2 }'::jsonb);
 app_config 
------------
 
(1 row)

Time: 0,670 ms
test=*&gt; select * from rls_test;
 id | org_id | unit_id | user_id 
----+--------+---------+---------
  5 |      1 |       2 |       3
(1 row)

Time: 1,038 ms
</code></pre>
<pre class=""lang-sql prettyprint-override""><code>test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
DO
Time: 26464,831 ms (00:26,465)
test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM * FROM rls_test;
  END LOOP;
END $$;
DO
Time: 29063,969 ms (00:29,064)
</code></pre>
<p><code>STABLE</code> function:</p>
<pre><code>test=&gt; \sf+ rls_guard 
        CREATE OR REPLACE FUNCTION public.rls_guard()
         RETURNS TABLE(org_id integer, unit_id integer, user_id integer)
         LANGUAGE sql
         STABLE
1       AS $function$
2       SELECT
3           app_config( 'org_id'  )::int,
4           app_config( 'unit_id' )::int,
5           app_config( 'user_id' )::int;
6       $function$
test=&gt; begin;
BEGIN
Time: 0,320 ms
test=*&gt; SELECT app_config( '{&quot;user_id&quot;:3, &quot;org_id&quot;:1, &quot;unit_id&quot;:2 }'::jsonb);
 app_config 
------------
 
(1 row)

Time: 0,734 ms
test=*&gt; select * from rls_test;
 id | org_id | unit_id | user_id 
----+--------+---------+---------
  5 |      1 |       2 |       3
(1 row)

Time: 1,007 ms
</code></pre>
<pre class=""lang-sql prettyprint-override""><code>test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
DO
Time: 20729,006 ms (00:20,729)
test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM * FROM rls_test;
  END LOOP;
END $$;
DO
Time: 23626,823 ms (00:23,627)
</code></pre>
<p><strong>UPD</strong></p>
<p>As requested here it is <code>app_config</code>:</p>
<pre class=""lang-sql prettyprint-override""><code> app_config(text)   | CREATE OR REPLACE FUNCTION public.app_config(key text)                                                         &gt;
                    |  RETURNS text                                                                                                  &gt;
                    |  LANGUAGE sql                                                                                                  &gt;
                    |  STABLE                                                                                                        &gt;
                    | AS $function$                                                                                                  &gt;
                    | SELECT NULLIF( current_setting( 'app.' || key,  true ), '' );                                                  &gt;
                    | $function$                                                                                                     &gt;
</code></pre>
<p>During the test both functions were switched to <code>IMMUTABLE</code> and back.</p>
<p>I am sorry, it seems this is most important part I missed: How <code>rls_guard</code> and <code>rls_test</code> belongs to each other.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE rls_test (
    id SERIAL,
    org_id  INT NOT NULL DEFAULT current_setting( 'app.org_id',   true )::int,
    unit_id INT NOT NULL DEFAULT current_setting( 'app.unit_id',  true )::int,
    user_id INT NOT NULL DEFAULT current_setting( 'app.user_id',  true )::int
);
-- TODO: add uniq constraint

INSERT INTO rls_test ( org_id, unit_id, user_id ) VALUES
( 1, 1, 1 ),
( 1, 1, 2 ),
( 1, 1, 3 ),
( 1, 2, 1 ),
( 1, 2, 3 ),
( 1, 3, 2 ),
( 2, 1, 1 );

ALTER TABLE rls_test FORCE  ROW LEVEL SECURITY;
ALTER TABLE rls_test ENABLE ROW LEVEL SECURITY;

-- If table has RESTRICTIVE rules it must have at least one permissive rule
-- https://www.postgresql.org/docs/current/sql-createpolicy.html#id-1.9.3.75.6
CREATE POLICY db_tenant_allow_any ON rls_test USING( true );

CREATE POLICY db_tenant_by ON rls_test AS RESTRICTIVE
USING( (org_id, unit_id, user_id) = (select (rls_guard()).*) );
</code></pre>
<p>So when we <code>select * from rls_test</code> implicit call to <code>rls_guard</code> is done.
Under root account where it passes RLS the picture looks like this:<br />
(<code>IMMUTABLE</code> case):</p>
<pre class=""lang-none prettyprint-override""><code>test=# DO $$
DECLARE
  i int;
BEGIN                     
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
DO

Time: 23648,919 ms (00:23,649)
test=# DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM * FROM rls_test;
  END LOOP;
END $$;
DO
Time: 1134,391 ms (00:01,134)
</code></pre>
<p><code>STABLE</code> case:</p>
<pre class=""lang-none prettyprint-override""><code>test=# DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
DO
Time: 20000,007 ms (00:20,000)

test=# DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM * FROM rls_test;
  END LOOP;
END $$;
DO
Time: 1140,360 ms (00:01,140)
</code></pre>
<p>Adding EXPLAIN ANALYSE:</p>
<pre><code>test=*# explain ( analyse, verbose, costs, settings, timing, summary, buffers) select rls_guard();
                                                          QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
 ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=31.094..31.114 rows=1 loops=1)
   Output: rls_guard()
   -&gt;  Result  (cost=0.00..0.01 rows=1 width=0) (actual time=3.538..3.539 rows=1 loops=1)
 Settings: jit_above_cost = '0', jit_inline_above_cost = '0', jit_optimize_above_cost = '0'
 Planning Time: 0.033 ms
 JIT:
   Functions: 1
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 0.097 ms (Deform 0.000 ms), Inlining 0.015 ms, Optimization 0.808 ms, Emission 2.704 ms, Total 3.624 ms
 Execution Time: 31.249 ms
(10 rows)

Time: 31,561 ms
test=*# explain ( analyse, verbose, costs, settings, timing, summary, buffers) select * from rls_test;
                                                  QUERY PLAN                                                  
--------------------------------------------------------------------------------------------------------------
 Seq Scan on public.rls_test  (cost=0.00..28.50 rows=1850 width=16) (actual time=0.013..0.015 rows=7 loops=1)
   Output: id, org_id, unit_id, user_id
   Buffers: shared hit=1
 Settings: jit_above_cost = '0', jit_inline_above_cost = '0', jit_optimize_above_cost = '0'
 Planning Time: 0.047 ms
 Execution Time: 0.044 ms
(6 rows)

Time: 0,408 ms
</code></pre>
<p>and regular account:<br />
<code>IMMUTABLE</code> case:</p>
<pre class=""lang-none prettyprint-override""><code>test=*&gt; explain ( analyse, verbose, costs, settings, timing, summary, buffers) select rls_guard();
                                                          QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
 ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=64.470..64.483 rows=1 loops=1)
   Output: rls_guard()
   Buffers: shared hit=6
   -&gt;  Result  (cost=0.00..0.01 rows=1 width=0) (actual time=3.169..3.169 rows=1 loops=1)
 Settings: jit_above_cost = '0', jit_inline_above_cost = '0', jit_optimize_above_cost = '0'
 Planning Time: 0.029 ms
 JIT:
   Functions: 1
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 0.086 ms (Deform 0.000 ms), Inlining 0.014 ms, Optimization 0.813 ms, Emission 2.335 ms, Total 3.248 ms
 Execution Time: 64.602 ms
(11 rows)

Time: 65,002 ms
test=*&gt; explain ( analyse, verbose, costs, settings, timing, summary, buffers) select * from rls_test;
                                                               QUERY PLAN                                                            &gt;
-------------------------------------------------------------------------------------------------------------------------------------&gt;
 Seq Scan on public.rls_test  (cost=765.26..807.64 rows=1 width=16) (actual time=72.676..72.678 rows=1 loops=1)
   Output: rls_test.id, rls_test.org_id, rls_test.unit_id, rls_test.user_id
   Filter: ((rls_test.org_id = (InitPlan 1).col1) AND (rls_test.unit_id = (InitPlan 1).col2) AND (rls_test.user_id = (InitPlan 1).col&gt;
   Rows Removed by Filter: 6
   Buffers: shared hit=1
   InitPlan 1
     -&gt;  Result  (cost=0.00..765.26 rows=1000 width=12) (actual time=59.178..59.191 rows=1 loops=1)
           Output: ((rls_guard())).org_id, ((rls_guard())).unit_id, ((rls_guard())).user_id
           -&gt;  ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=59.171..59.184 rows=1 loops=1)
                 Output: rls_guard()
                 -&gt;  Result  (cost=0.00..0.01 rows=1 width=0) (actual time=0.005..0.005 rows=1 loops=1)
 Settings: jit_above_cost = '0', jit_inline_above_cost = '0', jit_optimize_above_cost = '0'
 Planning Time: 0.080 ms
 JIT:
   Functions: 4
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 0.329 ms (Deform 0.075 ms), Inlining 4.316 ms, Optimization 4.535 ms, Emission 4.622 ms, Total 13.802 ms
 Execution Time: 73.047 ms
(18 rows)

Time: 73,507 ms

</code></pre>
<p><code>STABLE</code> case:</p>
<pre class=""lang-none prettyprint-override""><code>explain ( analyse, verbose, costs, settings, timing, summary, buffers) select rls_guard();
                                                          QUERY PLAN                                                          
------------------------------------------------------------------------------------------------------------------------------
 ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=30.851..30.872 rows=1 loops=1)
   Output: rls_guard()
   -&gt;  Result  (cost=0.00..0.01 rows=1 width=0) (actual time=3.987..3.988 rows=1 loops=1)
 Settings: jit_above_cost = '0', jit_inline_above_cost = '0', jit_optimize_above_cost = '0'
 Planning Time: 0.031 ms
 JIT:
   Functions: 1
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 0.099 ms (Deform 0.000 ms), Inlining 0.017 ms, Optimization 1.071 ms, Emission 2.893 ms, Total 4.080 ms
 Execution Time: 31.007 ms
(10 rows)

Time: 31,406 ms


explain ( analyse, verbose, costs, settings, timing, summary, buffers) select * from rls_test;
                                                               QUERY PLAN                                                            &gt;
-------------------------------------------------------------------------------------------------------------------------------------&gt;
 Seq Scan on public.rls_test  (cost=765.26..807.64 rows=1 width=16) (actual time=40.513..40.516 rows=1 loops=1)
   Output: rls_test.id, rls_test.org_id, rls_test.unit_id, rls_test.user_id
   Filter: ((rls_test.org_id = (InitPlan 1).col1) AND (rls_test.unit_id = (InitPlan 1).col2) AND (rls_test.user_id = (InitPlan 1).col&gt;
   Rows Removed by Filter: 6
   Buffers: shared hit=1
   InitPlan 1
     -&gt;  Result  (cost=0.00..765.26 rows=1000 width=12) (actual time=24.662..24.679 rows=1 loops=1)
           Output: ((rls_guard())).org_id, ((rls_guard())).unit_id, ((rls_guard())).user_id
           -&gt;  ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=24.652..24.669 rows=1 loops=1)
                 Output: rls_guard()
                 -&gt;  Result  (cost=0.00..0.01 rows=1 width=0) (actual time=0.005..0.005 rows=1 loops=1)
 Settings: jit_above_cost = '0', jit_inline_above_cost = '0', jit_optimize_above_cost = '0'
 Planning Time: 0.086 ms
 JIT:
   Functions: 4
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 0.354 ms (Deform 0.072 ms), Inlining 5.612 ms, Optimization 5.013 ms, Emission 5.199 ms, Total 16.178 ms
 Execution Time: 40.913 ms
(18 rows)
</code></pre>
<p>From the above we can see that for STABLE starting from here it works twice faster:
STABLE: -&gt;  ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=24.652..24.669 rows=1 loops=1)
IMMUTA: -&gt;  ProjectSet  (cost=0.00..5.27 rows=1000 width=32) (actual time=59.171..59.184 rows=1 loops=1)</p>
<p>ST:24 VS IM:59</p>
<p>I am just guessing that IMMUTABLE functions work a bit in a different way in compare to STABLE when Row Level Security is in effect.</p>
<p><strong>Side note:</strong>  After</p>
<pre class=""lang-sql prettyprint-override""><code>SET jit = on;
SET jit_above_cost = 0;                                              
SET jit_inline_above_cost = 0;
SET jit_optimize_above_cost = 0;
</code></pre>
<p>the</p>
<pre class=""lang-sql prettyprint-override""><code>DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
</code></pre>
<p>works <strong>1000</strong> times slower!</p>
<p>Results are pretty stable. I run any measurement multiple times. My test env does not run any heavy task in background. And the same results I see today after my PC was rebooted. I do not think this is a noise.</p>
<p>Btw.</p>
<pre><code>select version();
                                                              version                                                              
-----------------------------------------------------------------------------------------------------------------------------------
 PostgreSQL 17.6 (Ubuntu 17.6-1.pgdg24.04+1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit
(1 row)
</code></pre>
",3,4,1,2025-09-13T02:34:59+00:00,1,186,True
79763559,19328469,,postgresql,PQconnectdb extremely slow compared to psql.exe,"<p>I am attempting to add support for &quot;postgreSQL&quot; to my network handler. All is fine and well, I've had very little issues up until now but when I was profiling my initial connection, I realized it was taking a very long time. I thought this was a quirk of using windows for my host but when profiled the exact same connection info and query with &quot;psql.exe&quot; it was about 15ms, compared to my handlers 100-150ms.</p>
<p><strong>Important Notes</strong></p>
<ol>
<li><p>I use IIS10 with fastCGI for my network handling, I figured it might've been an issue with either more AV scrutiny or restricted CPU resources, so I made an example application which also uses libpq.dll; same problem.</p>
</li>
<li><p>I figured my dlls could be out of date, so I verified all of the hashes are exact to the ones used by psql.exe. No issue there.</p>
</li>
<li><p>I've tried non blocking &quot;PQconnectStart&quot; and handling polling myself, same issue.</p>
</li>
<li><p>I switched to pgbouncer for all connections, it improved the speed of both my executable and &quot;psql.exe&quot; but that gap of about 100-135ms still exists.</p>
</li>
<li><p>I've tried disabling authentication on both pgbouncer, and the postgre server itself. Saw a very minor improvement, still with a massive gap.</p>
</li>
<li><p>I tried fully and completely disabling windows AV and firewall, as well as running the program in an elevated shell. No change.</p>
</li>
<li><p>I have attempted to profile and monitor the applications execution, I am not the best at understanding these extensive monitor logs so I put it into an AI, this information may not be useful at all but it claims that the client is getting stuck attempting to provide the &quot;SCRAM proof&quot;, it notes (I also saw this) we receive connect packets very quickly, and then for some reason there is a large time gap between the next packet being sent from our client. In terms of useful information that's about all I got.</p>
</li>
</ol>
<p><strong>Minimal Reproducible Example</strong></p>
<pre class=""lang-cpp prettyprint-override""><code>auto Start = std::chrono::high_resolution_clock::now();

PGconn* PGConnection = PQconnectdb(&quot;host=127.0.0.1 hostaddr=127.0.0.1 port=6432 dbname=... user=... password=... sslmode=disable gssencmode=disable&quot;);
if (!PGConnection)
{
    return false;
}

if (PQstatus(PGConnection) == CONNECTION_BAD)
{
    PQfinish(PGConnection);
    return false;
}

auto End = std::chrono::high_resolution_clock::now();

std::cout &lt;&lt; &quot;PQConnection: &quot; &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(End - Start).count() &lt;&lt; &quot;\n&quot;;
system(&quot;pause&quot;);
</code></pre>
<p>Takes anywhere from 100 to 150ms.</p>
<pre><code>(Measure-Command { &amp; &quot;...\psql.exe&quot; &quot;host=127.0.0.1 port=6432 dbname=... user=... password=... sslmode=disable gssencmode=disable&quot; -c &quot;SELECT 1&quot; *&gt; $null }).TotalMilliseconds
</code></pre>
<p>Records execution taking 14-15ms</p>
<p><strong>End Goal</strong>
I'm primarily looking for an answer, maybe I am missing something crucial that's causing this hitch, that I can very easily resolve. However if this is just somehow a trick psql.exe uses to connect more efficiently I would be fine with that as well, this gap wouldn't bother me so much if it wasn't as large, I would just like to understand why at least.</p>
",0,0,0,2025-09-13T06:58:49+00:00,0,91,False
79763718,2444661,"Mumbai, India",postgresql,Undocumented message types in postgres logical replication,"<p>I have a program which receives postgres(16) logical replication stream. I am following <a href=""https://www.postgresql.org/docs/16/protocol-logicalrep-message-formats.html"" rel=""nofollow noreferrer"">documentation</a> to understand message types for logical replication.</p>
<p>Messages types I see are S, K, Z, W, d. But in above documentation I dont see Z,W,d.</p>
<p>Is it expected behaviour?</p>
",1,2,1,2025-09-13T12:40:07+00:00,1,69,True
79763756,5429268,United States,postgresql,pg_restore error must be member of role &quot;confluence&quot;,"<p>I made a backup of an existing posgres database. I didn't create the database, I inherited it. I've been tasked to restore the backup to a new database with a different database owner.
When I run the pg_restore command, I'm getting the following error:</p>
<pre><code>pg_restore: error: could not execute query: ERROR:  must be member of role &quot;confluence&quot;
Command was: ALTER TABLE public.usercontent_relation OWNER TO confluence;
</code></pre>
<p>On the new database I created the owner of the database is not 'confluence', it's 'confdemo_cop'. How do I fix this?</p>
",0,0,0,2025-09-13T14:07:29+00:00,0,38,False
79763777,61104,,postgresql,What is the recommended collation for a postgresql citext column?,"<p>I'm using postgres 16, and I have a number of tables where I need to treat their display ID columns as case insensitive, and also handle LIKE queries with wildcards (ex: <code>P%123%</code>). In order to handle these queries efficiently with range scans, I needed to set the collation for those columns to <code>C</code> rather than the default.</p>
<p>With the new requirement for case insensitive searching, I'm considering changing the column's datatype to <code>citext</code> (<a href=""https://www.postgresql.org/docs/current/citext.html"" rel=""nofollow noreferrer"">https://www.postgresql.org/docs/current/citext.html</a>). Will leaving the collation on these columns as <code>C</code> cause issues, since <code>C</code> is a case sensitive collation? What is the recommended collation for a <code>citext</code> column?</p>
",2,2,0,2025-09-13T14:45:20+00:00,1,129,True
79763947,4632019,Ukraine,postgresql,"Why is a function which returns a set (one or more rows, actually always one row in my case) twice as fast compared to a function with OUT args?","<p><strong>TL;DR;</strong><br />
When we use <code>OUT</code> syntax, it is twice slower in compare to <code>RETURNS TABLE</code>. <strong>But why??</strong></p>
<p>DB&lt;&gt;fiddle for <a href=""https://dbfiddle.uk/fz9L_wm0"" rel=""nofollow noreferrer""><code>OUT</code></a> and <a href=""https://dbfiddle.uk/uTkU1MT8"" rel=""nofollow noreferrer""><code>RETURNS TABLE</code></a> cases.</p>
<p>I thought that for the case, when I always return one row, returning the TABLE are the same as returning OUT args. It is also officially stated in <a href=""https://www.postgresql.org/docs/current/sql-createfunction.html#column_name"" rel=""nofollow noreferrer"">the doc</a>: (scroll to <code>column_name</code> part) that:</p>
<blockquote>
<p>CREATE [ OR REPLACE ] FUNCTION<br />
name ( [ [ argmode ] [ argname ] argtype [ { DEFAULT | = } default_expr ] [, ...]  &gt;] )<br />
[ RETURNS rettype<br />
| RETURNS TABLE ( column_name column_type [, ...] ) ]<br />
...<br />
<strong>column_name</strong><br />
The name of an output column in the RETURNS TABLE syntax. This is <strong>effectively another way of declaring a named OUT parameter</strong>, except that RETURNS TABLE also implies RETURNS SETOF.</p>
</blockquote>
<p>But this is not true for the table when Row Level Security is enabled for the table and this function is called to get values:
So for this table and data:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE rls_test (
    id SERIAL,
    org_id  INT NOT NULL DEFAULT current_setting( 'app.org_id',   true )::int,
    unit_id INT NOT NULL DEFAULT current_setting( 'app.unit_id',  true )::int,
    user_id INT NOT NULL DEFAULT current_setting( 'app.user_id',  true )::int
);
-- TODO: add uniq constraint

INSERT INTO rls_test ( org_id, unit_id, user_id ) VALUES
( 1, 1, 1 ),
( 1, 1, 2 ),
( 1, 1, 3 ),
( 1, 2, 1 ),
( 1, 2, 3 ),
( 1, 3, 2 ),
( 2, 1, 1 );

ALTER TABLE rls_test FORCE  ROW LEVEL SECURITY;
ALTER TABLE rls_test ENABLE ROW LEVEL SECURITY;

-- If table has RESTRICTIVE rules it must have at least one permissive rule
-- https://www.postgresql.org/docs/current/sql-createpolicy.html#id-1.9.3.75.6
CREATE POLICY db_tenant_allow_any ON rls_test USING( true );

CREATE POLICY db_tenant_by ON rls_test AS RESTRICTIVE
USING( (org_id, unit_id, user_id) = (select (rls_guard()).*) );
</code></pre>
<p>It does matter! how you return your ROW:<br />
<code>OUT</code> case:</p>
<pre class=""lang-none prettyprint-override""><code>CREATE OR REPLACE FUNCTION rls_guard( OUT org_id INT, OUT unit_id INT, OUT user_id INT )
AS $$
SELECT
  current_setting( 'app.org_id',  true )::int,
  current_setting( 'app.unit_id', true )::int,
  current_setting( 'app.user_id', true )::int;
$$ LANGUAGE sql STABLE;

DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
DO
Time: 9250,993 ms (00:09,251)  #1


test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM * FROM rls_test;
  END LOOP;
END $$;
DO
Time: 29104,855 ms (00:29,105)  #2 &lt;&lt;&lt;&lt;!!!!
</code></pre>
<p><code>RETURNS TABLE</code> case:</p>
<pre class=""lang-none prettyprint-override""><code>5b.
CREATE OR REPLACE FUNCTION rls_guard()
RETURNS TABLE( org_id INT, unit_id INT, user_id INT ) AS $$
SELECT
  current_setting( 'app.org_id',  true )::int,
  current_setting( 'app.unit_id', true )::int,
  current_setting( 'app.user_id', true )::int;
$$ LANGUAGE sql STABLE;

test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM rls_guard();
  END LOOP;
END $$;
DO
Time: 9886,400 ms (00:09,886)  #3

test=*&gt; DO $$
DECLARE
  i int;
BEGIN
  FOR i IN 1..1000000 LOOP
    PERFORM * FROM rls_test;
  END LOOP;
END $$;
DO
Time: 12695,723 ms (00:12,696)  #4 &lt;&lt;&lt;&lt;!!!!
</code></pre>
<p>From the above you can see that OUT work <strong>twice slower</strong>!!!</p>
<p>But why?? If it was supposed to be an <code>effectively another way of declaring a named OUT parameter</code>.</p>
<pre class=""lang-none prettyprint-override""><code>=&gt; select version();
                                                              version                                                              
-----------------------------------------------------------------------------------------------------------------------------------
 PostgreSQL 17.6 (Ubuntu 17.6-1.pgdg24.04+1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit
(1 row)
</code></pre>
<p>DB&lt;&gt;fiddle for <a href=""https://dbfiddle.uk/fz9L_wm0"" rel=""nofollow noreferrer""><code>OUT</code></a> and <a href=""https://dbfiddle.uk/uTkU1MT8"" rel=""nofollow noreferrer""><code>RETURNS TABLE</code></a> cases.</p>
<p><strong>UPD</strong><br />
As advised, I tried to optimize expression for POLICY. Though <code>OUT</code> and <code>RETURNS composite</code> works faster in compare to my original example, they still works slow. if number of rows increases by <code>x</code> rows in the table, then <code>RETURNS TABLE</code> lag by x seconds, but others lag by a factor of x. Eg. for 6 rows <code>RETURNS TABLE</code> takes 14s, <code>OUT/RETURNS composite</code> takes 69s.</p>
<p>I would agree will Adrian that execution time of functions <a href=""https://dbfiddle.uk/osNMgtbU"" rel=""nofollow noreferrer"">are pretty the same</a> (for <code>RETURNS TABLE</code> it takes 600ms slower per 1M calls) when we ignore the result. But accessing the result is an integral part of the process of working with functions and it is <a href=""https://dbfiddle.uk/xfy-qw75"" rel=""nofollow noreferrer""><strong>significantly different</strong></a> when we try to make use of it. At least it is twice faster for <code>RETURNS TABLE</code>.</p>
<p><a href=""https://pastebin.com/n3sxBxt6"" rel=""nofollow noreferrer"">Here</a> is long script with the test results if someone wants to reproduce this locally. From there we can see that for increased number of rows from 1 to 6 <code>RETURNS TABLE</code> speed degrades from 12.6 to 14.1 seconds and for <code>OUT/composite</code> speed degrades from 19.4 to 69.4 seconds.</p>
",3,4,1,2025-09-13T20:17:43+00:00,2,175,True
79763987,2326422,"Rural, IN",postgresql,Syntax battle using Postgres regex in query,"<p>This seems like it should be simple.</p>
<p>I have string data that looks like this:</p>
<pre><code>1-1-1-2 
1-1-1-2+ 
1-1-1-20 
1-1-1-21
</code></pre>
<p>I want to select just 1-1-1-2 and 1-1-1-2+ with the following query</p>
<pre><code>select * from dataset where id ~ '1-1-1-2\+*';
</code></pre>
<p>But when I run the query, it acts like a wildcard match:</p>
<pre><code>id |        surname        |      firstname      | birthdate  | deathdate
  |    service     
----------+-----------------------+---------------------+------------+----------
--+----------------
 1-1-1-20 | ? UNREADABLE-UNKNOWN  |                     | ?          |          
  | 
 1-1-1-2  | ROCKWELL              | Alphonzo P.         | 1841       | 1934     
  | Pvt. Civil War
 1-1-1-2+ | ROCKWELL              | Martha M. (DeSelms) | 1846       | 1929     
  | 
 1-1-1-21 | STEVENS               | Jessie Redmin       | 12/23/1874 | 11/20/187
6 | 
(4 rows)
</code></pre>
<p>It seems as if the + isn't being properly escaped.</p>
",0,1,1,2025-09-13T22:19:34+00:00,2,122,True
79764061,5489294,"Calgary, Canada",postgresql,How make @JoinColumns conditional?,"<p>I have the following scenario:</p>
<pre class=""lang-java prettyprint-override""><code>class AId {
String aId1;
String aId2;
}

@IdClass{AId.class}
class A {
@Id String aId1;
@Id String aId2;

String aType;  // possible values are 0,1

}

@IdClass{BId.class}
class B {
@Id String bId1;
@Id String bId2;

String bType; // possible values are 0,1,2

@ManyToOne (fetchType=&quot;EAGER&quot;)
@JoinColumn(name = bId1, referenceColumn = aId1)
@JoinColumn(name = bType, referenceColumn = aType)
A aOfB;      // B will have an A only if bType &lt;&gt; 2

}

</code></pre>
<p>So if <code>B={01,02,1}</code>, I want to perform the join with table A to obtain <code>A={01,03,1}</code><br />
But, if <code>B={01,02,2}</code> since <code>bType</code> is <code>2</code>, I know that it won't have an <code>A</code> element and I don't want to join when I fetch <code>B</code>. I am just unsure what the correct syntax is to achieve this.</p>
<p>Do I use <code>@JoinFormula</code>? But the examples that I've seen using <code>@JoinFormula</code> only relies on single column joins whereas in my case I need 2.</p>
<p>Update:
I was hoping that there would be a way to make the query dynamic. Example if bType&lt;&gt;2, I would have the following query:</p>
<pre><code>select
    b1_0.b_id1,
    b1_0.b_id2,
    a1_0.a_id1,
    a1_0.a_id2,
    a1_0.a_type,
    b1_0.bType 
from
    b_table b1_0 
left join
    a_table a1_0 
        on a1_0.a_id1=b1_0.b_id1 
        and a1_0.a_type=b1_0.b_type
where
    (
        b1_0.b_id1,b1_0.b_id2
    ) in ((?,?))
</code></pre>
<p>But if btype=2, then only:</p>
<pre><code>select
    b1_0.b_id1,
    b1_0.b_id2,
    b1_0.bType 
from
    b_table b1_0 
where
    (
        b1_0.b_id1,b1_0.b_id2
    ) in ((?,?))
</code></pre>
",1,1,0,2025-09-14T02:30:53+00:00,1,128,False
79764368,899021,,postgresql,Postgres FDW access on distant temporary tables,"<p>I set a local fwd server to connect to a distant host.
All works with existing distant tables I'm autorized to access to.
On the distant host I got no privileges to create tables but temporary ones.
When created I can access these temp tables when directly connecting on the host, they are on schema pg_temp_xx.
I now try to access them on my local fdw:</p>
<ul>
<li>first I import its definitions from the distant pg_temps_xx schema -&gt; all ok, on a named local schema (cause local pg_temp_xx is protected)</li>
<li>then when I select the distant temp table via my fwd server I got this message :
SQL Error [42501]: ERREUR: permission denied for schema pg_temp_xx</li>
</ul>
<p>I don't understand why with the same user I can access distant tables but not temporary ones (even firstly created on the fly). It's like privileges are depending on connection instance and not only role authorisation. Am I wrong anywhere?</p>
",0,0,0,2025-09-14T14:46:21+00:00,1,70,True
79764546,12108708,,postgresql,Laravel testing database table is being accessed by other users,"<p>I'm currently building a multi-tenant application using <code>stancl/tenancy</code> (every tenant uses their own database) in my Laravel application.</p>
<p>Whenever I'm writing tests, tests fail a lot due to the error</p>
<pre><code>Illuminate\Database\QueryException: SQLSTATE[55006]: Object in use: 7 FEHLER:  auf Datenbank »tenant_019949ce« wird von anderen Benutzern zugegriffen
DETAIL:  1 andere Sitzung verwendet die Datenbank. (Connection: tenant_host_connection, SQL: DROP DATABASE &quot;tenant_019949ce&quot;)
</code></pre>
<p>which means the table is used by another user. Tables are created when a tenant is created and should be deleted if a tenant gets deleted but it always fails in <code>tearDown</code> method.</p>
<p>My tests extend <code>TenancyTestCase</code> class:</p>
<pre><code>use Illuminate\Foundation\Testing\TestCase as BaseTestCase;

class TenancyTestCase extends BaseTestCase {

    use RefreshDatabase;

    private Tenant $tenant;

    protected function setUp(): void {
        parent::setUp();

        Config::set('tenancy.seeder_parameters.--class', TestDatabaseSeeder::class);

        $this-&gt;setupDefaultTenant();
        $this-&gt;forceRootUrl();

        $this-&gt;withoutVite();
    }

    private function setupDefaultTenant(): void {

        $this-&gt;tenant = Tenant::factory()-&gt;create();
        $this-&gt;tenant-&gt;domains()-&gt;save(Domain::factory([
            'domain' =&gt; 'tenant',
        ])-&gt;make());

        tenancy()-&gt;initialize($this-&gt;tenant);
    }

    private function forceRootUrl(): void {
        $parsed = parse_url(config('app.url'));
        $host = $parsed['host'] ?? 'localhost.test';
        $port = isset($parsed['port']) ? ':' . $parsed['port'] : '';

        URL::forceRootUrl('https://tenant.' . $host . $port);
    }

    public function tearDown(): void {
        tenancy()-&gt;end();
        $this-&gt;tenant-&gt;delete();

        parent::tearDown();
    }
}
</code></pre>
<p>I couldn't figure out why yet, any ideas on how to fix this?</p>
<p>My goal is to create a single database for each test and delete them if test is completed.</p>
<p>Thank you.</p>
<hr />
<p>EDIT:
If I comment <code>Jobs\DeleteDatabase::class</code> out, it works fine but the database will still persist which is not what I want. But tests are successful. So how to delete database correctly?</p>
<pre><code>Events\TenantDeleted::class =&gt; [
    JobPipeline::make([
        Jobs\DeleteDatabase::class,
    ])-&gt;send(function(Events\TenantDeleted $event) {
        return $event-&gt;tenant;
    })-&gt;shouldBeQueued(true),
],
</code></pre>
<p>I also found out that every tenants database that is created DURING the test itself (not in <code>setUp</code>) get's handled and deleted correctly. But not the original one.</p>
",0,0,0,2025-09-14T20:07:04+00:00,0,99,False
79764761,24888000,,postgresql,postgresql json arrays combine and group and sort,"<p>Please suggest me a solution to a certain problem. I have two tables. One is stored on one server, and the other on another. And I need to combine their data on daily statistics once in a while. The tables are identical in fields and structure. But I don't know how to combine the jsonb fields into one array by grouping them by some fields (cash_register field and products-&gt;productName field) and calculating the total number. This is what the two tables look like. Please note that the only difference in data will be in column info. And these are the columns that need to be combined correctly. Please help me write such a query.
My tables:</p>
<pre><code>+-------------+------------+---------------------------------------------------+
| employee_id |   date     |                        info                       |
+-------------+------------+---------------------------------------------------+
| 2223eb0f0d0x| 01/09/2025 | [
                                {
                                    &quot;cash_register&quot;: 1,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 4,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]                                |

| 2223eb0f0d0x| 02/09/2025 |   [
                                {
                                    &quot;cash_register&quot;: 1,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 3,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]     |

| 2223eb0f0d0x| 03/09/2025 |  [
                                {
                                    &quot;cash_register&quot;: 2,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 4,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]                                               |

| 2223eb0f0d0x| 04/09/2025 |   [
                                {
                                    &quot;cash_register&quot;: 1,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 4,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]                                               |
+-------------+------------+----------------------------------------------------+

+-------------+------------+--------------------------------------------------+
| employee_id |   date     |                    info                          |
+-------------+------------+--------------------------------------------------+
| 2223eb0f0d0x| 01/09/2025 | [
                                {
                                    &quot;cash_register&quot;: 2,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 4,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]                                |

| 2223eb0f0d0x| 02/09/2025 |   [
                                {
                                    &quot;cash_register&quot;: 1,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 2,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]     |

| 2223eb0f0d0x| 03/09/2025 |  [
                                {
                                    &quot;cash_register&quot;: 2,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 3,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]                                               |

| 2223eb0f0d0x| 04/09/2025 |   [
                                {
                                    &quot;cash_register&quot;: 1,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name1&quot;,
                                            &quot;count&quot;: 2
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                },
                                {
                                    &quot;cash_register&quot;: 4,
                                    &quot;products&quot;: [
                                        {
                                            &quot;productName&quot;: &quot;name8&quot;,
                                            &quot;count&quot;: 12
                                        },
                                        {
                                            &quot;productName&quot;: &quot;name2&quot;,
                                            &quot;count&quot;: 4
                                        }
                                    ]
                                }
                            ]                                               |
+-------------+------------+----------------------------------------------------+
</code></pre>
<p>result table (I made an example for one day, for the rest of the days the algorithm is the same):</p>
<pre><code>+-------------+------------+-------------------------------------------------+
| employee_id |   date     |                     info                        |
+-------------+------------+-------------------------------------------------+
| 2223eb0f0d0x| 01/09/2025 |  [
                                    {
                                        &quot;cash_register&quot;: 2,
                                        &quot;products&quot;: [
                                            {
                                                &quot;productName&quot;: &quot;name2&quot;,
                                                &quot;count&quot;: 4
                                            },
                                            {
                                                &quot;productName&quot;: &quot;name1&quot;,
                                                &quot;count&quot;: 2
                                            }
                                        ]
                                    },
                                    {
                                        &quot;cash_register&quot;: 4,
                                        &quot;products&quot;: [
                                            {
                                                &quot;productName&quot;: &quot;name8&quot;,
                                                &quot;count&quot;: 24
                                            },
                                            {
                                                &quot;productName&quot;: &quot;name2&quot;,
                                                &quot;count&quot;: 8
                                            }
                                        ]
                                    },
                                    {
                                        &quot;cash_register&quot;: 1,
                                        &quot;products&quot;: [
                                            {
                                                &quot;productName&quot;: &quot;name2&quot;,
                                                &quot;count&quot;: 4
                                            },
                                            {
                                                &quot;productName&quot;: &quot;name1&quot;,
                                                &quot;count&quot;: 2
                                            }
                                        ]
                                    }
                                ]                                |

+-------------+------------+-------------------------------------------------+
</code></pre>
<p>I am attaching a link to the query - <a href=""https://dbfiddle.uk/KEAm-g4s"" rel=""nofollow noreferrer"">https://dbfiddle.uk/KEAm-g4s</a></p>
",1,1,0,2025-09-15T05:21:51+00:00,1,81,True
79765477,5429268,United States,postgresql,Exception executing SQL update,"<p>I just restored a database using a different owner and database name. Upon starting the application, I'm getting the following error:</p>
<pre><code>    Exception executing SQL update &lt;ALTER TABLE \&quot;AO_9412A1_AOUSER\&quot; ADD CONSTRAINT       U_AO_9412A1_AOUSER_USERNAME UNIQUE (\&quot;USERNAME\&quot;)&gt;
org.postgresql.util.PSQLException: ERROR: could not create unique index &quot;u_ao_9412a1_aouser_username&quot;
  Detail: Key (&quot;USERNAME&quot;)=(8a48869174e61af50178d2469d490089) is duplicated
</code></pre>
<p>So, I assuming that '8a48869174e61af50178d2469d490089' is already in that table.</p>
<p>I do the following command</p>
<pre><code>select * from &quot;AO_9412A1_AOUSER&quot; where 'USERNAME' = '8a48869174e61af50178d2469d490089';
</code></pre>
<p>And I get</p>
<p>(0 rows)</p>
<p>Any tips?</p>
",0,0,0,2025-09-15T18:19:01+00:00,1,51,True
79765506,128967,"Los Angeles, CA",postgresql,Creating Postgres Users and Granting Permissions from Go,"<p>I'm trying to create database users in my Postgres database from Golang, and I can't seem to get queries to run.</p>
<p>I'm able to establish a connection, but I cannot run <code>create user X with password Y</code> and I've tried multiple ways of doing this, and searching the internet has not yielded any results. I'm building a database migrator wrapper utility and I need to manage these users manually as their credentials are being sourced from secrets. I'm using <code>pq</code> as my driver.</p>
<p>I've tried the following:</p>
<pre class=""lang-golang prettyprint-override""><code>// fails with syntax error re $
res, err := db.ExecContext(ctx, &quot;create user $1 with password $2&quot;, username, password)
// fails with syntax error re ?
res, err := db.ExecContext(ctx, &quot;create user ? with password ?&quot;, username, password)
</code></pre>
<p>The only examples I've found on the internet use <code>fmt.Sprintf</code> but obviously that's a bad idea due to SQL injection attacks.</p>
<p>Is there a specific function I need to be using when operating on Postgres users/roles/other items? I need to issue <code>grant</code> statements after this so I'm not sure if it will follow the same pattern.</p>
<p>I was unaware that <code>pg</code> is headed toward deprecation, so as recommended in the comments below, I have switched to <code>github.com/jackc/pgx/v5</code>. However, I'm still having issues and I can't see in the documentation how to accomplish what I'm looking to do.</p>
<p>I'm now operating with a <code>pgx.Conn</code> directly instead of through the <code>sql</code> facade so I can take full advantage of all of the features provided by <code>pgx</code>.</p>
<p>I'm running a local Docker container for <code>postgres:15.10</code> and I'm trying to create a user <code>janedoe</code> with a password of <code>password</code>.</p>
<p>Attempt 1: Use <code>pgx.Identifier</code> to Sanitize Both Username and Password</p>
<pre class=""lang-golang prettyprint-override""><code>queryTemplate := fmt.Sprintf(
    &quot;create user %s with password %s&quot;,
    pgx.Identifier{username}.Sanitize(),
    pgx.Identifier{password}.Sanitize(),
)

result, err := pgx.Exec(ctx, queryTemplate)
</code></pre>
<p>This returns <code>ERROR: syntax error at or near &quot;&quot;password&quot;&quot; (SQLSTATE 42601)</code>. I've tried including a final semicolon at the end of the statement and that doesn't fix things.</p>
<p>It seems that Postgres wants this string to not be double-quoted but single-quoted, and I don't see a way in <code>pgx</code> to specify the kind of escaping that I want.</p>
",1,1,0,2025-09-15T18:50:56+00:00,1,134,False
79765524,25868858,,postgresql,Tortoise ORM postgresql Encryption,"<p>I figured out how to decrypt fields</p>
<pre><code>class _Decrypt(Function):
    def __init__(self, term: Any, key, alias: str | None = None) -&gt; None:
        super().__init__(&quot;pgp_sym_decrypt&quot;, term, key, alias=alias)

class Decrypt(Aggregate):
    database_func = _Decrypt
</code></pre>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>await models.UserPhone.annotate(phone_decrypt=Decrypt(&quot;phone&quot;, key)).filter(phone_decrypt=phone)
</code></pre>
<p>Result:</p>
<pre class=""lang-sql prettyprint-override""><code>DEBUG:tortoise.db_client:SELECT pgp_sym_decrypt(&quot;phone&quot;,$1) &quot;phone_decrypt&quot;,1 FROM &quot;users_phones&quot; WHERE pgp_sym_decrypt(&quot;phone&quot;,$2)=$3 LIMIT $4: ['...', '...', '...', 1]
</code></pre>
<p>But i cant encrypt values while creating</p>
<p>I tried</p>
<pre class=""lang-py prettyprint-override""><code>class Encrypt(Function): # NOT WORKING
    def __init__(self, term: Any, key, alias: str | None = None) -&gt; None:
        super().__init__(&quot;pgp_sym_encrypt&quot;, term, key, alias=alias)

await models.UserPhone.create(phone=Encrypt(phone, key))
</code></pre>
<p>But this generating me only string containing my function</p>
<pre class=""lang-sql prettyprint-override""><code>DEBUG:tortoise.db_client:INSERT INTO &quot;users_phones&quot; (&quot;id&quot;,&quot;phone&quot;) VALUES ($1,$2): ['id', &quot;pgp_sym_encrypt('...','...')&quot;]
</code></pre>
<p>After it inserts second param tortoise throws an Exception</p>
<pre><code>tortoise.exceptions.OperationalError: invalid input for query argument $2: &quot;pgp_sym_encrypt('...','... (a bytes-like object is required, not 'str')
</code></pre>
<p>I think this is because function is not beeing called and interpreted as str</p>
<p>I tried to make get_sql() function in Encrypt, but its never beeing called</p>
<p>__str__ &lt;- this is called out then i pass Encrypt() as a value</p>
<p>function_cast() didnt work either because its being called then selecting a field
I need to encrypt only then creating or updating</p>
<p>So do any one have ideas on how to force tortoise use function instead of converting it to str?</p>
<p>Thanks for your attention ♥</p>
",1,1,0,2025-09-15T19:09:18+00:00,1,51,False
79765624,31496851,,postgresql,"Query that checks if an item name contains a substring, and if true, perform a second query that grabs an image with a specific name","<p>I have this simple query that makes an image appear in a report based on its name</p>
<pre><code>SELECT image_data 
FROM image 
WHERE ((image_name='logo'));
</code></pre>
<p>I was able to modify this query so that if the name of the image exactly matches the item number listed on the report, the image will appear. This way I'm not limited to one specific image.</p>
<pre><code>SELECT image_data
FROM image
WHERE (image_name = (SELECT item_number FROM wo 
         JOIN itemsite ON wo_itemsite_id=itemsite_id
         JOIN item ON itemsite_item_id=item_id
WHERE wo_id=&lt;? value(&quot;wo_id&quot;) ?&gt;));
</code></pre>
<p>Now I'm trying to figure out how to modify it further so that the image name doesn't have to be a one to one match.</p>
<p>I'm imagining a case expression like this</p>
<pre><code>SELECT image_data
FROM image
WHERE (image_name = (
    SELECT item_number, 
        CASE
            WHEN item_number LIKE 'ITEM_A%' THEN 'ITEM A IMAGE'
            WHEN item_number LIKE 'ITEM_B%' THEN 'ITEM B IMAGE' 
            ELSE ''
        END 
    FROM wo
         JOIN itemsite ON wo_itemsite_id=itemsite_id
         JOIN item ON itemsite_item_id=item_id
    WHERE wo_id=&lt;? value(&quot;wo_id&quot;) ?&gt;
));
</code></pre>
<p>Unfortunately this doesn't seem to work. I've tried putting the case expression inside &quot;WHERE&quot;, &quot;FROM&quot;, and &quot;SELECT&quot;, but no success. I even tried making two separate queries but that didn't work either. What am I doing wrong?</p>
<p>9/16/25 EDIT:
I think Zegarek's solution is what I'm looking for but I still can't get it to work. It might just be a syntax error on my end though.</p>
<p>my original modified query had some other JOINs that I initially cut out of my question for readability. I'm VERY new to SQL so I thought showing those JOINs might make things more confusing. I added them back in up above.</p>
<p>Here's how I tried to implement Zegarek's solution</p>
<pre><code>SELECT a.image_data,
      CASE WHEN b.item_number LIKE 'PRODUCT-ANLG%' THEN 'PRODUCT ANALOG DATASHEET PAGE 1'
            WHEN b.item_number LIKE 'PRODUCT 21%' THEN 'PRODUCT 21 DATASHEET' 
            ELSE ''
       END AS image_label
FROM image AS a JOIN wo AS b ON a.image_name=b.item_number
          JOIN itemsite ON wo_itemsite_id=itemsite_id
                  JOIN item ON itemsite_item_id=item_id 
WHERE (b.wo_id = &lt;? value(&quot;wo_id&quot;) ?&gt;);
</code></pre>
<p>I then named the query &quot;DataSheetImage&quot; and then called the image_data column on the report.
<a href=""https://i.sstatic.net/TvQKhDJj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TvQKhDJj.png"" alt=""enter image description here"" /></a></p>
<p>Unfortunately this still didn't work. I tried calling image_data, image_label,  a.image_data, and a.image_label, but the image won't appear.</p>
<p>I'm thinking the problem is either
A. a syntax issue with the JOINs.
or
B. me not understanding how the &quot;image_label&quot; alias works and how to properly call it.</p>
<p>Where am I going wrong? What am I misunderstanding here?</p>
<p>SOLUTION EDIT: I finally was able to get the query working with a lot of help from Zegarek. Here is the solution that ended up working for me in Postgresql 9.6</p>
<pre><code>SELECT image_data
FROM image
WHERE (image_name = (SELECT image_name
         FROM wo
         JOIN itemsite ON wo_itemsite_id=itemsite_id
         JOIN item ON itemsite_item_id=item_id
INNER JOIN image ON (REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')=REGEXP_REPLACE(item_number,'^PRODUCT.(\w+).*','\1'))
         WHERE wo_id= &lt;? value(&quot;wo_id&quot;) ?&gt; ));
</code></pre>
<p>The solution was to JOIN the wo and image tables together and then compare image_name and item_number using REGEXP_REPLACE. I don't fully understand how the REGEXP_REPLACE works yet, but I think I can figure out the rest from here.</p>
<p>Here is the db fiddle showing how the tables look <a href=""https://dbfiddle.uk/-VaSugr5"" rel=""nofollow noreferrer"">https://dbfiddle.uk/-VaSugr5</a></p>
<p>Thank you so much to Zegarek for helping me figure this out.</p>
",1,1,0,2025-09-15T21:18:27+00:00,1,170,True
79765852,7529280,,postgresql,MassTransit throws errors in log,"<p>In my solution that contains a project which should handle long jobs with MassTransit MassTransit produces weird error logs.</p>
<p>When I start the project in my container, everything seems to work fine, but the logs contain such entries sometimes. The service is being replicated via Docker Swarm to have multiple parallel instances working on potentially long running jobs.</p>
<p>Thats the RabbitMq config in my Program.cs</p>
<pre><code>cfg.UsingRabbitMq((context, rabbitCfg) =&gt;
{
    rabbitCfg.UseDelayedMessageScheduler();

    rabbitCfg.PrefetchCount = 1;
    rabbitCfg.Exclusive = false;
    rabbitCfg.Durable = true;

    rabbitCfg.Host(appSettings.BrokerSettings.HostName, &quot;/&quot;, h =&gt;
    {
        h.Username(appSettings.BrokerSettings.UserName);
        h.Password(appSettings.BrokerSettings.Password);
        h.Heartbeat(600);
    });

    rabbitCfg.ConfigureEndpoints(context);
});
</code></pre>
<p>And thats the config for a consumer:</p>
<pre><code>c.Options&lt;JobOptions&lt;UpdatePropertiesMessage&gt;&gt;(o =&gt; o
    .SetJobTimeout(TimeSpan.FromHours(5))
    .SetConcurrentJobLimit(1))
    .SetRetry(r =&gt; r.Interval(3, TimeSpan.FromMinutes(1)));
</code></pre>
<p>Thats the log:</p>
<pre><code>fail: Microsoft.EntityFrameworkCore.Database.Transaction[20205]

      An error occurred using a transaction.

fail: Microsoft.EntityFrameworkCore.Database.Transaction[20205]

      An error occurred using a transaction.

warn: MassTransit.ReceiveTransport[0]

      Transaction rollback failed

      System.InvalidOperationException: This NpgsqlTransaction has completed; it is no longer usable.

         at Npgsql.ThrowHelper.ThrowInvalidOperationException(String message)

         at Npgsql.NpgsqlTransaction.Rollback(Boolean async, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.RollbackAsync(CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.RollbackAsync(CancellationToken cancellationToken)

         at MassTransit.EntityFrameworkCoreIntegration.Saga.EntityFrameworkSagaRepositoryContextFactory`1.&lt;WithinTransaction&gt;g__Rollback|11_0[T](IDbContextTransaction transaction) in /_/src/Persistence/MassTransit.EntityFrameworkCoreIntegration/EntityFrameworkCoreIntegration/Saga/EntityFrameworkSagaRepositoryContextFactory.cs:line 181

warn: MassTransit.ReceiveTransport[0]

      R-RETRY rabbitmq://rabbitmq/JobType 01000000-0a00-0242-c5d8-08ddf4ea9d17 MassTransit.RetryPolicies.RetryConsumeContext&lt;MassTransit.Contracts.JobService.SetConcurrentJobLimit&gt;

      Npgsql.PostgresException (0x80004005): 40001: could not serialize access due to read/write dependencies among transactions

      

      DETAIL: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.

         at Npgsql.Internal.NpgsqlConnector.ReadMessageLong(Boolean async, DataRowLoadingMode dataRowLoadingMode, Boolean readingNotifications, Boolean isReadingPrependedMessage)

         at System.Runtime.CompilerServices.PoolingAsyncValueTaskMethodBuilder`1.StateMachineBox`1.System.Threading.Tasks.Sources.IValueTaskSource&lt;TResult&gt;.GetResult(Int16 token)

         at Npgsql.Internal.NpgsqlConnector.ExecuteInternalCommand(Byte[] data, Boolean async, CancellationToken cancellationToken)

         at Npgsql.NpgsqlTransaction.Commit(Boolean async, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.CommitAsync(CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.CommitAsync(CancellationToken cancellationToken)

         at MassTransit.EntityFrameworkCoreIntegration.Saga.EntityFrameworkSagaRepositoryContextFactory`1.WithinTransaction[T](DbContext context, CancellationToken cancellationToken, Func`1 callback) in /_/src/Persistence/MassTransit.EntityFrameworkCoreIntegration/EntityFrameworkCoreIntegration/Saga/EntityFrameworkSagaRepositoryContextFactory.cs:line 193

         at MassTransit.EntityFrameworkCoreIntegration.Saga.EntityFrameworkSagaRepositoryContextFactory`1.WithinTransaction[T](DbContext context, CancellationToken cancellationToken, Func`1 callback) in /_/src/Persistence/MassTransit.EntityFrameworkCoreIntegration/EntityFrameworkCoreIntegration/Saga/EntityFrameworkSagaRepositoryContextFactory.cs:line 210

         at MassTransit.EntityFrameworkCoreIntegration.Saga.EntityFrameworkSagaRepositoryContextFactory`1.WithinTransaction[T](DbContext context, CancellationToken cancellationToken, Func`1 callback) in /_/src/Persistence/MassTransit.EntityFrameworkCoreIntegration/EntityFrameworkCoreIntegration/Saga/EntityFrameworkSagaRepositoryContextFactory.cs:line 210

         at MassTransit.EntityFrameworkCoreIntegration.Saga.EntityFrameworkSagaRepositoryContextFactory`1.Send[T](ConsumeContext`1 context, IPipe`1 next) in /_/src/Persistence/MassTransit.EntityFrameworkCoreIntegration/EntityFrameworkCoreIntegration/Saga/EntityFrameworkSagaRepositoryContextFactory.cs:line 81

         at MassTransit.EntityFrameworkCoreIntegration.Saga.EntityFrameworkSagaRepositoryContextFactory`1.Send[T](ConsumeContext`1 context, IPipe`1 next) in /_/src/Persistence/MassTransit.EntityFrameworkCoreIntegration/EntityFrameworkCoreIntegration/Saga/EntityFrameworkSagaRepositoryContextFactory.cs:line 86

         at MassTransit.DependencyInjection.DependencyInjectionSagaRepositoryContextFactory`1.Send[T](ConsumeContext`1 context, Func`3 send) in /_/src/MassTransit/DependencyInjection/DependencyInjection/DependencyInjectionSagaRepositoryContextFactory.cs:line 59

         at MassTransit.Middleware.CorrelatedSagaFilter`2.Send(ConsumeContext`1 context, IPipe`1 next) in /_/src/MassTransit/Middleware/CorrelatedSagaFilter.cs:line 45

         at MassTransit.Middleware.CorrelatedSagaFilter`2.Send(ConsumeContext`1 context, IPipe`1 next) in /_/src/MassTransit/Middleware/CorrelatedSagaFilter.cs:line 62

         at MassTransit.Middleware.InMemoryOutboxFilter`2.Send(TContext context, IPipe`1 next) in /_/src/MassTransit/Middleware/InMemoryOutboxFilter.cs:line 35

         at MassTransit.Middleware.InMemoryOutboxFilter`2.Send(TContext context, IPipe`1 next) in /_/src/MassTransit/Middleware/InMemoryOutboxFilter.cs:line 45

         at MassTransit.Middleware.ScopeMessageFilter`1.Send(ConsumeContext`1 context, IPipe`1 next) in /_/src/MassTransit/Middleware/ScopeMessageFilter.cs:line 22

         at MassTransit.Middleware.ScopeMessageFilter`1.Send(ConsumeContext`1 context, IPipe`1 next) in /_/src/MassTransit/Middleware/ScopeMessageFilter.cs:line 22

         at MassTransit.Middleware.RetryFilter`1.MassTransit.IFilter&lt;TContext&gt;.Send(TContext context, IPipe`1 next) in /_/src/MassTransit/Middleware/RetryFilter.cs:line 47

        Exception data:

          Severity: ERROR

          SqlState: 40001

          MessageText: could not serialize access due to read/write dependencies among transactions

          Detail: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.

          Hint: The transaction might succeed if retried.

          File: predicate.c

          Line: 4662

          Routine: PreCommit_CheckForSerializationFailure

info: Microsoft.EntityFrameworkCore.Database.Command[20101]

      Executed DbCommand (0ms) [Parameters=[p0='?' (DbType = Guid)], CommandType='Text', CommandTimeout='30']

      SELECT m.&quot;CorrelationId&quot;, m.&quot;ActiveJobCount&quot;, m.&quot;ActiveJobs&quot;, m.&quot;ConcurrentJobLimit&quot;, m.&quot;CurrentState&quot;, m.&quot;GlobalConcurrentJobLimit&quot;, m.&quot;Instances&quot;, m.&quot;Name&quot;, m.&quot;OverrideJobLimit&quot;, m.&quot;OverrideLimitExpiration&quot;, m.&quot;Properties&quot;


      FROM (

          SELECT *, xmin FROM &quot;JobTypeSaga&quot; WHERE &quot;CorrelationId&quot; = @p0 FOR UPDATE


      ) AS m

      LIMIT 2

fail: Microsoft.EntityFrameworkCore.Database.Command[20102]

      Failed executing DbCommand (0ms) [Parameters=[@p1='?' (DbType = Guid), @p0='?'], CommandType='Text', CommandTimeout='30']

      UPDATE &quot;JobTypeSaga&quot; SET &quot;Instances&quot; = @p0

      WHERE &quot;CorrelationId&quot; = @p1;

fail: Microsoft.EntityFrameworkCore.Update[10000]

      An exception occurred in the database while saving changes for context type 'MassTransit.EntityFrameworkCoreIntegration.JobServiceSagaDbContext'.

      System.InvalidOperationException: An exception has been raised that is likely due to a transient failure.

       ---&gt; Microsoft.EntityFrameworkCore.DbUpdateException: An error occurred while saving the entity changes. See the inner exception for details.

       ---&gt; Npgsql.PostgresException (0x80004005): 40001: could not serialize access due to read/write dependencies among transactions

      

      DETAIL: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.

         at Npgsql.Internal.NpgsqlConnector.ReadMessageLong(Boolean async, DataRowLoadingMode dataRowLoadingMode, Boolean readingNotifications, Boolean isReadingPrependedMessage)

         at System.Runtime.CompilerServices.PoolingAsyncValueTaskMethodBuilder`1.StateMachineBox`1.System.Threading.Tasks.Sources.IValueTaskSource&lt;TResult&gt;.GetResult(Int16 token)

         at Npgsql.NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)

         at Npgsql.NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)

         at Npgsql.NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)

         at Npgsql.NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)

         at Npgsql.NpgsqlCommand.ExecuteDbDataReaderAsync(CommandBehavior behavior, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReaderAsync(RelationalCommandParameterObject parameterObject, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReaderAsync(RelationalCommandParameterObject parameterObject, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.ReaderModificationCommandBatch.ExecuteAsync(IRelationalConnection connection, CancellationToken cancellationToken)

        Exception data:

          Severity: ERROR

          SqlState: 40001

          MessageText: could not serialize access due to read/write dependencies among transactions

          Detail: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.

          Hint: The transaction might succeed if retried.

          File: predicate.c

          Line: 4603

          Routine: OnConflict_CheckForSerializationFailure

         --- End of inner exception stack trace ---

         at Microsoft.EntityFrameworkCore.Update.ReaderModificationCommandBatch.ExecuteAsync(IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(IEnumerable`1 commandBatches, IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(IEnumerable`1 commandBatches, IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(IEnumerable`1 commandBatches, IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IList`1 entriesToSave, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(StateManager stateManager, Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)

         at Npgsql.EntityFrameworkCore.PostgreSQL.Storage.Internal.NpgsqlExecutionStrategy.ExecuteAsync[TState,TResult](TState state, Func`4 operation, Func`4 verifySucceeded, CancellationToken cancellationToken)

         --- End of inner exception stack trace ---

         at Npgsql.EntityFrameworkCore.PostgreSQL.Storage.Internal.NpgsqlExecutionStrategy.ExecuteAsync[TState,TResult](TState state, Func`4 operation, Func`4 verifySucceeded, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)

      System.InvalidOperationException: An exception has been raised that is likely due to a transient failure.

       ---&gt; Microsoft.EntityFrameworkCore.DbUpdateException: An error occurred while saving the entity changes. See the inner exception for details.

       ---&gt; Npgsql.PostgresException (0x80004005): 40001: could not serialize access due to read/write dependencies among transactions

      

      DETAIL: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.

         at Npgsql.Internal.NpgsqlConnector.ReadMessageLong(Boolean async, DataRowLoadingMode dataRowLoadingMode, Boolean readingNotifications, Boolean isReadingPrependedMessage)

         at System.Runtime.CompilerServices.PoolingAsyncValueTaskMethodBuilder`1.StateMachineBox`1.System.Threading.Tasks.Sources.IValueTaskSource&lt;TResult&gt;.GetResult(Int16 token)

         at Npgsql.NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)

         at Npgsql.NpgsqlDataReader.NextResult(Boolean async, Boolean isConsuming, CancellationToken cancellationToken)

         at Npgsql.NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)

         at Npgsql.NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)

         at Npgsql.NpgsqlCommand.ExecuteDbDataReaderAsync(CommandBehavior behavior, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReaderAsync(RelationalCommandParameterObject parameterObject, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReaderAsync(RelationalCommandParameterObject parameterObject, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.ReaderModificationCommandBatch.ExecuteAsync(IRelationalConnection connection, CancellationToken cancellationToken)

        Exception data:

          Severity: ERROR

          SqlState: 40001

          MessageText: could not serialize access due to read/write dependencies among transactions

          Detail: Detail redacted as it may contain sensitive data. Specify 'Include Error Detail' in the connection string to include this information.

          Hint: The transaction might succeed if retried.

          File: predicate.c

          Line: 4603

          Routine: OnConflict_CheckForSerializationFailure

         --- End of inner exception stack trace ---

         at Microsoft.EntityFrameworkCore.Update.ReaderModificationCommandBatch.ExecuteAsync(IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(IEnumerable`1 commandBatches, IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(IEnumerable`1 commandBatches, IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(IEnumerable`1 commandBatches, IRelationalConnection connection, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IList`1 entriesToSave, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(StateManager stateManager, Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)

         at Npgsql.EntityFrameworkCore.PostgreSQL.Storage.Internal.NpgsqlExecutionStrategy.ExecuteAsync[TState,TResult](TState state, Func`4 operation, Func`4 verifySucceeded, CancellationToken cancellationToken)

         --- End of inner exception stack trace ---

         at Npgsql.EntityFrameworkCore.PostgreSQL.Storage.Internal.NpgsqlExecutionStrategy.ExecuteAsync[TState,TResult](TState state, Func`4 operation, Func`4 verifySucceeded, CancellationToken cancellationToken)

         at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
</code></pre>
",1,1,0,2025-09-16T06:41:23+00:00,0,73,False
79765980,16229462,,postgresql,Prisma P1000 Authentication Failed with Docker PostgreSQL - Complete Troubleshooting Guide,"<p>I'm encountering a persistent P1000 Authentication Failed error when trying to connect Prisma to a PostgreSQL database running in Docker. Despite having correct credentials and a running container, Prisma migrations fail with authentication errors.</p>
<p><strong>File Structure</strong></p>
<pre><code>ecomm/
├── prisma/
│   └── schema.prisma
├── src/
│   ├── generated/           # (if using custom Prisma output)
│   ├── bin.ts
│   ├── index.ts
│   └── db.ts
├── dist/                    # (generated after build)
│   ├── bin.js
│   ├── index.js
│   └── db.js
├── node_modules/
├── .env
├── .gitignore
├── package.json
├── package-lock.json
├── tsconfig.json
└── README.md
</code></pre>
<p><strong>package.json:</strong></p>
<pre><code>{
  &quot;name&quot;: &quot;ecomm&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;scripts&quot;: {
    &quot;build&quot;: &quot;prisma generate &amp;&amp; tsc&quot;,
    &quot;start&quot;: &quot;node dist/bin.js&quot;,
    &quot;dev&quot;: &quot;ts-node src/bin.ts&quot;
  },
  &quot;type&quot;: &quot;commonjs&quot;,
  &quot;dependencies&quot;: {
    &quot;@prisma/client&quot;: &quot;^6.16.1&quot;,
    &quot;express&quot;: &quot;^5.1.0&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;@types/express&quot;: &quot;^5.0.3&quot;,
    &quot;@types/node&quot;: &quot;^20.0.0&quot;,
    &quot;prisma&quot;: &quot;^6.16.1&quot;,
    &quot;typescript&quot;: &quot;^5.0.0&quot;,
    &quot;ts-node&quot;: &quot;^10.9.0&quot;
  }
}
</code></pre>
<p><strong>prisma schema</strong></p>
<pre><code>generator client {
  provider = &quot;prisma-client-js&quot;
  output   = &quot;../src/generated/prisma&quot;
}

datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}
model Request{
  id Int @id @default(autoincrement())
  a  Int
  b  Int
  answer Int
  type Type
}

enum Type{
  ADD
  MUL
}
</code></pre>
<p><strong>.env:</strong></p>
<pre><code>DATABASE_URL=&quot;postgresql://postgres:mysecretpassword@localhost:5432/postgres&quot;
</code></pre>
<p><strong>Error Details</strong>
Command that fails:</p>
<pre><code>npx prisma migrate dev --name init
</code></pre>
<p>Error output:</p>
<pre><code>Environment variables loaded from .env
Prisma schema loaded from prisma\schema.prisma
Datasource &quot;db&quot;: PostgreSQL database &quot;postgres&quot;, schema &quot;public&quot; at &quot;localhost:5432&quot;
Error: P1000: Authentication failed against database server, the provided database credentials for `postgres` are not valid.
Please make sure to provide valid database credentials for the database server at the configured address.
</code></pre>
<p><strong>Docker commands I've used:</strong></p>
<pre><code># Start PostgreSQL container
docker run -p 5432:5432 -e POSTGRES_PASSWORD=mysecretpassword -d postgres

# Verify container is running
docker ps
CONTAINER ID   IMAGE      COMMAND                  CREATED          STATUS          PORTS                    NAMES
f1e171459a61   postgres   &quot;docker-entrypoint.s…&quot;   26 minutes ago   Up 26 minutes   0.0.0.0:5432-&gt;5432/tcp   sweet_chaplygin
</code></pre>
<p><strong>What I've Tried</strong></p>
<ol>
<li><p>Verified container is running - docker ps shows the container is up</p>
</li>
<li><p>Double-checked credentials - Password matches in both Docker command
and DATABASE_URL</p>
</li>
<li><p>Tested different connection strings - Tried various formats with
same result</p>
</li>
<li><p>Restarted containers multiple times - Same error persists</p>
</li>
<li><p>Checked for port conflicts - Port 5432 appears to be free</p>
</li>
</ol>
",1,1,0,2025-09-16T08:33:18+00:00,0,75,False
79766781,1241610,Texas,postgresql,How to pass an array in sqlc to a database query with an &quot;IN&quot; operator,"<p>I have been struggling with passing an array/slice from Golang to a sqlc (postgres) database query <code>WHERE</code> clause that has an <code>IN</code> operator. There is very little documentation around this function and it leaves some info out (such as how you would mark what kind of array it is with something like <code>::text[]</code>). Whether I tried:</p>
<p><code>table.jsonb_col-&gt;&gt;'some_id' IN (sqlc.slice('list_of_ids')::text[])</code> or</p>
<p><code>table.jsonb_col-&gt;&gt;'some_id' IN (@list_of_ids::text[])</code></p>
<p>It didn't matter and I would always get the error</p>
<blockquote>
<p>ERROR: operator does not exist: text = text[] (SQLSTATE 42883)</p>
</blockquote>
<p>What I found out later is that it seems go is passing an array to the sql statement, but the sql is expecting a set of rows (or values, I'm not sure on the terminology) or something equivalent to that.</p>
<p>From what little I could find about the subject this is the way to do it, but it just didn't work.</p>
",2,2,0,2025-09-17T00:23:37+00:00,1,231,True
79767171,20213784,,postgresql,DB creation fails through the Azure web app with Azure Postgres Flexible Server,"<p>I have an Azure Postgres flexible server sitting in a VNet divided into multiple subnets. In a distinct subnet lies an azure app service I am trying to run by publishing it via gitlab. However shortly after launch, this app crashes due to the missing database schema. Basically I can connect to the database via the Azure web app (the <code>CanConnect()</code> check does not throw an exception), but any EF Core operation fails with an error:</p>
<blockquote>
<p>the table XX.YY does not exist</p>
</blockquote>
<p>Here is the faulty code:</p>
<pre class=""lang-cs prettyprint-override""><code>XXDbContext dbContext = scope.ServiceProvider.GetRequiredService&lt;XXDbContext&gt;();

// passes
var canConnect = await dbContext.Database.CanConnectAsync();

if (!canConnect)
{
    throw new ApplicationException(&quot;Cannot establish connection to the database&quot;);
}

var pendingMigrations = (await dbContext.Database.GetPendingMigrationsAsync()).ToList();
logger.LogInformation($&quot;Applying pending migrations in environment...&quot;);

foreach (var mug in pendingMigrations)
{
    logger.LogInformation(mug);
}

// fails to do anything
if (await TableExistsAsync(dbContext, &quot;Users&quot;, &quot;BL&quot;) == false)
{
    logger.LogWarning(&quot;Database tables do not exist. Creating..&quot;);
    dbContext.Database.EnsureCreated();
    logger.LogWarning(&quot;Database schema created successfully&quot;);
}

if (pendingMigrations.Any())
{
    var migrationsList = string.Join(&quot;, &quot;, pendingMigrations);
    logger.LogWarning($&quot;Database has pending migrations: {migrationsList}&quot;);
    await dbContext.Database.MigrateAsync();
    logger.LogInformation(&quot;Migrations applied successfully&quot;);
}

// crashes
var userCount = await dbContext.Users.CountAsync();
</code></pre>
<p>I have tried another approaches with</p>
<pre><code>var script = dbContext.Database.GenerateCreateScript();
dbContext.Database.ExecuteSqlRaw(script);
</code></pre>
<p>It also fails, this time with an SQL formatting error:</p>
<blockquote>
<p>FATAL: Database validation failed. The application cannot start.System.FormatException: Index (zero based) must be greater than or equal to zero and less than the size of the argument list.at System.Text.ValueStringBuilder.AppendFormatHelper</p>
</blockquote>
<p>For sake of completeness, here is an anonymized version of the connection string I use:</p>
<pre><code>Server=XXX.postgres.database.azure.com;Database=postgres;Port=5432;User Id=admin;Password=YYY;Ssl Mode=Require;
</code></pre>
<p>I can't execute any query directly in the Azure Portal as no query editor is available there, and I would greatly prefer to have some automatic behaviour here.</p>
<p>Is my only option to create a VM with bastion just to be able to execute migration SQL scripts?</p>
<p>Thank you for reading my question!</p>
",1,1,0,2025-09-17T10:23:57+00:00,0,40,False
79767372,2702630,"Tilburg, Netherlands",postgresql,FlywayDB doesn&#39;t create tables in specified schema,"<p>I'm using FlywayDB v11.13.0 (latest as from now) and am targeting it against a PostgreSQL 17.6 database. I specify a schema like this:</p>
<pre class=""lang-java prettyprint-override""><code>String connectionUrl = UriBuilder.fromUri(connectionUrl).replaceQueryParam(&quot;currentSchema&quot;, dbSchema).build().toString();

Flyway.configure()
            .cleanDisabled(false)
            .validateMigrationNaming(true)
            .dataSource(connectionUrl, databaseUsername, databasePassword)
            .defaultSchema(schemaName)
            .schemas(schemas.toArray(new String[0]))
            .locations(locations.toArray(new String[0]))
            .initSql(initSql.toString())
            .baselineOnMigrate(true)
            .outOfOrder(outOfOrder)
            .load();
</code></pre>
<p>And this is a piece of a migration script:</p>
<pre class=""lang-sql prettyprint-override""><code>create table ASSET (
  ID                 varchar(22)              not null,
  ATTRIBUTES         jsonb,
  CREATED_ON         timestamp with time zone not null,
  NAME               varchar(1023)            not null,
  PARENT_ID          varchar(22),
  PATH               ltree,
  REALM              varchar(255)             not null,
  TYPE               varchar(500)             not null,
  ACCESS_PUBLIC_READ boolean                  not null,
  VERSION            int8                     not null,
  primary key (ID),
  check (ID != PARENT_ID)
);
</code></pre>
<p>But the table is created in the <code>public</code> schema instead of my specific schema. Ive also tried adding to use the schema in the initSQL:</p>
<pre class=""lang-java prettyprint-override""><code>initSql.append(&quot;CREATE SCHEMA IF NOT EXISTS &quot;).append(schemaName).append(&quot;;&quot;);
initSql.append(&quot;SET search_path TO &quot;).append(schemaName).append(&quot;, public;&quot;);

</code></pre>
<p>But that doesn't help either.</p>
<p>The only thing what works is adding the <code>SET search_path TO </code> statement in the migration script itself, something which I don't prefer, specially if FlywayDB seems to support specifying schema's.</p>
<p>My question am I missing something here or is there a bug in FlywayDB?</p>
",0,0,0,2025-09-17T13:12:16+00:00,0,79,False
79767989,25886068,,postgresql,Ensure macaddr value is unique across columns,"<p>I have a table that represents the <a href=""https://www.postgresql.org/docs/current/datatype-net-types.html#DATATYPE-NET-TYPES-TABLE"" rel=""nofollow noreferrer"">MAC addresses</a> of some devices. The table has a column for Ethernet, bluetooth, &amp; WiFi, all nullable:</p>
<pre class=""lang-sql prettyprint-override""><code>create table device (
   id bigint generated by default as identity primary key
  ,wifi_mac macaddr8 unique
  ,ethernet_mac macaddr8 unique
  ,bluetooth_mac macaddr8 unique);
</code></pre>
<p>I have unique constraints in place that ensures each MAC address is unique, but that only works for each category of address. This does not enforce that an ethernet address cannot also exist in the WiFi column.</p>
<p>Using a multi-column unique constraint doesn't work either as it only ensure that the <em>combination</em> of MAC addresses are unique.</p>
<p>How can I ensure uniqueness of individual entries across these three columns?</p>
",-3,2,5,2025-09-18T04:45:32+00:00,2,149,True
79768381,1966792,"Chennai, India",postgresql,What is equivalant command in Oracle POSTGRESQL copy command,"<p>POSTGRESQL having command like copy to export CSV file , like any command is available in Oracle.</p>
<p>Example for POSTGRESQL
Copy (select * from emp) to 'D:/test.csv' with delimiter ',' csv header;</p>
",-1,0,1,2025-09-18T11:49:41+00:00,0,42,False
79768622,23205395,,postgresql,"asyncio.run with SQLAlchemy asyncio sessions works only once, then raises &quot;Future attached to a different loop&quot; and &quot;another operation is in progress&quot;","<p>I’m running into an issue when using asyncio.run together with SQLAlchemy (async) inside a Celery task.</p>
<p>When I call the function the first time, it works fine.
On the second call, I get:</p>
<pre><code>RuntimeError: Task &lt;Task pending name='Task-7' coro=&lt;test_db_tst() running at /app/test_asyncio_error.py:37&gt; cb=[_run_until_complete_cb() at /usr/local/lib/python3.11/asyncio/base_events.py:181]&gt; got Future &lt;Future pending cb=[BaseProtocol._on_waiter_completed()]&gt; attached to a different loop
</code></pre>
<p>On the third and subsequent calls, I get a database error:</p>
<pre><code>sqlalchemy.exc.InterfaceError: (sqlalchemy.dialects.postgresql.asyncpg.InterfaceError) &lt;class 'asyncpg.exceptions._base.InterfaceError'&gt;: cannot perform operation: another operation is in progress
[SQL: SELECT 1]
</code></pre>
<p>My DB setup is standard:</p>
<pre><code>class Database:
    def __init__(self, host: str):
        try:
            self.url = host.split('@')[1]
        except IndexError:
            self.url = host
        self._engine = create_async_engine(host, echo=settings.DEBUG)
        self._session_factory = async_sessionmaker(bind=self._engine, expire_on_commit=False)

    @property
    def engine(self) -&gt; AsyncEngine:
        return self._engine

    @property
    def session_factory(self) -&gt; async_sessionmaker[AsyncSession]:
        return self._session_factory


db_tst = Database(settings.db_url)

async def test_db_tst():
    &quot;&quot;&quot;Simple connect works, but execute raises the error&quot;&quot;&quot;
    _db_session = db_tst.session_factory()
    async with _db_session as session:
        await session.execute(text(&quot;SELECT 1&quot;))
        await session.close()
</code></pre>
<p>Celery task code:</p>
<pre><code># tasks.py
@app.task
def send_notifications_task():
    asyncio.run(send_notifications_batch())

# app.py
@signals.worker_process_init.connect
def setup_periodic_tasks(sender, **kwargs):
    asyncio.set_event_loop(asyncio.new_event_loop())
</code></pre>
<ul>
<li>If I call db.engine.dispose(), I get the same error, but then it works one more time (only once).</li>
<li>In another project I have exactly the same setup and it works fine.</li>
</ul>
<hr />
<ol>
<li>Is asyncio.run even the right approach here when working with SQLAlchemy async + Celery?</li>
<li>If not, what’s the correct way to run async DB code inside a Celery task that runs every 15 minutes via celery-beat?</li>
</ol>
",0,0,0,2025-09-18T15:12:54+00:00,2,127,False
79768868,1276213,,postgresql,Enum mapping hell PostgreSQL =&gt; EF Core 9,"<p>I am creating a brand new project to prototype an ASP.NET Core 9 Web API backend with PostgreSQL using the latest Entity Framework Core 9 and Npsgsql 9. Everything was going well until I hit a table with a PostgreSQL enum. Whatever I try, I can't get it to work!</p>
<p>Trying to figure this out on my own has been rough because of different versions requiring different approaches so it's got me all confused.</p>
<p>Right now, I have a table in PostgreSQL called <code>locations</code>, and it has a column <code>location_type</code> whose datatype is <code>locationtype</code> (in DBeaver it says: public.&quot;locationtype&quot;) which is declared as an enum type with values <code>[FIXED, MOBILE, TEMPORARY]</code>.</p>
<p>On the C# side, I created an enum:</p>
<pre><code>public enum LocationType
{
    FIXED,
    MOBILE,
    TEMPORARY
}
</code></pre>
<p>And then I registered it in the builder:</p>
<pre><code>builder.Services.AddDbContext&lt;MedstickDbContext&gt;(options =&gt;
        options.UseNpgsql(builder.Configuration.GetConnectionString(&quot;PostgresDEV&quot;),
        o =&gt; o.MapEnum&lt;LocationType&gt;(&quot;locationtype&quot;))
    .UseSnakeCaseNamingConvention());
</code></pre>
<p>And then in the <code>DbContext</code>, I also registered it:</p>
<pre><code>protected override void OnModelCreating(ModelBuilder modelBuilder)
{
    modelBuilder
        .HasPostgresEnum&lt;LocationType&gt;();
}
</code></pre>
<p>And the <code>Location</code> class has a property defined like this:</p>
<pre><code>public partial class Location
{
    public string Id { get; set; } = null!;

    public string Name { get; set; } = null!;

    public LocationType? LocationType { get; set; }
}
</code></pre>
<p>And that's all I have right now. When I try to run a query to get location results, I get the following error:</p>
<blockquote>
<p>System.InvalidCastException: Received enum value 'FIXED' from database which wasn't found on enum MyApp.Models.LocationType</p>
</blockquote>
<p>And I just can't get past this.</p>
<p>I had previous iterations where I tried using what appears to be older version solution with doing explicit calls to</p>
<pre><code>modelBuilder entity.Property HasConversion&lt;string&gt;() 
</code></pre>
<p>but while they were running I was seeing integers in the results instead of the string value and then when I tried applying filtering I was getting conversion errors there so that never completely worked either.</p>
<p>Please tell me there is something I can still do before I give up on this trial!</p>
",2,2,0,2025-09-18T19:49:11+00:00,0,279,False
79769100,31496851,,postgresql,Query result overriding in db&lt;&gt;fiddle and xtuple,"<p>This a follow-up to a question from 9/15/25: <a href=""https://stackoverflow.com/questions/79765624/query-that-checks-if-an-item-name-contains-a-substring-and-if-true-perform-a-s"">Query that checks if an item name contains a substring, and if true, perform a second query that grabs an image with a specific name</a></p>
<p>I have a query that grabs image data from tables joined on substrings. The item substring I need to match changes depending on the item so I need to run multiple queries. Example:
<a href=""https://dbfiddle.uk/ZrUkfuab"" rel=""nofollow noreferrer"">https://dbfiddle.uk/ZrUkfuab</a></p>
<pre class=""lang-sql prettyprint-override""><code>create table image(
    image_name text,
    image_data bytea
);

INSERT INTO image
VALUES     ( 'PRODUCT ANALOG DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT ANALOG DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT MAIN DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT FRAM DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT AAAA DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')),
        ('PRODUCT BD DATASHEET',
         Convert_to(Md5(Random() :: text), 'UTF8')); 

create table wo(wo_id int,item_number text);
insert into wo values(1,'PRODUCT - MAIN BD - FRAM-MSTRW/O FBR')
                ,(2,'PRODUCT-ANLG-GIC2B-2AIN')
                ,(3,'PRODUCT - ANALOG BD - 4CT/4AIN')
                ,(4,'PRODUCT-AAAA-BBBBB-4XX');

create table Patterns(Pattern_id int, PatternString text);
insert into Patterns values(1,'^PRODUCT - MAIN BD -.(\w+).*')
                ,(2,'^PRODUCT-ANLG.(\w+).*')
                ,(3,'^PRODUCT -.(\w+).*')
                ,(4,'^PRODUCT.(\w+).*');
</code></pre>
<p>This successfully returns the image data:</p>
<pre><code>SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 3),'\1')
WHERE wo_id=3;
</code></pre>
<p>A different pattern results in zero matches, no results:</p>
<pre><code>SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 4),'\1')
WHERE wo_id=3;
</code></pre>
<p>This third block combines both queries. The query that doesn’t return anything is first, followed by one that does. This block successfully returns the image data, from the second query in it.</p>
<pre><code>SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 4),'\1')
WHERE wo_id=3;

SELECT item_number, image_name, image_data 
FROM wo
JOIN image ON REGEXP_REPLACE(image_name,'^PRODUCT.(\w+).*','\1')
             =REGEXP_REPLACE(item_number,(SELECT PatternString 
                                          FROM Patterns 
                                          WHERE Pattern_id = 3),'\1')
WHERE wo_id=3;
</code></pre>
<p>However, when I flip the order of queries in this block, no image data is returned. The (lack of) result of the second query seems to be overriding the result of the first query.</p>
<p><strong>How to get around it? For loops? Case Expressions? CTE’s?</strong></p>
<p>I really don’t want to give each query its own dedicated block because it would make organizing the report template a complete nightmare.</p>
",-1,1,2,2025-09-19T03:54:56+00:00,1,129,True
79769268,20240707,,postgresql,Ignoring accents in a PostgREST API query with the PostgreSQL unaccent extension,"<p>I am using a PostgREST API that queries a PostgreSQL database. In my table, I have a <code>name</code> field containing values with accents such as <code>Léonard</code>. I want to be able to perform a query via the PostgREST API while ignoring accents. For example, a query for <code>leonard</code> should return the row containing <code>Léonard</code>.</p>
<p>I have already installed the <code>unaccent</code> extension on my PostgreSQL schema and it works fine when I run SQL code directly on the table.</p>
<p>How can I modify the following query using <code>unaccent</code> so that accents are ignored?</p>
<blockquote>
<p>http://localhost:3000/people?name=ilike.leonard</p>
</blockquote>
<p>I found resources for using the unaccent extension, but none for using it in combination with the PostgREST API.</p>
",2,2,0,2025-09-19T08:05:15+00:00,1,66,True
79769415,23598941,,postgresql,Laravel 9 RefreshDatabase trait connects to wrong DB / user in GitHub Actions CI,"<p>I'm running Laravel 9 feature tests with the RefreshDatabase trait in GitHub Actions. Locally, everything works fine, but on CI, it seems that RefreshDatabase tries to connect with the wrong database or user. But actually i tryied to log it when workflow was running and its connecting correctly to postgre user..
Here's my setup:</p>
<p>phpunit.ci.xml</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;phpunit xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:noNamespaceSchemaLocation=&quot;./vendor/phpunit/phpunit/phpunit.xsd&quot;
         bootstrap=&quot;vendor/autoload.php&quot;
         colors=&quot;true&quot;&gt;
    &lt;testsuites&gt;
        &lt;testsuite name=&quot;Unit&quot;&gt;
            &lt;directory suffix=&quot;Test.php&quot;&gt;./tests/Unit&lt;/directory&gt;
            &lt;exclude&gt;./tests/Unit/PaymentManagement&lt;/exclude&gt;
        &lt;/testsuite&gt;
        &lt;testsuite name=&quot;Feature&quot;&gt;
            &lt;directory suffix=&quot;Test.php&quot;&gt;./tests/Feature&lt;/directory&gt;
            &lt;exclude&gt;./tests/Feature/Machines&lt;/exclude&gt;
        &lt;/testsuite&gt;
        &lt;testsuite name=&quot;Modules&quot;&gt;
            &lt;directory suffix=&quot;Test.php&quot;&gt;./Modules/*/Tests/Feature&lt;/directory&gt;
            &lt;directory suffix=&quot;Test.php&quot;&gt;./Modules/*/Tests/Unit&lt;/directory&gt;
        &lt;/testsuite&gt;
    &lt;/testsuites&gt;
    &lt;filter&gt;
        &lt;whitelist processUncoveredFilesFromWhitelist=&quot;true&quot;&gt;
            &lt;directory suffix=&quot;.php&quot;&gt;./app&lt;/directory&gt;
        &lt;/whitelist&gt;
    &lt;/filter&gt;
    &lt;php&gt;
        &lt;server name=&quot;APP_ENV&quot; value=&quot;testing.ci&quot;/&gt;
        &lt;server name=&quot;BCRYPT_ROUNDS&quot; value=&quot;4&quot;/&gt;
        &lt;server name=&quot;CACHE_DRIVER&quot; value=&quot;array&quot;/&gt;
        &lt;server name=&quot;MAIL_DRIVER&quot; value=&quot;array&quot;/&gt;
        &lt;server name=&quot;QUEUE_CONNECTION&quot; value=&quot;sync&quot;/&gt;
        &lt;server name=&quot;SESSION_DRIVER&quot; value=&quot;array&quot;/&gt;
        &lt;server name=&quot;TELESCOPE_ENABLED&quot; value=&quot;false&quot;/&gt;
        &lt;server name=&quot;DB_CONNECTION&quot; value=&quot;pgsql&quot;/&gt;
        &lt;server name=&quot;DB_HOST&quot; value=&quot;127.0.0.1&quot;/&gt;
        &lt;server name=&quot;DB_PORT&quot; value=&quot;5432&quot;/&gt;
        &lt;server name=&quot;DB_DATABASE&quot; value=&quot;main_testing&quot;/&gt;
        &lt;server name=&quot;DB_USERNAME&quot; value=&quot;postgres&quot;/&gt;
        &lt;server name=&quot;DB_PASSWORD&quot; value=&quot;secret&quot;/&gt;

        &lt;server name=&quot;DB_DATABASE_WAM&quot; value=&quot;maw_testing&quot;/&gt;
        &lt;server name=&quot;DB_USERNAME_WAM&quot; value=&quot;postgres&quot;/&gt;
        &lt;server name=&quot;DB_PASSWORD_WAM&quot; value=&quot;secret&quot;/&gt;

        &lt;server name=&quot;DB_DATABASE_VISITOR&quot; value=&quot;visitor_testing&quot;/&gt;
        &lt;server name=&quot;DB_USERNAME_VISITOR&quot; value=&quot;postgres&quot;/&gt;
        &lt;server name=&quot;DB_PASSWORD_VISITOR&quot; value=&quot;secretx&quot;/&gt;
        &lt;server name=&quot;DB_DATABASE_VISITOR&quot; value=&quot;warehouse_testing&quot;/&gt;
        &lt;server name=&quot;DB_USERNAME_VISITOR&quot; value=&quot;postgres&quot;/&gt;
        &lt;server name=&quot;DB_PASSWORD_VISITOR&quot; value=&quot;secretx&quot;/&gt;
        &lt;server name=&quot;DB_DATABASE_VISITOR&quot; value=&quot;import_testing&quot;/&gt;
        &lt;server name=&quot;DB_USERNAME_VISITOR&quot; value=&quot;postgres&quot;/&gt;
        &lt;server name=&quot;DB_PASSWORD_VISITOR&quot; value=&quot;secretx&quot;/&gt;
        &lt;ini name=&quot;output_buffering&quot; value=&quot;0&quot;/&gt;
        &lt;ini name=&quot;implicit_flush&quot; value=&quot;1&quot;/&gt;
    &lt;/php&gt;
&lt;/phpunit&gt;
</code></pre>
<p>This is .env.testing.ci</p>
<pre class=""lang-bash prettyprint-override""><code>APP_NAME=testapp
APP_ENV=testing
APP_URL=http://localhost
APP_DEBUG=true
LOG_CHANNEL=stack

DB_CONNECTION=pgsql

MY_DB_HOST=127.0.0.1
MY_DB_PORT=3308
MY_DB_DATABASE=media_testing
MY_DB_USERNAME=root
MY_DB_PASSWORD=

DB_HOST=127.0.0.1
DB_PORT=5432
DB_DATABASE=main_testing
DB_USERNAME=postgres
DB_PASSWORD=secret

DB_DATABASE_WAM=maw_testing
DB_USERNAME_WAM=postgres
DB_PASSWORD_WAM=secret

DB_DATABASE_VISITOR=visitor_testing
DB_USERNAME_VISITOR=postgres
DB_PASSWORD_VISITOR=secret

DB_DATABASE_STORE=warehouse_testing
DB_USERNAME_STORE=postgres
DB_PASSWORD_STORE=secret

DB_DATABASE_ORDERS_IMPORT=import_testing
DB_USERNAME_ORDERS_IMPORT=postgres
DB_PASSWORD_ORDERS_IMPORT=secret 
</code></pre>
<p>And this is workflow</p>
<pre class=""lang-yaml prettyprint-override""><code>on: workflow_dispatch


jobs:
  testapp-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: secret
          POSTGRES_DB: main_testing
        ports:
          - 5432:5432
        options: &gt;-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3

      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: &gt;-
          --health-cmd &quot;redis-cli ping&quot;
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3

    steps:
      - uses: actions/checkout@v3

      - name: Set up PHP
        uses: shivammathur/setup-php@v2
        with:
          php-version: '8.2'
          extensions: pgsql, redis, bcmath, gd, intl, mbstring, pdo, tokenizer, xml, curl
          ini-values: post_max_size=256M, upload_max_filesize=256M, memory_limit=512M
          tools: composer:v2

      - name: Configure composer auth
        run: composer config --global --auth github-oauth.github.com ${{ secrets.MOPS_TEST_COMPOSER_TOKEN }}          

      - name: Install dependencies
        run: composer install --prefer-dist --no-progress

      - name: Copy .env
        run: cp .env.testing.ci .env

      - name: create extra databases
        env:
          PGPASSWORD: secret
        run: |
          psql -h 127.0.0.1 -U postgres -c &quot;CREATE DATABASE maw_testing;&quot;
          psql -h 127.0.0.1 -U postgres -c &quot;CREATE DATABASE visitor_testing;&quot;
          psql -h 127.0.0.1 -U postgres -c &quot;CREATE DATABASE warehouse_testing;&quot;
          psql -h 127.0.0.1 -U postgres -c &quot;CREATE DATABASE import_testing;&quot;
          
      - name: Run migrations
        run: php artisan migrate --force

      - name: Run tests
        run: php artisan test --configuration=phpunit.ci.xml
</code></pre>
<p>example of test</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php

namespace Tests\Feature\ProductionDashboard;

use Mockery;
use App\Models\Machine;
use Tests\TestCase;
use App\Models\ProductionDashboard;
use App\Enums\ProductionDashboardType;
use Illuminate\Foundation\Testing\WithFaker;
use Illuminate\Foundation\Testing\RefreshDatabase;
use App\Libraries\ProductionDashboardLibrary\MachineJobAggregator;

class ProductionDashboardCrudTest extends TestCase
{
    use RefreshDatabase, WithFaker;

    protected function tearDown(): void
    {
        Mockery::close();
        parent::tearDown();
    }


    public function test_grid_returns_json()
    {
        $machine = Machine::factory()-&gt;create();

        $aggregatorMock = Mockery::mock('alias:' . MachineJobAggregator::class);
        $aggregatorMock-&gt;shouldReceive('initializeMachineData')
            -&gt;andReturn([
                'ready_jobs' =&gt; [
                    ['job' =&gt; 1, 'jobjobmachine' =&gt; 1, 'producttype' =&gt; 'Test']
                ]
            ]);
        $aggregatorMock-&gt;shouldReceive('aggregate')
            -&gt;andReturn(['total' =&gt; 1]);

        $url = '/api/production-dashboard/grid/ready_jobs?params[machine_id]=' . $machine-&gt;id;
        $response = $this-&gt;getJson($url);

        $response-&gt;assertStatus(200)
            -&gt;assertJsonStructure([
                'data',
                'links',
                'meta',
            ]);
    }
</code></pre>
<p>When i remove trait for RefreshDatabase it works well. Or it works well even with DatabaseTransaction trait, but I wasnt able figure out how solve problem with RefreshDatabse trait. With presence of RefreshDatabase trai all test alway fails, not just example above.</p>
",0,1,1,2025-09-19T10:49:01+00:00,1,53,True
79771250,9298910,,postgresql,Why is jsonb_build_object() not IMMUTABLE?,"<p>I tried, unsuccessfully, using <code>jsonb_build_object()</code> in a <code>GENERATED ALWAYS AS () STORED</code> column. After troubleshooting why that did not work, I noticed that the function is marked as <code>STABLE</code> and not <code>IMMUTABLE</code>.</p>
<p>What is the reason for this? I find it particularly strange that <code>jsonb_object()</code> is <code>IMMUTABLE</code>, altough they both do pretty much the same thing.</p>
",2,2,0,2025-09-22T05:26:36+00:00,1,106,True
79771769,5489294,"Calgary, Canada",postgresql,Single Save with JPA CascadeType.PERSIST resulting to wrong order of insetion,"<p>I have the following scenario:</p>
<pre><code>class A {
 @OneToMany(mappedBy = &quot;a&quot;, cascade = {CascadeType.PERSIST})
 List&lt;B&gt; bsOfA;
 @OneToMany(mappedBy = &quot;a&quot;, cascade = {CascadeType.PERSIST}) 
 List&lt;C&gt; csOfA;
}

class B {
 @OneToMany(mappedBy = &quot;b&quot;, cascade = {CascadeType.PERSIST})
 List&lt;D&gt; dsOfB;
 @ManyToOne
 A a;
}

class C {
 @OneToMany(mappedBy = &quot;c&quot;, cascade = {CascadeType.PERSIST})
 List&lt;D&gt; dsOfC;
 @ManyToOne
 A a;
}

class D {
 @ManyToOne
 C c;
 @ManyToOne
 B b;
 @ManyToOne
 A a;
}
</code></pre>
<p>Then, I am attempting to save A along with all its associated entities as follows:</p>
<pre><code>a = new A(aId);
b = new B(bId);
c = new C(cId);
d = new D(dId);
a.setBsOfA(List.of(b));
a.setCsOfA(List.of(c));
b.setDsOfB(List.of(d));
c.setDsOfC(List.of(d));
repo.save(a)
</code></pre>
<p>Now my problem is, I am getting a foreign key violation because the order of insertion is incorrect. Checking my logs, I am seeing the following order of insertion:
a -&gt; b -&gt; d (error c not yet existing)</p>
<p>How can I enforce Hibernate to insert in the correct order:
a -&gt; b -&gt; c -&gt; d OR
a -&gt; c -&gt; b -&gt; d</p>
",0,0,0,2025-09-22T15:02:18+00:00,1,83,False
79771875,1103606,,postgresql,"Oracle function to_char(character varying, unknown) does not exist","<p>I need to migrate this Oracle SQL to postgresql:</p>
<pre class=""lang-sql prettyprint-override""><code>create table tokens(id, date_of_birth)
  as values(1, '2025-09-22'::varchar(20));

    SELECT 
        s.id,
        to_char(s.date_of_birth, 'fmMM') dobMonth,
        to_char(s.date_of_birth, 'fmYYYY') dobYear,
        to_char(s.date_of_birth, 'fmDD') dobDay,
        s.type
    FROM 
        tokens s
    WHERE 
        s.token_id = :profileId
</code></pre>
<p>But I get an error:</p>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>ERROR:  function to_char(character varying, unknown) does not exist
LINE 1: select to_char(date_of_birth, 'fmMM') from tokens;
              ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
</code></pre>
</blockquote>
<p>Do you know what is the proper way to solve this issue?</p>
",0,2,2,2025-09-22T17:04:06+00:00,2,267,True
79772908,6855607,,postgresql,Postgres stored procedure doesn&#39;t return if running longer than 5 minutes,"<p>I have the following simple Postgres stored procedure created that just sleeps a number of minutes as passed in to call:</p>
<pre class=""lang-sql prettyprint-override""><code>create procedure sleep_test(delay_minutes numeric)
    language plpgsql
as
$$
BEGIN
    RAISE NOTICE 'Starting procedure. Sleeping for % minutes...', delay_minutes;
    PERFORM pg_sleep(delay_minutes * 60); -- Function call is in seconds
    RAISE NOTICE 'Finished sleeping.';

END;
$$;
</code></pre>
<p><strong>Running the procedure for four minutes:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>call sleep_test(4)
</code></pre>
<p>If I run the the procedure for four minutes in a worksheet, all works fine. The procedure runs (has state=='active' in pg_stat_activity) for 4 minutes and returns back to the calling worksheet:</p>
<p><strong>Running the procedure for five minutes:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>call sleep_test(5)
</code></pre>
<p>If I run it for 5 minutes or more, though, the process runs for 5 minutes (has state=='active' in pg_stat_activity), but after five minutes it goes into an &quot;idle&quot; state in pg_stat_activity and <em>does not return back to the calling worksheet</em>. From the worksheet's perspective it thinks the procedure is still running (indefinitely).</p>
<p><strong>DB Settings (in case they are useful):</strong></p>
<p>server_version == 12.22</p>
<p>statement_timeout == 0</p>
<p>idle_in_transaction_session_timeout==1d</p>
<p><strong>Clients used to test - same issue with both:</strong></p>
<p>Datagrip 2025.1 Query Console</p>
<p>Python 3.9 script using psycopg2 2.9.9</p>
<p><strong>Database server config:</strong></p>
<p>It is an AWS Postgres RDS database. Sorry... I am not familiar with where to exactly find OS and version in AWS.</p>
",1,1,0,2025-09-23T17:29:05+00:00,0,100,False
79773654,1382234,,postgresql,Can&#39;t run cursor in PostgreSQL,"<p>I'm new with PostgreSQL. Running sql using pgAdmin 4.
I'm trying to run simple cursor:</p>
<pre><code>DECLARE 
    data_cursor CURSOR FOR
        SELECT &quot;ID&quot;
        FROM public.&quot;MY_TABLE&quot;

BEGIN
    OPEN data_cursor;
    BEGIN
        LOOP
            FETCH NEXT FROM data_cursor INTO id;
            EXIT WHEN NOT FOUND;

            RAISE NOTICE 'Value 1: %', id;
            
        END LOOP;
        CLOSE data_cursor;
        COMMIT;
        
    END
END ;
</code></pre>
<p>But got an error:</p>
<pre><code>ERROR:  syntax error
LINE 9:  OPEN data_cursor;
</code></pre>
<p>Simply can't see what is wrong here.</p>
",0,0,0,2025-09-24T11:25:40+00:00,1,119,True
79773723,2615905,,postgresql,How to improve word_similarity query performance in postgresql?,"<p>In a project, we are using postgresql v12.11 (updating is sadly not an option at the moment).</p>
<p>Consider the following relations:</p>
<pre class=""lang-sql prettyprint-override""><code>document (
  id uuid primary key
)

page (
  id uuid primary key,
  document_id uuid not null references document
)

word (
  id uuid primary key,
  page_id uuid not null references page,
  text text not null
)
</code></pre>
<p>I had to implement a feature to find words across given documents by a given similarity threshold quickly, and after some research chose to use trigram matching with a <code>GIN</code> index.</p>
<p>This are my indices:</p>
<pre><code>CREATE INDEX word_page_id_idx ON word (page_id);
CREATE INDEX word_text_idx ON word USING GIN (text gin_trgm_ops);
CREATE INDEX page_document_id ON page (document_id);
</code></pre>
<p>This is the query I came up with:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT w.*, word_similarity(:searchString, w.text) similarity
    FROM document d
             INNER JOIN page p ON p.document_id = d.id
             INNER JOIN word w ON w.page_id = p.id
    WHERE d.id IN (:documentIds)
    AND :searchString &lt;% w.text
    ORDER BY similarity DESC, w.text;
</code></pre>
<p>It worked very good in the beginning, but now with ~20kk rows in the words table it starts to get slower, especially with multiple documents.</p>
<p>My knowledge about indices and the query planner is not deep enough to understand what exactly is going on here, or why the query is getting slower.</p>
<p>Here is what <code>EXPLAIN ANALYZE</code> gives me for a query with 18 document ids. I wonder why the plan considers <code>185700</code> word rows, although the given documents in total have only around 9k words associated with them, but where do I proceed from here? AI suggested to create a combined GIN index on (page_id, text) which does not work...</p>
<pre><code>+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|QUERY PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Sort  (cost=11351.77..11351.79 rows=8 width=63) (actual time=3825.241..3825.247 rows=74 loops=1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|  Sort Key: (word_similarity('05.06.2025'::text, w.text)) DESC, w.text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|  Sort Method: quicksort  Memory: 35kB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|  -&gt;  Nested Loop  (cost=248.89..11351.65 rows=8 width=63) (actual time=130.456..3825.106 rows=74 loops=1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|        -&gt;  Nested Loop  (cost=0.83..138.23 rows=45 width=16) (actual time=0.517..4.050 rows=28 loops=1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|              -&gt;  Index Only Scan using document_pkey on document d  (cost=0.41..31.98 rows=18 width=16) (actual time=0.010..0.217 rows=18 loops=1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                    Index Cond: (id = ANY ('{470d35da-a65e-4eb5-aae1-1ce9334ff617,44f05ab8-94f7-4e90-9008-a224b0f2c458,c9b8161d-2975-4902-884f-11c658960ca0,9315c0ec-b460-4044-b4a3-79b77f40faea,b6e0a7ef-37c3-4540-aeb9-424567e3c51f,0b0adbbe-7d70-484d-a589-f5952dd9c4ae,5f51a47e-11cc-4bf4-a3f6-d1452262d82f,099dfc29-1803-4a87-8820-891697b26047,3e6a8d9c-a40b-4980-a49e-8285eee4dedc,4078b105-aff4-478d-97ae-2b785b1bdd08,30b7a7f6-b67a-4f0c-8cfb-e686a01b8e89,7265f939-7f47-4264-8a72-43f71312ba74,3e03b9b5-2c24-4d7a-b063-5bec34ad6e1e,23f69244-81a9-4dbc-984d-21bfd2bd0147,39837f1c-491e-4b43-9c86-713c7b16ec7a,5ba1dce8-97e0-4e22-8685-e018c4dbbf31,15e34009-1ed7-470a-9b16-34fc416595eb,9fb7604b-eafd-49be-b8cf-d9f2c5d663bf}'::uuid[]))|
|                    Heap Fetches: 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|              -&gt;  Index Scan using page_document_id on page p  (cost=0.42..5.86 rows=4 width=32) (actual time=0.203..0.209 rows=2 loops=18)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|                    Index Cond: (document_id = d.id)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|        -&gt;  Bitmap Heap Scan on word ow  (cost=248.06..249.18 rows=1 width=59) (actual time=136.266..136.449 rows=3 loops=28)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|              Recheck Cond: (page_id = p.id)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|              Filter: ('05.06.2025'::text &lt;% text)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|              Rows Removed by Filter: 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|              Heap Blocks: exact=50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|              -&gt;  BitmapAnd  (cost=248.06..248.06 rows=1 width=0) (actual time=135.254..135.254 rows=0 loops=28)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|                    -&gt;  Bitmap Index Scan on word_page_id_idx  (cost=0.00..9.16 rows=761 width=0) (actual time=0.021..0.021 rows=251 loops=28)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|                          Index Cond: (page_id = p.id)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|                    -&gt;  Bitmap Index Scan on word_text_idx  (cost=0.00..233.26 rows=21568 width=0) (actual time=135.229..135.229 rows=185700 loops=28)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|                          Index Cond: (text %&gt; '05.06.2025'::text)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|Planning Time: 26.187 ms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|Execution Time: 3825.320 ms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>
",1,1,0,2025-09-24T12:43:40+00:00,1,122,True
79774404,5327579,,postgresql,Why are some records not added to the m2m table?,"<p>Why aren't some records being added to the m2m table?
If the table already contains a record with a specific topic and system (service can be any), then the next addition of a record with a similar topic and system does not occur.</p>
<pre><code>with transaction.atomic():
    chat_topic = ChatTopics.objects.get(ct_id=chat_topic_id)
    system = Systems.objects.get(sys_id=system_id)
    service = ServiceCatalog.objects.get(sc_id=service_id)

    chat_topic.ct_systems.add(system, through_defaults={&quot;ctm_service&quot;: service})
</code></pre>
<p>I don't get any errors when run this code.</p>
<p>Expected result:
<a href=""https://i.sstatic.net/26sxl9NM.png"" rel=""nofollow noreferrer"">Expected result</a></p>
<p>Actual result:
<a href=""https://i.sstatic.net/xF4kGGTi.png"" rel=""nofollow noreferrer"">Actual result</a></p>
<p><strong>Models:</strong></p>
<pre><code>class Systems(BaseModel):
    sys_id = models.AutoField(primary_key=True)
    sys_name = models.CharField(max_length=256, unique=True, null=False)

    def __str__(self):
        return self.sys_name

    class Meta:
        verbose_name = &quot;Systems&quot;
        verbose_name_plural = &quot;Systems&quot;
        ordering = ['sys_id']
</code></pre>
<pre><code>class ServiceCatalog(BaseModel):
    sc_id = models.AutoField(primary_key=True)
    sc_service = models.CharField(max_length=250, unique=True, null=False)

    def __str__(self):
        return self.sc_service

    class Meta:
        verbose_name = &quot;Service catalog&quot;
        verbose_name_plural = &quot;Service catalog&quot;
        ordering = ['sc_id']
</code></pre>
<pre><code>class ChatTopics(BaseModel):
    ct_id = models.AutoField(primary_key=True)
    ct_systems = models.ManyToManyField(Systems, blank=True, through=&quot;chattopics_m2m&quot;)
    ct_chat_id = models.CharField(max_length=20, null=False, blank=False)
    ct_chat_name = models.CharField(max_length=50, null=False, blank=False)
    ct_topic_id = models.CharField(max_length=10, null=True, default=None, blank=True)
    ct_topic_name = models.CharField(max_length=50, null=True, default=None, blank=True)
    
    def __str__(self):
        return f'{self.ct_chat_name} ({self.ct_topic_name})'
    
    class Meta:
        verbose_name = 'Topics'
        verbose_name_plural = 'Topics'
        
        constraints = [
            models.UniqueConstraint(fields=('ct_chat_id', 'ct_topic_id'), nulls_distinct=False, name='chattopics_ct_chat_id_ct_topic_id_unique')
        ]
</code></pre>
<pre><code>class ChatTopics_m2m(BaseModel):
    ctm_id = models.AutoField(primary_key=True)
    ctm_chat_topic = models.ForeignKey(ChatTopics, on_delete=models.CASCADE, null=False)
    ctm_system = models.ForeignKey(Systems, on_delete=models.CASCADE, null=False)
    ctm_service = models.ForeignKey(ServiceCatalog, on_delete=models.CASCADE, null=True, blank=True)
    
    class Meta:
        ordering = ['ctm_system']
</code></pre>
",2,2,0,2025-09-25T05:52:49+00:00,1,76,True
79774453,31567684,,postgresql,How to connect FastAPI with PostgreSQL using SQLAlchemy and async sessions?,"<p>I’m building a small project using FastAPI + PostgreSQL and I want to use async SQLAlchemy sessions for queries.
I’ve seen examples with SessionLocal, but I’m confused about how to properly set up:</p>
<blockquote>
<p>engine (async)</p>
<p>session dependency injection</p>
<p>creating models and migrations (with Alembic)</p>
</blockquote>
<p>Here’s my minimal setup so far:</p>
<pre><code>from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

DATABASE_URL = &quot;postgresql+asyncpg://user:password@localhost/db&quot;

engine = create_async_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)
</code></pre>
",0,0,0,2025-09-25T06:50:27+00:00,1,547,True
79774536,10944834,,postgresql,Does PostgreSQL use a multi column unique index when foreign key column order differs?,"<p>I am having EDB data base(Postgress with oracle compatibility). In data base it have two table called <code>institution</code> and <code>customer</code>. Each customer need to belong to an institution. <code>Intitution</code> has unique constraint with columns <code>(inst_id, inst_code)</code> in that order created by,</p>
<pre><code>ALTER TABLE IF EXISTS institution
ADD CONSTRAINT &quot;UK_institution_inst_code&quot; UNIQUE (inst_id, inst_code);
</code></pre>
<p>And <code>customer</code> table also has two columns with names <code>institution_id</code>, <code>institution_code</code>. I am setting foreign key from <code>customer</code> to <code>institution</code> by executing,</p>
<pre class=""lang-none prettyprint-override""><code>    ADD CONSTRAINT fk_customer_institution_id_institution_code FOREIGN KEY (institution_code, institution_id)
    REFERENCES institution (inst_code,inst_id) MATCH SIMPLE
    ON UPDATE NO ACTION
       ON DELETE NO ACTION;
</code></pre>
<p>Since there is a unique key in institution with <code>(inst_id, inst_code)</code> postgress allow to create foreign key and It is validating when I <code>Insert</code>,<code>Update</code>, or <code>Delete</code>. When enforcing the foreign key, does PostgreSQL actually use that unique index even though the column order is different?
Or do I need to create a matching unique key (or swap column order) to get efficient index usage?</p>
",2,2,0,2025-09-25T08:07:43+00:00,2,75,True
79774734,9298910,,postgresql,Postgres sort 1-N relationship,"<p>Suppose we have a table <code>items</code> and <code>properties</code>. Users can create items and also assign user-created properties to them. For example:</p>
<p>Items:</p>
<pre><code>id | name
---|--------
1  | First
2  | Second
3  | Third
</code></pre>
<p>Properties:</p>
<pre><code>id | itemId | name | value
---|--------|------|-------
1  | 1      | A    | 1
2  | 1      | B    | 2
3  | 1      | C    | 3
---|--------|------|-------
4  | 2      | A    | 2
5  | 2      | B    | 3
6  | 2      | C    | 5
---|--------|------|-------
7  | 3      | A    | 2
8  | 3      | B    | 4
</code></pre>
<p>This means, the Item #1 has the name &quot;First&quot; and the properties A=1, B=2, C=3. Item #2 has the name &quot;Second&quot; and the properties A=2, B=3, C=5. Lastly, the item #3 has the name &quot;Third&quot; and the properties A=2, B=4. Also, the properties per item are unique. So there is no item with two A properties.</p>
<p>Now, what I need to do is to sort the items by their properties. For example, I need to sort by A ASC and B DESC. That should result in:</p>
<ul>
<li>Item #1 (A=1,B=2)</li>
<li>Item #3 (A=2,B=4)</li>
<li>Item #2 (A=2,B=3)</li>
</ul>
<p>Also, the system needs to be able to account for empty values (ordering by C should show Item #3 last).</p>
<p>What is the best way to accomplish this? The issue would be trivial if there was a column for each property in the items table. However, this is not possible since there is a non-fixed amount of properties (since they can be added at runtime and different items have different properties).</p>
",0,1,1,2025-09-25T11:13:48+00:00,1,60,True
79775847,1424729,"Moskva, Москва, Россия",postgresql,"Micronaut reactive, Hibernate, Vert, Postgres - correct configuration?","<p>I try to make my first project with Micronaut and can't figure out how to confihure the datasource:</p>
<pre><code>jpa.default.properties.hibernate.hbm2ddl.auto=create-drop
jpa.default.properties.hibernate.show_sql=true
jpa.default.properties.hibernate.connection.db-type=postgres

# Micronaut JPA/Hibernate Reactive configuration
jpa.default.reactive=true
jpa.default.properties.hibernate.connection.url=vertx-reactive:postgresql://localhost:5432/postgres
jpa.default.properties.hibernate.connection.username=postgres
jpa.default.properties.hibernate.connection.password=postgres
</code></pre>
<p>This causes</p>
<blockquote>
<p>org.hibernate.service.spi.ServiceException: Unable to create requested
service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment] due to:
Cannot invoke &quot;String.length()&quot; because &quot;path&quot; is null</p>
</blockquote>
",0,0,0,2025-09-26T11:28:42+00:00,0,37,False
79775896,10300113,Austria,postgresql,Oracle&#39;s optimizer_dynamic_sampling in postgres,"<p>In Oracle, when I had a misbehaving query, I would run the query once with <a href=""https://blogs.oracle.com/optimizer/post/dynamic-sampling-and-its-impact-on-the-optimizer"" rel=""nofollow noreferrer"">optimizer_dynamic_sampling</a> at levels 8, 9 or 10. That would give me the plan that the optimizer would pick if it had perfect knowledge about how many rows it will get from each table. I found that info quite useful to better and faster understand what an optimal plan for the query at hand could be.</p>
<p>I am wondering whether there is a similar tool for postgres? A tool that gives me the execution plan that postgres would pick if it had perfect knowledge of how many rows it will have to deal with for each table.</p>
",1,1,0,2025-09-26T11:59:20+00:00,3,107,True
79776078,31578296,,postgresql,sequelize relationships Cannot read properties of undefined (reading &#39;hasMany&#39;),"<p>I'm trying to integrate sequelize with the db conection to postgres and i am getting an error where relationships are built.
i don't know if i'm missing something at this point</p>
<p>index.js</p>
<pre><code>'use strict';
const fs = require('fs');
const path = require('path');
const Sequelize = require('sequelize');
const process = require('process');
const basename = path.basename(__filename);
const env = process.env.NODE_ENV || 'development';
const config = require(__dirname + '/../config/config.json')[env];
const db = {};

let sequelize;
if (config.use_env_variable) {
  sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
  sequelize = new Sequelize(config.database, config.username, config.password, config);
}

fs
  .readdirSync(__dirname)
  .filter(file =&gt; {
    return (
      file.indexOf('.') !== 0 &amp;&amp;
      file !== basename &amp;&amp;
      file.slice(-3) === '.js' &amp;&amp;
      file.indexOf('.test.js') === -1
    );
  })
  .forEach(file =&gt; {
    const model = require(path.join(__dirname, file))(sequelize, Sequelize.DataTypes);
    db[model.name] = model;
  });

Object.keys(db).forEach(modelName =&gt; {
  if (db[modelName].associate) {
    db[modelName].associate(db);
  }
});

db.sequelize = sequelize;
db.Sequelize = Sequelize;

module.exports = db;
</code></pre>
<p>user.js</p>
<pre><code>'use strict';

const { Model } = require('sequelize');

module.exports = (sequelize, DataTypes) =&gt; {
  class User extends Model {
    static associate(models) {
      // Define asociaciones aquí
      // Ejemplo: Usuario.hasMany(models.Post, { foreignKey: 'usuarioId' });
      this.hasMany(models.Project, {foreignKey: 'UserId'})
    }
  }
  User.init({
    id: {
      type: DataTypes.INTEGER,
      primaryKey: true,
      autoIncrement: true
    },
    email: {
      type: DataTypes.STRING,
      allowNull: false,
      unique: true,
      validate: {
        isEmail: true
      }
    },
    password: {
      type: DataTypes.STRING,
    },
    
  }, {
    sequelize,
    modelName: 'User'
  });
  
  return User;
};

</code></pre>
<pre><code>'use strict';

const { Model } = require('sequelize');

module.exports = (sequelize, DataTypes) =&gt; {
  class Project extends Model {
    static associate(models) {
      
      this.hasMany(models.Timeentries, {foreignKey: 'UserId'})
    }
  }
  
  Project.init({
    id: {
      type: DataTypes.INTEGER,
      primaryKey: true,
      autoIncrement: true
    },
    title: {
      type: DataTypes.STRING,
      allowNull: false,
      unique: true
    },
    description: {
        type: DataTypes.STRING,
        allowNull: true,
      },
    urgency: {
      type: DataTypes.STRING,
    },
    start_date:{
        type: DataTypes.DATEONLY,
    },
    finish_date:{
        type: DataTypes.DATEONLY,
    }
  }, {
    sequelize,
    modelName: 'Project',
  });
  
  return Project;
};
</code></pre>
<p>Timeentries.js</p>
<pre><code>   'use strict';
const { Model } = require('sequelize');
const { Project } = require('./project')

module.exports = (sequelize, DataTypes) =&gt; {
  class Timeentries extends Model {
    static associate(models) {
      // Define asociaciones aquí
      // Ejemplo: Usuario.hasMany(models.Post, { foreignKey: 'usuarioId' });
      Project.hasMany(models.Timeentries, {foreignKey: 'ProjectId'})
    }
  }
  
  Timeentries.init({
    id: {
      type: DataTypes.INTEGER,
      primaryKey: true,
      autoIncrement: true
    },
    duration: {
      type: DataTypes.DATE,
    },
    date: {
        type: DataTypes.DATE,
      },
    urgency: {
      type: DataTypes.STRING,
    }
  }, {
    sequelize,
    modelName: 'Timeentries',

  });
  
  return Timeentries;
};
</code></pre>
<p>I'm trying to integrate sequelize with the db conection to postgres and i am getting an error where relationships are built.
i don't know if i'm missing something at this point</p>
<p>i tried to fill the association par in the index.js but had problems too, also tried to user the User.define, and no init but it failed as well for other reasons.
At first place in users and project i had</p>
<p><code>User.hasMany(models.Project, {foreignKey: 'UserId}</code>
and
<code>Project.hasMany(models.Timeentries, {foreignKey: 'UserId'})</code></p>
<p>and it was the exactly same</p>
<p>I think may be due to the way to import/export functions</p>
",-1,0,1,2025-09-26T14:43:31+00:00,2,56,False
79776168,1051933,"Milan, Italy",postgresql,Multi-table check in Check Constraint or in stored procedure,"<p>I am working on a data model for recording daily construction work reports in PostgreSQL.</p>
<p>A work report is composed by many rows, of three exclusive subtypes: People, Materials and Equipment.</p>
<p>Each work report is daily and details work performed on one job and has one Foreman.</p>
<p>Currently the data model has the following relevant tables (I am omitting details of the WorkReportRow and subtypes):</p>
<pre><code>Job
+---------------------+
| JobCode          PK |
+---------------------+
| JobName             |
| Address             |
+---------------------+

WorkReport
+---------------------+
| JobCode        PK.1 |
| Date           PK.2 |
+---------------------+
| Foreman          FK | -&gt; To PersonCode in Person table
+---------------------+

Person
+---------------------+
| PersonCode       PK |
+---------------------+
| InsuranceNumber     |
+---------------------+

Role
+---------------------+
| RoleCode         PK |
+---------------------+

PersonRole
+---------------------+
| RoleCode    PK.1 FK | -&gt; to RoleCode in Role table
| PersonCode  PK.2 FK | -&gt; to PersonCode in Person table
+---------------------+
</code></pre>
<p>Please note that a person can have more than one role.</p>
<p>I have to enforce a rule where the foreman for each <code>WorkReport</code> is a <code>Person</code> that in the <code>Role</code> has at least <code>Foreman</code>.</p>
<p>Due to the nature of the constraint I'd like for it to be enforced in the database. Ideally I'd add a view called <code>Foreman</code>:</p>
<pre><code>CREATE OR REPLACE VIEW Foremen AS
  SELECT p.PersonCode FROM Person p
  JOIN PersonRole pr on p.PersonCode = pr.PersonCode
  WHERE pr.RoleCode = 'Foreman'
</code></pre>
<p>And then in the <code>WorkReport</code> table the <code>Foreman</code> FK would point to the <code>Foremen</code> view, the problem is that PostgreSQL apparently doesn't support FK to views.</p>
<p>The two options I see now are:</p>
<p><strong>1.</strong> a <code>CHECK</code> constraint in the <code>WorkReport</code> table like this:</p>
<pre><code>ADD CONSTRAINT chk_foreman
CHECK (
  EXISTS (
    SELECT 1
    FROM &quot;PersonRole&quot; pr
    WHERE pr.&quot;PersonCode&quot; = &quot;WorkReport&quot;.&quot;Foreman&quot;
    AND pr.&quot;RoleCode&quot; = 'Foreman'
  )
);
</code></pre>
<p>I like this solution as it is in the &quot;schema&quot; of the database, therefore impossible to violate, but I don't like to have to per form a query in a check as it looks wasteful.</p>
<p><strong>EDIT</strong>: it was correctly pointed out in the comments that CHECK doesn't support queries to other tables in the database.<br />
The correct statement defined in the SQL standard would be ASSERTION, which is unfortunately not implemented in any RBDMS.<br />
This leaves, without duplicating data with a <code>Foremen</code> table or other data model changes (Subtyped for Person) only the following solution.</p>
<p><strong>2.</strong> A check in the stored procedure (current solution)</p>
<p>Due to the need to keep integrity (between WorkReport, WorkReportRow and RowPerson/RowMaterial/RowEquipment) WorkReport insertion is already handled by a stored procedure.</p>
<p>Currently I added this check in the stored procedure:</p>
<pre><code>  SELECT EXISTS (
    SELECT 1
    FROM &quot;PersonRole&quot;
    WHERE &quot;PersonCode&quot; = P_Foreman
    AND &quot;RoleCode&quot; = 'Foreman'
  ) INTO is_foreman;

  IF NOT is_foreman THEN
    RAISE EXCEPTION 'Error: Person % is not a Foreman.', P_Foreman;
  END IF;
</code></pre>
<p>The issue I have with the current implementation is that the check is performed at the Procedure level, and not as a schema constraint, which I think logically would be the best solution.</p>
<p>Do you have any input on this based on the relational model rules and standard?</p>
",2,2,0,2025-09-26T16:13:38+00:00,3,139,True
79776223,18347948,,postgresql,CREATE FUNCTION resulting in Unterminated dollar-quoted string error,"<p>I am posting this question here because I feel like emails I have sent to PostgreSQL mailing lists (pgsql-novice@lists.postgresql.org in particular) have not been going through.  I am fairly certain that the question below did not get through, but I apologize if anyone has already seen this question....</p>
<p>I am using a HeidiSQL client to query a PostgreSQL database.  I find myself very often creating multiple step queries similar to the following where I enter the same dates in multiple places...</p>
<pre><code>WITH q_step_1 AS (
   SELECT .... FROM ...
   WHERE ord_date &gt;= '2024-01-01'),
q_step_2 AS (
   SELECT ... FROM ...
   WHERE inv_date &gt;= '2024-01-01')
SELECT ...
FROM q_step_1
   LEFT JOIN q_step_2 ON .....
</code></pre>
<p>I would like to come up with a way that I can define the dates I want in one place, and refer to these definitions in my actual queries.</p>
<p>I am looking into using variables and/or parameters, but the information I found made me believe that in my particular environment, these might not be viable options (please correct me if I am wrong).  The most promising option seemed to be creating a FUNCTION similar to the following:</p>
<pre><code>CREATE FUNCTION pk_get_date() RETURNS DATE
   AS $$SELECT '2024-01-01'::DATE;$$
   LANGUAGE SQL IMMUTABLE;
</code></pre>
<p>I have tried modifying the above in a number of ways, but I always get the following error:</p>
<pre class=""lang-none prettyprint-override""><code>ERROR: unterminated dollar-quoted string at or near &quot;$$SELECT '2024-01-01'::DATE&quot;
LINE 2: RETURNS DATE AS $$SELECT '2024-01-01'::DATE
</code></pre>
<p>If anyone here can suggest how I might be able to get around this error, or suggest a different / better way I can accomplish my objective, I would greatly appreciate it.</p>
",2,2,0,2025-09-26T17:17:13+00:00,2,97,True
79776386,18347948,,postgresql,can&#39;t CREATE FUNCTION - permission denied for schema public SQL state: 42501,"<p>My job requires me to write queries that retrieve data from a PostgreSQL database that serves as a data warehouse that combines data from several related companies.  I find myself very often creating multiple step queries similar to the following where I enter the same dates in multiple places...</p>
<pre><code>WITH q_step_1 AS (
   SELECT .... FROM ...
   WHERE ord_date &gt;= '2024-01-01'),
q_step_2 AS (
   SELECT ... FROM ...
   WHERE inv_date &gt;= '2024-01-01')
SELECT ...
FROM q_step_1
   LEFT JOIN q_step_2 ON .....
</code></pre>
<p>I would like to come up with a way that I can define the dates I want in one place, and refer to these definitions in my actual queries.  Until today, I have been using the HeidiSQL client.  I first looked into using variables and/or parameters, but the information I found made me believe that at least with HeidiSQL, these might not be viable options (please correct me if I am wrong).  The most promising option seemed to be creating a FUNCTION similar to the following:</p>
<pre><code>CREATE FUNCTION pk_get_date() 
RETURNS DATE AS $$SELECT '2024-01-01'::DATE;$$ 
LANGUAGE SQL
IMMUTABLE;
</code></pre>
<p>When I tried to create this function in HeidiSQL, I would get an &quot;unterminated dollar-quoted string&quot; error, which was due to a limitation of the HeidiSQL client I learned about in the following thread...</p>
<p><a href=""https://stackoverflow.com/questions/79776223/create-function-resulting-in-unterminated-dollar-quoted-string-error"">CREATE FUNCTION resulting in Unterminated dollar-quoted string error</a></p>
<p>... My next step was to download and install pgAdmin 4.  When I try to define the above function in a query in pgAdmin 4, I get the following error...</p>
<pre><code>ERROR: permission denied for schema public
SQL state: 42501
</code></pre>
<p>I do not have administrator access to this PostgreSQL database.  Could my problem simply be that the people who configured this database gave me the permissions necessary to write and execute queries, but not to create a function?</p>
<p>If this is my problem, can anyone here suggest an alternative approach that might allow me to accomplish my objective, which is to store a hard-coded date in a variable, parameter, function, or any available holding area that would allow me to use a reference to this date in the queries I write?</p>
<p>Any suggestions will be greatly appreciated.</p>
<p>Thanks in advance,
Paul</p>
",1,1,0,2025-09-26T21:01:09+00:00,1,88,True
79776839,13518461,,postgresql,Hibernate creates tables but data.sql not populating,"<p>I’m working on a Spring Boot 3.x application with PostgreSQL.</p>
<p>In my dev profile, I want Hibernate to create the tables, and then Spring Boot to populate the question table using data.sql.</p>
<p>Here is my application.yml setup (multi-profile):</p>
<pre><code># =====================================================
# Default profile (shared settings)
# =====================================================
spring:
  datasource:
    driver-class-name: org.postgresql.Driver
  jpa:
    properties:
      hibernate:
        format_sql: true
server:
  port: 8080
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,trace
  tracing:
    enabled: true
#  otlp:
#    tracing:
#      endpoint: http://localhost:4318/v1/traces
logging:
  level:
    root: INFO
    org.springframework.web: DEBUG
  pattern:
    level: &quot;%5p [traceId=%X{traceId:-}, spanId=%X{spanId:-}]&quot;
---
# =====================================================
# Dev profile
# =====================================================
spring:
  config:
    activate:
      on-profile: dev
  datasource:
    url: jdbc:postgresql://localhost:5432/tiktak_db
    username: tiktak_user
    password: tiktak_password
  jpa:
    hibernate:
      ddl-auto: create-drop   # Hibernate creates tables
    show-sql: true
defer-datasource-initialization: true  # ensure data.sql runs AFTER schema creation
  sql:
    init:
      mode: always            # ensure data.sql runs
      continue-on-error: true
  flyway:
    enabled: false            # disable Flyway in dev
---
# =====================================================
# Prod profile
# =====================================================
spring:
  config:
    activate:
      on-profile: prod
  datasource:
    url: jdbc:postgresql://prod-db:5432/tiktak_db
    username: tiktak_user
    password: tiktak_password
  jpa:
    hibernate:
      ddl-auto: validate      # Hibernate validates schema only
    show-sql: false
  flyway:
    enabled: true
    locations: classpath:db/migration
  sql:
    init:
      mode: never
</code></pre>
<p>My data.sql file is in src/main/resources/:</p>
<pre><code>INSERT INTO question(code, text) VALUES
('Q1','Are the headlights intact?'),
('Q2','Is there visible rust on the chassis?'),
('Q3','Are the tires worn out?');
</code></pre>
<p>✅ What works</p>
<p>With spring.jpa.hibernate.ddl-auto=create-drop, Hibernate correctly creates the question table in my Dockerized Postgres.</p>
<p>❌ What doesn’t work</p>
<p>data.sql is not executed — the question table stays empty.</p>
<p>I don’t see any INSERT logs for data.sql.</p>
<p><strong>What I’ve tried:</strong></p>
<p>Verified sql.init.mode=always.</p>
<p>Placed data.sql under src/main/resources.</p>
<p>Tried create-drop for ddl-auto.</p>
<p>Disabled Flyway in dev (flyway.enabled=false).</p>
<p>Looked through startup logs — no sign of data.sql being</p>
<p>executed.</p>
<p>❓ <strong>Question:</strong></p>
<p>How can I configure Spring Boot so that in the dev profile it:</p>
<p>1. Uses Hibernate (ddl-auto=create-drop) to create the schema, and</p>
<p>2. Executes data.sql to populate the question table (with PostgreSQL running in Docker)?</p>
",0,1,1,2025-09-27T15:01:15+00:00,1,106,False
79777138,23343676,,postgresql,PostgreSql Database Design,"<p>While designing social media application, i want my my users to have unique email and unique username, but the problem is, I am providing an otp verification and my table have coulmn id int primary key, (createdAt,updatedAt,deletedAt timestamp), username varchar unique, email varchar unique, passwordHash varchar,is_verified bool</p>
<p>so while trying to sign up if username and email is unique, user data is stored in table with is_verified column false, upon otp verification only is_verified column becomes true. but the question is, if users tried to sign up and didn't do otp verification and other users come to signup and see that username is already taken,
and people can also do try to take sign up with random email, this will also take away lot of emails, i can clean up is_verified false columns after 24 hours, but still it's not good from user perspective right? like when user tried to sign up and for some reason he couldn't sign up at the moment and when he tries again he sees already email and usename is taken.
i could do one thing that is, if the email is already there as is_verified false, i could overwrite new details onto that column.
again another problem comes is when a genuine user tries to sign up, a fake user can overwrite this details using the genuine user's email
how can i handle this?</p>
<p>How would do you handle this? am i overthinking too much? or is this a real scenario?
or should i just make a temporary users table and remove is_verified columns from users table and move this details to temporary users and only after otp verification should i move details from temporary users to users?
but again there same problem can arrive like if no unique constraint give to temporary table, duplicate email or username may present in temporary users table and after otp verification, they can't be inserted on users table, if i am providing unique constraints in temporary users table, the problem just goes back to first problem?</p>
",-3,0,3,2025-09-28T05:25:35+00:00,2,101,True
79777414,12267189,,postgresql,User context and permissions when writing files from inside a plsh function in PostgreSQL on Ubuntu,"<p>I have a database on a Ubuntu system, that belongs to the postgresql user <code>kalle</code> (the owner of the database and the tables).
That user is also the owner of the trigger function <code>getimage</code>, written in the language <code>plsh</code>.</p>
<p>If someone does an insert in a database table <code>picturetable</code> a trigger is called, that starts a small python script copying an image to a directory:</p>
<pre><code>CREATE OR REPLACE FUNCTION getimage() RETURNS trigger AS $$
#!/bin/bash
..
...
python3 /my_path/pictures.py $uebergabe
...
$$ LANGUAGE plsh;
</code></pre>
<p>The images shall be copied to the directory:
<code>/bla/blabla/my_image_directory</code></p>
<p>This all works perfectly if I have done a <code>chmod 777</code> to <code>my_image_directory</code> (which does not seem to be the right way).</p>
<p>At the Ubuntu server <code>my_image_directory</code> has the owner <code>root</code> and the group <code>www-data</code>.</p>
<p>If I do a <code>chmod 775 my_image_directory</code> and afterwards I insert an entry to the table, it does not work at all. In the PostgreSQL log I get:</p>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>PermissionError: [Errno 13] Permission denied: '/bla/blabla/my_image_directory/Q20250928163831030.jpg'
2025-09-28 14:39:04.239 UTC [459416] kalle@picturetable STATEMENT: 
INSERT INTO &quot;public&quot;.&quot;digitalisierung&quot;(&quot;geom&quot;,&quot;digitalisierer&quot;,&quot;art&quot;,&quot;text&quot;,&quot;bild&quot;,&quot;bezeichnung&quot;,&quot;datum&quot;,&quot;bild_blob&quot;) 
VALUES (st_geomfromwkb($1::bytea,4326),'hallo','fund_sonde','Xxxx','Q20250928163831030.jpg','Qqqq','2025-09-28',NULL)
RETURNING &quot;fid&quot;
</code></pre>
</blockquote>
<p><strong>My question is:</strong> what rights are necessary to <code>my_image_directory</code> to save a file there, that is triggered in the mentioned way? I don't want to set a <code>chmod 777</code> (or do I have to do that?)</p>
<p>I tried <code>chmod 775</code> with <code>kalle:www-data</code> and also with <code>postgres:www-data</code>, but I still get the
<code>PermissionError: [Errno 13] Permission denied: '/bla/blabla/my_image_directory/Q20250928163831030.jpg'</code></p>
<hr />
<p>Another weird thing: If I have done a <code>777</code> to <code>my_image_directory</code>, the images is saved there, but the owner in the Ubuntu filesystem is <code>postgres</code> and NOT the user <code>kalle</code>, that writes in the table and is the owner of the trigger.</p>
",2,2,0,2025-09-28T15:28:36+00:00,1,84,True
79777829,10203944,,postgresql,DBeaver isn&#39;t refreshing the tables in the database navigator. I could see the tables when I view the schema explictly,"<p>I'm on dbeaver Version 25.0.5.202505181758. I could see the table being created on this fresh postgres instance, but couldn't see the table in the database navigator.</p>
<p>I have tried to refresh the database navigator manually, edit the connection and refresh it again. Still not getting the table listed in under the public schema.</p>
<p>What other information I can provide to solve this issue?</p>
<p><a href=""https://i.sstatic.net/j4DzSqFd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j4DzSqFd.png"" alt=""enter image description here"" /></a></p>
",0,0,0,2025-09-29T07:07:02+00:00,0,136,False
79778099,9137657,,postgresql,Creating a build ledger for a dbt model,"<p>I am working on a PostgreSQL + dbt data platform where a mart table provides data for an API. Every time a row in the model is updated (incremental update) last_update timestamp should also be updated to communicate the API user that changes have been made.</p>
<p>However, this data needs to persistent so that when we do scheduled full refreshes, the last_update dates can be repopulated into the corresponding rows in the mart table from a persistent build ledger. Does someone have suggestions for an optimal pattern to implement this?</p>
<p>Options I've been considering</p>
<ol>
<li><p>Just use another dbt model (risking over-running rows with full-refresh)</p>
</li>
<li><p>Post-hook (cumbersome logic)</p>
</li>
</ol>
<p>Are these my only options? Or is there something I have overlooked here?</p>
",1,1,0,2025-09-29T12:38:37+00:00,1,58,True
79778744,22131651,,postgresql,Django migrations error: psycopg2.errors.InsufficientPrivilege: permission denied for schema public. How to resolve this error?,"<p>I’m trying to run Django migrations on my local PostgreSQL database, but I’m hitting a permission error.</p>
<p><strong>Setup:</strong></p>
<ul>
<li>Windows 10</li>
<li>PostgreSQL installed locally</li>
<li>Django project using PostgreSQL (psycopg2)</li>
</ul>
<p>Steps I did:</p>
<ol>
<li><p>Created a database in PostgreSQL:</p>
<p>CREATE DATABASE tradingdb;</p>
</li>
<li><p>Created a user:</p>
<p>CREATE USER myuser WITH PASSWORD 'mypassword';</p>
</li>
<li><p>Granted privileges:</p>
<p>GRANT ALL PRIVILEGES ON DATABASE tradingdb TO myuser;</p>
</li>
<li><p>Updated my Django settings.py:</p>
<p>DATABASES = {
'default': {
'ENGINE': 'django.db.backends.postgresql',
'NAME': 'tradingdb',
'USER': 'myuser',
'PASSWORD': 'mypassword',
'HOST': 'localhost',
'PORT': '5432',
}
}</p>
</li>
<li><p>Ran:</p>
<p>python manage.py migrate</p>
</li>
</ol>
<p><strong>Error I get:</strong></p>
<pre><code>django.db.utils.ProgrammingError: permission denied for schema public
LINE 1: CREATE TABLE &quot;django_migrations&quot; (&quot;id&quot; bigint NOT NULL PRIMA...
</code></pre>
",0,0,0,2025-09-30T06:08:12+00:00,1,78,True
79779353,2813152,Germany,postgresql,failed to send batch: ERROR: Out of memory (SQLSTATE XX000),"<p>When calling <code>supabase db reset</code> I get this error: <code>failed to send batch: ERROR: Out of memory (SQLSTATE XX000)</code>.</p>
<p>It breaks because of one migration where I create a db function that uses the postgres net extension.</p>
<pre><code>-- Function to send Slack notification for new users
CREATE OR REPLACE FUNCTION public.send_slack_new_user_notification(new_user_record jsonb)
    RETURNS void
    LANGUAGE plpgsql
    SECURITY DEFINER
    set search_path = public
AS
$function$
DECLARE
  _project_url text;
  _anon_key text;
  _url text;
  _headers jsonb;
BEGIN
  -- Get project URL and anon key dynamically
  SELECT public.get_project_url() INTO _project_url;
  SELECT public.get_anon_key() INTO _anon_key;

  -- Construct the full function URL
  _url := format('%s/functions/v1/on_new_user_slack_notify', _project_url);

  -- Construct the headers with the anon key
  _headers := format('{&quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: &quot;Bearer %s&quot;}', _anon_key)::jsonb;

  -- Perform a web request to our Edge Function
  select &quot;net&quot;.&quot;http_post&quot;(
    url:= _url,
    body:=jsonb_build_object('record', new_user_record),
    headers:= _headers
  );
END;
$function$;
</code></pre>
<p>When I comment the lines where I make the request it works.</p>
<p>I tried to increase memory in db.settings (shared_buffers, work_mem, maintenance_work_mem) but it did not help.</p>
<p>Does anyone has an idea how to resolve this?</p>
",0,0,0,2025-09-30T17:08:56+00:00,0,94,False
79779750,20903742,,postgresql,django admin migration with postgres_fdw,"<p>I use postgres_fdw to create relations between my databases
Here I have a foreign model with the name:</p>
<pre><code>class User(AbstractUser):
    ...
    class Meta:
        db_table = '&quot;foreign_user&quot;.&quot;accounts__user&quot;'
        managed = False
</code></pre>
<p>which pointing to this model in another django project:</p>
<pre><code>class User(AbstractUser):
    ...
    class Meta:
        db_table = 'accounts__user'
        managed = False
</code></pre>
<p>I did run the <code>python manage.py makemigrations</code> and <code>python manage.py migrate</code> I got an error from django admin</p>
<blockquote>
<p>Applying admin.0001_initial...Traceback (most recent call last):
...
django.db.utils.ProgrammingError: referenced relation &quot;accounts__user&quot; is not a table</p>
</blockquote>
<p>but when I ran migrations with specific apps (my installed app only) and the admin migrations didn't run, everything worked well and all relations with <strong>User</strong> model created.</p>
<p>I also made sure that I set <code>AUTH_USER_MODEL='accounts.User</code>
I already made the <strong>foreing server</strong>s and <strong>foreign schema</strong>s as needed
I did check the database, and all of the tables and relations were fine</p>
",1,1,0,2025-10-01T07:31:17+00:00,1,50,True
79779757,1424729,"Moskva, Москва, Россия",postgresql,How to have fixed Postgres port in Quarkus dev container,"<p>Each time my Quarkus app restarts it has a different Postgres port. There is <code>testcontainers.reuse.enable=false</code> in the <code>~/.testcontainers.properties</code>. MacOS, run from Idea.</p>
",1,1,0,2025-10-01T07:42:55+00:00,1,76,True
79779871,2616474,France,postgresql,Postgresql with daterange on the same day,"<p>I'm trying to create a virtual column called <code>period</code> created by 2 other columns <code>starting_date</code> (time) and <code>ending_date</code> (time).</p>
<p>The aim is to avoid overlapping of periods for the same user.</p>
<p>Here is my current SQL</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE public.classrooms (
    id bigint generated by default as identity primary key,
    levels character varying[] DEFAULT '{}'::character varying[] NOT NULL,
    year integer NOT NULL,
    created_at timestamp(6) without time zone NOT NULL,
    updated_at timestamp(6) without time zone NOT NULL
);
CREATE TABLE public.absences (
    id bigint generated by default as identity primary key,
    classroom_id bigint NOT NULL,
    absentable_type character varying NOT NULL,
    absentable_id bigint NOT NULL,
    absence_type integer DEFAULT 0,
    starting_date timestamp(6) without time zone NOT NULL,
    ending_date timestamp(6) without time zone NOT NULL,
    period daterange GENERATED ALWAYS AS (daterange((starting_date)::date, ((ending_date)::date + 1), '[)'::text)) STORED,
    created_at timestamp(6) without time zone NOT NULL,
    updated_at timestamp(6) without time zone NOT NULL
);
CREATE EXTENSION IF NOT EXISTS btree_gist;
ALTER TABLE ONLY public.absences
    ADD CONSTRAINT index_absences_no_overlapping_periods 
    EXCLUDE USING gist (  absentable_id WITH =
                        , absentable_type WITH =
                        , absence_type WITH =
                        , period WITH &amp;&amp;);
</code></pre>
<pre class=""lang-sql prettyprint-override""><code>INSERT INTO &quot;absences&quot; 
(&quot;classroom_id&quot;, &quot;absentable_type&quot;, &quot;absentable_id&quot;, &quot;absence_type&quot;, &quot;starting_date&quot;, &quot;ending_date&quot;, &quot;created_at&quot;, &quot;updated_at&quot;) 
VALUES 
(1, 'Student', 147, 0, '2025-09-29 06:30:00', '2025-09-29 10:00:00', '2025-10-01 09:06:37.736167', '2025-10-01 09:06:37.736167') 
RETURNING &quot;id&quot;, &quot;period&quot;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">id</th>
<th style=""text-align: left;"">period</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">[2025-09-29,2025-09-30)</td>
</tr>
</tbody>
</table></div>
<p>First I was using <code>daterange((starting_date)::date, (ending_date)::date)</code> but if <code>starging_date</code> and <code>ending_date</code> are on the same day, Postgresql stores <code>ending_date + 1 day</code>.</p>
<p>My current usage of <code>daterange</code> on <code>AS</code> uses<br />
<code>daterange((starting_date)::date, ((ending_date)::date + 1), '[)'::text)</code>.</p>
<p>On both cases, range is over 2 days.</p>
<p>Maybe my approach using virtual column to add a constrain on is not correct. Changing from <code>daterange</code> to <code>tsrange</code> side-steps the issue.</p>
",1,1,0,2025-10-01T09:35:52+00:00,1,74,True
79780370,26559755,,postgresql,Converting DateTime calculation from SQL Server to Postgres,"<p>There are two dates of type datetimeoffsets.
The dates are combined in the following way to create a third date;
Get the date part from date one.
Get the time part from date two, removing milliseconds and not rounding the seconds.
Get the timezone offset value from date one.
Add these together to get date three.</p>
<p>In SQL Server the following SQL returns date as 2024-01-01 04:05:06.0000000 +02:30</p>
<pre><code>DECLARE @date1 DATETIMEOFFSET = '1 Jan 2024 01:02:03.1234567 +02:30',
        @date2 DATETIMEOFFSET = '2 Feb 2025 04:05:06.8888888 +01:30';
SELECT DATETIMEOFFSETFROMPARTS(DATEPART(YEAR, @date1),
                               DATEPART(MONTH, @date1),
                               DATEPART(DAY, @date1),
                               DATEPART(HOUR, @date2),
                               DATEPART(MINUTE, @date2),
                               DATEPART(SECOND, @date2),
                               0,
                               DATEPART(TZOFFSET, @date1) / 60,
                               DATEPART(TZOFFSET, @date1) % 60,
                               7);
</code></pre>
<p>How do I do this in Postgres to return <code>2024-01-01 04:05:06.0000000 +02:30</code>?</p>
<p>My Postgres timezone is UTC.</p>
<p>I tried below which returns <code>2023-12-31 02:35:06 +00:00</code>;</p>
<pre><code>WITH input AS (SELECT TIMESTAMPTZ '2024-01-01 01:02:03.1234567+02:30' AS dDate1,
                      TIMESTAMPTZ '2025-02-02 04:05:06.8888888+01:30' AS dDate2),
     parts AS (SELECT dDate1::DATE                              AS date_part,
                      TO_CHAR(dDate2, 'HH24:MI:SS')             AS time_part,
                      EXTRACT(TIMEZONE_HOUR FROM dDate1)::int   AS tz_hour,
                      EXTRACT(TIMEZONE_MINUTE FROM dDate1)::int AS tz_minute
               FROM input)
SELECT (
           date_part || ' ' || time_part ||
           CASE WHEN tz_hour &gt;= 0 THEN ' +' ELSE ' -' END ||
           LPAD(ABS(tz_hour)::text, 2, '0') || ':' ||
           LPAD(tz_minute::text, 2, '0')
           ) AS result
FROM parts;
</code></pre>
<p>and below which returns <code>2024-01-01 04:05:06.000000</code>, with no offset and offsets hardcoded.</p>
<pre><code>WITH input AS (SELECT TIMESTAMPTZ '2024-01-01 01:02:03.1234567+02:30' AS dDate1,
                      TIMESTAMPTZ '2025-02-02 04:05:06.8888888+01:30' AS dDate2)
select (dDate1::timestamp at time zone '+02:30')::date + date_trunc('second', dDate2::timestamp at time zone '+01:30')::time
FROM input;
</code></pre>
",2,2,0,2025-10-01T18:39:11+00:00,2,88,True
79780392,434949,"Landgraaf, Netherlands",postgresql,Postgresql stops using filter index,"<p>I have a very simple table for a task queue and keep taking records out one by one. If I add the <code>filter_selector</code> after filling/resetting the table, postgresql will use it and run super fast. But if I reset the table by calling <code>UPDATE &quot;public&quot;.&quot;testqueue&quot; T SET &quot;status&quot;=0</code> then it will start using the primary key which is 10 times slower.</p>
<p>How can I make sure postgresql keeps using the <code>filter_selector</code>?</p>
<p>Table:</p>
<pre><code>CREATE TABLE PUBLIC.&quot;testqueue&quot; (
    &quot;id&quot; BIGINT NOT NULL GENERATED ALWAYS AS IDENTITY,
    &quot;guid&quot; &quot;uuid&quot; NOT NULL DEFAULT GEN_RANDOM_UUID (),
    &quot;created&quot; TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT NOW(),
    &quot;status&quot; SMALLINT NOT NULL DEFAULT 0,
    &quot;notbefore&quot; TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT NOW(),
    &quot;task&quot; CHARACTER VARYING(255) NOT NULL,
    &quot;data&quot; &quot;text&quot; NULL,
    &quot;completed&quot; TIMESTAMP WITHOUT TIME ZONE NULL,
    PRIMARY KEY (ID)
)


INSERT INTO &quot;public&quot;.&quot;testqueue&quot; (&quot;task&quot;) SELECT 'DoSomething' FROM GENERATE_SERIES(1, 1000000)
</code></pre>
<p>Filter index:</p>
<pre><code>CREATE  INDEX &quot;filter_selector&quot; ON &quot;public&quot;.&quot;testqueue&quot; (&quot;id&quot;) INCLUDE (&quot;status&quot;,&quot;notbefore&quot;) WHERE &quot;status&quot;=0
</code></pre>
<p>Explain before updating every status to 0, after ~100000 runs:</p>
<pre><code>Update on public.testqueue t  (cost=0.98..9.02 rows=1 width=40) (actual time=0.063..0.065 rows=1 loops=1)
  Output: t.id
  Buffers: shared hit=16
  CTE cte
    -&gt;  Limit  (cost=0.42..0.55 rows=1 width=14) (actual time=0.032..0.032 rows=1 loops=1)
          Output: testqueue.id, testqueue.ctid
          Buffers: shared hit=6
          -&gt;  LockRows  (cost=0.42..129018.39 rows=999967 width=14) (actual time=0.031..0.032 rows=1 loops=1)
                Output: testqueue.id, testqueue.ctid
                Buffers: shared hit=6
                -&gt;  Index Scan using filter_selector on public.testqueue  (cost=0.42..119018.72 rows=999967 width=14) (actual time=0.027..0.028 rows=1 loops=1)
                      Output: testqueue.id, testqueue.ctid
                      Filter: ((testqueue.status = 0) AND (testqueue.notbefore &lt;= now()))
                      Buffers: shared hit=5
  -&gt;  Nested Loop  (cost=0.42..8.46 rows=1 width=40) (actual time=0.047..0.047 rows=1 loops=1)
        Output: '1'::smallint, t.ctid, cte.*
        Inner Unique: true
        Buffers: shared hit=10
        -&gt;  CTE Scan on cte  (cost=0.00..0.02 rows=1 width=40) (actual time=0.037..0.037 rows=1 loops=1)
              Output: cte.*, cte.id
              Buffers: shared hit=6
        -&gt;  Index Scan using testqueue_pkey on public.testqueue t  (cost=0.42..8.44 rows=1 width=14) (actual time=0.008..0.008 rows=1 loops=1)
              Output: t.ctid, t.id
              Index Cond: (t.id = cte.id)
              Buffers: shared hit=4
Planning Time: 0.121 ms
Execution Time: 0.088 ms
</code></pre>
<p>Explain after updating every status to 0, after ~100000 runs:</p>
<pre><code>Update on public.testqueue t  (cost=0.98..9.02 rows=1 width=40) (actual time=25.514..25.517 rows=1 loops=1)
  Output: t.id
  Buffers: shared hit=97078 dirtied=9
  CTE cte
    -&gt;  Limit  (cost=0.42..0.56 rows=1 width=14) (actual time=25.478..25.479 rows=1 loops=1)
          Output: testqueue.id, testqueue.ctid
          Buffers: shared hit=97072 dirtied=9
          -&gt;  LockRows  (cost=0.42..133659.01 rows=999967 width=14) (actual time=25.477..25.478 rows=1 loops=1)
                Output: testqueue.id, testqueue.ctid
                Buffers: shared hit=97072 dirtied=9
                -&gt;  Index Scan using testqueue_pkey on public.testqueue  (cost=0.42..123659.34 rows=999967 width=14) (actual time=25.466..25.467 rows=1 loops=1)
                      Output: testqueue.id, testqueue.ctid
                      Filter: ((testqueue.status = 0) AND (testqueue.notbefore &lt;= now()))
                      Rows Removed by Filter: 100038
                      Buffers: shared hit=97071 dirtied=9
  -&gt;  Nested Loop  (cost=0.42..8.46 rows=1 width=40) (actual time=25.498..25.499 rows=1 loops=1)
        Output: '1'::smallint, t.ctid, cte.*
        Inner Unique: true
        Buffers: shared hit=97076 dirtied=9
        -&gt;  CTE Scan on cte  (cost=0.00..0.02 rows=1 width=40) (actual time=25.487..25.488 rows=1 loops=1)
              Output: cte.*, cte.id
              Buffers: shared hit=97072 dirtied=9
        -&gt;  Index Scan using testqueue_pkey on public.testqueue t  (cost=0.42..8.44 rows=1 width=14) (actual time=0.007..0.007 rows=1 loops=1)
              Output: t.ctid, t.id
              Index Cond: (t.id = cte.id)
              Buffers: shared hit=4
Planning:
  Buffers: shared hit=6 dirtied=1
Planning Time: 0.147 ms
Execution Time: 25.547 ms
</code></pre>
<p>Select:</p>
<pre><code>  WITH CTE AS (
    SELECT &quot;id&quot; 
    FROM &quot;public&quot;.&quot;testqueue&quot;
    WHERE &quot;status&quot;=0 
    AND &quot;notbefore&quot;&lt;=NOW()
    ORDER BY &quot;id&quot;
    LIMIT 1
    FOR UPDATE SKIP LOCKED
  )

  UPDATE &quot;public&quot;.&quot;testqueue&quot; T 
  SET &quot;status&quot;=1
  FROM CTE
  WHERE CTE.&quot;id&quot;=T.&quot;id&quot;
  RETURNING T.&quot;id&quot;
</code></pre>
<p><strong>Update on comments:</strong></p>
<p>I have included the entire explain with verbose, buffer and settings checked. I have changed the <code>guid</code> back to <code>id</code>. That was an attempt to trigger postgresql into not using the pkey, but it didn't work. The <code>testqueue6</code> was a simple token I forgot to remove because I have been running multiple test.</p>
<p>Changing <code>SELECT *</code> to <code>SELECT &quot;id&quot;</code> in the <code>CTE</code> doesn't change anything.</p>
<p>The whole thing is a follow up on the <a href=""https://stackoverflow.com/q/79775969/434949"">same question on SQL server</a> which included a tip about the filtered index. The index does work very well and does not seem to be the problem on it's own. It's just that postgresql doesn't use it all the time.</p>
<p>When using the filtered index it can handle roughly 2500 tasks per second no matter how many have been handled. With the primary key it slows down over time, starting at 800 a second and ending on about 10 per second when the list gets used more.</p>
<p>This gets reflected in the <code>EXPLAIN</code> as well. When using the <code>filter_selector</code> it always says <code>Rows Removed by Filter: 0</code>. But when using the primary key it changes to <code>Rows Removed by Filter: 100009</code> when running it over 100000 times.</p>
",2,2,0,2025-10-01T19:09:21+00:00,4,103,True
79780405,2800519,,postgresql,Does the pg.Pool from node-postgres support acquireTimeoutMillis?,"<p>I'm having a hard time with an app that has some spikes in connections. I think if acquireTimeoutMillis was working these would be queued pre-database. Trying to set it in testing doesn't seem to be affecting the tests.</p>
<p>The way I'm testing is to have a config like:</p>
<pre><code>      max: 1,
      idleTimeoutMillis: 30000,
      connectionTimeoutMillis: 5000,
      acquireTimeoutMillis: 1,
</code></pre>
<p>I would expect doing a lot of concurrent requests would fail here as the pool would be tied up and acquiring a connection would not work.</p>
<p>I'm a bit confused as the doc in <a href=""https://github.com/brianc/node-postgres/tree/master/packages/pg-pool"" rel=""nofollow noreferrer"">https://github.com/brianc/node-postgres/tree/master/packages/pg-pool</a> which states that all options from <a href=""https://github.com/coopernurse/node-pool"" rel=""nofollow noreferrer"">https://github.com/coopernurse/node-pool</a> should work. What am I missing?</p>
",2,2,0,2025-10-01T19:28:45+00:00,1,60,True
79780715,9298910,,postgresql,Postgres Tree Structure To Array,"<p>I have a table with a simple tree structure like this:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE &quot;foo&quot; (
    &quot;fooId&quot; INT PRIMARY KEY,
    &quot;parentFooId&quot; INT NULL REFERENCES &quot;foo&quot; (&quot;fooId&quot;)
)
</code></pre>
<p>When querying all of the (direct and indirect) parents of an element, I would need to do a recursive query. However, I am in a situation where that is not very practical.</p>
<p>So, what I want to do instead is to have a second table that stores all of an elements parent elements in an array (this table will automatically be updated via a trigger).</p>
<p>For example, this data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fooId</th>
<th>parentFooId</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>null</td>
</tr>
<tr>
<td>2</td>
<td>null</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
</tr>
<tr>
<td>7</td>
<td>5</td>
</tr>
</tbody>
</table></div>
<p>would result in these two trees:
<a href=""https://i.sstatic.net/bZ09X1aU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZ09X1aU.png"" alt=""Tree"" /></a></p>
<p>This, in turn, should result in this lookup-table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>childId</th>
<th>parents</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>{}</td>
</tr>
<tr>
<td>2</td>
<td>{}</td>
</tr>
<tr>
<td>3</td>
<td>{1}</td>
</tr>
<tr>
<td>4</td>
<td>{1}</td>
</tr>
<tr>
<td>5</td>
<td>{1,3}</td>
</tr>
<tr>
<td>6</td>
<td>{2}</td>
</tr>
<tr>
<td>7</td>
<td>{1,3,5}</td>
</tr>
</tbody>
</table></div>
<p>Note:</p>
<ol>
<li>The order of the elements in the array does not matter to me.</li>
<li>I would also be OK if the elements without any children would not be in the lookup table.</li>
</ol>
<p>I am struggeling to find the correct SQL for this job. I've tried something like this:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH RECURSIVE &quot;parents&quot; AS (
SELECT &quot;foo&quot;.&quot;fooId&quot;, &quot;foo&quot;.&quot;parentFooId&quot;, &quot;foo&quot;.&quot;fooId&quot; AS &quot;parentId&quot;
FROM &quot;foo&quot;
    
UNION

SELECT &quot;nextFoo&quot;.&quot;fooId&quot;, &quot;nextFoo&quot;.&quot;parentFooId&quot;, &quot;parent&quot;.&quot;parentId&quot; AS &quot;parentId&quot;
FROM &quot;parents&quot; &quot;parent&quot;
    JOIN &quot;foo&quot; &quot;nextFoo&quot; ON (&quot;nextFoo&quot;.&quot;fooId&quot; = &quot;parent&quot;.&quot;parentFooId&quot;)
)
SELECT &quot;parentId&quot;, array_agg(&quot;fooId&quot;) AS &quot;parents&quot; FROM &quot;parents&quot; WHERE &quot;fooId&quot; != &quot;parentId&quot; GROUP BY &quot;parentId&quot;;
</code></pre>
<p>What I have tried here is to pass down the parent's ID using <code>parentId</code>. However, this does not work and the resulting <code>parents</code> array only ever contains the direct children.</p>
",1,1,0,2025-10-02T08:05:27+00:00,3,85,True
79780906,12980093,,postgresql,Why correlated scalar is 10x times slower in MySQL comparing to PG,"<p>Let's create a table with 3000 rows</p>
<pre><code>create table tt(id int, txt text);

insert into tt
with recursive r(id) as
(select 1 union all select id + 1 from r where id &lt; 3e3)
select id, concat('name', id)
from r;
</code></pre>
<p>The same query in both databases results in very different performance:</p>
<pre><code>select sum(id), 
       sum((select count(*) 
            from tt t1 
            where t1.id = t2.id)) cnt
from tt t2
</code></pre>
<p><strong>MYSQL</strong></p>
<pre><code>mysql&gt; explain analyze
    -&gt; select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
    -&gt; from tt t2\G
*************************** 1. row ***************************
EXPLAIN: -&gt; Aggregate: sum(t2.id), sum((select #2))  (cost=602 rows=1) (actual time=7542..7542 rows=1 loops=1)
    -&gt; Table scan on t2  (cost=302 rows=3000) (actual time=0.025..2.75 rows=3000 loops=1)
-&gt; Select #2 (subquery in projection; dependent)
    -&gt; Aggregate: count(0)  (cost=62.5 rows=1) (actual time=2.51..2.51 rows=1 loops=3000)
        -&gt; Filter: (t1.id = t2.id)  (cost=32.5 rows=300) (actual time=1.25..2.51 rows=1 loops=3000)
            -&gt; Table scan on t1  (cost=32.5 rows=3000) (actual time=0.00256..2.31 rows=3000 loops=3000)

1 row in set, 1 warning (7.54 sec)
</code></pre>
<p><strong>PG</strong></p>
<pre><code>postgres=# explain analyze
postgres-# select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
postgres-# from tt t2;
                                                   QUERY PLAN
-----------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=163599.50..163599.51 rows=1 width=40) (actual time=684.339..684.340 rows=1 loops=1)
   -&gt;  Seq Scan on tt t2  (cost=0.00..47.00 rows=3000 width=4) (actual time=0.013..0.223 rows=3000 loops=1)
   SubPlan 1
     -&gt;  Aggregate  (cost=54.50..54.51 rows=1 width=8) (actual time=0.227..0.227 rows=1 loops=3000)
           -&gt;  Seq Scan on tt t1  (cost=0.00..54.50 rows=1 width=0) (actual time=0.113..0.223 rows=1 loops=3000)
                 Filter: (id = t2.id)
                 Rows Removed by Filter: 2999
 Planning Time: 0.663 ms
 Execution Time: 684.512 ms
(9 rows)
</code></pre>
<p>As you can see the difference is ~7.0 seconds vs ~0.7 seconds.</p>
<p>So it both cases it does not unnest subquery and executes it 3000 times.</p>
<p>But in MySQL one execution takes 2+ ms while in PG it takes 0.2 ms.</p>
<p><strong>Question asked to understand this difference and not to make query faster.</strong></p>
<p><strong>Obviously we can create an index or rewrite to explicit join.</strong></p>
<pre><code>select sum(t1.id), sum(cnt) cnt
from tt t2
join (select id, sum(1) cnt from tt group by id) t1 on t1.id = t2.id;
</code></pre>
<p>PS. Both RDBMS have default settings.</p>
<pre><code>mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 8.0.43    |
+-----------+
1 row in set (0.00 sec)

postgres=# select version();
                          version
------------------------------------------------------------
 PostgreSQL 15.1, compiled by Visual C++ build 1914, 64-bit
(1 row)
</code></pre>
<p><strong>UPDATE</strong></p>
<p>As requested in the comments - adding comparison on Ubuntu.</p>
<p><strong>Still 10x difference.</strong></p>
<p><strong>MYSQL</strong></p>
<pre><code>mysql&gt; explain analyze
    -&gt; select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
    -&gt; from tt t2\G
*************************** 1. row ***************************
EXPLAIN: -&gt; Aggregate: sum(t2.id), sum((select #2))  (cost=600 rows=1) (actual time=5416..5416 rows=1 loops=1)
    -&gt; Table scan on t2  (cost=300 rows=3000) (actual time=0.0244..5.38 rows=3000 loops=1)
-&gt; Select #2 (subquery in projection; dependent)
    -&gt; Aggregate: count(0)  (cost=60.3 rows=1) (actual time=1.79..1.79 rows=1 loops=3000)
        -&gt; Filter: (t1.id = t2.id)  (cost=30.3 rows=300) (actual time=0.906..1.79 rows=1 loops=3000)
            -&gt; Table scan on t1  (cost=30.3 rows=3000) (actual time=0.00668..1.56 rows=3000 loops=3000)

1 row in set, 1 warning (5.41 sec)
</code></pre>
<p><strong>PG</strong></p>
<pre><code>postgres=# explain analyze
select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
from tt t2;
                                                   QUERY PLAN                                                    
-----------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=163599.50..163599.51 rows=1 width=40) (actual time=526.720..526.721 rows=1 loops=1)
   -&gt;  Seq Scan on tt t2  (cost=0.00..47.00 rows=3000 width=4) (actual time=0.012..0.301 rows=3000 loops=1)
   SubPlan 1
     -&gt;  Aggregate  (cost=54.50..54.51 rows=1 width=8) (actual time=0.175..0.175 rows=1 loops=3000)
           -&gt;  Seq Scan on tt t1  (cost=0.00..54.50 rows=1 width=0) (actual time=0.087..0.173 rows=1 loops=3000)
                 Filter: (id = t2.id)
                 Rows Removed by Filter: 2999
 Planning Time: 0.078 ms
 Execution Time: 526.766 ms
(9 rows)
</code></pre>
",0,4,4,2025-10-02T12:16:14+00:00,2,223,True
79781144,17303566,,postgresql,Filtering Out Null Keys in JSON Output Using Apache AGE Cypher Query with json_object_agg,"<p>Is there a way to filter out null keys for my json? This is using Apache AGE extension:</p>
<pre><code>SELECT json_object_agg(k, v)
FROM cypher('hermech', $$
  MATCH (a:contact_name)
  WHERE id(a) = 10133099161583644
  OPTIONAL MATCH (a)-[:DIRECT_EMAIL_ADDRESS]-&gt;(b:direct_email_address)
  OPTIONAL MATCH (a)&lt;-[:CONTACT_NAME]-(c:account_name)
  OPTIONAL MATCH (c)-[:MAIN_PHONE_NUMBER]-&gt;(d:main_phone_number)
  OPTIONAL MATCH (c)-[:MAIN_STREET_ADDRESS]-&gt;(e:main_street_address)
  OPTIONAL MATCH (c)-[:MAIN_DOMAIN]-&gt;(f:main_domain)
  WITH [a,b,c,d,e,f] AS nodes
  UNWIND nodes AS n
  RETURN label(n) AS k, n.value AS v
$$) AS (k text, v text);
</code></pre>
<p>It breaks because <code>label(n)</code> doesn't exist since <code>f</code> doesn't exist in the dataset so it's a non-existent key in the json output.</p>
<p>I tried putting a <code>WHERE</code> clause after <code>UNWIND</code> but apparently that isn't proper syntax according to the parser.</p>
",1,1,0,2025-10-02T16:55:06+00:00,1,56,False
79781388,31618388,,postgresql,Searchkick hanging on postgres schema calls during BulkReindexJob,"<p>We are running a large model reindex asyncronously with a <code>Model.reindex(mode: :async)</code> call.</p>
<p>When we do this, we will see a rise in the the database calls to the following call to Postgres Schema information with PgClass#find</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT c.relname 
FROM pg_class c 
LEFT JOIN pg_namespace n ON n.oid = c.relnamespace 
WHERE n.nspname = ANY (current_schemas(false)) AND c.relname = ? AND c.relkind IN (?);
</code></pre>
<p>This statement seems to be coming from <code>data_source_sql</code> in <code>active_record/connection_adapters/postgresql/schema_statements.rb</code>.</p>
<p>We are creating and loading the schema_cache.yml file on boot with sidekiq that is running the BulkReindexJobs for searchkick.</p>
<p>Any clue why we are getting such locking on this <code>data_source_sql</code> statement when running parallel BulkReindexJobs?</p>
<p>Tried debugging, increasing an decresing parallel workers.  Confirmed the use of a schema_cache.yml file to prevent needing to (I thought) do this query</p>
",0,0,0,2025-10-03T01:21:16+00:00,0,50,False
79782665,2052584,,postgresql,Query upcoming birthday date,"<p>My table looks like</p>
<pre><code>ID | PersonName | Birthday   | IntDate
--------------------------------------
 1 | Joe        | 1977-08-20 |     820
 2 | Sandy      | 1985-02-27 |     227
 3 | Jane       | 1981-11-01 |    1101
</code></pre>
<p>The <code>INDEX</code>ed helper column <code>IntDate</code> is just an int = <code>month*100+days</code>, written at insert/update and used to speed up queries for all birthdays in a given date range (e.g. between [1001;1031] for October) independent of the year. (So I don't have to filter based on <code>EXTRACT</code> or <code>DATE_PART</code> functions for day and month in the WHERE clause, as the birthday DATE includes a year in the past.)</p>
<p>Given a certain date (e.g. <code>2025-05-01</code>), I want to query the next birthday date of the persons and the upcoming anniversary (what birthday is coming).
Then I want to sort the list by the next birthday date (this or next year).</p>
<p>For <code>2025-05-01</code>, the list might look like:</p>
<pre><code>ID | PersonName | Birthday   | NextBirthday ^ | Anniversary (at upcoming birthday)
-----------------------------------------------------------
 1 | Joe        | 1977-08-20 |   2025-08-20   |          48
 3 | Jane       | 1981-11-01 |   2025-11-01   |          44
 2 | Sandy      | 1985-02-27 |   2026-02-27   |          41
</code></pre>
<p>Is that possible using an SQL SELECT query in PostgreSQL?</p>
",1,3,2,2025-10-04T20:40:30+00:00,3,161,True
79783577,25190704,,postgresql,TimescaleDB hierarchical continuous aggregate with same time_bucket intermittently not refreshing,"<p>I’m working with hierarchical continuous aggregates in TimescaleDB and noticed inconsistent behavior. According to the documentation:</p>
<p>&quot;The time bucket of a continuous aggregate should be greater than or equal to the time bucket of the underlying continuous aggregate. It also needs to be a multiple of the underlying time bucket.&quot;</p>
<p>Question:</p>
<p>Is it expected that a hierarchical CAGG with the same time_bucket may have intermittent automatic refresh issues, even though the documentation seems to allow it?</p>
<p>Is this setup considered supported, or does Timescale only guarantee proper invalidation propagation when the parent bucket is strictly larger and a multiple of the child bucket?</p>
<p>Any clarification or guidance on best practices for hierarchical CAGGs in this scenario would be appreciated.</p>
<p>Setup:</p>
<p>Lower-level CAGG: time_bucket('1 hour', ...), multiple columns.</p>
<p>Higher-level hierarchical CAGG: time_bucket('1 hour', ...), aggregates fewer columns.</p>
<p>Automatic refreshes (policy-based hourly jobs and real-time refresh) sometimes work, sometimes do not.</p>
<p>Manual refresh_continuous_aggregate() always works.</p>
",0,0,0,2025-10-06T10:29:07+00:00,0,41,False
79783671,31550797,,postgresql,Daylight Savings Time function in Postgres,"<p>I am trying to write a function which, given a timestamp dt_, will return TRUE if dt_ falls withing daylight savings time, and FALSE otherwise.</p>
<pre><code>CREATE OR REPLACE FUNCTION is_dst(dt_ timestamp)
    RETURNS boolean
    LANGUAGE 'plpgsql'
    COST 100
    VOLATILE PARALLEL UNSAFE
AS $BODY$
/*
    Returns true if dt_ is within DST and false otherwise 
*/
DECLARE
    boydt timestamp without time zone;
    startdt timestamp without time zone;
    enddt timestamp without time zone;
    dst_start timestamp without time zone;
    dst_end timestamp without time zone;
    mar record;
    nov record;
BEGIN
    boydt := date_trunc('year', dt_);               --First day of year
    startdt := boydt + interval '2 months'; --First day of March
    enddt := startdt + interval '1 months' - interval '1 day';  --Last day of March
    
    SELECT dt, extract(dow from dt) dow,
        row_number() over (partition by extract(dow from dt) order by dt) rn
    into mar
    FROM generate_series(startdt, enddt, '1 day'::interval) dt;
    
    startdt := boydt + interval '10 months';    --First day of November
  enddt := startdt + interval '1 months' - interval '1 day';    --Last day of November
    
    SELECT dt, extract(dow from dt) dow,
        row_number() over (partition by extract(dow from dt) order by dt) rn
    into nov
    FROM generate_series(startdt, enddt, '1 day'::interval) dt;
    
    SELECT dt into dst_start from mar where dow = 0 and rn = 2; --Second Sunday in March
    SELECT dt into dst_end from nov where dow = 0 and rn = 1;   --First Sunday in November
    
    --RETURN (dt_ between dst_start and dst_end);
    RETURN (dt_ between startdt and enddt);
END;
$BODY$;

ALTER FUNCTION is_dst(timestamp)
    OWNER TO postgres;
</code></pre>
<p>The issue is every time I try to run the function (example below):</p>
<pre><code>select is_dst('2025-06-01')  --expect this to return true
</code></pre>
<p>it returns the error message 'relation &quot;mar&quot; does not exist'. I cannot figure out what the reason for the error is.</p>
<p>Can anyone please indicate where the error in my code is?</p>
",-2,0,2,2025-10-06T12:15:52+00:00,2,67,True
79783748,7856143,,postgresql,Cloud Run Spring Boot App fails to connect to Cloud SQL (PostgreSQL) via Proxy: Connection refused,"<p>I'm deploying a Spring Boot application ( forestplus-back ) to Google Cloud Run ( europe-southwest1 ) and trying to connect it to a Cloud SQL PostgreSQL instance ( forsest-plus-db , also in europe-southwest1 ).</p>
<p>The application consistently fails to start with a Connection to localhost:5432 refused error, even after extensive troubleshooting.</p>
<p>My application works correctly when connecting to an external PostgreSQL database (Supabase) using its direct URL. This confirms my Spring Boot application code, environment variable parsing, and web server startup logic are all functioning as expected. The problem is specific to Cloud SQL connectivity via the recommended proxy.</p>
<pre><code>Error Message:
org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

</code></pre>
<p>This error appears during application startup (from Spring Boot's HikariCP connection pool initialization), leading to org.springframework.context.ApplicationContextException: Unable to start web server .</p>
<ul>
<li>Cloud Run Configuration (Verified):</li>
<li>Cloud Run Service: forestplus-back</li>
<li>Region: europe-southwest1</li>
<li>Container Port: 8080</li>
<li>Environment Variables:</li>
<li>DATABASE_URL : jdbc:postgresql://localhost:5432/forsest-plus-db (also tested with postgres database on a new instance)</li>
<li>DATABASE_USERNAME : (Correct username for Cloud SQL instance)</li>
<li>DATABASE_PASSWORD : (Correct password for Cloud SQL instance)</li>
<li>Cloud SQL Connections: The instance nomadic-raceway-473907-s9:europe-southwest1:forsest-plus-db (and later nomadic-raceway-473907-s9:europe-southwest1:test-cloudsql-instance ) is correctly listed and associated under the &quot;Connections&quot; section of the Cloud Run service configuration. This has been removed and re-added multiple times, followed by new deployments.</li>
<li>Service Account: The Cloud Run service account ( 143810247129-compute@developer.gserviceaccount.com ) has the Cloud SQL Client ( roles/cloudsql.client ) IAM role on the project nomadic-raceway-473907-s9 .</li>
<li>Cloud SQL Instance Configuration (Verified):</li>
<li>Instance ID: forsest-plus-db (and test-cloudsql-instance for testing)</li>
<li>Region: europe-southwest1</li>
<li>Database Engine: PostgreSQL</li>
<li>Status: RUNNABLE (green in console).</li>
<li>Public IP Connectivity: Enabled.</li>
<li>Private IP Connectivity: Disabled.</li>
<li>Authorized Networks: Initially only my office IP. Later 0.0.0.0/0 was added (temporarily for debugging), but the error persisted.</li>
<li>User/Password: Verified correct and able to connect from external tools (e.g., local psql from my office IP).</li>
<li>Cloud SQL Admin API: Enabled in the project.</li>
</ul>
<p><strong>Troubleshooting Steps Performed:</strong></p>
<ul>
<li>Confirmed Spring Boot application works with Supabase.</li>
<li>Verified DATABASE_URL format for proxy: jdbc:postgresql://localhost:5432/forsest-plus-db .</li>
<li>Ensured Cloud SQL instance is listed in Cloud Run &quot;Connections&quot; (removed and re-added multiple times).</li>
<li>Verified Cloud SQL Client IAM role for Cloud Run service account.</li>
<li>Verified Cloud SQL Admin API is enabled.</li>
<li>Tested with 0.0.0.0/0 in Cloud SQL authorized networks (no change).</li>
<li>Created an entirely new Cloud SQL PostgreSQL instance ( test-cloudsql-instance ) with default settings, public IP enabled, 0.0.0.0/0 authorized, and tried connecting Cloud Run to it. Result: Same Connection to localhost:5432 refused error.</li>
<li>Searched Cloud Logging extensively for &quot;cloud_sql_proxy&quot; or related terms. No logs from the proxy itself are visible, indicating it's either not starting or failing silently.</li>
</ul>
<p><strong>Problem:</strong></p>
<ul>
<li>The consistent &quot;Connection to localhost:5432 refused&quot; error, coupled with the absence of Cloud SQL Proxy logs, strongly suggests that the Cloud SQL Proxy sidecar process is not being correctly injected or executed within the Cloud Run container by the Google Cloud platform, despite all standard configurations being in place. This happens even when connecting to a freshly created Cloud SQL instance.</li>
</ul>
<p>Given all these checks, what could be preventing the Cloud SQL Proxy from starting or functioning correctly within my Cloud Run service?</p>
",0,0,0,2025-10-06T13:33:47+00:00,1,56,True
79784467,12892937,,postgresql,PostgreSQL correlated query ignore WHERE clause,"<p>For each <code>customer_id</code>, I want to find the first delivery order. For each row of <code>Delivery</code>, I compare <code>order_date</code> with the smallest <code>order_date</code> of that <code>customer_id</code>.</p>
<p>Why does the <code>SELECT</code> statement below only return the row that contains the smallest <code>order_date</code>?</p>
<p><a href=""https://onecompiler.com/postgresql/43ywzu7fb"" rel=""nofollow noreferrer"">Code.</a></p>
<pre><code>CREATE TABLE IF NOT EXISTS Delivery (
  delivery_id int,
  customer_id int,
  order_date date,
  customer_pref_delivery_date date
);

TRUNCATE TABLE Delivery;

INSERT INTO Delivery (delivery_id, customer_id, order_date, customer_pref_delivery_date)
VALUES (1, 1, '2019-08-01', '2019-08-02');
INSERT INTO Delivery VALUES (2, 2, '2019-08-02', '2019-08-02');
INSERT INTO Delivery VALUES (3, 1, '2019-08-11', '2019-08-12');
INSERT INTO Delivery VALUES (4, 3, '2019-08-24', '2019-08-24');
INSERT INTO Delivery VALUES (5, 3, '2019-08-21', '2019-08-22');
INSERT INTO Delivery VALUES (6, 2, '2019-08-11', '2019-08-13');
INSERT INTO Delivery VALUES (7, 4, '2019-08-09', '2019-08-09');

-- Write your PostgreSQL query statement below
WITH temp AS (
    SELECT
        d.delivery_id,
        d.customer_id,
        d.order_date,
        d.customer_pref_delivery_date
    FROM Delivery d
    WHERE order_date = (SELECT MIN(order_date) FROM Delivery D
                        WHERE d.customer_id = D.customer_id)
)
select * from temp;
</code></pre>
<p>Input:</p>
<pre><code>Delivery table:
+-------------+-------------+------------+-----------------------------+
| delivery_id | customer_id | order_date | customer_pref_delivery_date |
+-------------+-------------+------------+-----------------------------+
| 1           | 1           | 2019-08-01 | 2019-08-02                  |
| 2           | 2           | 2019-08-02 | 2019-08-02                  |
| 3           | 1           | 2019-08-11 | 2019-08-12                  |
| 4           | 3           | 2019-08-24 | 2019-08-24                  |
| 5           | 3           | 2019-08-21 | 2019-08-22                  |
| 6           | 2           | 2019-08-11 | 2019-08-13                  |
| 7           | 4           | 2019-08-09 | 2019-08-09                  |
+-------------+-------------+------------+-----------------------------+
</code></pre>
<p>Output:</p>
<pre><code> delivery_id | customer_id | order_date | customer_pref_delivery_date 
-------------+-------------+------------+-----------------------------
           1 |           1 | 2019-08-01 | 2019-08-02
(1 row)
</code></pre>
",2,2,0,2025-10-07T10:53:11+00:00,1,83,True
79784613,256965,Finland,postgresql,How to insert into postgres temporal tables with kyselyjs?,"<p>I have a database that has some temporal tables. I have zod schemas for those tables and I am using those zod schemas to define the tables in Kysely. The schemas can be as follows:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE temporal (
  version_id    SERIAL PRIMARY KEY,
  some_id       TEXT,
  some_data     TEXT,
  valid_from    TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  valid_to      TIMESTAMP DEFAULT NULL,
  is_current    BOOLEAN NOT NULL DEFAULT TRUE
);
</code></pre>
<pre class=""lang-js prettyprint-override""><code>const TemporalTableSchema = z.object({
  versionId: z.number(),
  someId: z.string(),
  someData: z.string(),
  validFrom: z.coerce.date(),
  validTo: z.coerce.date().optional().nullable(),
  isCurrent: z.boolean()
})

type TemporalTableSchema = z.infer&lt;typeof TemporalTableSchema&gt;
</code></pre>
<p>Now if I want to insert into a such table I would never pass the version id, is_current or the timestamps myself (I have a trigger to update the valid_to and is_current fields when needed). So how can I do this with <a href=""https://kysely.dev/"" rel=""nofollow noreferrer"">Kysely</a>?</p>
<p>If I create my Kysely instance using the type from the zod schema Kysely is complaining that the insert is missing some required fields. This is what I would like to do somehow:</p>
<pre class=""lang-js prettyprint-override""><code>const db = new Kysely&lt;{ temporal: TemporalTableSchema }&gt;(someConfig)

await db.insertInto('temporal').values({
  someId: someData.id,
  someData: someData.data,
})
</code></pre>
<p>however, here Kysely would complain as it sees that for example versionId field is required based on the zod schema. Then on the other hand when querying data I would like to be able to validate and parse the returned data with the given schema and now all those fields should be in the data</p>
<pre class=""lang-js prettyprint-override""><code>// Here I want all the fields including the values from the autogenerated fields
const result = await db.selectFrom('temporal').selectAll()
</code></pre>
<p>So is there a way to tell Kysely that when inserting we don't need all the fields from a schema? I wouldn't want to mark all those fields optional as it would be annoying to do all kind of checks when using the data after it is queried from the db.</p>
",2,2,0,2025-10-07T13:49:46+00:00,1,106,True
79784758,31645448,,postgresql,The postgres update in docker compose broke the database,"<p>I decided to just update the postgres version via <code>docker compose pull</code>. And now I have an error due to a lack of rights. What's wrong?
My docker-compose.yaml</p>
<pre><code>services:
  postgres:
    env_file: .env
    image: 'postgres:latest'
    container_name: general_postgres
    environment:
      POSTGRES_USER: postgres_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: postgres_db
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - '5050:5432'
    volumes:
      - './pgdata:/var/lib/postgresql/data/pgdata'
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 1024M
        reservations:
          cpus: '0.25'
          memory: 256M
    command: |
      postgres -c max_connections=1000
               -c shared_buffers=256MB
               -c effective_cache_size=768MB
               -c maintenance_work_mem=64MB
               -c checkpoint_completion_target=0.7
               -c wal_buffers=16MB
               -c default_statistics_target=100
               -c password_encryption=scram-sha-256
    healthcheck:
      test:
        - CMD-SHELL
        - pg_isready -U postgres_user -d postgres_db
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    tty: true
    stdin_open: true
</code></pre>
<p>docker compose logs</p>
<pre><code>general_postgres  | mkdir: cannot create directory ‘/var/lib/postgresql/data’: Permission denied
general_postgres exited with code 1 (restarting)
general_postgres  | mkdir: cannot create directory ‘/var/lib/postgresql/data’: Permission denied
general_postgres exited with code 1 (restarting)
general_postgres  | mkdir: cannot create directory ‘/var/lib/postgresql/data’: Permission denied
general_postgres exited with code 1 (restarting)
general_postgres  | mkdir: cannot create directory ‘/var/lib/postgresql/data’: Permission denied
</code></pre>
<p>I tried doing <code>chown -Rv root:root pgdata</code>, but it didn't help. I have no idea what happened. There may have been BREAKING CHANGES in postgres. idk</p>
<p>UPDATE: I found a version of the past postgres in ENV using <code>docker image ls</code> and <code>docker image history --no-trunk</code>. It was 17.6-1.pgdg13+1</p>
",3,3,0,2025-10-07T16:32:13+00:00,1,186,True
79785110,2519395,"Ben Arous, Tunisia",postgresql,Custom postgres dialect isn&#39;t being picked up by spring,"<p>I have a Spring project with a PostgreSQL database. I'm trying to add a custom dialect, but I've noticed that Spring is not loading it.</p>
<p>My dialect:</p>
<pre class=""lang-java prettyprint-override""><code>package org.company.configuration;

public class CustomPostgreSQLDialect extends PostgreSQLDialect {

    private static final Logger LOGGER = LoggerFactory.getLogger(CustomPostgreSQLDialect.class);

    public CustomPostgreSQLDialect() {
        super();
        LOGGER.warn(&quot;=== CustomPostgreSQLDialect INITIALIZED ===&quot;);
    }
}
</code></pre>
<p>and this is how I use it in the properties file:</p>
<pre class=""lang-yaml prettyprint-override""><code>spring:
  jpa:
    database: POSTGRESQL
    database-platform: org.company.configuration.CustomPostgreSQLDialect
</code></pre>
<p>at startup I don't see the log <code>=== CustomPostgreSQLDialect INITIALIZED ===</code>, any idea what might be the issue? I've tried also to call it like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>spring:
  jpa:
    properties:
      hibernate:
        dialect: org.company.configuration.CustomPostgreSQLDialect
</code></pre>
<p>This didn't work either. I tried recompiling and force cache cleaning via the command <code>mvn clean compile -U</code>, but still no luck!</p>
",0,0,0,2025-10-08T05:52:16+00:00,1,136,False
79785192,31649047,,postgresql,Power BI DirectQuery to Redshift: ODBC error with pg_catalog.date_add(...) does not exist,"<p>I'm using Power BI Desktop with Amazon Redshift in DirectQuery mode. Data loads initially (sometimes at least / sometimes it has errors), but when I scroll down in a visual (e.g., table), I eventually hit this error:</p>
<p><code>ODBC Error: ERROR [42883] ERROR: function pg_catalog.date_add(&quot;unknown&quot;, double precision, timestamp without time zone) does not exist</code></p>
<p>It seems Power BI is trying to use a date_add function from pg_catalog, but that function doesn't exist in Redshift.</p>
<p>My question is:
Is there a workaround or fix to prevent this error? Can I solve it by adjusting column types or applying transformations?</p>
",0,0,0,2025-10-08T07:46:07+00:00,0,75,False
79785552,2417205,,postgresql,C# NET Core LINQ To Entities doubling subquery for sorting,"<p>I am working with EF Core 6.0 Web APi.</p>
<p>Npgsql.EntityFrameworkCore.PostgreSQL 6.0.29</p>
<p>PostgreSQL 16.3</p>
<p>I have database (Postgres) with musical entities and lessons. Lessons related to musical entities by navigation property. I want to sort musical entities by latest lesson date. I have projection as domain model.</p>
<pre><code>new DomainMusicalEntity
{
    Id = entity.Id,
    OwnerId = entity.OwnerId,
    CreationDate = entity.CreationDate,
    Description = entity.Description,
    Title = entity.Title,
    IsPublic = entity.IsPublic,
    IsCompleted = entity.IsCompleted,
    YoutubeLink = entity.YoutubeLink,
    Author = entity.Author,
    UniqueId = entity.UniqueId,
    OriginId = entity.OriginId,
    DefaultFile = entity.MusicalEntityFiles.FirstOrDefault(x =&gt; x.IsDefault == true),
    LastLessonDate = entity.MusicalEntitiesToLessons.Select(x=&gt;x.Lesson.Date).FirstOrDefault(),
}
</code></pre>
<p>Have sort in SQL by LINQ to Entities.</p>
<pre><code>MusicalEntitySort.LastLessonDate =&gt; filter.SortDirection == SortDirection.Desc
     ? query.OrderByDescending(x =&gt; x.LastLessonDate)
     : query.OrderBy(x =&gt; x.LastLessonDate),
</code></pre>
<p>In SQL I have this query(postgresql):</p>
<pre><code>SELECT t.id, t.owner_id, t.creation_date, t.description, t.title, t.is_public, t.is_completed, t.youtube_link, t.author, t.unique_id, t.origin_id, (
          SELECT l0.date
          FROM musical_entities_to_lessons AS m2
          INNER JOIN lessons AS l0 ON m2.lesson_id = l0.id
          WHERE t.id = m2.musical_entity_id
          LIMIT 1), t0.id, t0.file_path, t0.is_default, t0.musical_entity_id, t0.original_name
      FROM (
          SELECT m.id, m.author, m.creation_date, m.description, m.is_completed, m.is_public, m.origin_id, m.owner_id, m.title, m.unique_id, m.youtube_link
          FROM musical_entities AS m
          WHERE m.owner_id = @__filter_OwnerId_0
          LIMIT @__p_2 OFFSET @__p_1
      ) AS t
      LEFT JOIN (
          SELECT t1.id, t1.file_path, t1.is_default, t1.musical_entity_id, t1.original_name
          FROM (
              SELECT m1.id, m1.file_path, m1.is_default, m1.musical_entity_id, m1.original_name, ROW_NUMBER() OVER(PARTITION BY m1.musical_entity_id ORDER BY m1.id) AS row
              FROM musical_entity_files AS m1
              WHERE m1.is_default = TRUE
          ) AS t1
          WHERE t1.row &lt;= 1
      ) AS t0 ON t.id = t0.musical_entity_id
      ORDER BY (
          SELECT l.date
          FROM musical_entities_to_lessons AS m0
          INNER JOIN lessons AS l ON m0.lesson_id = l.id
          WHERE t.id = m0.musical_entity_id
          LIMIT 1) DESC
</code></pre>
<p>I see that I have a doubling in SELECT and in ORDER BY.</p>
<p>How can I get rid of that? I tried a lot of ways, but didn't succeed.</p>
<p>It example is simple. In my business logic that I want to implement, I want to take the maximum of these dates and filter by concrete lesson statuses. But this simple example gives the same result as sql doubling problem. In my use case I tried different prodections, but I always have double and triple queries like in the example above.</p>
<p>I know about views and subquery. But I have this architecture with a generic repository like this.</p>
<pre><code>public Task&lt;TDomainModel[]&gt; FindDomainAsync(TFilter? filter, CancellationToken cancellationToken = default)
{
    IQueryable&lt;TEntity&gt; query = DbSet;

    query = ApplyFilterInternal(query, filter);
    query = GetPagedQuery(query, filter);
    query = query.AsNoTracking();

    var domainQuery = SelectFieldsProjection(query, filter!);

    return ApplySort(domainQuery, filter).ToArrayAsync(cancellationToken);
}
</code></pre>
",1,1,0,2025-10-08T14:13:40+00:00,1,111,True
79785651,3430463,"Grenoble, France",postgresql,Why does the same query breaks when upgrading from postgre 14.17 to 14.19 when using timestamp?,"<p>After our provider upgraded our pg-14 database from 14.17 to 14.19, with timescaledb extension 2.5.2, our app completely broke <strong>because of awful database performances</strong>.</p>
<p>Our analysis showed that the issue was from our queries that used the timestamp type. The following query takes ages to execute (5-10mn)</p>
<p>We generate them with psycopg cursor as such</p>
<pre class=""lang-py prettyprint-override""><code>QUERY = &quot;&quot;&quot;SELECT m.time, m.value
                   FROM measurements m
                   WHERE m.series_id = %(series_id)s
                   AND m.time &gt;= %(start)s
                   AND m.time &lt;= %(end)s
                   ORDER BY m.time&quot;&quot;&quot;
cur.execute(QUERY, {&quot;series_id&quot;: SERIES_ID, &quot;start&quot;: START, &quot;end&quot;: END})
</code></pre>
<p>This generates the following query</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT m.time, m.value
                   FROM measurements m
                   WHERE m.time &gt;= '2020-01-02T00:00:00+00:00'::timestamp
                   AND m.time &lt;= '2023-01-01T00:00:00+00:00'::timestamp
                   AND m.series_id= %s
                   ORDER BY m.time

</code></pre>
<p>When we analyze it, we get
Query read 1.7 GB from disk (or system disk cache)</p>
<p><a href=""https://i.sstatic.net/pB56KRYf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pB56KRYf.png"" alt=""explain analyze"" /></a></p>
<p>Full analyze: <a href=""https://explain.depesz.com/s/TGIg#stats"" rel=""nofollow noreferrer"">https://explain.depesz.com/s/TGIg#stats</a></p>
<p>I have read that best practice is to use timestamptz (<a href=""https://wiki.postgresql.org/wiki/Don%27t_Do_This#Don.27t_use_timestamp_.28without_time_zone.29"" rel=""nofollow noreferrer"">source</a>) but I cannot find what could have caused that change in the release notes for <a href=""https://www.postgresql.org/docs/release/14.18/"" rel=""nofollow noreferrer"">14.18</a> nor <a href=""https://www.postgresql.org/docs/release/14.19/"" rel=""nofollow noreferrer"">14.19</a>.</p>
<p><strong>If this type is not recommended, why does cursor generates queries like that ?</strong></p>
<p>To fix this, we executed a query built with a python string instead of passing arguments to cursor. The following query executes instantaneously.</p>
<pre class=""lang-py prettyprint-override""><code>QUERY = f&quot;&quot;&quot;SELECT m.time, m.value
                   FROM measurements m
                   WHERE m.series_id = '{SERIES_ID}'
                   AND m.time &gt;= '{START}'
                   AND m.time &lt;= '{END}'
                   ORDER BY m.time&quot;&quot;&quot; 
cur.execute(QUERY)
</code></pre>
<p>This generates the following query</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT m.time, m.value
                   FROM measurements m
                   WHERE m.time &gt;= '2020-01-02T00:00:00+00:00'
                   AND m.time &lt;= '2023-01-01T00:00:00+00:00'
                   AND m.series_id = %s
                   ORDER BY m.time
</code></pre>
<p><strong>This query works just fine, do you know what caused this huge performance change between the pg versions ?</strong></p>
<p>When we analyze it we get
Query read 4.6 MB from disk (or system disk cache)
<a href=""https://i.sstatic.net/0kxunS0C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0kxunS0C.png"" alt=""explain analyze"" /></a></p>
<p>Full analyze: <a href=""https://explain.depesz.com/s/tVBM#stats"" rel=""nofollow noreferrer"">https://explain.depesz.com/s/tVBM#stats</a></p>
<hr />
<p><strong>Some information about the table</strong></p>
<p>Column types</p>
<pre><code>column     | data type
-----------+--------------
time       | timestamptz 
value      | float8
series_id  | text
</code></pre>
<p>Indexes</p>
<pre><code>tablename               |indexname                                                 |indexdef                                                                                                |
------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------------------------+
measurements            |measurements_series_id_time                               |CREATE INDEX measurements_series_id_time ON public.measurements USING btree (series_id, &quot;time&quot; DESC) 
measurements            |measurements_pk                                           |CREATE UNIQUE INDEX measurements_pk ON public.measurements USING btree (&quot;time&quot;, series_id)
measurements            |measurements_time_idx                                     |CREATE INDEX measurements_time_idx ON public.measurements USING btree (&quot;time&quot; DESC) 
</code></pre>
<hr />
<p><strong>Some information about the execution environment</strong></p>
<ul>
<li>python : ^3.10</li>
<li>psycopg: ^3.1.4</li>
</ul>
",0,0,0,2025-10-08T15:51:29+00:00,1,128,True
79785709,24286848,,postgresql,Copy parts of different PostgreSQL tables to a single file and restore them,"<p>Let’s say I have two different schemas with 2 tables each:</p>
<ul>
<li><code>schema_one</code> with tables <code>this</code>, <code>that</code></li>
<li><code>schema_two</code> with tables <code>here</code>, <code>there</code></li>
</ul>
<p>The tables have the following structure:</p>
<ul>
<li><p><code>schema_one.this</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Apple</td>
</tr>
<tr>
<td>2</td>
<td>Orange</td>
</tr>
</tbody>
</table></div>
</li>
<li><p><code>schema_one.that</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>location</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>new_jersey</td>
<td>Apple</td>
</tr>
<tr>
<td>alabama</td>
<td>Orange</td>
</tr>
</tbody>
</table></div>
</li>
<li><p><code>schema_two.here</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Banana</td>
</tr>
<tr>
<td>2</td>
<td>Pear</td>
</tr>
</tbody>
</table></div>
</li>
<li><p><code>schema_two.there</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>favourite_colour</th>
<th>favourite_fruit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Red</td>
<td>Grape</td>
</tr>
<tr>
<td>2</td>
<td>Blue</td>
<td>Watermelon</td>
</tr>
</tbody>
</table></div>
</li>
</ul>
<p>I want to copy data from all 4 tables, but to save on number of files I’m trying to do something like this:</p>
<pre class=""lang-bash prettyprint-override""><code>psql -f dump_all_schemas.sql &gt; output.sql
</code></pre>
<p><code>dump_all_schemas.sql</code> looks like this:</p>
<pre class=""lang-sql prettyprint-override""><code>COPY (SELECT * FROM schema_one.this WHERE favourite_fruit = ‘Apple’) TO STDOUT;
COPY (SELECT * FROM schema_one.that WHERE id=‘1’) TO STDOUT;
COPY (SELECT * FROM schema_two.here WHERE favourite_fruit = ‘Banana’) TO STDOUT;
COPY (SELECT * FROM schema_two.there WHERE favourite_colour = ‘Red’) TO STDOUT;
</code></pre>
<p>Is it possible to do the reverse of this restore the data to the correct tables? If not, what might be a good solution to this?</p>
<p>I have tried searching around but I can’t seem to discern one way or the other if it’s even possible to do what I’m doing. I know I can get the data from into the file, but the reverse is causing a headache.</p>
",1,1,0,2025-10-08T17:16:19+00:00,1,113,True
79786127,30873012,,postgresql,node-pg pool setup in a server vs serverless lambda,"<p>so we currently have a serverless in which we use like 3 endpoint.. all of them are very important.</p>
<p>and a nestjs server</p>
<p>we were using raw connections from pg-node till now and just switched to using pg pool
we are currently using a connection from the pool for each api. ig we could use the pool itself?</p>
<p>and as for lambda the same.</p>
<p>but we have been facing this issue</p>
<pre><code>&quot;errorType&quot;: &quot;Error&quot;,
&quot;errorMessage&quot;: &quot;Connection terminated unexpectedly&quot;,
</code></pre>
<p>and this happens midway of an api sometimes in lambda. not sure about nest but these error do happen in both and very often. we also have a rds proxy setup with idle connection timeout as 2 minute</p>
<pre><code>  max: 1000,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 20000,
  keepAlive: true,
  keepAliveInitialDelayMillis: 10000,
</code></pre>
<p>this is our pool config
can anyone help in finding the root cause cuz of which the error is thrown? also we didnt have this problem before setting up the proxy and pool.. is there anything wrong with the config..</p>
<p>we use
context.callbackWaitsForEmptyEventLoop = false;
in all function</p>
",0,0,0,2025-10-09T07:23:07+00:00,0,30,False
79786132,3108935,Europe,postgresql,AWS DMS Keeps silently failing with resource temporarily unavailable (apr status = 11),"<p>I'm using AWS DMS to migrate from PostgreSQL 9.6 to PostgreSQL 17.5 and the DMS task keeps failing on a single table with this error</p>
<pre><code>atfo_read: status__ == APR_SUCCESS failed  (at_file.c:1000)
Resource temporarily unavailable (apr status = 11) [1000158]  (at_file.c:1000)
rep_net_server_select: Server poll timeout. failed  (at_repnet.c:489)

</code></pre>
<p>Has anyone had similar issues? I can’t find anything related to this. I’ve tried running it without batch apply, changing the LOB size settings, and nothing seems to work. Note that the emails table contains columns with user-defined HTML content. All other tables are loaded properly, except this one which causes the whole process to fail.</p>
<p>Full AWS DMS logs:</p>
<pre><code>1759993354000,2025-10-09T07:02:34 [SOURCE_CAPTURE  ]I:  Source database version number is: 90624  (postgres_endpoint_util.c:696)
1759993354000,2025-10-09T07:02:34 [SOURCE_UNLOAD   ]I:  PostgreSQL database server encoding is 'UTF8'  (postgres_endpoint_imp.c:1123)
1759993354000,2025-10-09T07:02:34 [SOURCE_UNLOAD   ]I:  Postgres Endpoint created using default mode  (postgres_endpoint_imp.c:1210)
1759993354000,2025-10-09T07:02:34 [TASK_MANAGER    ]I:  Starting component st_2_x  (subtask.c:1478)
1759993355000,2025-10-09T07:02:35 [TASK_MANAGER    ]I:  Starting component st_2_y  (subtask.c:1478)
1759993355000,&quot;2025-10-09T07:02:35 [TARGET_LOAD     ]I:  No records received to load or apply on target , waiting for data from upstream  (streamcomponent.c:2084)&quot;
1759993355000,&quot;2025-10-09T07:02:35 [TABLES_MANAGER  ]I:  Next table to load 'public'.'emails' ID = 1, order = 0  (tasktablesmanager.c:2284)&quot;
1759993355000,2025-10-09T07:02:35 [SORTER          ]I:  Start collecting changes for table id = 1  (sorter_transaction.c:2486)
1759993355000,2025-10-09T07:02:35 [TASK_MANAGER    ]I:  Start loading table 'public'.'emails' (Id = 1) by subtask 2. Start load timestamp 000640B462AC1680  (replicationtask_util.c:781)
1759993355000,2025-10-09T07:02:35 [SOURCE_UNLOAD   ]I:  Latest lsn position is '00001D60/67714140' for table 'public'.'emails'  (postgres_endpoint_unload.c:434)
1759993355000,2025-10-09T07:02:35 [SOURCE_UNLOAD   ]I:  Sent unloaded record 1 to internal queue  (streamcomponent.c:3154)
1759993355000,2025-10-09T07:02:35 [SOURCE_UNLOAD   ]I:  Calculated batch used for UNLOAD size is 1 rows per fetch.  (postgres_endpoint_unload.c:182)
1759993355000,2025-10-09T07:02:35 [SOURCE_UNLOAD   ]I:  REPLICA IDENTITY information for table 'public'.'emails': Query status='Success' Type='DEFAULT' Description='Old values of the Primary Key columns (if any) will be captured.'  (postgres_endpoint_unload.c:195)
1759993355000,&quot;2025-10-09T07:02:35 [TARGET_LOAD     ]I:  Table 'public'.'emails' contains LOB columns, change working mode to default mode  (odbc_endpoint_imp.c:6482)&quot;
1759993355000,&quot;2025-10-09T07:02:35 [TARGET_LOAD     ]I:  Table 'public'.'emails' has Optimized Full LOB Support, inline limit = 65536 bytes  (odbc_endpoint_imp.c:6488)&quot;
1759993355000,2025-10-09T07:02:35 [TARGET_LOAD     ]I:  Loaded record 2 to temporary csv file  (csv_target.c:2042)
1759993356000,2025-10-09T07:02:36 [TARGET_LOAD     ]I:  Loaded temporary csv file /rdsdbdata/data/tasks/&lt;redacted&gt;/data_files/1/LOAD00000001.csv to target  (csv_target.c:1989)
1759993357000,2025-10-09T07:02:37 [IO              ]D:  atfo_read: status__ == APR_SUCCESS failed  (at_file.c:1000)
1759993357000,2025-10-09T07:02:37 [IO              ]D:  Resource temporarily unavailable (apr status = 11) [1000158]  (at_file.c:1000)
1759993358000,2025-10-09T07:02:38 [IO              ]D:  rep_net_server_select: Server poll timeout. failed  (at_repnet.c:489)
</code></pre>
<p>Task config:</p>
<pre><code>{
    &quot;TargetMetadata&quot;: {
        &quot;ParallelApplyBufferSize&quot;: 0,
        &quot;ParallelApplyQueuesPerThread&quot;: 0,
        &quot;ParallelApplyThreads&quot;: 0,
        &quot;TargetSchema&quot;: &quot;&quot;,
        &quot;InlineLobMaxSize&quot;: 64,
        &quot;ParallelLoadQueuesPerThread&quot;: 0,
        &quot;SupportLobs&quot;: true,
        &quot;LobChunkSize&quot;: 64,
        &quot;TaskRecoveryTableEnabled&quot;: false,
        &quot;ParallelLoadThreads&quot;: 0,
        &quot;LobMaxSize&quot;: 0,
        &quot;BatchApplyEnabled&quot;: true,
        &quot;FullLobMode&quot;: true,
        &quot;LimitedSizeLobMode&quot;: false,
        &quot;LoadMaxFileSize&quot;: 0,
        &quot;ParallelLoadBufferSize&quot;: 0
    },
    &quot;FullLoadSettings&quot;: {
        &quot;CommitRate&quot;: 10000,
        &quot;StopTaskCachedChangesApplied&quot;: false,
        &quot;StopTaskCachedChangesNotApplied&quot;: false,
        &quot;MaxFullLoadSubTasks&quot;: 8,
        &quot;TransactionConsistencyTimeout&quot;: 600,
        &quot;CreatePkAfterFullLoad&quot;: false,
        &quot;TargetTablePrepMode&quot;: &quot;DO_NOTHING&quot;
    },
    &quot;Logging&quot;: {
        &quot;EnableLogging&quot;: true,
        &quot;EnableLogContext&quot;: false,
        &quot;LogComponents&quot;: [
            {
                &quot;Id&quot;: &quot;SOURCE_UNLOAD&quot;,
                &quot;Severity&quot;: &quot;LOGGER_SEVERITY_DEFAULT&quot;
            },
            {
                &quot;Id&quot;: &quot;SOURCE_CAPTURE&quot;,
                &quot;Severity&quot;: &quot;LOGGER_SEVERITY_DEFAULT&quot;
            },
            {
                &quot;Id&quot;: &quot;TARGET_LOAD&quot;,
                &quot;Severity&quot;: &quot;LOGGER_SEVERITY_DEFAULT&quot;
            },
            {
                &quot;Id&quot;: &quot;TARGET_APPLY&quot;,
                &quot;Severity&quot;: &quot;LOGGER_SEVERITY_DEFAULT&quot;
            },
            {
                &quot;Id&quot;: &quot;TASK_MANAGER&quot;,
                &quot;Severity&quot;: &quot;LOGGER_SEVERITY_DEFAULT&quot;
            }
        ],
        &quot;CloudWatchLogGroup&quot;: &quot;dms-tasks-&lt;redacted&gt;&quot;,
        &quot;CloudWatchLogStream&quot;: &quot;dms-task-&lt;redacted&gt;&quot;
    },
    &quot;ControlTablesSettings&quot;: {
        &quot;historyTimeslotInMinutes&quot;: 5,
        &quot;CommitPositionTableEnabled&quot;: false,
        &quot;HistoryTimeslotInMinutes&quot;: 5,
        &quot;StatusTableEnabled&quot;: false,
        &quot;SuspendedTablesTableEnabled&quot;: false,
        &quot;HistoryTableEnabled&quot;: false,
        &quot;ControlSchema&quot;: &quot;&quot;,
        &quot;FullLoadExceptionTableEnabled&quot;: false
    },
    &quot;StreamBufferSettings&quot;: {
        &quot;StreamBufferCount&quot;: 3,
        &quot;CtrlStreamBufferSizeInMB&quot;: 5,
        &quot;StreamBufferSizeInMB&quot;: 8
    },
    &quot;ChangeProcessingDdlHandlingPolicy&quot;: {
        &quot;HandleSourceTableDropped&quot;: true,
        &quot;HandleSourceTableTruncated&quot;: true,
        &quot;HandleSourceTableAltered&quot;: true
    },
    &quot;ErrorBehavior&quot;: {
        &quot;FailOnNoTablesCaptured&quot;: false,
        &quot;ApplyErrorUpdatePolicy&quot;: &quot;LOG_ERROR&quot;,
        &quot;FailOnTransactionConsistencyBreached&quot;: false,
        &quot;RecoverableErrorThrottlingMax&quot;: 1800,
        &quot;DataErrorEscalationPolicy&quot;: &quot;SUSPEND_TABLE&quot;,
        &quot;ApplyErrorEscalationCount&quot;: 0,
        &quot;RecoverableErrorStopRetryAfterThrottlingMax&quot;: false,
        &quot;RecoverableErrorThrottling&quot;: true,
        &quot;ApplyErrorFailOnTruncationDdl&quot;: false,
        &quot;DataMaskingErrorPolicy&quot;: &quot;STOP_TASK&quot;,
        &quot;DataTruncationErrorPolicy&quot;: &quot;LOG_ERROR&quot;,
        &quot;ApplyErrorInsertPolicy&quot;: &quot;LOG_ERROR&quot;,
        &quot;EventErrorPolicy&quot;: &quot;IGNORE&quot;,
        &quot;ApplyErrorEscalationPolicy&quot;: &quot;LOG_ERROR&quot;,
        &quot;RecoverableErrorCount&quot;: -1,
        &quot;DataErrorEscalationCount&quot;: 50,
        &quot;TableErrorEscalationPolicy&quot;: &quot;STOP_TASK&quot;,
        &quot;RecoverableErrorInterval&quot;: 5,
        &quot;ApplyErrorDeletePolicy&quot;: &quot;IGNORE_RECORD&quot;,
        &quot;TableErrorEscalationCount&quot;: 50,
        &quot;FullLoadIgnoreConflicts&quot;: true,
        &quot;DataErrorPolicy&quot;: &quot;LOG_ERROR&quot;,
        &quot;TableErrorPolicy&quot;: &quot;SUSPEND_TABLE&quot;
    },
    &quot;ChangeProcessingTuning&quot;: {
        &quot;StatementCacheSize&quot;: 50,
        &quot;CommitTimeout&quot;: 5,
        &quot;RecoveryTimeout&quot;: -1,
        &quot;BatchApplyPreserveTransaction&quot;: true,
        &quot;BatchApplyTimeoutMin&quot;: 1,
        &quot;BatchSplitSize&quot;: 0,
        &quot;BatchApplyTimeoutMax&quot;: 30,
        &quot;MinTransactionSize&quot;: 5000,
        &quot;MemoryKeepTime&quot;: 60,
        &quot;BatchApplyMemoryLimit&quot;: 1000,
        &quot;MemoryLimitTotal&quot;: 2048
    },
    &quot;ValidationSettings&quot;: {
        &quot;EnableValidation&quot;: false,
        &quot;ValidationMode&quot;: &quot;ROW_LEVEL&quot;,
        &quot;ThreadCount&quot;: 5,
        &quot;FailureMaxCount&quot;: 10000,
        &quot;TableFailureMaxCount&quot;: 1000,
        &quot;HandleCollationDiff&quot;: false,
        &quot;ValidationOnly&quot;: false,
        &quot;RecordFailureDelayLimitInMinutes&quot;: 0,
        &quot;SkipLobColumns&quot;: false,
        &quot;ValidationPartialLobSize&quot;: 0,
        &quot;ValidationQueryCdcDelaySeconds&quot;: 0,
        &quot;PartitionSize&quot;: 10000
    },
    &quot;PostProcessingRules&quot;: null,
    &quot;CharacterSetSettings&quot;: null,
    &quot;LoopbackPreventionSettings&quot;: null,
    &quot;BeforeImageSettings&quot;: null,
    &quot;TTSettings&quot;: {
        &quot;TTS3Settings&quot;: null,
        &quot;TTRecordSettings&quot;: null,
        &quot;EnableTT&quot;: false
    },
    &quot;FailTaskWhenCleanTaskResourceFailed&quot;: false
}
</code></pre>
",0,0,0,2025-10-09T07:27:28+00:00,0,73,False
79786624,31658981,,postgresql,Sql injection protection when sending json as a sql function parameter,"<p>I am sending a json array with filter parameters from ExtJs in the sql function parameter.</p>
<p>filters look like this:</p>
<p><code>[{&quot;field&quot;:&quot;product_type_id&quot;,&quot;data&quot;:{&quot;type&quot;:&quot;number&quot;,&quot;value&quot;:&quot;43&quot;,&quot;comparison&quot;:&quot;in&quot;}},{&quot;field&quot;:&quot;code&quot;,&quot;data&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;value&quot;:&quot;RRR&quot;,&quot;comparison&quot;:&quot;like&quot;}}]</code></p>
<p>I would like to protect myself against possible SQL injection, but my sanitizeString function removes all characters such as <code>{, &quot;, :</code> etc. This prevents me from sending json, so I cannot use it for parameters. Can you tell me how to best protect myself through sql injection and at the same time pass json in the parameter without any problems?</p>
<pre><code>// this.DBModels?.execQuery
 execQuery&lt;T = any&gt;(query: string, callback: (err: Error, data: T[]) =&gt; void) {
    this.db.driver.execQuery(query, (err: Error, data: T[]): void =&gt; {
      callback(err, data)
    })
  }

   this.DBModels?.execQuery(
      `SELECT * FROM ${sanitizeString(functionName)}('${
        functionParameter
      }')`,
</code></pre>
",1,1,0,2025-10-09T16:02:01+00:00,2,114,True
79787172,3010249,,postgresql,Find existing roles without being able to connect to local Postgres cluster,"<p>I have a Postgres data directory, which I have mounted in Docker. The database started without errors. However, this database does not contain a <code>postgres</code> role and I forgot which roles I had set up!</p>
<p>So, how can I get a list of roles that are configured in the database, without knowing any? Is there a command line tool? Is it embedded in a file in the postgres datadir?</p>
<p>By running <code>docker exec -it &lt;postgres_container&gt; bash</code>, I became root in the container. Running <code>psql</code> gave</p>
<pre><code>psql: error: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: 
FATAL:  role &quot;root&quot; does not exist
</code></pre>
<p>This is expected. However, if I <code>su postgres</code> and then <code>psql</code>, I get</p>
<pre><code>psql: error: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: 
FATAL:  role &quot;postgres&quot; does not exist
</code></pre>
<p>When trying to create the role <code>postgres</code> with <code>createuser -s postgres</code> I also get</p>
<pre><code>createuser: error: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: 
FATAL:  role &quot;postgres&quot; does not exist
</code></pre>
",3,3,0,2025-10-10T09:16:42+00:00,1,99,True
79789193,31678955,,postgresql,postgresql pglogical replication and Segmentation error on bidirectional replication,"<p>I am configuring a bidirectional replication between node1 and node2.
The configuration of node1 as publisher and node2 as subscriber is correct: the data is replicated and no errors occur.
The reverse configuration (node2 as publisher and node1 as subscriber) is accepted but, when replication in this direction is required, the databases enter an automatic recovery loop.</p>
<p>I would like to understand whether this is actually a bug or if I am doing something wrong in the configuration.
Thank you for your attention.</p>
<p>Below are the steps to reproduce my problem:</p>
<ul>
<li>dbtest01 = node1</li>
<li>dbtest02 = node2</li>
</ul>
<p>NB:
The two nodes contain the same data.
Two-way replication is on the same table (store_level) and is implemented using filters (on id_store field).</p>
<p>System info:
OS: Linux myserver 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 GNU/Linux
Postgres: 15
pglogical: 2.4.2</p>
<p><strong>Create databases</strong></p>
<pre><code>createdb -h 127.0.0.1 -U postgres dbtest01
createdb -h 127.0.0.1 -U postgres dbtest02
</code></pre>
<p><strong>dbtest01: create tables</strong></p>
<pre><code>psql -h 127.0.0.1 -U postgres

\c dbtest01

create table store_level (
  id_item varchar(20) not null,
  id_store varchar(10) not null,
  quantity  decimal(12,3)
);

alter table store_level
add constraint pk_store_level
primary key (id_item, id_store);
</code></pre>
<p><strong>dbtest01: add data</strong></p>
<pre><code>insert into store_level (id_item, id_store, quantity) values ('ITEM01', 'STORE_A', 10);
insert into store_level (id_item, id_store, quantity) values ('ITEM02', 'STORE_A', 11);
insert into store_level (id_item, id_store, quantity) values ('ITEM01', 'STORE_B', 12);
insert into store_level (id_item, id_store, quantity) values ('ITEM02', 'STORE_B', 13);
insert into store_level (id_item, id_store, quantity) values ('ITEM01', 'STORE_C', 14);
insert into store_level (id_item, id_store, quantity) values ('ITEM02', 'STORE_C', 15);
</code></pre>
<p><strong>dbtest02: create tables</strong></p>
<pre><code>psql -h 127.0.0.1 -U postgres

\c dbtest02

create table store_level (
  id_item varchar(20) not null,
  id_store varchar(10) not null,
  quantity  decimal(12,3)
);

alter table store_level
add constraint pk_store_level
primary key (id_item, id_store);
</code></pre>
<p><strong>dbtest02: add data</strong></p>
<pre><code>insert into store_level (id_item, id_store, quantity) values ('ITEM01', 'STORE_A', 10);
insert into store_level (id_item, id_store, quantity) values ('ITEM02', 'STORE_A', 11);
insert into store_level (id_item, id_store, quantity) values ('ITEM01', 'STORE_B', 12);
insert into store_level (id_item, id_store, quantity) values ('ITEM02', 'STORE_B', 13);
insert into store_level (id_item, id_store, quantity) values ('ITEM01', 'STORE_C', 14);
insert into store_level (id_item, id_store, quantity) values ('ITEM02', 'STORE_C', 15);
</code></pre>
<p><strong>dbtest01: define publisher</strong></p>
<pre><code>\c dbtest01

CREATE EXTENSION IF NOT EXISTS pglogical;

GRANT USAGE ON SCHEMA pglogical TO postgres;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA pglogical TO postgres;

SELECT pglogical.create_node(
    node_name := 'node1',
    dsn := 'host=127.0.0.1 port=5432 dbname=dbtest01 user=postgres password=MyPassword'
);

SELECT pglogical.create_replication_set(
    set_name := 'replication_set_a1'
);


SELECT pglogical.replication_set_add_table(set_name := 'replication_set_a1', relation := 'public.store_level',     synchronize_data := false, row_filter := 'id_store &lt;&gt; ''STORE_B''');
</code></pre>
<p><strong>dbtest02: define subscriber</strong></p>
<pre><code>\c dbtest02

CREATE EXTENSION IF NOT EXISTS pglogical;

GRANT USAGE ON SCHEMA pglogical TO postgres;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA pglogical TO postgres;

SELECT pglogical.create_node(
    node_name := 'node2',
    dsn := 'host=192.168.1.56 port=5432 dbname=dbtest02 user=postgres password=MyPassword'
);


SELECT pglogical.create_subscription(
    subscription_name := 'sub_node2_to_node1',
    provider_dsn := 'host=127.0.0.1 port=5432 dbname=dbtest01 user=postgres password=MyPassword',
    replication_sets := ARRAY['replication_set_a1'],
    synchronize_structure := false,
    synchronize_data := false
);
</code></pre>
<p><strong>dbtest01: update data</strong></p>
<p>After applying the following update, replication behaves correctly.
<code>update store_level set quantity = 100 + quantity;</code></p>
<h1><strong>INVERSE REPLICATION: FROM NODE2 TO NODE1</strong></h1>
<p><strong>dbtest02: define publisher</strong></p>
<pre><code>SELECT pglogical.create_replication_set(
    set_name := 'replication_set_a2'
);


SELECT pglogical.replication_set_add_table(set_name := 'replication_set_a2', relation := 'public.store_level',
synchronize_data := false, row_filter := 'id_store = ''STORE_B''');
</code></pre>
<p><strong>dbtest01: define subscriber</strong></p>
<pre><code>\c dbtest01

SELECT pglogical.create_subscription(
    subscription_name := 'sub_node1_to_node2',
    provider_dsn := 'host=127.0.0.1 port=5432 dbname=dbtest02 user=postgres password=MyPassword',
    replication_sets := ARRAY['replication_set_a2'],
    synchronize_structure := false,
    synchronize_data := false
);
</code></pre>
<p><strong>dbtest02: update data</strong></p>
<p>The following instruction causes the error and sends the database into a loop.
<code>update store_level set quantity = 1000 + quantity;</code></p>
<p>Log content follows (/etc/postgresql/15/main/postgresql.conf)</p>
<pre><code>
2025-10-13 11:06:00.743 CEST [92108] [sconosciuto]@postgres LOG:  manager worker [92108] at slot 2 generation 9 detaching cleanly
2025-10-13 11:06:00.743 CEST [92109] [sconosciuto]@dbtest01 LOG:  starting apply for subscription sub_node1_to_node2
2025-10-13 11:06:00.752 CEST [92112] [sconosciuto]@template1 LOG:  manager worker [92112] at slot 2 generation 11 detaching cleanly
2025-10-13 11:06:00.786 CEST [92115] postgres@dbtest02 LOG:  la decodifica logica ha trovato un punto consistente a D/B5208258
2025-10-13 11:06:00.786 CEST [92115] postgres@dbtest02 DETTAGLI:  Non ci sono transazioni in corso.
2025-10-13 11:06:00.786 CEST [92115] postgres@dbtest02 ISTRUZIONE:  CREATE_REPLICATION_SLOT &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot; LOGICAL pglogical_output
2025-10-13 11:06:00.787 CEST [92115] postgres@dbtest02 LOG:  snapshot di decidifica logica esportati: &quot;00000016-0000E67E-1&quot; con 0 ID di transazione
2025-10-13 11:06:00.787 CEST [92115] postgres@dbtest02 ISTRUZIONE:  CREATE_REPLICATION_SLOT &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot; LOGICAL pglogical_output
2025-10-13 11:06:00.807 CEST [92119] postgres@dbtest02 LOG:  avvio della decodifica logica per lo slot &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot;
2025-10-13 11:06:00.807 CEST [92119] postgres@dbtest02 DETTAGLI:  Commit delle transazioni streaming dopo D/B5208290, lettura del WAL dopo D/B5208258.
2025-10-13 11:06:00.807 CEST [92119] postgres@dbtest02 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot; LOGICAL D/B5208290 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a2', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92109')
2025-10-13 11:06:00.807 CEST [92119] postgres@dbtest02 LOG:  la decodifica logica ha trovato un punto consistente a D/B5208258
2025-10-13 11:06:00.807 CEST [92119] postgres@dbtest02 DETTAGLI:  Non ci sono transazioni in corso.
2025-10-13 11:06:00.807 CEST [92119] postgres@dbtest02 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot; LOGICAL D/B5208290 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a2', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92109')
2025-10-13 11:06:50.529 CEST [92109] [sconosciuto]@dbtest01 LOG:  CONFLICT: remote UPDATE on relation public.store_level (local index pk_store_level). Resolution: apply_remote.
2025-10-13 11:06:50.529 CEST [92109] [sconosciuto]@dbtest01 DETTAGLI:  existing local tuple {id_item[varchar]:ITEM01 id_store[varchar]:STORE_B quantity[numeric]:112.000} xid=537434,origin=0,timestamp=2025-10-13 10:59:16.883319+02; remote tuple {id_item[varchar]:ITEM01 id_store[varchar]:STORE_B quantity[numeric]:1012.000} in xact origin=7,timestamp=2025-10-13 11:06:50.527877+02,commit_lsn=0/B5208C10
2025-10-13 11:06:50.529 CEST [92109] [sconosciuto]@dbtest01 CONTESTO:  apply UPDATE from remote relation public.store_level in commit before D/B5208C10, xid 537442 committed at 2025-10-13 11:06:50.527877+02 (action #2) from node replorigin 7
2025-10-13 11:06:50.530 CEST [92109] [sconosciuto]@dbtest01 LOG:  CONFLICT: remote UPDATE on relation public.store_level (local index pk_store_level). Resolution: apply_remote.
2025-10-13 11:06:50.530 CEST [92109] [sconosciuto]@dbtest01 DETTAGLI:  existing local tuple {id_item[varchar]:ITEM02 id_store[varchar]:STORE_B quantity[numeric]:113.000} xid=537434,origin=0,timestamp=2025-10-13 10:59:16.883319+02; remote tuple {id_item[varchar]:ITEM02 id_store[varchar]:STORE_B quantity[numeric]:1013.000} in xact origin=7,timestamp=2025-10-13 11:06:50.527877+02,commit_lsn=0/B5208C10
2025-10-13 11:06:50.530 CEST [92109] [sconosciuto]@dbtest01 CONTESTO:  apply UPDATE from remote relation public.store_level in commit before D/B5208C10, xid 537442 committed at 2025-10-13 11:06:50.527877+02 (action #3) from node replorigin 7
2025-10-13 11:06:50.532 CEST [92010] postgres@dbtest01 LOG:  ricezione dati dal client fallita: Connessione interrotta dal corrispondente
2025-10-13 11:06:50.532 CEST [92010] postgres@dbtest01 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest02_node1_sub_node3f3d779&quot; LOGICAL D/B5201308 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a1', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '91999')
2025-10-13 11:06:50.532 CEST [92010] postgres@dbtest01 LOG:  fine del file inaspettato sulla connessione di standby
2025-10-13 11:06:50.532 CEST [92010] postgres@dbtest01 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest02_node1_sub_node3f3d779&quot; LOGICAL D/B5201308 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a1', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '91999')
2025-10-13 11:06:50.532 CEST [57369] LOG:  processo di lavoro in background &quot;pglogical apply 1272091:1060992920&quot; (PID 91999) è stato terminato dal segnale 11: Errore di segmentazione
2025-10-13 11:06:50.532 CEST [57369] LOG:  interruzione di tutti gli altri processi attivi del server
2025-10-13 11:06:50.587 CEST [57369] LOG:  tutti i processi server sono terminati; re-inizializzazione
2025-10-13 11:06:51.055 CEST [92130] LOG:  il database è stato interrotto; l'ultimo segno di vita risale alle 2025-10-13 11:00:30 CEST
2025-10-13 11:06:53.899 CEST [92130] LOG:  il database non è stato arrestato correttamente; ripristino automatico in corso
2025-10-13 11:06:53.902 CEST [92130] LOG:  stato di replica ripristinato dal nodo 1 a 4/43E38E0
2025-10-13 11:06:53.902 CEST [92130] LOG:  stato di replica ripristinato dal nodo 2 a C/90274FF0
2025-10-13 11:06:53.902 CEST [92130] LOG:  stato di replica ripristinato dal nodo 3 a C/90210310
2025-10-13 11:06:53.902 CEST [92130] LOG:  stato di replica ripristinato dal nodo 4 a B/6BD06ED8
2025-10-13 11:06:53.902 CEST [92130] LOG:  stato di replica ripristinato dal nodo 5 a C/9020EA10
2025-10-13 11:06:53.902 CEST [92130] LOG:  stato di replica ripristinato dal nodo 6 a D/B5201C50
2025-10-13 11:06:53.925 CEST [92130] LOG:  il redo inizia in D/B5202050
2025-10-13 11:06:53.926 CEST [92130] LOG:  lunghezza del record a D/B5208F30 non valida: atteso 24, ricevuto 0
2025-10-13 11:06:53.926 CEST [92130] LOG:  ripetizione eseguita con utilizzo del sistema D/B5208EE8: CPU: utente: 0.00 s, sistema: 0.00 s, passati: 0.00 s
2025-10-13 11:06:53.973 CEST [92131] LOG:  inizio checkpoint: end-of-recovery immediate wait
2025-10-13 11:06:54.010 CEST [92131] LOG:  checkpoint completato: ha scritto 42 buffer (0.0%); 0 file WAL aggiunti, 0 rimossi, 0 riciclati; scrittura=0.029 s, sincronizzazione=0.004 s, totale=0.039 s; file di sincronizzazione=36, più lungo=0.002 s, medio=0.001 s; distanza=27 kB, stima=27 kB
2025-10-13 11:06:54.015 CEST [57369] LOG:  il database è pronto ad accettare connessioni
2025-10-13 11:06:54.016 CEST [92135] LOG:  starting pglogical supervisor
2025-10-13 11:06:54.023 CEST [92137] [sconosciuto]@postgres LOG:  manager worker [92137] at slot 0 generation 1 detaching cleanly
2025-10-13 11:06:54.040 CEST [92139] [sconosciuto]@template1 LOG:  manager worker [92139] at slot 0 generation 3 detaching cleanly
2025-10-13 11:06:54.090 CEST [92145] [sconosciuto]@dbtest01 LOG:  starting pglogical database manager for database dbtest01
2025-10-13 11:06:54.094 CEST [92146] [sconosciuto]@dbtest01 LOG:  starting apply for subscription sub_node1_to_node2
2025-10-13 11:06:54.115 CEST [92147] postgres@dbtest02 LOG:  avvio della decodifica logica per lo slot &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot;
2025-10-13 11:06:54.115 CEST [92147] postgres@dbtest02 DETTAGLI:  Commit delle transazioni streaming dopo D/B5208860, lettura del WAL dopo D/B5208860.
2025-10-13 11:06:54.115 CEST [92147] postgres@dbtest02 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot; LOGICAL D/B5208C40 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a2', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92146')
2025-10-13 11:06:54.115 CEST [92147] postgres@dbtest02 LOG:  la decodifica logica ha trovato un punto consistente a D/B5208860
2025-10-13 11:06:54.115 CEST [92147] postgres@dbtest02 DETTAGLI:  Non ci sono transazioni in corso.
2025-10-13 11:06:54.115 CEST [92147] postgres@dbtest02 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest01_node2_sub_nodeeb86277&quot; LOGICAL D/B5208C40 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a2', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92146')
2025-10-13 11:06:55.089 CEST [92148] [sconosciuto]@dbtest02 LOG:  starting pglogical database manager for database dbtest02
2025-10-13 11:06:55.092 CEST [92149] [sconosciuto]@dbtest02 LOG:  starting apply for subscription sub_node2_to_node1
2025-10-13 11:06:55.110 CEST [92150] postgres@dbtest01 LOG:  D/B5201C50 has been already streamed, forwarding to D/B5208860
2025-10-13 11:06:55.110 CEST [92150] postgres@dbtest01 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest02_node1_sub_node3f3d779&quot; LOGICAL D/B5201C50 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a1', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92149')
2025-10-13 11:06:55.111 CEST [92150] postgres@dbtest01 LOG:  avvio della decodifica logica per lo slot &quot;pgl_dbtest02_node1_sub_node3f3d779&quot;
2025-10-13 11:06:55.111 CEST [92150] postgres@dbtest01 DETTAGLI:  Commit delle transazioni streaming dopo D/B5208860, lettura del WAL dopo D/B5208860.
2025-10-13 11:06:55.111 CEST [92150] postgres@dbtest01 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest02_node1_sub_node3f3d779&quot; LOGICAL D/B5201C50 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a1', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92149')
2025-10-13 11:06:55.111 CEST [92150] postgres@dbtest01 LOG:  la decodifica logica ha trovato un punto consistente a D/B5208860
2025-10-13 11:06:55.111 CEST [92150] postgres@dbtest01 DETTAGLI:  Non ci sono transazioni in corso.
2025-10-13 11:06:55.111 CEST [92150] postgres@dbtest01 ISTRUZIONE:  START_REPLICATION SLOT &quot;pgl_dbtest02_node1_sub_node3f3d779&quot; LOGICAL D/B5201C50 (expected_encoding 'UTF8', min_proto_version '1', max_proto_version '1', startup_params_format '1', &quot;binary.want_internal_basetypes&quot; '1', &quot;binary.want_binary_basetypes&quot; '1', &quot;binary.basetypes_major_version&quot; '1500', &quot;binary.sizeof_datum&quot; '8', &quot;binary.sizeof_int&quot; '4', &quot;binary.sizeof_long&quot; '8', &quot;binary.bigendian&quot; '0', &quot;binary.float4_byval&quot; '0', &quot;binary.float8_byval&quot; '1', &quot;binary.integer_datetimes&quot; '0', &quot;hooks.setup_function&quot; 'pglogical.pglogical_hooks_setup', &quot;pglogical.forward_origins&quot; '&quot;all&quot;', &quot;pglogical.replication_set_names&quot; 'replication_set_a1', &quot;relmeta_cache_size&quot; '-1', pg_version '150002', pglogical_version '2.4.2', pglogical_version_num '20402', pglogical_apply_pid '92149')
2025-10-13 11:06:55.112 CEST [57369] LOG:  processo di lavoro in background &quot;pglogical apply 1272091:1060992920&quot; (PID 92149) è stato terminato dal segnale 11: Errore di segmentazione
2025-10-13 11:06:55.112 CEST [57369] LOG:  interruzione di tutti gli altri processi attivi del server
2025-10-13 11:06:55.116 CEST [57369] LOG:  tutti i processi server sono terminati; re-inizializzazione
2025-10-13 11:06:55.315 CEST [92151] LOG:  il database è stato interrotto; l'ultimo segno di vita risale alle 2025-10-13 11:06:54 CEST
2025-10-13 11:06:58.173 CEST [92151] LOG:  il database non è stato arrestato correttamente; ripristino automatico in corso
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 1 a 4/43E38E0
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 2 a C/90274FF0
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 3 a C/90210310
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 4 a B/6BD06ED8
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 5 a C/9020EA10
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 6 a D/B5201C50
2025-10-13 11:06:58.176 CEST [92151] LOG:  stato di replica ripristinato dal nodo 7 a D/B5208C40
2025-10-13 11:06:58.198 CEST [92151] LOG:  lunghezza del record a D/B5208FA8 non valida: atteso 24, ricevuto 0
2025-10-13 11:06:58.198 CEST [92151] LOG:  redo non richiesto
2025-10-13 11:06:58.245 CEST [92152] LOG:  inizio checkpoint: end-of-recovery immediate wait
2025-10-13 11:06:58.278 CEST [92152] LOG:  checkpoint completato: ha scritto 2 buffer (0.0%); 0 file WAL aggiunti, 0 rimossi, 0 riciclati; scrittura=0.027 s, sincronizzazione=0.002 s, totale=0.035 s; file di sincronizzazione=3, più lungo=0.001 s, medio=0.001 s; distanza=0 kB, stima=0 kB
2025-10-13 11:06:58.283 CEST [57369] LOG:  il database è pronto ad accettare connessioni

... -&gt; loop

</code></pre>
",1,1,0,2025-10-13T10:52:26+00:00,1,99,True
79789546,2327342,"S&#227;o Paulo, SP",postgresql,Duplicate key violation when updating &quot;owner&quot; entity of a one-to-one relation with a new &quot;owned&quot; value,"<p>I have two entities, <code>Vehicle</code> and <code>Driver</code>, with a one-to-one relationship. I also have a database constraint to ensure the uniques of a vehicle-to-driver relationship.</p>
<pre><code>@Entity
class Vehicle(
  @Id
  val id: UUID
) {
  
  @OneToOne(optional = true)
  @JoinColumn(name=&quot;driver_id&quot;, unique=true, nullable=true)
  var driver: Driver? = null

  ...
}
</code></pre>
<pre><code>@Entity
class Driver(
  @Id
  val id: UUID,
  var licenseNumber: String
)
</code></pre>
<p>The problem I'm facing is:</p>
<p>If I have a driver that is already associated to a vehicle, and I try to remove it from the current vehicle and associate it to a new vehicle, I get <code>duplicate key value violates unique constraint &quot;unique_vehicle_id_driver_id_idx&quot;</code>.</p>
<p>Sample code (in a @Transactional method)</p>
<pre><code>val vehicleA = vehicleRepository.findById(&quot;vehicleAId&quot;)

val vehicleADriver = vehicleA.driver

vehicleA.driver = null

val vehicleB = vehicleRepository.findById(&quot;vehicleBId&quot;)

vehicleB.driver = vehicleADriver

vehicleRepository.save(vehicleA)
vehicleRepository.save(vehicleB) 

// Exception throw! Duplicate key value violates unique constraint &quot;unique_vehcle_id_driver_id_idx&quot;

</code></pre>
<p>However, if I map the relation bidirectionally, the update works correctly without exception.</p>
<pre><code>@Entity
class Driver(
  @Id
  val id: UUID,
  var licenseNumber: String
) {
  
  @OneToOne(mappedBy=&quot;driver&quot;)
  var vehicle: Vehicle? = null

}
</code></pre>
<ul>
<li>Why it only works with the bidirectional mapping?</li>
<li>Is there a way to make it work without bidirectional mapping?</li>
<li>I don't want to use intermediate <code>saveAndFlush</code> call to avoid database round-trips</li>
</ul>
",1,1,0,2025-10-13T18:17:37+00:00,1,83,True
79790438,3514830,,postgresql,Different versions of pg_stat_statements on different databases on the same instance?,"<p>I'm writing some code to capture some query info in aurora postgres. When I take data from pg_stat_statements view from 3 different databases, I can see different columns returned.</p>
<p>Turns out the extversion on each is different (1.11, 1.6 and 1.7) - which I found out by running</p>
<pre><code>SELECT * FROM pg_extension WHERE extname = 'pg_stat_statements';
</code></pre>
<p>I also found out that there are multiple versions available</p>
<pre><code>SELECT * FROM pg_available_extension_versions WHERE name = 'pg_stat_statements';
</code></pre>
<p>Question: can I get the same version enabled/installed on all databases? in other words, can I control the version that gets enabled/installed when I run the below?</p>
<pre><code>CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
</code></pre>
",1,1,0,2025-10-14T17:06:51+00:00,1,88,True
79790643,742402,,postgresql,How to remove optional prefixes and suffixes from string,"<p>Product codes may contain 5 different prefixes CK0, CK, C, 0, K0
and two character suffixes from AA to AZ: AA,AB,...,AZ</p>
<p>Sample codes:</p>
<pre><code>CK04721310AE
CK04721310AD
CK04721310AC
CK04721310AB
4721310AE
4721310AC
K04721310AE
04721310AE
C4721310
</code></pre>
<p>How to remove those this prefix and or suffix from code? Result should be <strong>4721310</strong> for all those codes (here with prefix and suffix in italics):</p>
<p><em>CK0</em><strong>4721310</strong><em>AE</em><br />
<em>CK0</em><strong>4721310</strong><em>AD</em><br />
<em>CK0</em><strong>4721310</strong><em>AC</em><br />
<em>CK0</em><strong>4721310</strong><em>AB</em><br />
<strong>4721310</strong><em>AE</em><br />
<strong>4721310</strong><em>AC</em><br />
<em>K0</em><strong>4721310</strong><em>AE</em><br />
<em>0</em><strong>4721310</strong><em>AE</em><br />
<em>C</em><strong>4721310</strong></p>
<p>Tried</p>
<pre><code>select substring( 'CK04721310AE' , '(?:CK0|CK|C|0|K0)?(.+)(A[A-Z])?' )
</code></pre>
<p>But this does not remove suffix for unknown reason.
It returns suffix also:</p>
<blockquote>
<p>4721310AE</p>
</blockquote>
<p>How to get code without suffix and prefix ?</p>
<p>Using PostgreSQL 15.2</p>
",-7,0,7,2025-10-14T21:37:58+00:00,1,176,True
79790915,9888512,Germany,postgresql,Axon Framework with postgresql: Using Bytea instead of OID not working,"<p>I'm using axon framework 4.12.1 with postgresql (17), flyway and spring boot (3.5.6).
Following the <a href=""https://docs.axoniq.io/axon-framework-reference/4.12/tuning/rdbms-tuning/#_postgresql_lob_annotated_columns_and_default_hibernate_mapping"" rel=""nofollow noreferrer"">recommendations</a> I changed the sql migration files to use bytea instead of oid and configured the appropriate orm.xml.</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;entity-mappings version=&quot;2.0&quot; xmlns=&quot;http://java.sun.com/xml/ns/persistence/orm&quot;&gt;
    &lt;mapped-superclass access=&quot;FIELD&quot; metadata-complete=&quot;false&quot;
                       class=&quot;org.axonframework.eventhandling.AbstractSequencedDomainEventEntry&quot;&gt;
        &lt;attributes&gt;
            &lt;id name=&quot;globalIndex&quot;&gt;
                &lt;generated-value strategy=&quot;SEQUENCE&quot; generator=&quot;mySequenceGenerator&quot;/&gt;
                &lt;sequence-generator name=&quot;mySequenceGenerator&quot; sequence-name=&quot;domain_event_entry_seq&quot;
                                    allocation-size=&quot;1&quot;/&gt;
            &lt;/id&gt;
            &lt;basic name=&quot;metaData&quot;&gt;
                &lt;column name=&quot;meta_data&quot; column-definition=&quot;BYTEA&quot;/&gt;
            &lt;/basic&gt;
            &lt;basic name=&quot;payload&quot;&gt;
                &lt;column name=&quot;payload&quot; column-definition=&quot;BYTEA&quot;/&gt;
            &lt;/basic&gt;
        &lt;/attributes&gt;
    &lt;/mapped-superclass&gt;
    &lt;entity class=&quot;org.axonframework.modelling.saga.repository.jpa.SagaEntry&quot;&gt;
        &lt;attribute-override name=&quot;serializedSaga&quot;&gt;
            &lt;column name=&quot;serialized_saga&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
    &lt;/entity&gt;
    &lt;entity class=&quot;org.axonframework.eventhandling.tokenstore.jpa.TokenEntry&quot;&gt;
        &lt;attribute-override name=&quot;token&quot;&gt;
            &lt;column name=&quot;token&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
    &lt;/entity&gt;
    &lt;entity class=&quot;org.axonframework.eventsourcing.eventstore.jpa.SnapshotEventEntry&quot;&gt;
        &lt;attribute-override name=&quot;metaData&quot;&gt;
            &lt;column name=&quot;meta_data&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
        &lt;attribute-override name=&quot;payload&quot;&gt;
            &lt;column name=&quot;payload&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
    &lt;/entity&gt;
    &lt;entity class=&quot;org.axonframework.eventhandling.deadletter.jpa.DeadLetterEntry&quot;&gt;
        &lt;attribute-override name=&quot;diagnostics&quot;&gt;
            &lt;column name=&quot;diagnostics&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
        &lt;attribute-override name=&quot;metaData&quot;&gt;
            &lt;column name=&quot;meta_data&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
        &lt;attribute-override name=&quot;payload&quot;&gt;
            &lt;column name=&quot;payload&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
        &lt;attribute-override name=&quot;token&quot;&gt;
            &lt;column name=&quot;token&quot; column-definition=&quot;BYTEA&quot;&gt;&lt;/column&gt;
        &lt;/attribute-override&gt;
    &lt;/entity&gt;
&lt;/entity-mappings&gt;
</code></pre>
<p>when using the app it's now failing with</p>
<blockquote>
<p>org.hibernate.exception.SQLGrammarException: could not execute
statement [ERROR: column &quot;meta_data&quot; is of type bytea but expression
is of type bigint   Hint: You will need to rewrite or cast the
expression.   Position: 179] [insert into domain_event_entry
(aggregate_identifier,event_identifier,meta_data,payload,payload_revision,payload_type,sequence_number,time_stamp,type,global_index)
values (?,?,?,?,?,?,?,?,?,?)]</p>
</blockquote>
<p>domain event entry table definition:</p>
<pre class=""lang-sql prettyprint-override""><code>create table public.domain_event_entry
(
    global_index         bigserial
        constraint pk_domain_event_entry
            primary key,
    aggregate_identifier varchar(255) not null,
    sequence_number      bigint       not null,
    type                 varchar(255),
    event_identifier     varchar(255) not null
        constraint uk_domain_event_entry_identifier
            unique,
    meta_data            bytea,
    payload              bytea        not null,
    payload_revision     varchar(255),
    payload_type         varchar(255) not null,
    time_stamp           varchar(255) not null,
    constraint uk_domain_event_entry
        unique (aggregate_identifier, sequence_number)
);

create unique index idx_domain_event_entry_aggregate_seq
    on public.domain_event_entry (aggregate_identifier, sequence_number);

create unique index idx_domain_event_entry_event_id
    on public.domain_event_entry (event_identifier);

</code></pre>
<p>application.properties (excerpt):</p>
<pre><code>spring.datasource.driverClassName=org.postgresql.Driver
spring.jpa.database-platform=com.haufe.zeva.infrastructure.persistence.ByteaEnforcedPostgresSQLDialect # the dialect I copied from the documentation
spring.jpa.generate-ddl=false
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.hbm2ddl.auto=none
spring.jpa.properties.jakarta.persistence.schema-generation.database.action=none
spring.jpa.properties.jakarta.persistence.schema-generation.create-source=none
spring.jpa.properties.jakarta.persistence.schema-generation.drop-source=none
spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
spring.jpa.hibernate.naming.strategy=org.hibernate.cfg.EJB3NamingStrategy
spring.jpa.properties.hibernate.jdbc.lob.non_contextual_creation=true
spring.jpa.open-in-view=false
spring.jpa.properties.hibernate.multiTenancy=DISCRIMINATOR
spring.jpa.properties.hibernate.tenant_identifier_resolver=com.som.path.TenantResolver
spring.jpa.properties.hibernate.hbm2ddl.halt_on_error=false
# axon framework
axon.axonserver.enabled=false
</code></pre>
<p>How to make the configuration work with <code>bytea</code>?</p>
",1,1,0,2025-10-15T08:00:42+00:00,1,93,False
79791075,742402,,postgresql,How to split row to mutiple rows based on quantity,"<p>Stock status table contains product codes and quantities. Same product code may be in multiple rows, no primary key:</p>
<pre><code>Product Quantity
Code1   2
Code2   1
Code1   3
Code3   3
</code></pre>
<p>How to split it to multiple rows based on quantity? Result should be</p>
<pre><code>Code1
Code1
Code2
Code1
Code1
Code1
Code3
Code3
Code3
</code></pre>
<p>Result can be unordered.
Using Postgresql 17</p>
",-4,0,4,2025-10-15T10:46:37+00:00,1,92,True
79791280,20176201,,postgresql,Missing constructor when using SELECT NEW DTO with query Java Persist Query Language,"<p>I am using <strong>Spring Data JPA with Hibernate 6</strong>. I want to fetch a list of cars into a DTO using JPQL <code>SELECT NEW</code> syntax, with <code>LEFT JOIN</code>, <code>GROUP BY</code>, and <code>CASE</code> conditions.</p>
<p>I keep getting the following error:</p>
<pre class=""lang-java prettyprint-override""><code>Caused by: java.lang.IllegalArgumentException: org.hibernate.query.SemanticException: Missing constructor for type 'CarsResponseDto' [    SELECT NEW com.duydev.backend.presentation.dto.response.CarsResponseDto(
        c.id,
        c.brand,
        c.model,
        c.year,
        c.pricePerHour
    )
    FROM CarsEntity c
    LEFT JOIN c.location loc
    LEFT JOIN c.bookings b
    WHERE (:__$synthetic$__1 IS NULL OR c.brand = :__$synthetic$__2)
      AND (:__$synthetic$__3 IS NULL OR c.year = :__$synthetic$__4)
      AND (:__$synthetic$__5 IS NULL OR loc.province = :__$synthetic$__6)
      AND (:__$synthetic$__7 IS NULL OR loc.ward = :__$synthetic$__8)
      AND (:__$synthetic$__9 IS NULL OR c.pricePerHour &gt;= :__$synthetic$__10)
      AND (:__$synthetic$__11 IS NULL OR c.pricePerHour &lt;= :__$synthetic$__12)
      AND (b.endTime &gt;= :__$synthetic$__13 OR b.id IS NULL)
    GROUP BY c.id
    HAVING SUM(
        CASE
            WHEN b.id IS NOT NULL
                 AND b.status = 'CONFIRMED'
                 AND b.endTime &gt; :__$synthetic$__14
                 AND b.startTime &lt; :__$synthetic$__15
            THEN 1
            ELSE 0
        END
    ) = 0
]
</code></pre>
<p>My file CarsRepository:</p>
<pre><code>package com.duydev.backend.domain.repositories;

import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import com.duydev.backend.domain.model.CarsEntity;
import com.duydev.backend.presentation.dto.request.RequestGetCarsDto;
import com.duydev.backend.presentation.dto.response.CarsResponseDto;

@Repository
public interface CarsRepository extends JpaRepository&lt;CarsEntity, Long&gt; {
    @Query(&quot;&quot;&quot;
                SELECT NEW com.duydev.backend.presentation.dto.response.CarsResponseDto(
                    c.id,
                    c.brand,
                    c.model,
                    c.year,
                    c.pricePerHour
                )
                FROM CarsEntity c
                LEFT JOIN c.location loc
                LEFT JOIN c.bookings b
                WHERE (:#{#requestGetCarsDto.getBrand()} IS NULL OR c.brand = :#{#requestGetCarsDto.getBrand()})
                  AND (:#{#requestGetCarsDto.getYear()} IS NULL OR c.year = :#{#requestGetCarsDto.getYear()})
                  AND (:#{#requestGetCarsDto.getProvince()} IS NULL OR loc.province = :#{#requestGetCarsDto.getProvince()})
                  AND (:#{#requestGetCarsDto.getWard()} IS NULL OR loc.ward = :#{#requestGetCarsDto.getWard()})
                  AND (:#{#requestGetCarsDto.getMinPrice()} IS NULL OR c.pricePerHour &gt;= :#{#requestGetCarsDto.getMinPrice()})
                  AND (:#{#requestGetCarsDto.getMaxPrice()} IS NULL OR c.pricePerHour &lt;= :#{#requestGetCarsDto.getMaxPrice()})
                  AND (b.endTime &gt;= :#{#requestGetCarsDto.getStartTime()} OR b.id IS NULL)
                GROUP BY c.id
                HAVING SUM(
                    CASE
                        WHEN b.id IS NOT NULL
                             AND b.status = 'CONFIRMED'
                             AND b.endTime &gt; :#{#requestGetCarsDto.getStartTime()}
                             AND b.startTime &lt; :#{#requestGetCarsDto.getEndTime()}
                        THEN 1
                        ELSE 0
                    END
                ) = 0
            &quot;&quot;&quot;)
    public Page&lt;CarsResponseDto&gt; findCars(@Param(&quot;requestGetCarsDto&quot;) RequestGetCarsDto requestGetCarsDto,
            Pageable pageable);
}
</code></pre>
<p>This is my CarsEntity:</p>
<pre><code>
package com.duydev.backend.domain.model;

import java.math.BigDecimal;
import java.util.List;

import org.hibernate.annotations.ColumnTransformer;

import jakarta.persistence.CascadeType;
import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.JoinColumn;
import jakarta.persistence.ManyToOne;
import jakarta.persistence.OneToMany;
import jakarta.persistence.OneToOne;
import jakarta.persistence.Table;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;

@Getter
@Setter
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Entity
@Table(name = &quot;tbl_cars&quot;)
public class CarsEntity extends AbstractEntity&lt;Long&gt; {

    @ManyToOne
    @JoinColumn(name = &quot;owner_id&quot;, referencedColumnName = &quot;id&quot;)
    private User user;

    @Column(name = &quot;brand&quot;)
    private String brand;

    @Column(name = &quot;model&quot;)
    private String model;

    @Column(name = &quot;license_plate&quot;)
    private String licensePlate;

    @Column(name = &quot;year&quot;)
    private Integer year;

    @Column(name = &quot;price_per_hour&quot;)
    private BigDecimal pricePerHour;

    @OneToOne(cascade = CascadeType.ALL)
    @JoinColumn(name = &quot;location_id&quot;, referencedColumnName = &quot;id&quot;)
    private LocationEntity location;

    @Column(name = &quot;images&quot;, columnDefinition = &quot;jsonb&quot;)
    @ColumnTransformer(write = &quot;?::jsonb&quot;)
    private String images;

    @OneToMany(mappedBy = &quot;car&quot;)
    private List&lt;BookingEntity&gt; bookings;
}
</code></pre>
<p>Here is my CarsResponseDto:</p>
<pre><code>package com.duydev.backend.presentation.dto.response;

import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;

@Getter
@Setter
@NoArgsConstructor
public class CarsResponseDto {
    Long id;
    String brand;
    String model;
    Integer year;
    Double pricePerHour;

    public CarsResponseDto(Long id, String brand, String model, Integer year, Double pricePerHour) {
        this.id = id;
        this.brand = brand;
        this.model = model;
        this.year = year;
        this.pricePerHour = pricePerHour;
    }
}
</code></pre>
<p>This is my schema Cars:</p>
<pre><code>CREATE TABLE tbl_cars (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    created_by BIGINT,
    updated_by BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    owner_id BIGINT NOT NULL,
    brand VARCHAR(50) NOT NULL,
    model VARCHAR(50) NOT NULL,
    year INT NOT NULL,
    license_plate VARCHAR(20) UNIQUE NOT NULL,
    price_per_hour DECIMAL(10, 2) NOT NULL,
    location_id BIGINT,
    images JSONB,
    FOREIGN KEY (owner_id) REFERENCES tbl_user(id) ON DELETE CASCADE,
    FOREIGN KEY (location_id) REFERENCES tbl_locations(id) ON DELETE SET NULL
);
</code></pre>
<p>How can I fix this error while keeping the <code>CASE</code> expressions and DTO projection?</p>
",4,4,0,2025-10-15T14:32:10+00:00,1,133,True
79791669,3163203,,postgresql,How to use the type of a referenced column as a VARIADIC argument type in a (Postgres) SQL function or procedure definition?,"<p><a href=""https://www.postgresql.org/docs/17/sql-createfunction.html"" rel=""nofollow noreferrer"">The PostgreSQL documentation for <code>argtype</code></a> in function and procedure definitions states:</p>
<blockquote>
<p>The type of a column is referenced by writing <code>table_name.column_name%TYPE</code>.</p>
</blockquote>
<p>This works fine for <code>IN</code> mode arguments, but for <code>VARIADIC</code> it will say:</p>
<blockquote>
<p>ERROR: VARIADIC parameter must be an array</p>
</blockquote>
<p>If brackets are added to the end to make the array type ‘<code>table_name.column_name%TYPE[]</code>’, it will say:</p>
<blockquote>
<p>ERROR:  syntax error at or near &quot;[&quot;</p>
</blockquote>
<p>even though <code>INTEGER[]</code> would work.</p>
<p>If <a href=""https://www.postgresql.org/docs/current/plpgsql-declarations.html#PLPGSQL-DECLARATION-TYPE"" rel=""nofollow noreferrer"">the <code>ARRAY[]</code> syntax</a> is used, it will say:</p>
<blockquote>
<p>ERROR:  syntax error at or near &quot;ARRAY&quot;</p>
</blockquote>
<p>Is there a working syntax, or is this broken?</p>
<p>Complete SQL script for replication:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE SCHEMA s;

CREATE TABLE s.&quot;the table&quot; (
    a INTEGER,
    b INTEGER,
    c INTEGER
);

CREATE PROCEDURE s.&quot;the table operation&quot;(
          IN a s.&quot;the table&quot;.a%TYPE,
          IN b s.&quot;the table&quot;.b%TYPE,
    VARIADIC c s.&quot;the table&quot;.c%TYPE[]
)
LANGUAGE plpgsql AS $$
BEGIN
END;
$$;

DROP SCHEMA s CASCADE;
</code></pre>
",3,3,0,2025-10-15T23:13:50+00:00,2,91,True
79793181,890438,,postgresql,PostgresSQL join two tables to summarize data by 3 columns in one table,"<p>I have the following two tables below</p>
<p>Job summary table</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">id</th>
<th>Task</th>
<th>success</th>
<th>fail</th>
<th style=""text-align: right;"">time</th>
<th style=""text-align: center;"">org_id</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td>task1</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">5</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td>task1</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">6</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td>task1</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">4</td>
<td>task1</td>
<td>FALSE</td>
<td>TRUE</td>
<td style=""text-align: right;"">8</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">5</td>
<td>task2</td>
<td>FALSE</td>
<td>TRUE</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">6</td>
<td>task2</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">7</td>
<td>task2</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">6</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">8</td>
<td>task3</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">6</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">9</td>
<td>task3</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">8</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">10</td>
<td>task3</td>
<td>TRUE</td>
<td>FALSE</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table></div>
<p>Orgs table</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">id</th>
<th>org_name</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td>sales</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td>ops</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td>maint</td>
</tr>
</tbody>
</table></div>
<p>My task at hand is to generate a single query where I have following joined over <code>org_id</code> and grouped by task</p>
<ul>
<li>Total job count summarized by task name</li>
<li>Cumulative time elapsed for all tasks by task name</li>
<li>Average run time across all tasks by task name</li>
</ul>
<p>I have built the query below which gives me the summary of task count but when adding sum of time and average, and then <strong>trying to add the time elapsed for the <em>last</em> execution</strong> of this task, PostgreSQL forces me to use this last <code>time</code> in the <code>group by</code> (&quot;<code>time should be used as part of the group by</code>&quot;), and when doing so, <strong>the <code>group by</code> doesn't retain 1 row per task anymore</strong>, but instead the query then outputs as many rows as there are different <code>time</code>s for this task.</p>
<pre><code>select 
    task.name as TN, 
    sum(case when task.failed=false then 1 else 0 END) as successful_jobs, 
    sum(case when failed=true then 1 else 0 END) as failed_jobs,
    --sum(time) as cumulative_sum,
    --avg(time) as average_sum,
    -- Here how to add time of the last execution?
    -- I don't want max(time), I want time of the max(id)
    -- any use of column time makes it mandatory in the group by,
    -- which then breaks the results by outputting as many rows as there are different time for this task name,
    -- instead of 1 row per task name.
from 
    tasktable as task
join 
    orgtable as org on org.id = task.org_id
where
    org.org_name = 'sales' 
group by 
    task.name
order by 
    successful_jobs desc
</code></pre>
<p>Expected end state</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>TN</th>
<th style=""text-align: right;"">successful_jobs</th>
<th style=""text-align: right;"">failed_jobs</th>
<th style=""text-align: right;"">cumulative sum</th>
<th style=""text-align: right;"">average sum</th>
<th style=""text-align: right;"">last execution time</th>
</tr>
</thead>
<tbody>
<tr>
<td>task1</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">29</td>
<td style=""text-align: right;"">7.25</td>
<td style=""text-align: right;"">8</td>
</tr>
<tr>
<td>task3</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">24</td>
<td style=""text-align: right;"">8</td>
<td style=""text-align: right;"">10</td>
</tr>
<tr>
<td>task2</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">19</td>
<td style=""text-align: right;"">6.333333333</td>
<td style=""text-align: right;"">6</td>
</tr>
</tbody>
</table></div>
<p>Appreciate any help building this query</p>
",3,3,0,2025-10-17T14:55:49+00:00,2,94,True
79793556,31710185,,postgresql,I have a question about which approach is more performant in Prisma when fetching related records,"<p>I have two entities with a one-to-many relationship, something like this:</p>
<pre><code>model ParentEntity {
  id       Int              @id @default(autoincrement())
  name     String
  children ChildEntity[]
}

model ChildEntity {
  id        Int             @id @default(autoincrement())
  parentId  Int
  parent    ParentEntity    @relation(fields: [parentId], references: [id])
}
</code></pre>
<p>In one of my API routes, I need to fetch all children related to a specific parent. Currently, I see two possible approaches:</p>
<p>findMany directly on ChildEntity</p>
<pre><code>const children = await prisma.childEntity.findMany({
  where: { parentId: id },
});
</code></pre>
<p>OR
findUnique on ParentEntity with include</p>
<pre><code>const parent = await prisma.parentEntity.findUnique({
  where: { id },
  include: { children: true },
});
</code></pre>
<p>From a performance and best practices perspective, which approach is more efficient?</p>
<p>Considerations:</p>
<p>The number of children can be large (hundreds);</p>
<p>In some cases, I don’t need all the parent data, just the children;</p>
<p>The database in use is PostgreSQL.</p>
<p>Should I prioritize using findMany directly on the related entity, or use include within findUnique for consistency and potential relational caching?</p>
",0,0,0,2025-10-18T04:35:12+00:00,1,66,True
79793569,16835244,,postgresql,Docker exec with psql single result hangs,"<p>I have a bash script that runs docker exec into a Postgresql container to retrieve the maximum ID of a DB table:</p>
<pre class=""lang-bash prettyprint-override""><code>sudo docker exec -it my-database psql -qtAX -U postgres -c &quot;select MAX(rwi.id) from myschema.myimporttable rwi;&quot; myschema
</code></pre>
<p>If I run that statement from the host command line like this, it immediately returns the correct response, say <code>66</code>.</p>
<p>If I instead assign it to a variable in a bash script, it hangs indefinitely:</p>
<pre class=""lang-bash prettyprint-override""><code>result=$(sudo docker exec -it my-database psql -qtAX -U postgres -c &quot;select MAX(rwi.id) from myschema.myimporttable rwi;&quot; myschema)
</code></pre>
<p>Checking <code>pg_activity</code>, it shows <code>ClientRead</code> in idle state for that query. I need to <code>kill -9</code> to terminate this.</p>
<p>If I log-in to the container and run</p>
<pre class=""lang-bash prettyprint-override""><code>result=$(psql -qtAX -U postgres -c &quot;select MAX(rwi.id) from myschema.myimporttable rwi;&quot; myschema)
</code></pre>
<p>it does not hang. I assume it is something with my <code>docker exec</code> that causes the hang.</p>
<p>It worked for years without issues, and now it does not anymore. Docker version upgrade?</p>
",2,3,1,2025-10-18T05:16:54+00:00,0,78,False
79793773,157171,Netherlands,postgresql,Postgres Docker container not shutting down from vscode Rebuild command,"<p>I have a very simple Docker project setup with a vscode devcontainer.</p>
<p>There are two Docker containers, <code>dev</code> and <code>db</code></p>
<p>This works! I open the project in the devcontainer and everything works fine, until i have to rebuild the container: When I do the vscode  <code>devcontainers: Reload container</code> command the <code>dev</code> service container is closing and deleted as expected. However, the <code>db</code> service container is not. The restart policy even set to &quot;no&quot;, it just refuses to shut down at all, causing a new error because the new rebuilder db container wants to use the port that is still in use on the old container.</p>
<p>It looks like the default setting in devcontainer.json <code>&quot;shutdownAction&quot;: &quot;stopCompose&quot;</code> is completely ignored by Docker or by the db container itself. (i tried to explicitly set that setting in the devcontainer.json as well without success).</p>
<p>Here are the relevant files:</p>
<p><code>docker-compose.yml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>services: 
  dev: 
    container_name: trading-platform-dev
    build:
      context: .
      dockerfile: Dockerfile
    ports: 
      - &quot;3000:3001&quot; 
    volumes:
      - .:/workspace:cached
    depends_on:
      - db
    tty: true
    stdin_open: true

  db:
    container_name: trading-platform-db
    image: postgres:17-alpine
    restart: &quot;no&quot;
    environment: 
      POSTGRES_USER: user 
      POSTGRES_PASSWORD: password
      POSTGRES_DB: trading_platform_db
    ports:
      - &quot;5432:5432&quot;
    volumes:
      - postgres_dev_data:/var/lib/postgresql/data

volumes:
  postgres_dev_data:
</code></pre>
<hr />
<p><code>Dockerfile</code></p>
<pre class=""lang-none prettyprint-override""><code>FROM node:22-slim

# Install Required packages
RUN apt-get update &amp;&amp; apt-get install -y openssl curl git

# Activate Corepack
RUN corepack enable

# Disable npm update notifier 
RUN npm config set update-notifier false --global

# Create and set working directory
WORKDIR /workspace

# Switch to non-root user
RUN chown -R node: /workspace
USER node
</code></pre>
<hr />
<p><code>.devcontainer/devcontainer.json</code></p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;Trading Platform Dev&quot;,
    &quot;dockerComposeFile&quot;: &quot;../docker-compose.yml&quot;,
    &quot;service&quot;: &quot;dev&quot;,
    &quot;workspaceFolder&quot;: &quot;/workspace&quot;,
    &quot;customizations&quot;: {
        &quot;vscode&quot;: {
            &quot;extensions&quot;: [
                &quot;ms-vscode-remote.remote-containers&quot;,
                &quot;Prisma.prisma&quot;,
                &quot;esbenp.prettier-vscode&quot;,
                &quot;dbaumer.vscode-eslint&quot;,
                &quot;firsttris.vscode-jest-runner&quot;
            ]
        }
    }
}
</code></pre>
",-2,0,2,2025-10-18T13:19:31+00:00,1,73,True
79794098,23301497,,postgresql,trouble connecting my database to Django server on deployment machine,"<p>I'm deploying to AWS Linux 2023 and my postgresql database in on aws RDS. I've installed psql and checked if db is accessible in my instance. I've also checked that the environmental variables are fetching exactly as expected in my settings.py. and I even ssh and applied the migrations myself. but I keep getting the following issue:</p>
<pre><code>[stderr] raise dj_exc_value.with_traceback(traceback) from exc_value
[stderr] File &quot;/home/ec2-user/.local/share/virtualenvs/app-UPB06Em1/lib/python3.13/site-packages/django/db/backends/base/base.py&quot;, line 279, in ensure_connection
[stderr] self.connect()
[stderr] ~~~~~~~~~~~~^^
[stderr] File &quot;/home/ec2-user/.local/share/virtualenvs/app-UPB06Em1/lib/python3.13/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
[stderr] return func(*args, **kwargs)
[stderr] File &quot;/home/ec2-user/.local/share/virtualenvs/app-UPB06Em1/lib/python3.13/site-packages/django/db/backends/base/base.py&quot;, line 256, in connect
[stderr] self.connection = self.get_new_connection(conn_params)
[stderr] ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
[stderr] File &quot;/home/ec2-user/.local/share/virtualenvs/app-UPB06Em1/lib/python3.13/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
[stderr] return func(*args, **kwargs)
[stderr] File &quot;/home/ec2-user/.local/share/virtualenvs/app-UPB06Em1/lib/python3.13/site-packages/django/db/backends/postgresql/base.py&quot;, line 332, in get_new_connection
[stderr] connection = self.Database.connect(**conn_params)
[stderr] File &quot;/home/ec2-user/.local/share/virtualenvs/app-UPB06Em1/lib64/python3.13/site-packages/psycopg2/__init__.py&quot;, line 122, in connect
[stderr] conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[stderr]django.db.utils.OperationalError: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: No such file or directory
[stderr] Is the server running locally and accepting connections on that socket?
[stderr]
[stderr]2025-10-19 04:29:22,032 INFO Starting server at tcp:port=8000:interface=127.0.0.1
[stderr]2025-10-19 04:29:22,032 INFO HTTP/2 support not enabled (install the http2 and tls Twisted extras)
[stderr]2025-10-19 04:29:22,033 INFO Configuring endpoint tcp:port=8000:interface=127.0.0.1
[stderr]2025-10-19 04:29:22,033 INFO Listening on TCP address 127.0.0.1:8000
</code></pre>
<p>settings.py:</p>
<pre><code>...
DATABASES = {
    &quot;default&quot;: {
        &quot;ENGINE&quot;: &quot;django.db.backends.postgresql&quot;,
        &quot;NAME&quot;: DB_NAME,
        &quot;USER&quot;: DB_USER,
        &quot;PASSWORD&quot;: DB_PASSWORD,
        &quot;HOST&quot;: DB_HOST,
        &quot;PORT&quot;: DB_PORT,
    }
}
...
</code></pre>
<p>any possible ideas why this issue is happening?</p>
<p>Edit:
I'm using daphne to run the app and codedeploy to deploy the app. also note that the migration didn't get applied when code deploy run my start_app.sh</p>
<ul>
<li>python3.13</li>
<li>pipenv</li>
</ul>
<p>my start_app.sh:</p>
<pre><code>cd /home/ec2-user/app


if [ ! -f /etc/ssl/private/privkey.pem ]; then
    sudo mkdir -p /etc/ssl/private /etc/ssl/certs
    sudo openssl req -x509 -nodes -days 365 \
        -newkey rsa:2048 \
        -keyout /etc/ssl/private/privkey.pem \
        -out /etc/ssl/certs/fullchain.pem \
        -subj &quot;/C=US/ST=State/L=City/O=MyOrg/OU=MyDept/CN=localhost&quot;
    sudo chmod 600 /etc/ssl/private/privkey.pem
    sudo chmod 644 /etc/ssl/certs/fullchain.pem
fi

python3.13 -m pipenv install --deploy --ignore-pipfile
python3.13 -m pipenv run python manage.py migrate

pkill -f daphne || true

python3.13 -m pipenv run daphne -b 127.0.0.1 -p 8000 core.asgi:application &amp;

DAPHNE_PID=$!

# Check if the process is still running
sleep 10
if ! kill -0 &quot;$DAPHNE_PID&quot; 2&gt;/dev/null; then
    echo &quot;Daphne failed to start — exiting.&quot;
    exit 1
fi

sudo systemctl restart nginx
</code></pre>
",0,0,0,2025-10-19T05:12:47+00:00,1,73,False
79794139,31714830,,postgresql,psycopg.OperationalError: server closed the connection unexpectedly when using LangGraph AsyncPostgresSaver with FastAPI and SQLAlchemy,"<p>I'm integrating LangGraph (which uses <code>psycopg</code>) with an existing SQLAlchemy ORM (which uses <code>asyncpg</code>) inside a FastAPI app.</p>
<p>The ORM has been part of the project for a long time - stable and used across all services. The issues began after adding LangGraph persistence (<code>AsyncPostgresStore</code>, <code>AsyncPostgresSaver</code>) for agent state and checkpoints. Both LangGraph and SQLAlchemy connect to the same PostgreSQL database.</p>
<p>Some of the repository classes based on the ORM are also used inside the LangGraph flow (within tools and agent logic).</p>
<p>After several concurrent or long-running requests, I consistently get connection errors such as:</p>
<pre><code>psycopg.OperationalError: consuming input failed: server closed the connection unexpectedly
    This probably means the server terminated abnormally
    before or while processing the request.
</code></pre>
<p>It sometimes triggers follow-up errors like:</p>
<pre class=""lang-lua prettyprint-override""><code>asyncpg.exceptions.ConnectionDoesNotExistError: connection was closed in the middle of operation
sqlalchemy.exc.DBAPIError: connection was closed in the middle of operation
dishka.exceptions.ExitError: Cleanup context errors
</code></pre>
<p>The failures seem to happen when both SQLAlchemy and LangGraph are trying to close connections or roll back sessions.</p>
<p>Current setup:</p>
<pre><code>from collections.abc import AsyncIterator
from typing import cast
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.store.postgres.aio import AsyncPostgresStore
from psycopg import AsyncConnection
from psycopg.rows import DictRow
from psycopg_pool import AsyncConnectionPool
from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)

async def get_async_engine(
    dsn: PostgresDsn,
) -&gt; AsyncIterator[AsyncEngine]:
    async_engine = create_async_engine(
        url=dsn,
        pool_size=5,
        max_overflow=10,
        pool_pre_ping=True,
    )
    yield async_engine
    await async_engine.dispose()

def get_async_session_factory(
    engine: AsyncEngine,
) -&gt; async_sessionmaker[AsyncSession]:
    return async_sessionmaker(
        bind=engine,
        class_=AsyncSession,
        autoflush=False,
        expire_on_commit=False,
    )

async def get_main_async_session(
    async_session_factory: async_sessionmaker[AsyncSession],
) -&gt; AsyncIterator[MainAsyncSession]:
    async with async_session_factory() as session:
        yield cast(MainAsyncSession, session)

async def get_connection_pool(
    settings: AppSettings,
) -&gt; AsyncIterator[AsyncConnectionPool[AsyncConnection[DictRow]]]:
    pool: AsyncConnectionPool[AsyncConnection[DictRow]] = AsyncConnectionPool(conninfo=settings.postgres.psycopg_dsn)
    await pool.open()
    try:
        yield pool
    finally:
        await pool.close()

async def get_store(
    connection_pool: AsyncConnectionPool[AsyncConnection[DictRow]],
) -&gt; AsyncIterator[AsyncPostgresStore]:
    store = AsyncPostgresStore(connection_pool)
    yield store

async def get_checkpointer(
    connection_pool: AsyncConnectionPool[AsyncConnection[DictRow]],
) -&gt; AsyncIterator[AsyncPostgresSaver]:
    saver = AsyncPostgresSaver(connection_pool)
    yield saver

# Dishka injection
# SQLAlchemy Persistence 
provider.provide(source=get_async_engine, scope=Scope.APP)
provider.provide(source=get_async_session_factory, scope=Scope.APP)
provider.provide(source=get_main_async_session, scope=Scope.REQUEST)

# LangGraph Persistence
provider.provide(source=get_connection_pool, scope=Scope.APP)
provider.provide(source=get_store, scope=Scope.REQUEST)
provider.provide(source=get_checkpointer, scope=Scope.REQUEST)
</code></pre>
<p>I expected LangGraph and SQLAlchemy to share the same PostgreSQL database without connection issues.</p>
",2,2,0,2025-10-19T07:20:40+00:00,0,193,False
79794512,6808672,Turkey,postgresql,.NET / EF Core / AutoMapper causing PostgreSQL &quot;invalid input syntax for type json&quot; error when saving entity,"<p>I'm working on a <code>.NET 8</code> project using <code>Microsoft.EntityFrameworkCore 8.0.7</code> with PostgreSQL as the database. I have a table that contains a <code>jsonb</code> column, and I'm using AutoMapper to map a DTO to the entity.</p>
<p>Here is my entity:</p>
<pre><code>public partial class PickingAlgorithmConfig
{
    public int Id { get; set; }
    public string Name { get; set; } = null!;
    public string? RuleJson { get; set; }  // jsonb column in PostgreSQL
    public int WarehouseId { get; set; }
    
    // Other properties omitted for brevity
}
</code></pre>
<p>Here is my DTO:</p>
<pre><code>public class PickingAlgorithmConfigUpsertRequest
{
    public int Id { get; set; }
    public string Name { get; set; }
    public int WarehouseId { get; set; }
    public string? RuleJson { get; set; } // can be empty string or valid JSON
}
</code></pre>
<p>I set up AutoMapper like this:</p>
<pre><code>CreateMap&lt;PickingAlgorithmConfigUpsertRequest, PickingAlgorithmConfig&gt;()
    .ForMember(dest =&gt; dest.RuleJson,
               opt =&gt; opt.MapFrom(src =&gt; string.IsNullOrWhiteSpace(src.RuleJson) ? null : src.RuleJson))
    .ReverseMap();
</code></pre>
<p>And my service method:</p>
<pre><code>public async Task&lt;ApiResponse&lt;PickingAlgorithmConfigDto&gt;&gt; AddOrUpdateAsync(PickingAlgorithmConfigUpsertRequest upsertRequest, bool isUpdate)
{
    try
    {
        PickingAlgorithmConfig entity;
        var warehouse = await _warehouseRepository.GetByIdAsync(upsertRequest.WarehouseId);

        if (warehouse == null)
        {
            return new ApiResponse&lt;PickingAlgorithmConfigDto&gt;
            {
                Success = false,
                Data = null,
                Message = &quot;Warehouse not found.&quot;
            };
        }

        if (isUpdate)
        {
            entity = await _pickingAlgorithmConfigRepository.GetByIdAsync(upsertRequest.Id);
            if (entity == null)
            {
                return new ApiResponse&lt;PickingAlgorithmConfigDto&gt;
                {
                    Success = false,
                    Data = null,
                    Message = &quot;Picking algorithm configuration not found.&quot;
                };
            }

            _mapper.Map(upsertRequest, entity);
            await _pickingAlgorithmConfigRepository.UpdateAsync(entity);
        }
        else
        {
            entity = _mapper.Map&lt;PickingAlgorithmConfig&gt;(upsertRequest);
            await _pickingAlgorithmConfigRepository.AddAsync(entity);
        }

        await _pickingAlgorithmConfigRepository.SaveChangesAsync();

        
        var dto = _mapper.Map&lt;PickingAlgorithmConfigDto&gt;(entity);

        return new ApiResponse&lt;PickingAlgorithmConfigDto&gt;
        {
            Success = true,
            Data = dto,
            Message = &quot;Operation success.&quot;
        };
    }
    catch (Exception ex)
    {
        return new ApiResponse&lt;PickingAlgorithmConfigDto&gt;
        {
            Success = false,
            Data = null,
            Message = $&quot;Operation failed: {ex.Message}&quot;
        };
    }
}
</code></pre>
<p>When I try to add a new record, I sometimes get this PostgreSQL error:</p>
<pre><code>22P02: invalid input syntax for type json
</code></pre>
<p>I want AutoMapper to set RuleJson to null if the incoming string is empty or not valid JSON, to avoid this error.</p>
<p>I've seen some questions about PostgreSQL JSON errors, but they don't involve AutoMapper mapping from a DTO.</p>
<p>My question:
How can I configure AutoMapper (or handle this in the service) so that <code>RuleJson</code> is <code>null</code> whenever the input string is empty or invalid JSON, and thus prevent the PostgreSQL invalid <code>input syntax for type json</code> error?</p>
",2,2,0,2025-10-19T22:15:15+00:00,1,134,True
79794872,9740712,India,postgresql,How to specify the name while concurrently removing indexes from database,"<p>I have some field in my table for which i need to remove indexing. In the django application, i saw that i could do this using the migrations.RemoveIndexConcurrently() method. However im having confusions on how to specify the name attribute with it. The previously said indexed fields were added during the time of creating the table and hence there is no separate AddIndex migration. Need to remove indexing for these fields in 2 different environments and when i looked up the names using</p>
<pre><code>SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'my_db_table_name'
</code></pre>
<p>i saw indexnames like
<code>user_secondary_mail_779d505a_like</code>, which could be different in the second environment. Is there any way i could specify the names of the fields so that i could run the migration in both environments. Any help would be greatly appreciated!</p>
",0,1,1,2025-10-20T11:59:50+00:00,0,127,False
79795578,8060017,,postgresql,Why doesn&#39;t index creation work any more in Postgres v17 and v18?,"<p>Considering the following SQL code:</p>
<pre class=""lang-sql prettyprint-override""><code>create table test (a int, b int) ;
create type tst as (a int, b int) ;
create function tst (a int, b int) returns tst[] language sql immutable as 
  $$ select array( select row(a,b) :: tst) ; $$ ;
create index index_test on test using gin (tst(a,b) array_ops) ;
</code></pre>
<p>This SQL code works well with Postgres until v16, see <a href=""https://dbfiddle.uk/rbnfJHRs"" rel=""nofollow noreferrer"">dbfiddle</a>, but it doesn't work any more with Postgres v17 and v18, see <a href=""https://dbfiddle.uk/Ynk3HZIT"" rel=""nofollow noreferrer"">dbfiddle</a>.</p>
<blockquote>
<p>ERROR:  type &quot;tst&quot; does not exist<br />
LINE 1:  select array( select row(a,b) :: tst) ;<br />
QUERY:   select array( select row(a,b) :: tst) ;<br />
CONTEXT:  SQL function &quot;tst&quot; during inlining</p>
</blockquote>
<p>What has changed with Postgres v17 &amp; v18, and how to fix this issue?</p>
",0,4,4,2025-10-21T08:03:08+00:00,1,219,True
79795933,256965,Finland,postgresql,How to handle inserts and updates on postgres temporal tables?,"<p>I have couple temporal tables in my db.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE temporal (
  version_id    SERIAL PRIMARY KEY,
  some_id       TEXT,
  some_data     TEXT,
  valid_from    TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  valid_to      TIMESTAMP DEFAULT NULL,
  is_current    BOOLEAN NOT NULL DEFAULT TRUE
);
</code></pre>
<p>I have created a trigger that acts on inserts</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE TRIGGER my_trigger
  BEFORE INSERT ON temporal
  FOR EACH ROW 
  EXECUTE manage_temporal_version();
</code></pre>
<p>The <code>plpgsql</code> function the trigger calls is quite usual temporal versioning trigger that basically does two things:</p>
<ol>
<li>It checks against some unique fields whether the table already has that row and if it's currently the active row or not</li>
<li>If such row is found, it deprecates the previous</li>
<li>Continues to insert a new row with the new data</li>
</ol>
<pre class=""lang-sql prettyprint-override""><code>    CREATE OR REPLACE FUNCTION manage_record_version()
      RETURNS TRIGGER AS $$
    DECLARE
      existing_row RECORD;
      where_clause TEXT;
      col_name TEXT;
      i INTEGER;
    BEGIN
      -- Build WHERE clause dynamically based on trigger arguments
      where_clause := 'is_current = TRUE';
  
      -- TG_ARGV contains the column names passed to the trigger
      FOR i IN 0 .. TG_NARGS - 1 LOOP
        col_name := TG_ARGV[i];
        
        -- Add condition for each key column
        where_clause := where_clause || format(' AND %I = $1.%I', col_name, col_name);
      END LOOP;
    
      -- Find existing current row with matching key columns
      EXECUTE format('SELECT * FROM %I WHERE %s FOR UPDATE', TG_TABLE_NAME, where_clause)
      INTO existing_row
      USING NEW;
    
      -- If a current row exists, version it
      IF FOUND THEN

        -- Close the existing version
        EXECUTE format(
          'UPDATE %I SET valid_to = CURRENT_TIMESTAMP, is_current = FALSE WHERE version_id = $1',
          TG_TABLE_NAME
        ) USING existing_row.version_id;
    
        -- Set up NEW record for the new version
        NEW.version_id := nextval(format('%s_version_id_seq', TG_TABLE_NAME));
        NEW.valid_from := CURRENT_TIMESTAMP;
        NEW.valid_to := NULL;
        NEW.is_current := TRUE;
        
        RETURN NEW;
      END IF;
    
      -- If no existing current row, proceed with normal insert
      RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;

</code></pre>
<p>I was thinking about that I wouldn't like to let people run direct update queries on the table, because the table rows shouldn't be updated directly due to this temporal versioning. I was trying an approach that I will change the trigger to act before insert or update and then during insert I'll do the above procedure and during updates I will simply check whether the update is happening on already closed row or on the currently active row. If updating closed row an exception is thrown preventing the update and if updating the current row the update would follow similar procedure as the insert. Sounds good in theory, but there's one big problem:</p>
<p>The inserts and updates inside the triggered function will invoke the trigger as well leading to infinitely cascading triggers.</p>
<p>Is there any way to make this work? Or is there any commonly agreed good way to handle inserts and updates on temporal tables?</p>
",1,1,0,2025-10-21T14:31:23+00:00,1,81,True
79796121,31729609,,postgresql,RLS usage on low-medium sized projects. +Security concerns,"<p>Consider this simple Row Level Security example:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE accounts (
  id integer PRIMARY KEY,
  email text,
  password text,
);

-- creating RLS policy
ALTER TABLE accounts ENABLE ROW LEVEL SECURITY;

CREATE POLICY own_rows
ON accounts
FOR SELECT
USING (owner_id = current_setting('somenamespace.current_user'));

-- example query using RLS
set_config('somenamespace.current_user', 123, true)  -- true for local scoped variable (lives till the end of tx)
SELECT email,password FROM accounts
</code></pre>
<p>PostgreSQL RLS is basically just the same as writing <code>WHERE user_id = $1</code> clause at the end of each query that must access a private row. In practice that means on each query you must write something like <code>set_config('')</code> or <code>SET LOCAL</code> first, followed by the main query, all that wrapped in a transaction. This raises the following complexities:</p>
<ol>
<li>It significantly increases amount of code required on a backend - <code>tx.Begin + tx.Commit + tx.Rollback + set_config + core query</code> vs <code>just query</code></li>
<li>Uncertainty pertaining <code>SET</code> command - commonly used in guides (!!!) on RLS while clearly being a security flaw if used like:</li>
</ol>
<pre class=""lang-sql prettyprint-override""><code>-- declare policy
...
USING (owner_id = current_user)

-- execute query
SET current_user = 123
SELECT email,password FROM accounts
</code></pre>
<p>since <code>SET</code> command works per connection - once returned to connection pool <code>SET</code> leftovers remain in that connection, which may cause unrelated query/user to access data of previous user. Some manuals suggest mitigation by prepending <code>SET LOCAL</code> or <code>RESET</code> command which is even more peculiar (I bet I even saw that in Supabase doc):</p>
<pre class=""lang-sql prettyprint-override""><code>RESET current_user
SET current_user = 123
...
</code></pre>
<p>However, it's still unclear if <code>SET arbitrary_namespace.current_user = 123</code> remains in a connection, i.e. <a href=""https://www.bytebase.com/blog/postgres-row-level-security-footguns/#10-connection-pooling-context-loss"" rel=""nofollow noreferrer"">https://www.bytebase.com/blog/postgres-row-level-security-footguns/#10-connection-pooling-context-loss</a> (note dot namespace for the variable)</p>
<pre class=""lang-sql prettyprint-override""><code>-- Use application-controlled session variables
-- App sets per transaction:
SET app.user_id = 'user-uuid';
SET app.tenant_id = 'tenant-uuid';
</code></pre>
<blockquote>
<p>App sets per transaction</p>
</blockquote>
<p>NO IT IS NOT !!!! -&gt; <a href=""https://www.postgresql.org/docs/current/sql-set.html"" rel=""nofollow noreferrer"">https://www.postgresql.org/docs/current/sql-set.html</a></p>
<blockquote>
<p>Once the surrounding transaction is committed, the effects will persist until the end of the session, unless overridden by another SET.</p>
</blockquote>
<p>Session AFAIU means connection, which means literally any consequent query executed by the client who got assigned that connection by the Connection Pool!</p>
<ol start=""3"">
<li>It introduces indexing/performance issues compared to simple <code>WHERE</code> clause, I'll omit that part but it may be found on the link above too</li>
</ol>
<p><strong>SUMMARIZE</strong>:</p>
<ol>
<li>Considering the above, why even use RLS instead of simple WHERE clause or an SQL function which takes user_id as an argument? For the sake of &quot;readability&quot; and &quot;explicit declarative permissions&quot;? Apart from that, what real benefits it gives compared to <code>get_user_auth_pair(user_id) returning email,password</code>?</li>
<li>What am I missing in bytebase implementation? Because concerns related to SET leftovers in a connection are definitely confirmed by multiple sources.</li>
</ol>
",1,2,1,2025-10-21T18:08:30+00:00,4,128,True
79796335,17981284,,postgresql,Django ORM fails to generate valid sql for JSONb contains,"<p>lets start with the error first from my logs:</p>
<pre><code>2025-10-21 19:18:11,380 ERROR api.services.observium_port_status_service Error getting port status from store: invalid input syntax for type json
LINE 1: ...&quot; WHERE &quot;api_jsonstatestore&quot;.&quot;filter_key_json&quot; @&gt; '''{&quot;type&quot;...
                                                             ^
DETAIL:  Token &quot;'&quot; is invalid.
CONTEXT:  JSON data, line 1: '...
</code></pre>
<p>and the query:</p>
<pre class=""lang-py prettyprint-override""><code>state = (
   JsonStateStore.objects.select_for_update()
   .filter(filter_key_json__contains={&quot;type&quot;: &quot;observium_port_status&quot;, &quot;observium_port_id&quot;: observium_port_id})
   .first()
)
</code></pre>
<p>here is an example record</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>created</th>
<th>touched</th>
<th>filter_key_json</th>
<th>data_json</th>
</tr>
</thead>
<tbody>
<tr>
<td>33</td>
<td>2025-10-21 18:19:59.873 -0500</td>
<td>2025-10-21 18:44:57.047 -0500</td>
<td>{&quot;type&quot;: &quot;observium_port_status&quot;, &quot;observium_port_id&quot;: 987}</td>
<td>redacted</td>
</tr>
</tbody>
</table></div>
<p>and the model:</p>
<pre class=""lang-py prettyprint-override""><code>class JsonStateStore(models.Model):
    created = models.DateTimeField(auto_now_add=True)
    touched = models.DateTimeField(auto_now=True)
    filter_key_json = models.JSONField()
    data_json = models.JSONField()
    
    class Meta:
        verbose_name = &quot;JSON State Store&quot;
        verbose_name_plural = &quot;JSON State Stores&quot;

    def __str__(self):
        return f&quot;JsonStateStore(key={self.filter_key_json}, created={self.created}, touched={self.touched})&quot;

    def save(self, *args, **kwargs):
        self.touched = timezone.now()
        super().save(*args, **kwargs)
</code></pre>
<p>I am on django <code>4.2.2</code> <br>
my database backend is <code>timescale.db.backends.postgis</code> (<a href=""https://github.com/jamessewell/django-timescaledb"" rel=""nofollow noreferrer"">github</a> | <a href=""https://pypi.org/project/django-timescaledb/"" rel=""nofollow noreferrer"">pypi</a>) and i am on version <code>0.2.13</code> of that package</p>
<p>I cannot identify any syntax error in the QuerySet call and I cannot figure out what is going wrong here</p>
<p>My current workaround is</p>
<pre class=""lang-py prettyprint-override""><code>lock_sql = (
   &quot;&quot;&quot;
   SELECT id, created, touched, filter_key_json, data_json
   FROM api_jsonstatestore
   WHERE filter_key_json @&gt; %s::jsonb
   ORDER BY id ASC
   LIMIT 1
   FOR UPDATE
   &quot;&quot;&quot;
)
payload = {&quot;type&quot;: &quot;observium_port_status&quot;, &quot;observium_port_id&quot;: observium_port_id}
records = list(JsonStateStore.objects.raw(lock_sql, [json.dumps(payload)]))
state = records[0] if records else None
</code></pre>
<p>if anyone has any insight on why the ORM method does not work and would like to share I would appreciate it</p>
",0,0,0,2025-10-22T01:20:03+00:00,0,113,False
79796620,22177234,,postgresql,unable to obtain isolated JDBC connection,"<pre class=""lang-none prettyprint-override""><code>org.postgresql.util.PSQLException: FATAL: invalid value for parameter &quot;TimeZone&quot;: &quot;Asia/Calcutta&quot;
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2734) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2859) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.core.v3.QueryExecutorImpl.&lt;init&gt;(QueryExecutorImpl.java:178) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:344) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:57) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:277) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.Driver.makeConnection(Driver.java:448) ~[postgresql-42.7.7.jar:42.7.7]
    at org.postgresql.Driver.connect(Driver.java:298) ~[postgresql-42.7.7.jar:42.7.7]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:144) ~[HikariCP-6.3.3.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:370) ~[HikariCP-6.3.3.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:207) ~[HikariCP-6.3.3.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:488) ~[HikariCP-6.3.3.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:576) ~[HikariCP-6.3.3.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:97) ~[HikariCP-6.3.3.jar:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111) ~[HikariCP-6.3.3.jar:na]
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:126) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:485) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.resource.transaction.backend.jdbc.internal.JdbcIsolationDelegate.delegateWork(JdbcIsolationDelegate.java:61) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:334) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:129) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:81) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.boot.model.relational.Database.&lt;init&gt;(Database.java:45) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:226) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:194) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1442) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1513) ~[hibernate-core-6.6.29.Final.jar:6.6.29.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:66) ~[spring-orm-6.2.11.jar:6.2.11]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:390) ~[spring-orm-6.2.11.jar:6.2.11]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:419) ~[spring-orm-6.2.11.jar:6.2.11]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:400) ~[spring-orm-6.2.11.jar:6.2.11]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:366) ~[spring-orm-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1873) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1822) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:607) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:529) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:339) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:373) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:691) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:513) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1375) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1205) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:569) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:529) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:339) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:373) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1725) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1474) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:606) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:529) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:339) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:373) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1698) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1643) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:913) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:791) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:240) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1395) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1232) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:569) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:529) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:339) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:373) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1760) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1643) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:913) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:791) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:240) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1395) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1232) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:569) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:529) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:339) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:373) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:337) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-6.2.11.jar:6.2.11]
    at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:230) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:184) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:179) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:164) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.ServletContextInitializerBeans.&lt;init&gt;(ServletContextInitializerBeans.java:96) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:271) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:245) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4464) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:317) ~[na:na]
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java) ~[na:na]
    at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:81) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145) ~[na:na]
    at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:773) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:317) ~[na:na]
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java) ~[na:na]
    at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:81) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145) ~[na:na]
    at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.StandardService.startInternal(StandardService.java:412) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.apache.catalina.startup.Tomcat.start(Tomcat.java:438) ~[tomcat-embed-core-10.1.46.jar:10.1.46]
    at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.&lt;init&gt;(TomcatWebServer.java:107) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:517) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:219) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:193) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:167) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:621) ~[spring-context-6.2.11.jar:6.2.11]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:439) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:318) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1361) ~[spring-boot-3.5.6.jar:3.5.6]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1350) ~[spring-boot-3.5.6.jar:3.5.6]
    at com.deepanshu.expensetracker.ExpensetrackerApplication.main(ExpensetrackerApplication.java:10) ~[main/:na]

2025-10-22T14:50:55.785+05:30  WARN 16800 --- [expensetracker] [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 22023
2025-10-22T14:50:55.785+05:30 ERROR 16800 --- [expensetracker] [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : FATAL: invalid value for parameter &quot;TimeZone&quot;: &quot;Asia/Calcutta&quot;
2025-10-22T14:50:55.787+05:30  WARN 16800 --- [expensetracker] [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata
</code></pre>
<pre><code>spring.datasource.url=jdbc:postgresql://localhost:5432/springdatabase?serverTimezone=Asia/Kolkata
spring.datasource.username=appuser
spring.datasource.password=apppassword
spring.datasource.driver-class-name=org.postgresql.Driver
spring.jpa.hibernate.ddl-auto=update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.datasource.hikari.initialization-fail-timeout=60000
spring.datasource.hikari.connection-timeout=30000
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.format_sql=true
logging.level.org.hibernate.SQL=DEBUG
logging.level.org.hibernate.engine.jdbc.spi.SqlExceptionHelper=TRACE
</code></pre>
<p>it i use this credential on gui database there is working but i am getting error here please help me but i am getting this issue</p>
",-4,2,6,2025-10-22T09:32:33+00:00,1,313,True
79796876,10188661,"Minsk, Belarus",postgresql,Do you need index on is_archived if not many rows are archived?,"<p>Let's say I have book table:</p>
<pre><code>id: int
is_archived: bool
creator_id: int
created_at: Date
</code></pre>
<p>There are like 1,000,000+ books and only 10 have <code>is_archived=true</code>.</p>
<p>In queries you always filter <code>is_archived=false</code>, like:</p>
<pre><code>SELECT * 
FROM book 
WHERE creator_id = 1 
  AND is_archived = false 
LIMIT 10

SELECT * 
FROM book 
WHERE created_at &gt; NOW() 
  AND is_archived = false 
LIMIT 10
</code></pre>
<p>You have separate indexes on <code>creator_id</code> and <code>created_at</code>, does it make sense to delete them and create instead <code>creator_id+is_archived=false</code> and <code>created_at+is_archived=false</code>?</p>
<p>My understanding is that <code>creator_id</code> index is enough because it will be used by postgres first to let's say get 10,000 rows with <code>creator_id=1</code> and then just do sequential scan to pick first 10 with <code>is_archived=false</code></p>
",0,0,0,2025-10-22T14:16:22+00:00,1,88,True
79797165,2671149,,postgresql,Postgresql server starts in nix-shell but I can&#39;t use it (connection failed),"<p>I want to run postgresql within a nix-shell. I followed the instructions on this page: <a href=""https://mgdm.net/weblog/postgresql-in-a-nix-shell/"" rel=""nofollow noreferrer"">https://mgdm.net/weblog/postgresql-in-a-nix-shell/</a></p>
<p>PostgreSQL installs properly in the shell (it's not installed on the main system at all), and I can start the server:</p>
<pre><code>[nix-shell:~/Prog/Rust/ibis]$ pg_ctl -D .tmp/mydb -l logfile -o &quot;--unix_socket_directories='$PWD'&quot; start
waiting for server to start.... done
server started
</code></pre>
<p>But then if I try to create a database it gives me an error about connection on server socket. I tried two ways and they give the same error:</p>
<pre><code>[nix-shell:~/Prog/Rust/ibis]$ createdb mydb
createdb: error: connection to server on socket &quot;/run/postgresql/.s.PGSQL.5432&quot; failed: No such file or directory
        Is the server running locally and accepting connections on that socket?
</code></pre>
<p>and</p>
<pre><code>[nix-shell:~/Prog/Rust/ibis]$ psql -c &quot;CREATE USER ibis WITH PASSWORD 'censored' SUPERUSER;&quot; -U postgres
psql: error: connection to server on socket &quot;/run/postgresql/.s.PGSQL.5432&quot; failed: No such file or directory
        Is the server running locally and accepting connections on that socket?
</code></pre>
<p>I have created a /run/postgresql/ folder within the project folder. I used chmod to give full permissions to all.</p>
<p>What am I not understanding here? I can start and stop the server, and the folder exists within the project folders.</p>
",0,0,0,2025-10-22T19:14:20+00:00,1,74,True
79797221,10708345,Madrid,postgresql,npm drizzle-kit and AWS RDS: Error: getaddrinfo EAI_AGAIN,"<p>I don't seem to be able to push my configuration to RDS</p>
<pre class=""lang-json prettyprint-override""><code>// package.json
&quot;drizzle-kit&quot;: &quot;^0.31.5&quot;,
</code></pre>
<pre class=""lang-js prettyprint-override""><code>// drizzle.config.ts
import { defineConfig, Config } from 'drizzle-kit';
const dbCredentials = {
  ssl: true,
  host: db-dev.abc123def456.eu-west-1.rds.amazonaws.com, // RDS console's &quot;Connectivity &amp; security &gt; Endpoint&quot;
  port: 5432, // RDS console's &quot;Connectivity &amp; security &gt; Port&quot;
  user: postgres, // RDS console's &quot;Configuration &gt; Master username&quot;
  password: ********, // RDS console's &quot;Configuration &gt; Master password&quot;
  database: db-dev, // RDS console's &quot;DB identifier&quot;
};

export default defineConfig({
  out: './drizzle',
  dialect: 'postgresql',
  schema: './src/providers/db.schema.ts',
  casing: 'snake_case',
  dbCredentials,
} satisfies Config);
</code></pre>
<pre class=""lang-js prettyprint-override""><code>import { pgTable, varchar, timestamp } from 'drizzle-orm/pg-core';

export const passkeyChallenges = pgTable('passkey-challenges', {
  timestamp: timestamp('timestamp').notNull().defaultNow(),
  id: varchar().notNull().unique(),
  challenge: varchar().notNull(),
});

</code></pre>
<p>ERROR:</p>
<pre><code>[⣻] Pulling schema from database...
Error: getaddrinfo EAI_AGAIN db-dev.abc123def456.eu-west-1.rds.amazonaws.com
    at /giggle/giggle-app-gateway/node_modules/pg-pool/index.js:45:11
    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)
    at async Object.query (/giggle/giggle-app-gateway/node_modules/drizzle-kit/bin.cjs:80607:26)
    at async fromDatabase2 (/giggle/giggle-app-gateway/node_modules/drizzle-kit/bin.cjs:19259:25) {
  errno: -3001,
  code: 'EAI_AGAIN',
  syscall: 'getaddrinfo',
  hostname: 'db-dev.abc123def456.eu-west-1.rds.amazonaws.com'
}
</code></pre>
<p>STEPS TAKEN:
I believe this is related to the security group.
I have tried to add new Inbound Rules:</p>
<ul>
<li>type <code>PostgreSQL</code>:
<ul>
<li>all IPv4</li>
<li>all IPv6</li>
</ul>
</li>
<li>type <code>All Traffic</code>:
<ul>
<li>my CIDR</li>
</ul>
</li>
</ul>
<p>I keep getting the same error. Could it be related the AWS' recent meltdown? Everything seems to be in order.</p>
",0,0,0,2025-10-22T20:48:04+00:00,1,47,True
79797577,2713729,"Oulu, Finland",postgresql,Why does Postgres automatically create a count field for grouped selects?,"<p>By accident, I just learned that PostgreSQL seems to create a <code>count</code> field out of thin air for any <code>SELECT ... GROUP BY</code> queries.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE employees(age INT, name TEXT);
INSERT INTO employees VALUES(25, 'Name1');
INSERT INTO employees VALUES(25, 'Name2');
INSERT INTO employees VALUES(48, 'Name3');

SELECT e.age, e.count
FROM employees e
GROUP BY e.age
</code></pre>
<p>Returns:</p>
<pre><code>age  count
25   2
48   1
</code></pre>
<p>(as seen <a href=""https://dbfiddle.uk/Hr9qCPz7"" rel=""nofollow noreferrer"">in a fiddle</a> that shows it exists at least since 9.3)</p>
<p>I can't seem to find any documentation for this behaviour and it doesn't seem to happen for any other aggregate functions except for <code>count</code>. Is this a bug, a hidden feature or is there some actual documentation about it that I just can't find?</p>
<p>I ran into this because my table was supposed to have a real <code>count</code> column, so I selected <code>MIN(e.count)</code>. But my schema was outdated so the <code>count</code> column did not exist. Instead of &quot;column 'count' not found&quot; error I got a very mysterious &quot;aggregate functions cannot be nested&quot; error message. That's consistent with Postgres converting it under the hood into <code>MIN(COUNT(e.*))</code>.</p>
",4,4,0,2025-10-23T08:45:33+00:00,0,72,False
79798160,23593613,,postgresql,SQLAlchemy Dealing with null/none type in where statement,"<p>I have a query that I am attempting to convert from Oracle into sqlalchemy, but I'm running into issues with this time conversion part. There is a chance that the conversion value is Null and so needs to be treated like 0 if so.</p>
<pre><code>stmt = (
    select(QcSodTable)
    .join(
        LocalTimeTable,
        and_(
            QcSodTable.platformid == LocalTimeTable.platformid,
            QcSodTable.networktype == LocalTimeTable.networktype
        ),
        isouter=True,
    )
    .where(        
        ((QcSodTable.datetime + cast(literal(1.5) + ' DAYS', Interval)) - (LocalTimeTable.time_conv / 24) &lt; func.now(),
    )
</code></pre>
<p>This is the query as is right now. If the time_conv exists, this works as expected. If time conv is Null, it returns no results, if I take out the time_conv part, it returns as I expect. Is there a way to force time_conv to act as 0 when it is Null?</p>
",1,1,0,2025-10-23T20:01:35+00:00,1,60,True
79798449,5259182,,postgresql,Apache HOP PostgreSQL Bulk Loader Boolean Bug?,"<p>I am experimenting with Apache HOP to become the successor of the (dead end) SSIS ETL platform. I am very impressed with the quality and the performance of Apache HOP so far.</p>
<p>I am trying to load data from Table Input (jdbc, progress openedge) into postgresql via the PostgreSQL Bulk Loader. I think I found a bug in the process:</p>
<p>If I run a pipeline that uses a table input step and have HOP find some fields are Boolean types it will represent like this:</p>
<p><a href=""https://i.sstatic.net/WxdYmorw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WxdYmorw.png"" alt=""boolean field"" /></a></p>
<p>Based on DDL this is correct - this is a boolean field.</p>
<p><a href=""https://i.sstatic.net/530IVwzH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/530IVwzH.png"" alt=""enter image description here"" /></a></p>
<p>My problem now is that if I run this pipeline there seems to be a problem between the &quot;internal data model&quot; of HOP (whatever this may be) and the data handed over to the PostgreSQL Bulk Loader step:</p>
<pre><code>2025/10/24 08:10:01 - PostgreSQL Bulk Loader.0 - ERROR: Error in transform 
2025/10/24 08:10:01 - PostgreSQL Bulk Loader.0 - ERROR: org.postgresql.util.PSQLException: ERROR: invalid input syntax for type boolean: &quot;1.0&quot;   Where: COPY glimsds_order, line 1, column ord_addtobudget: &quot;1.0&quot;
</code></pre>
<p>Is this a bug someone can reproduce? I had the same problem with other data sources like salesforce (boolean).</p>
<p>I was able to mitigate this problem (accidentally) by using a formula and have it look for 1.0 and translate it to 1 but this actually never worked as intended. What fixed the problem was casting it to string (by accident) and this then lead to 1 instead of 1.0 what postgresql happily loaded.</p>
<p>Is there a way to further look into the inner data model or system of HOP? Is there a way to tell HOP this is a String (N) not a Boolean? Any other elegant solution?</p>
<p>Thanks! :)</p>
",0,0,0,2025-10-24T06:32:03+00:00,0,71,False
79798524,26430106,,postgresql,EXPLAIN CREATE TABLE throws a syntax error at integer type specification - but query runs successfully,"<pre><code>EXPLAIN CREATE TABLE TestTbl (
    EId integer PRIMARY KEY,
    FullName VARCHAR(50) NOT NULL
)
</code></pre>
<p>This query results in an error:</p>
<blockquote>
<p>ERROR:  syntax error at or near &quot;integer&quot;<br />
LINE 2:     EId integer PRIMARY KEY</p>
</blockquote>
<p>But when I execute this without <code>EXPLAIN</code>, the query is successful:</p>
<pre><code>CREATE TABLE TestTbl (
    EId integer PRIMARY KEY,
    FullName VARCHAR(50) NOT NULL
)
</code></pre>
<p>This query runs perfectly fine, and creates this table.</p>
<p>What can be the reason? I am new to PostgreSQL. Am I doing something wrong?</p>
",3,3,0,2025-10-24T07:48:49+00:00,2,119,True
79799934,165927,,postgresql,PSQLException: ERROR: buffer is pinned in InvalidateBuffer on CREATE DATABASE command,"<p>Working with Amazon RDS - PostgreSQL 16, my Java application started receiving the following error when trying to run the <code>CREATE DATABASE</code> command</p>
<pre><code>Caused by: org.postgresql.util.PSQLException: ERROR: buffer is pinned in InvalidateBuffer
  at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2734)
  at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)
  at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)
  at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:518)
  at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:435)
  at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:357)
  at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:342)
  at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:318)
  at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:313)
</code></pre>
<p>postgresql driver version 42.7.7</p>
",1,1,0,2025-10-26T09:07:12+00:00,1,77,True
79802168,12749540,,postgresql,Spring R2DBC save returns id null,"<p>I'm using R2DBC with <code>R2dbcRepository</code> in Spring Boot.</p>
<p>I have a table called <code>Dispatches</code> with Jakarta annotations but when I invoke the save() method on my repository the save method returns the same entity field values before calling save(). I've tried with different approaches but the output is the same.</p>
<p>I'm using a Postgres database.</p>
<p>Here's my entity:</p>
<pre><code>@Table(name=&quot;dispatches&quot;)
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Data
public class Dispatches {
    
    @Id
    @Column(nullable = false, updatable = false)
    @SequenceGenerator(
            name = &quot;dispatches_seq1&quot;,
            sequenceName = &quot;dispatches_seq1&quot;,
            allocationSize = 1,
            initialValue = 10000
    )
    @GeneratedValue(
            strategy = GenerationType.AUTO
    )
    private Integer id;

    @Column(length = 100)
    private String origin;

    @Column(length = 100)
    private String target;

    @Column(precision = 20, scale = 2)
    private BigDecimal paymentValue;

    @Column(nullable = false)
    private Integer paymentMethod;

    @Column(nullable = false)
    
    private LocalDate startDate;

    @Column
    
    private LocalDate endDate;

    @Column(precision = 3, scale = 2)
    private BigDecimal kmDone;
    
    @Column(name=&quot;customer_id&quot;)
    private Integer customerId;
    
    @Column(name=&quot;drone_id&quot;)
    private Integer droneId;
    
    @Column(name=&quot;creation_date&quot;)
    private LocalDate creationDate;
/*
   @ManyToOne(fetch = FetchType.LAZY)
   @JoinColumn(name = &quot;customer_id&quot;, nullable = false)
    private Customer customer;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = &quot;drone_id&quot;, nullable = false)
    private Drone drone;    
   
    @Transient
    private List&lt;DispatchCart&gt; dispatchDispatchCarts = new ArrayList&lt;&gt;();

    @Transient
    private List&lt;DispatchComments&gt; dispatchDispatchComments = new ArrayList&lt;&gt;();
    */

}

</code></pre>
<p>and here's my repository interface. I removed the <code>@Entity</code> annotation but it didn't work either:</p>
<pre><code>public interface DispatchRepository extends R2dbcRepository&lt;Dispatches, Integer&gt; {
    
    Flux&lt;Dispatches&gt; findByCustomerId(Integer customerId);
    
    public Mono&lt;Dispatches&gt; findByOriginAndTargetAndStartDate(String origin, String target, LocalDate startDate);
    
    public Mono&lt;Dispatches&gt; findByOriginAndTarget(String origin, String target);

}

</code></pre>
<p>The method where I'm mapping the output of save() is this:</p>
<pre><code>return this.repository.save(dispatches).map(d-&gt;{
            log.info(&quot;Saved dispatches &quot;+dispatches);
            log.info(&quot;Saved dispatches &quot;+d.getId());
            log.info(&quot;Saved dispatches 2 &quot;+dispatches.getId());
            DispatchDto dispatchDto = DispatchMapper.INSTANCE.toDto(d);
            return ResponseWrapper.&lt;DispatchDto&gt;builder()
                    .data(dispatchDto)
                    .message(&quot;OK&quot;)
                    .build();
        });
</code></pre>
<p>But the output of logs are</p>
<p><strong>Saved dispatches null</strong></p>
<p><strong>Saved dispatches 2 null</strong></p>
<p>Does someone know if some Maven database module or annotation conflicts with R2DBC libraries?</p>
",1,1,0,2025-10-27T19:11:04+00:00,1,69,False
79802208,31766313,,postgresql,SELECT jsonb data based on specific children values/properties,"<p>Assume I have many rows with jsonb data like this in their respective columns:</p>
<pre><code>{
  ...,
  &quot;participants&quot;: {
    &quot;one_uuid&quot;: {
      &quot;id&quot;: &quot;another_uuid&quot;,
      &quot;tags&quot;: []
    },
    &quot;two_uuid&quot;: {
      &quot;id&quot;: &quot;another_two_uuid&quot;,
      &quot;tags&quot;: []
    },
    &quot;three_uuid&quot;: {
      &quot;id&quot;: &quot;another_three_uuid&quot;, // (this will be different for each row, important thing is tag = booked)
      &quot;tags&quot;: [
        &quot;booked&quot;
      ]
    }
  },
  ...
}
</code></pre>
<p>How can I (if possible) do a <code>SELECT</code> and end up with <code>another_three_uuid</code> as that column value (and other values for the other rows) because it has a tag = booked? I already have the condition for tag = booked working on the <code>WHERE</code>. Do I need a CASE here?</p>
<p>So that I end up with result like:</p>
<pre><code>selected_column     selected_column_2
-------------------------------------
some_value         another_three_uuid
other_value        (another uuid from &quot;id&quot; with tag = booked)
...
</code></pre>
<p>My last resort is doing a <code>SELECT</code> on the whole <code>participants</code> value, filtering out the objects that don't have <code>booked</code> tags, and grabbing the necessary <code>id</code> in the JavaScript code that fetches this data.</p>
",1,1,0,2025-10-27T20:14:40+00:00,3,113,True
79802852,3373511,,postgresql,Sudden CPU spike on AWS RDS (PostgreSQL) after running fine for the entire last month,"<p>I encountered an issue where our AWS RDS (PostgreSQL) instance suddenly experienced a high CPU spike, even though it had been running normally for the entire previous month.</p>
<p>Currently, the system is back to normal, but we are still unsure of the root cause. Here are the details:</p>
<p><strong>Problem</strong></p>
<p>On the day of the incident, the database experienced unusually high CPU usage. The main cause was a query counting transactions (COUNT transaction) that suddenly took much longer to execute than usual. From our investigation, we found that the system had been operating normally in the month prior, and the team did not deploy any new code or make changes right before the issue occurred.</p>
<p>The problematic query was:</p>
<pre><code>SELECT COUNT(*) AS &quot;cnt&quot; 
FROM &quot;tickets&quot; AS t 
WHERE t.id = $1 
  AND t.category = $2 
  AND t.round_id = $3;
</code></pre>
<p><strong>Resolution / What we did to restore normal operation</strong></p>
<p>1.Added a multi-column index to the query to speed up transaction lookups.</p>
<p>2.Deleted old unused transaction data (about 17 million rows, approximately 5 GB).</p>
<p><strong>Questions / Looking for advice</strong></p>
<p>Has anyone experienced a sudden CPU spike like this on RDS even though the system was running normally for weeks?</p>
<p>Could there be underlying reasons why a query that previously ran fine suddenly became slow?</p>
<p>Are there any proactive monitoring or indexing strategies to prevent this in the future?</p>
<p><img src=""https://i.sstatic.net/TM1EUGXJ.png"" alt=""image1"" /></p>
<p><img src=""https://i.sstatic.net/988lqXKN.png"" alt=""image2"" /></p>
<p><img src=""https://i.sstatic.net/WiaWehXw.png"" alt=""image3"" /></p>
<p><img src=""https://i.sstatic.net/pU6dIofg.png"" alt=""image4"" /></p>
<p><img src=""https://i.sstatic.net/4JkWphLj.png"" alt=""image5"" /></p>
",0,0,0,2025-10-28T13:13:02+00:00,0,131,False
79802904,4730536,,postgresql,How to create an index on a JSONB field used with ILIKE in PostgreSQL 17?,"<p>Do you know how I could create an index that optimizes the following query, which is running on PostgreSQL 17?</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
    sboms.application, 
    sboms.application_version AS applicationVersion, 
    component -&gt;&gt; 'name' AS dependency, 
    component -&gt;&gt; 'version' AS dependencyVersion, 
    sboms.last_modified_date AS lastUpdated
FROM sboms, jsonb_array_elements(sbom -&gt; 'components') AS component
WHERE component -&gt;&gt; 'name' ILIKE CONCAT('%', :searchTerm, '%')
  AND sboms.application ILIKE CONCAT('%', :application, '%');
</code></pre>
<p>I already have an index on <code>sboms.application</code> (which is a regular column), but I can’t find an efficient way to create an index for the <code>JSONB</code> column (<code>sbom</code>), specifically for this condition:</p>
<pre class=""lang-sql prettyprint-override""><code>WHERE component -&gt;&gt; 'name' ILIKE CONCAT('%', :searchTerm, '%')
</code></pre>
<p>The JSON structure looks like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;components&quot;: [
    {
      &quot;name&quot;: ...
    }
  ]
}
</code></pre>
<p>Do you have any idea how I could optimize this query so that it can take advantage of an index?</p>
",1,1,0,2025-10-28T14:09:11+00:00,1,50,True
79803102,3049628,"Watford, United Kingdom",postgresql,Postgres not using indexes when checking for nulls,"<p>I have two tables. They each have around 2.3 million rows in them.</p>
<p>They are running on PostgreSQL 17.4</p>
<pre><code>CREATE TABLE keyeddata (
    category text NULL,
    key1 text NULL,
    key2 text NULL,
    key3 text NULL,
    key4 text NULL,
    &quot;parameter&quot; text NULL,
    value text NULL,
    meta1 text NULL,
    meta2 text NULL,
    &quot;version&quot; int4 NULL,
    updatedatetime timestamp NULL
);

CREATE UNIQUE INDEX keyeddataidx ON keyeddata USING btree (category, key1, key2, key3, key4, parameter) NULLS NOT DISTINCT;

CREATE TABLE keyeddataaudit (
    category text NULL,
    key1 text NULL,
    key2 text NULL,
    key3 text NULL,
    key4 text NULL,
    &quot;parameter&quot; text NULL,
    value text NULL,
    meta1 text NULL,
    meta2 text NULL,
    &quot;version&quot; int4 NULL,
    updatedatetime timestamp NULL
);
CREATE UNIQUE INDEX keyeddataauditidx ON keyeddataaudit USING btree (category, key1, key2, key3, key4, parameter, version) NULLS NOT DISTINCT;
</code></pre>
<p>I want to delete values from the audit table if the following 2 requirements are satisfied:</p>
<ol>
<li>update datetime is past a certain date</li>
<li>there is a row with identical keys in keyeddata (or keyeddataaudit, I don't mind which) with a higher version</li>
</ol>
<p>The idea is to delete old values, but only if there is a more recent one.</p>
<p>I can get the same performance issues with either a select or a delete, so these examples are using a select.</p>
<p>If I run this query:</p>
<pre><code>select count(*) from keyeddataaudit a, keyeddata t
  WHERE a.updatedatetime &lt; '2025-10-01' AND
  (a.category = t.category) AND
  (a.key1 = t.key1 ) AND
  (a.key2 = t.key2 ) AND
  (a.key3 = t.key3 ) AND
  (a.key4 = t.key4 ) AND
  (a.parameter = t.parameter ) AND
  a.version &lt; t.version;
</code></pre>
<p>Then it hits the index and completes in under a second.</p>
<p>However, that doesn't handle nulls.</p>
<p>If I change it to:</p>
<pre><code>select count(*) from keyeddataaudit a, keyeddata t
  WHERE a.updatedatetime &lt; '2025-10-01' AND
  (a.category is not distinct from t.category) AND
  (a.key1 is not distinct from t.key1 ) AND
  (a.key2 is not distinct from t.key2 ) AND
  (a.key3 is not distinct from t.key3 ) AND
  (a.key4 is not distinct from t.key4 ) AND
  (a.parameter is not distinct from t.parameter ) AND
  a.version &lt; t.version;
</code></pre>
<p>Or if I try</p>
<pre><code>select count(*) from keyeddataaudit a, keyeddata t
  WHERE a.updatedatetime &lt; '2025-10-01' AND
  (a.category = t.category OR (a.category is null and t.category is null)) AND
  (a.key1 = t.key1 OR (a.key1 is null and t.key1 is null) ) AND
  (a.key2 = t.key2 OR (a.key2 is null and t.key2 is null) ) AND
  (a.key3 = t.key3 OR (a.key3 is null and t.key3 is null) ) AND
  (a.key4 = t.key4 OR (a.key4 is null and t.key4 is null) ) AND
  (a.parameter = t.parameter OR (a.parameter is null and t.parameter is null) ) AND
  a.version &lt; t.version;
</code></pre>
<p>Then in both of these the query has run for over 5 minutes without completing before I cancelled it.</p>
<p>How do I change either the indexes or the queries so it actually uses them, please? Or alternatively is there some other way to achieve this with reasonable performance?</p>
<p>Edit:
This completes in 48 seconds:</p>
<pre><code> select count(*) from spreads.keyeddataaudit a, spreads.keyeddata t
  WHERE a.updatedatetime &lt; '2025-10-01' AND
  (coalesce(a.category, 'null') = coalesce(t.category, 'null')) AND
  (coalesce(a.key1, 'null') = coalesce(t.key1, 'null')) AND
  (coalesce(a.key2, 'null') = coalesce(t.key2, 'null')) AND
  (coalesce(a.key3, 'null') = coalesce(t.key3, 'null')) AND
  (coalesce(a.key4, 'null') = coalesce(t.key4, 'null')) AND
  (coalesce(a.parameter, 'null') = coalesce(t.parameter, 'null')) AND
  a.version &lt; t.version;
</code></pre>
<p>But would get confused if data ever contained 'null'</p>
<p>Which is solved by this, which completes in 1m29s.</p>
<pre><code> select * from spreads.keyeddataaudit a, spreads.keyeddata t
  WHERE a.updatedatetime &lt; '2025-10-01' AND
  (coalesce(a.category, 'null') = coalesce(t.category, 'null') and coalesce(a.category, 'null2') = coalesce(t.category, 'null2')) AND
  (coalesce(a.key1, 'null') = coalesce(t.key1, 'null') and coalesce(a.key1, 'null2') = coalesce(t.key1, 'null2')) AND
  (coalesce(a.key2, 'null') = coalesce(t.key2, 'null') and coalesce(a.key2, 'null2') = coalesce(t.key2, 'null2')) AND
  (coalesce(a.key3, 'null') = coalesce(t.key3, 'null') and coalesce(a.key3, 'null2') = coalesce(t.key3, 'null2')) AND
  (coalesce(a.key4, 'null') = coalesce(t.key4, 'null') and coalesce(a.key4, 'null2') = coalesce(t.key4, 'null2')) AND
  (coalesce(a.parameter, 'null') = coalesce(t.parameter, 'null') and coalesce(a.parameter, 'null2') = coalesce(t.parameter, 'null2')) AND
  a.version &lt; t.version;
</code></pre>
<p>But it really feels like there should be a better way!</p>
",2,2,0,2025-10-28T17:22:54+00:00,1,102,True
79803126,23593613,,postgresql,SqlAlchemy Update statement binding entire row as values,"<p>I'm not sure if this is possible based off of the docs, but I have a text based query that updates all row values every time it is run. Originally this was done via Oracle and its cursor, passing in the row and binding it to the params. We are converting over to postgres and sqlalchemy, so in trying to convert some of these things, I'm running into an issue where I seem to have to bind every single parameter every time I call the query, otherwise it will error out depending on what I tried.</p>
<p>The query:</p>
<pre><code>update_qc_sod = text(f&quot;&quot;&quot;
update {QcSodTable.__table__.fullname}
stn_ops = :stn_ops,
precip_trace = :precip_trace,
precip = :precip,
precip_period = :precip_period,
temp_max = :temp_max,
temp_min = :temp_min
where platformid = :platformid and
trim(networktype) = :networktype and
datetime = :datetime
&quot;&quot;&quot;)
</code></pre>
<p>I have tried to execute in these ways:
(only update the values that were changed this loop)</p>
<pre><code>connection.execute(queries.update_qc_sod.bindparams(qc_flag=qc_sod_new_row.QcSodTable.qc_flag, remarks=qc_sod_new_row.QcSodTable.remarks))
</code></pre>
<p>(Try to update all rows)</p>
<pre><code>connection.execute(queries.update_qc_sod.bindparams(qc_sod_new_row))
</code></pre>
<p>(Try to pass in as parameters)</p>
<pre><code>connection.execute(queries.update_qc_sod, qc_sod_new_row)
</code></pre>
<p>I have quite a few places this exists and it would be far easier to use it as it was instead of making a query for each one of these with the correct params to bind only. I could manually add each parameter to bind but that would make it incredibly ugly and large. If that is the only way, then I will attempt that but if there is a better, cleaner way to pass in an entire row as an update parameter, that would be fantastic</p>
<p>Edit:</p>
<p>So to clarify what I'm attempting to do: I have a select statement that uses a union to grab rows from the QcSodTable. After that, the result is run through a series of quality controls to make sure all the data is valid. If the data needed to be altered, then the data would be inserted right after so that it would be ready for other checks down the line that may need to use that data.</p>
<pre><code>qc_sod_row = connection.execute(queries.get_qc_sod_data).all()[0][0]
</code></pre>
<p>Here is a quick example of one of the qc checks:</p>
<pre><code>def check_datetimes(connection, qc_sod_row, algorithm, is_good):
    &quot;&quot;&quot;
    if local_time or date does not exist, then add remarks and
    add alg flag of C00/01
    &quot;&quot;&quot;
    qc_sod_new_row = copy.deepcopy(qc_sod_row)

    if not is_good:
        qc_sod_new_row.remarks = utils.update_remarks(qc_sod_row.remarks, algorithm)
        qc_sod_new_row.qc_flag = &quot;F&quot;
        # Update the QC_SOD table with new values
        diction = utils.bind_qc_sod(queries.update_qc_sod, qc_sod_new_row)
        connection.execute(queries.update_qc_sod, diction)

    return qc_sod_new_row
</code></pre>
<p>The error I am running into is during the connection.execute where I am trying to pass in the params that it errors with a multitude of issues depending on how I try to bind the params.</p>
<p>The diction var above is a dictionary of each rowname and the parameter I want to bind</p>
<pre><code>    diction = dict(
        stn_ops= qc_sod.stn_ops,
        precip_trace= qc_sod.precip_trace,
        precip= qc_sod.precip,
        precip_period= qc_sod.precip_period,
        temp_max= qc_sod.temp_max,
        temp_min= qc_sod.temp_min,
        wind_gust_dir= qc_sod.wind_gust_dir
    )
</code></pre>
<p>So in the above example, remarks would concat a new remark and update the qc_flag. All the other data would remain the same, so I want to update only those fields, but the query itself is used in other areas to update other columns as well (precip, temp_max, etc).</p>
<p>This is how it was done previously via the OracleCX library:</p>
<pre><code>def chk_localtime(qc_sod_row, time_conv, sod_connection):
    ALG_FLAG = &quot;C00&quot;
    qc_sod_row_new = qc_sod_row.copy()
    
    if not local_time_check(time_conv):
        cursor_sod = sod_connection.cursor()

        qc_sod_row_new[&quot;REMARKS&quot;] = utils.update_remarks(qc_sod_row, ALG_FLAG)
        qc_sod_row_new[&quot;QC_FLAG&quot;] = &quot;F&quot;
        
        #Update the QC_SOD table with new values
        cursor_sod.execute(qc_sod_queries.update_qc_sod, qc_sod_row_new)
        
        cursor_sod.close()
        
    return qc_sod_row_new
</code></pre>
<p>Essentially, I have a row that will be updated in some columns and not others, I want to update the columns that have been changed. Since the column that can change is always different, it would be great to just pass in the new, updated row as all the parameters and just update what was changed if that makes sense.</p>
<p>Another thing I have tried is to use SqlAlchemy to create the statement:</p>
<pre><code>update_qc_sod = (
    update(QcSodTable)
    .where(
        QcSodTable.platformid == bindparam(&quot;platformid&quot;),
        func.trim(QcSodTable.networktype) == bindparam(&quot;networktype&quot;),
        QcSodTable.datetime == bindparam(&quot;datetime&quot;)
    )
    .values(
        stn_ops=bindparam(&quot;stn_ops&quot;),
        precip_trace=bindparam(&quot;precip_trace&quot;),
        precip=bindparam(&quot;precip&quot;),
        precip_period=bindparam(&quot;precip_period&quot;),
        temp_max=bindparam(&quot;temp_max&quot;),
        temp_min=bindparam(&quot;temp_min&quot;),
        wind_gust_dir=bindparam(&quot;wind_gust_dir&quot;),
    )
)
</code></pre>
<p>Hopefully that adds some clarity, if not I can add more info/examples</p>
",2,2,0,2025-10-28T17:59:41+00:00,1,85,True
79803688,30676690,,postgresql,"LATERAL conversion issues when Calcite performs SQL rewrite, NullPointerException","<p>I'm trying to use Calcite+PostgreSQL to rewrite SQL queries, the following error occurs during runtime. How should I solve it?
The main warning is like this：</p>
<blockquote>
<p>[warn] call_rewriter failed at idx=1: Java rewriter failed (code=1). stderr=Exception in thread &quot;main&quot; java.lang.NullPointerException: rightResult.neededAlias is null</p>
</blockquote>
<p>The whole error of one SQL during rewriting as follows：</p>
<pre><code>[warn] call_rewriter failed
at idx=1: Java rewriter failed (code=1). stderr=Exception in thread &quot;main&quot; java.lang.NullPointerException: 
rightResult.neededAlias is null, 
node is SELECT * FROM (`db`.`CUSTOMER`AS`$cor5`, LATERAL (SELECT * FROM `db`.`CUSTOMER_ADDRESS`WHERE`$cor5`.`c_current_addr_sk`=`ca_address_sk`) AS `t15`) AS `$cor3`, LATERAL (SELECT * FROM (SELECT * FROM `db`.`CUSTOMER_DEMOGRAPHICS`WHERE`cd_marital_status` = 'U' AND CAST(`cd_education_status`AS CHAR(15)) = 'Advanced Degree') AS`t16`WHERE`$cor3`.`c_current_cdemo_sk`=`cd_demo_sk`) AS `t17\`\`
</code></pre>
<p>after this warning，there are some lines I suppose have something to do with calcite, I intercepted some of the content as follows：</p>
<pre><code>    at org.apache.calcite.util.ReflectUtil$2.invoke(ReflectUtil.java:533)   at org.apache.calcite.rel.rel2sql.RelToSqlConverter.dispatch(RelToSqlConverter.java:155)
    at org.apache.calcite.rel.rel2sql.RelToSqlConverter.visitInput(RelToSqlConverter.java:163)
    at org.apache.calcite.rel.rel2sql.SqlImplementor.visitInput(SqlImplementor.java:233)
    at org.apache.calcite.rel.rel2sql.SqlImplementor.visitInput(SqlImplementor.java:221)
    at org.apache.calcite.rel.rel2sql.SqlImplementor.visitRoot(SqlImplementor.java:197)
    at com.sure.calcite.RewriterCli.main(RewriterCli.java:101)         Suppressed: java.lang.Throwable: Error while converting RelNode to SqlNode: LogicalAggregate(group=[{}], EXPR$0=[MIN($0)], EXPR$1=[MIN($22)], EXPR$2=[MIN($53)], EXPR$3=[MIN($46)])   LogicalFilter(condition=[=($127, $153)])
</code></pre>
<p>The codes snippet of the error part are as follows，：</p>
<pre class=""lang-py prettyprint-override""><code># PG connection
    runner = PgRunner(
        host=args.pg_host, port=args.pg_port, user=args.pg_user,
        password=args.pg_password, db=args.dataset
    )

    logger = RewriteLogger(out_file)
    df = pd.read_csv(csv_path)
    n_total = len(df)
    if args.limit and args.limit &gt; 0:
        n_total = min(n_total, args.limit)
    print(f&quot;[info] loaded {n_total} queries from {csv_path}&quot;)

    successes = 0
    for idx in range(n_total):
        row = df.iloc[idx]
        original_sql = str(row.get(&quot;original_sql&quot;) or row.get(&quot;sql&quot;) or row.get(&quot;query&quot;) or &quot;&quot;).strip()
        if not original_sql:
            continue

        # === Calling the Java Rewriter (Note: Returns a tuple, not a dictionary) ===
        #⚠️error part⚠️
        try:
            rewritten_sql, rules_applied = call_rewriter(
                args.dataset, original_sql, jar_path=args.jar, schema_path=schema_path
            )
        except Exception as e:
            ⚠️print(f&quot;[warn] call_rewriter failed at idx={idx}: {e}&quot;)
            continue

        if not rewritten_sql or not rewritten_sql.strip():
            continue

        # === Actual Execution Time (EXPLAIN ANALYZE) ===
     
        try:
            t_before, plan_before = runner.explain_analyze_time(original_sql)
        except Exception as e:
            ⚠️print(f&quot;[warn] explain_analyze original failed idx={idx}: {e}&quot;)
            continue

        try:
            t_after, plan_after = runner.explain_analyze_time(rewritten_sql)
        except Exception as e:
            ⚠️print(f&quot;[warn] explain_analyze rewritten failed idx={idx}: {e}&quot;)
            continue
</code></pre>
<p>Defination of call_rewriter：</p>
<pre class=""lang-py prettyprint-override""><code>def call_rewriter(db_id: str,
                  sql: str,
                  jar_path: Optional[str] = None,
                  schema_path: Optional[str] = None) -&gt; Tuple[str, List[str]]:
    &quot;&quot;&quot;
    调用 Java 重写器，返回 (rewritten_sql, rules_applied)。
    Java CLI 约定输出 JSON：
      {&quot;rewritten_sql&quot;:&quot;...&quot;, &quot;rules_applied&quot;:[...], &quot;plan_before&quot;:&quot;...&quot;, &quot;plan_after&quot;:&quot;...&quot;}
    &quot;&quot;&quot;
    jar = _auto_find_jar(jar_path)
    if not schema_path or not os.path.exists(schema_path):
        raise FileNotFoundError(f&quot;schema_path 不存在：{schema_path}&quot;)

    cmd = [&quot;java&quot;, &quot;-jar&quot;, jar, &quot;--schema&quot;, schema_path, &quot;--sql&quot;, sql]
    proc = subprocess.run(cmd, text=True, capture_output=True)

    # 优先从 stdout 抓 JSON；不行就从 stderr 抓（过滤掉日志噪声）
    obj = _extract_json(proc.stdout) or _extract_json(proc.stderr)
    if obj is None:
        # 仍没有 JSON，抛错（上层会跳过这条）
        raise RuntimeError(
            f&quot;Java rewriter failed (code={proc.returncode}). &quot;
            f&quot;stderr={proc.stderr.strip()}&quot;
        )

    rewritten = obj.get(&quot;rewritten_sql&quot;) or &quot;&quot;
    rules = obj.get(&quot;rules_applied&quot;) or []
    if not isinstance(rewritten, str):
        rewritten = str(rewritten)
    if not isinstance(rules, list):
        rules = []
    return rewritten, rules
</code></pre>
<p>Class PgRunner :</p>
<pre class=""lang-py prettyprint-override""><code>class PgRunner:
    def __init__(self, host=&quot;&quot;, port=, user=&quot;&quot;, password=&quot;&quot;, db=&quot;dsb&quot;):
        self.conn = psycopg2.connect(host=host, port=port, user=user, password=password, dbname=db)
        self.conn.autocommit = True

    def close(self):
        try: self.conn.close()
        except Exception: pass

    def explain_analyze(self, sql: str) -&gt; Dict[str, Any]:
        with self.conn.cursor() as cur:
            cur.execute(&quot;EXPLAIN (ANALYZE, BUFFERS) &quot; + sql)
            rows = cur.fetchall()
        plan_text = &quot;\n&quot;.join([r[0] for r in rows])
        m = _EXEC_TIME_RE.search(plan_text)
        exec_ms = float(m.group(1)) if m else float(&quot;nan&quot;)
        return {&quot;execution_time_ms&quot;: exec_ms, &quot;plan_text&quot;: plan_text}

    def checksum_of_results(self, sql: str, max_rows: int = 100000) -&gt; Tuple[str, int]:
        text = f&quot;SELECT * FROM ({sql}) t&quot;
        md5 = hashlib.md5(); n = 0
        with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.itersize = 1000
            cur.execute(text)
            for row in cur:
                n += 1
                if n &gt; max_rows: break
                md5.update(json.dumps(row, sort_keys=True, default=str).encode(&quot;utf-8&quot;))
        return md5.hexdigest(), n
</code></pre>
<p>The code in the Java file is as follows：</p>
<pre class=""lang-java prettyprint-override""><code>// Configure Calcite settings
SchemaPlus root = Frameworks.createRootSchema(true);
SchemaPlus schemaPlus = root.add(spec.schema, new SimpleSchema(spec));

SqlParser.Config parserCfg = SqlParser.config()
        .withCaseSensitive(false)
        .withUnquotedCasing(Casing.TO_LOWER)
        .withQuotedCasing(Casing.UNCHANGED)
        .withQuoting(Quoting.DOUBLE_QUOTE)
        .withConformance(SqlConformanceEnum.BABEL);
        //.withLex(Lex.POSTGRESQL);  // The runtime error indicated that POSTGRESQL was not present, so I commented out this line.

Properties props = new Properties();
props.setProperty(&quot;caseSensitive&quot;, &quot;false&quot;);

FrameworkConfig cfg = Frameworks.newConfigBuilder()
        .defaultSchema(schemaPlus)
        .parserConfig(parserCfg)
        .context(Contexts.of(new CalciteConnectionConfigImpl(props)))
        .build();

//Rule set + Dialect
HepProgramBuilder hp = new HepProgramBuilder();
RuleSets.addDefaultRules(hp);  // Current set of rules: The rule set is one I defined myself, which I have included in the code snippet below.
SqlDialect dialect = pickDialect(spec.dialect);

// RelNode before and after rule rewriting
RelNode before = planner.rel(validated).project();
String planBefore = RelOptUtil.toString(before);

HepPlanner hep = new HepPlanner(hp.build());
RuleCaptureListener listener = new RuleCaptureListener();// Listener for recording hit rules
hep.addListener(listener);
hep.setRoot(before);
RelNode after = hep.findBestExp();            
String planAfter = RelOptUtil.toString(after);

// ★ Location where the NPE was triggered（RelNode→SQL）
RelToSqlConverter conv = new RelToSqlConverter(PostgresqlSqlDialect.DEFAULT);
String sqlAfter = conv
    .visitRoot(after)                        
    .asStatement()
    .toSqlString(PostgresqlSqlDialect.DEFAULT)
    .getSql();
</code></pre>
<p>Partial interception of the code defined by the rule set：</p>
<pre class=""lang-java prettyprint-override""><code>    public static void addDefaultRules(HepProgramBuilder hp) {
        hp.addMatchOrder(org.apache.calcite.plan.hep.HepMatchOrder.TOP_DOWN);

        // Filter相关
        hp.addRuleInstance(CoreRules.FILTER_MERGE);
        hp.addRuleInstance(CoreRules.FILTER_MULTI_JOIN_MERGE);
        hp.addRuleInstance(CoreRules.FILTER_REDUCE_EXPRESSIONS);
        hp.addRuleInstance(CoreRules.FILTER_INTO_JOIN);
        ...
</code></pre>
<p>It should be noted that what I want to achieve in my complete code is to read SQL, rewrite it using calcite, obtain the rule name used in the rewrite, and then use postgreSQL to actually execute the SQL before and after the rewrite to obtain the actual execution time, and record the SQL before and after the rewrite, execution time, rule name, logical plan and other information.
The SQLI use is in a csv file, which is from dsb.</p>
<pre class=""lang-sql prettyprint-override""><code>select min(item1.i_item_sk), min(item2.i_item_sk), min(s1.ss_ticket_number), min(s1.ss_item_sk) 
FROM item AS item1, item AS item2, store_sales AS s1, store_sales AS s2, date_dim, customer, customer_address, customer_demographics 
WHERE item1.i_item_sk &lt; item2.i_item_sk 
  AND s1.ss_ticket_number = s2.ss_ticket_number 
  AND s1.ss_item_sk = item1.i_item_sk and s2.ss_item_sk = item2.i_item_sk 
  AND s1.ss_customer_sk = c_customer_sk 
  and c_current_addr_sk = ca_address_sk 
  and c_current_cdemo_sk = cd_demo_sk 
  AND d_year between 2000 and 2000 + 1 
  and d_date_sk = s1.ss_sold_date_sk 
  and item1.i_category in ('Books', 'Children') 
  and item2.i_manager_id between 6 and 25 
  and cd_marital_status = 'M' 
  and cd_education_status = 'Advanced Degree' 
  and s1.ss_list_price between 87 and 101 
  and s2.ss_list_price between 87 and 101  ;
</code></pre>
",0,2,2,2025-10-29T10:49:34+00:00,0,70,False
79803876,22442419,,postgresql,Java socket connection to GCP Cloud PostgreSQL - Service account authentication,"<p>We are trying to connect to the GCP PostgreSQL DB from our Java application using socket connections and service account for authentication.</p>
<pre class=""lang-java prettyprint-override""><code>String dbUser = &quot;user@abc.iam.gserviceaccount.com&quot;;
String dbPass = &quot;&quot;;
String dbName = &quot;test&quot;;
String instanceConnectionName = &quot;my-project:europe-west1:my-instance&quot;;
String serviceAccountPath = &quot;/path/ords-labs-esb-sql-user-sa-key.json&quot;;

String jdbcUrl =
    String.format(&quot;jdbc:postgresql://google/&quot;
                  + &quot;%s?cloudSqlInstance=%s&amp;socketFactory=com.google.cloud.sql.&quot;
                  + &quot;postgres.SocketFactory&amp;useSSL=false&quot;,
        dbName, instanceConnectionName);

// Set the environment variable for authentication
System.setProperty(&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;, serviceAccountPath);

try {
    Class.forName(&quot;org.postgresql.Driver&quot;);

    Connection conn = DriverManager.getConnection(jdbcUrl, dbUser, dbPass);

    debugLog(&quot;Connected to PostgreSQL on GCP successfully!&quot;);
    // Perform DB operations here
} catch (Exception e) {
    debugLog(e.toString());
}
</code></pre>
<p>We are getting this error:</p>
<blockquote>
<p>org.postgresql.util.PSQLException: Something unusual has occurred to cause the driver to fail. Please report this exception.</p>
</blockquote>
<p>We are using the below jars:</p>
<p><a href=""https://i.sstatic.net/53NmzSJH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/53NmzSJH.jpg"" alt=""enter image description here"" /></a></p>
<p>We are not sure what is causing this error. The same call works if I use basic authentication. We had set up a temporary username/password user account in GCP PostGreSQL which works via this code.</p>
",0,0,0,2025-10-29T14:02:06+00:00,0,59,False
79804089,22271956,,postgresql,PgCat sometimes routes write queries to replicas,"<p>I’m testing a <strong>TimescaleDB cluster</strong> (1 primary + 2 replicas) behind <strong>PgCat</strong>, running in Docker Swarm.
Sometimes write queries (<code>CREATE TABLE</code>, <code>INSERT</code>) fail with:</p>
<blockquote>
<p>psycopg.errors.ReadOnlySqlTransaction: cannot execute INSERT in a read-only transaction</p>
</blockquote>
<p>Other times the exact same script runs correctly and writes to the primary.
This happens inconsistently across runs.</p>
<hr />
<h3>Example logs</h3>
<pre class=""lang-bash prettyprint-override""><code># python 
p.py Traceback (most recent call last):
  File &quot;//p.py&quot;, line 27, in &lt;module&gt;
    cur.execute(&quot;&quot;&quot;
psycopg.errors.ReadOnlySqlTransaction: cannot execute INSERT in a read-only transaction

# python p.py
Inserted rows:
(1, 'alpha', 10)
(2, 'beta', 20)
(3, 'gamma', 30)

Queried rows:
id=1, name=alpha, value=10
id=2, name=beta, value=20
id=3, name=gamma, value=30

# python p.py
Traceback (most recent call last):
  File &quot;//p.py&quot;, line 27, in &lt;module&gt;
    cur.execute(&quot;&quot;&quot;
psycopg.errors.ReadOnlySqlTransaction: cannot execute INSERT in a read-only transaction

# python p.py
Inserted rows:
(4, 'alpha', 10)
(5, 'beta', 20)
(6, 'gamma', 30)

Queried rows:
id=1, name=alpha, value=10
id=2, name=beta, value=20
id=3, name=gamma, value=30
id=4, name=alpha, value=10
id=5, name=beta, value=20
id=6, name=gamma, value=30

# python p.py
psycopg.errors.ReadOnlySqlTransaction: cannot execute CREATE TABLE in a read-only transaction
</code></pre>
<p>So PgCat occasionally routes write queries to replicas, even though the configuration should prevent that.</p>
<hr />
<h3>PgCat config (<code>pgcat.toml</code>)</h3>
<pre class=""lang-ini prettyprint-override""><code>[general]
host = &quot;0.0.0.0&quot;
port = 6432
admin_username = &quot;---TIMESCALEDB_PGCAT_USER---&quot;
admin_password = &quot;---TIMESCALEDB_PGCAT_PASSWORD---&quot;
pool_mode = &quot;transaction&quot;
server_lifetime = 3600
idle_timeout = 30000
connect_timeout = 5000
healthcheck_timeout = 1000
healthcheck_delay = 30000
log_level = &quot;info&quot;
log_client_connections = true
log_client_disconnections = true

[pools.dev]
pool_size = 15
database = &quot;---TIMESCALEDB_DB_NAME---&quot;
default_role = &quot;primary&quot;
query_parser_enabled = true
primary_reads_enabled = true

[pools.dev.users.0]
username = &quot;---TIMESCALEDB_USER---&quot;
password = &quot;---TIMESCALEDB_PASSWORD---&quot;
pool_size = 15

[pools.dev.shards.0]
database = &quot;---TIMESCALEDB_DB_NAME---&quot;
servers = [
    [ &quot;timescaledb-master&quot;, 5432, &quot;primary&quot; ],
    [ &quot;timescaledb-replica1&quot;, 5432, &quot;replica&quot; ],
    [ &quot;timescaledb-replica2&quot;, 5432, &quot;replica&quot; ]
]
</code></pre>
<hr />
<h3>Test script (<code>psycopg</code>)</h3>
<pre class=""lang-py prettyprint-override""><code>import psycopg

conn = psycopg.connect(
    dbname=&quot;dev&quot;,
    user=&quot;wizabot&quot;,
    password=&quot;crptx123&quot;,
    host=&quot;timescaledb_proxy&quot;,
    port=6432,
)

cur = conn.cursor()

cur.execute(&quot;&quot;&quot;
CREATE TABLE IF NOT EXISTS test_table (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    value INTEGER NOT NULL
);
&quot;&quot;&quot;)
conn.commit()

cur.execute(&quot;&quot;&quot;
INSERT INTO test_table (name, value)
VALUES ('alpha', 10), ('beta', 20), ('gamma', 30)
RETURNING id, name, value;
&quot;&quot;&quot;)
print(&quot;Inserted rows:&quot;)
for row in cur.fetchall():
    print(row)
conn.commit()

cur.execute(&quot;SELECT id, name, value FROM test_table ORDER BY id;&quot;)
for row in cur.fetchall():
    print(f&quot;id={row[0]}, name={row[1]}, value={row[2]}&quot;)

cur.close()
conn.close()
</code></pre>
<hr />
<h3>docker compose (swarm mode) config:</h3>
<pre class=""lang-yaml prettyprint-override""><code>...
  timescaledb-master:
    image: timescaledb:latest
    networks:
      - wizabot_internal_net
    environment:
      POSTGRES_USER: ${TIMESCALEDB_USER?}
      POSTGRES_PASSWORD: ${TIMESCALEDB_PASSWORD?}
      POSTGRES_DB: ${TIMESCALEDB_DB_NAME?}
      POSTGRES_HOST_AUTH_METHOD: md5
      POSTGRES_INITDB_ARGS: &quot;--auth-host=md5&quot;
    command: &gt;
      postgres
      -c wal_level=replica
      -c max_wal_senders=10
      -c max_replication_slots=10
      -c hot_standby=on
      -c shared_preload_libraries=timescaledb
    volumes:
      - wizabot_timescaledb_master:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.db_vm == true
          - node.hostname == wiza-swarm-manager

  timescaledb-replica1:
    image: timescaledb:latest
    networks:
      - wizabot_internal_net
    environment:
      POSTGRES_USER: ${TIMESCALEDB_USER?}
      POSTGRES_PASSWORD: ${TIMESCALEDB_PASSWORD?}
      POSTGRES_DB: ${TIMESCALEDB_DB_NAME?}
      PGUSER: ${TIMESCALEDB_USER?}
      PGPASSWORD: ${TIMESCALEDB_PASSWORD?}
      POSTGRES_MASTER_HOST: timescaledb-master
      POSTGRES_MASTER_PORT: 5432
    volumes:
      - wizabot_timescaledb_replica1:/var/lib/postgresql/data
    depends_on:
      - timescaledb-master

  timescaledb-replica2:
    image: timescaledb:latest
    networks:
      - wizabot_internal_net
    environment:
      POSTGRES_USER: ${TIMESCALEDB_USER?}
      POSTGRES_PASSWORD: ${TIMESCALEDB_PASSWORD?}
      POSTGRES_DB: ${TIMESCALEDB_DB_NAME?}
      PGUSER: ${TIMESCALEDB_USER?}
      PGPASSWORD: ${TIMESCALEDB_PASSWORD?}
      POSTGRES_MASTER_HOST: timescaledb-master
      POSTGRES_MASTER_PORT: 5432
    volumes:
      - wizabot_timescaledb_replica2:/var/lib/postgresql/data
    depends_on:
      - timescaledb-master

  timescaledb_proxy:
    image: pgcat:latest
    networks:
      - wizabot_internal_net
    depends_on:
      - timescaledb-master
      - timescaledb-replica1
      - timescaledb-replica2
    environment:
      TIMESCALEDB_USER: ${TIMESCALEDB_USER?}
      TIMESCALEDB_PASSWORD: ${TIMESCALEDB_PASSWORD?}
      TIMESCALEDB_DB_NAME: ${TIMESCALEDB_DB_NAME?}
    deploy:
      replicas: 2
      placement:
        preferences:
          - spread: node.id
...
</code></pre>
<hr />
<h3>Environment</h3>
<ul>
<li><strong>PgCat:</strong> latest (<code>postgresml/pgcat</code>)</li>
<li><strong>TimescaleDB:</strong> 2.x (PostgreSQL 15)</li>
<li><strong>Python:</strong> psycopg 3.x</li>
<li><strong>Deployment:</strong> Docker Swarm</li>
</ul>
<hr />
<h3>Question</h3>
<p>Why does PgCat sometimes route write queries to replicas even when:</p>
<ul>
<li><code>pool_mode = &quot;transaction&quot;</code></li>
<li><code>query_parser_enabled = true</code></li>
<li><code>default_role = &quot;primary&quot;</code></li>
<li><code>primary_reads_enabled = true</code></li>
</ul>
<p>Is there a known issue with query routing or connection reuse between replicas and primaries?</p>
",0,0,0,2025-10-29T17:44:25+00:00,1,76,False
79804253,4758517,,postgresql,Example of partial aggregation,"<p>As Postgresql documentation states in # <a href=""https://www.postgresql.org/docs/current/xaggr.html#XAGGR-PARTIAL-AGGREGATES"" rel=""nofollow noreferrer"">36.12.4. Partial Aggregation</a> aggregate functions can support partial aggregation.
To do this with custom aggregate function as again the <a href=""https://www.postgresql.org/docs/current/sql-createaggregate.html"" rel=""nofollow noreferrer"">documentation says</a> we have to define COMBINEFUNC.
I have tried to do this, but it is never called. Is there any example of how do call this <em>partial aggregation</em> correctly. I didn't find either.</p>
<p>The general idea is that I have for example a table</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>type</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table></div>
<p>And I want sum values for each type (eg, for type=1 - 1+2=3 and for type=2 - 3+4=7) and then combine it with by for example multiplying the subset results (eg, 3*7=21).</p>
<p>I thought it could be achieved with partial aggregation but I didn't find how to do this correctly. Or maybe I do it completely wrong?</p>
<p><strong>Update</strong></p>
<p>Adding example SQL</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION my_sum_sfunc(state integer, value integer) 
RETURNS integer 
LANGUAGE plpgsql
AS $$
BEGIN
    RAISE NOTICE 'SUM % + %', state, value;
    RETURN state + value;
END;
$$;

CREATE OR REPLACE FUNCTION my_sum_combinefunc(state1 integer, state2 integer)
RETURNS integer 
LANGUAGE plpgsql
AS $$
BEGIN
    RAISE NOTICE 'COMBINE % * %', state1, state2;
    RETURN state1 * state2;
END;
$$;

CREATE AGGREGATE my_sum(integer) (
    SFUNC = my_sum_sfunc,
    STYPE = integer,
    INITCOND = '0',
    COMBINEFUNC = my_sum_combinefunc,
    PARALLEL = SAFE
);

CREATE TABLE my_table (
    id integer PRIMARY KEY,
    &quot;type&quot; integer NOT NULL,
    value integer
);

INSERT INTO my_table (id, &quot;type&quot;, value)
VALUES
    (1, 1, 1),
    (2, 1, 2),
    (3, 2, 3),
    (4, 2, 4);
</code></pre>
<p>I've tried simple</p>
<pre><code>SELECT my_sum(v.value)
FROM my_table v
</code></pre>
<p>Also tried</p>
<pre><code>SELECT my_sum(v.value) OVER (PARTITION BY v.&quot;type&quot;)
FROM my_table v
</code></pre>
<p>None of these makes call to combine function</p>
<hr />
<p><strong>Update 2</strong>
Posting my full bitwise aggregate functions as asked</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION bit_mask_and_state(agg_state integer[], cur_value integer[])
 RETURNS integer[]
 LANGUAGE plpgsql
 IMMUTABLE
AS $function$
DECLARE res integer[];
BEGIN
    select array_agg(coalesce(a,x'FFFFFFFF'::integer) &amp; coalesce(b,x'FFFFFFFF'::integer)) as s
    into res
    from unnest(agg_state, cur_value) x(a,b);
    return res;
END;
$function$
;

CREATE OR REPLACE AGGREGATE bit_array_and(integer[]) (
    SFUNC = bit_mask_and_state,
    STYPE = integer[]
);

CREATE OR REPLACE FUNCTION bit_mask_or_state(agg_state integer[], cur_value integer[])
 RETURNS integer[]
 LANGUAGE plpgsql
 IMMUTABLE
AS $function$
DECLARE res integer[];
BEGIN
    select array_agg(coalesce(a,0) | coalesce(b,0)) as s
    into res
    from unnest(agg_state, cur_value) x(a,b);
    return res;
END;
$function$
;

CREATE OR REPLACE AGGREGATE bit_array_or(integer[]) (
    SFUNC = bit_mask_or_state,
    STYPE = integer[]
);
</code></pre>
<p>Tried to create aggregate like this</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE AGGREGATE my_sum(integer[]) (
    SFUNC = bit_mask_or_state,
    STYPE = integer[],
    COMBINEFUNC = bit_mask_or_state,
    PARALLEL = SAFE
);
</code></pre>
<p>But it didn't work as I expected. Ended up with the following</p>
<pre class=""lang-sql prettyprint-override""><code>    select bit_mask_and_state(
            bit_array_or(r.mask_array) FILTER (WHERE r.&quot;type&quot; = 1),
            bit_array_or(r.mask_array) FILTER (WHERE r.&quot;type&quot; = 2)
        )
    from ... r
</code></pre>
",2,3,1,2025-10-29T21:55:38+00:00,2,144,True
79804551,22272977,,postgresql,How to use jsonb functions in hibernate 6?,"<p>We use Postgres <code>jsonb_agg</code> function in criteria query API like this:</p>
<pre><code>cb.function(
                JSONB_AGG_FUNCTION_NAME,
                JsonNode.class,
                someJoin.get(someField)
            )
</code></pre>
<p>and map a result into projection with constructor:</p>
<pre><code>Projection (&lt;some args&gt;, com.fasterxml.jackson.databind.JsonNode node)
</code></pre>
<p>In Hibernate 5 we registered <code>jsonb_agg</code> function using this approach:</p>
<pre><code>public class ExtendedPgDialect extends PostgreSQLDialect {

public static final String JSONB_AGG_FUNCTION_NAME = &quot;jsonb_agg&quot;;

public ExtendedPgDialect() {
    registerFunction(JSONB_AGG_FUNCTION_NAME,
            new StandardSQLFunction(JSONB_AGG_FUNCTION_NAME, JsonNodeBinaryType.INSTANCE));
     }
}
</code></pre>
<p>But in Hibernate 6.6.33.Final we can't register that function and can't use it for querying data.
Also we used (<code>JsonNodeBinaryType</code>) type from io.hypersistence:hypersistence-utils-hibernate-53 library. How to register <code>jsonb_agg</code> function in Hibernate 6 for mapping to <code>JsonNode</code> java type?</p>
",2,2,0,2025-10-30T08:54:22+00:00,1,77,False
79805212,521586,,postgresql,Problem updating Postgres ENUM from DuckDB,"<p>I'm on DuckDB 1.4.1 experiencing difficulty updating a Postgres 17.6 ENUM field status:</p>
<pre><code>CREATE TYPE mystatus_enum AS ENUM (
    'IN_STOCK', 'OUT_OF_STOCK', 'NOT_FOUND', 'NOT_A_PRODUCT'
);

CREATE TABLE mytable
(
    id INTEGER primary key,
    status mystatus_enum
);
</code></pre>
<p>All of the following give me:</p>
<blockquote>
<p>Not implemented Error: Enums in Postgres must be named - unnamed enums are not supported. Use CREATE TYPE to create a named enum.</p>
</blockquote>
<p>Making a similar update from within Postgres without DuckDB is not a problem.</p>
<p>Both for DuckDB sources:</p>
<pre><code>update mypg.mytable set status=ddb.status where ddb.id=mypg.mytable.id;
update mypg.mytable set status=ddb.status::varchar where ddb.id=mypg.mytable.id;
update mypg.mytable set status=ddb.status::text where ddb.id=mypg.mytable.id;
update mypg.mytable set status=ddb.status::mypg.mystatus_enum where ddb.id=mypg.mytable.id;
</code></pre>
<p>and PG sources:</p>
<pre><code>update mypg.mytable set status=mypg.mytable2.status where mypg.mytable2.id=mypg.mytable.id;
</code></pre>
<p>and as above but with PG sources.</p>
<p>I've also tried using an equivalent ENUM declared in DuckDB.</p>
<p>Any suggestions for how I can avoid this error?
I'd like to avoid my last resort of switching the Postgres field from ENUM to VARCHAR as it is a very large and active table.</p>
",1,1,0,2025-10-30T20:57:38+00:00,0,113,False
79805414,2239301,India,postgresql,Best way to write an Oracle Procedure using Spring Data Jpa,"<p>I am new to JPA and learning it. Now I have an Oracle procedure in system, which now needs to be made DB Agnostic, that can run on Postgres. For that I want to write the same functionality of procedure using Spring Boot JPA. I need some guidance on how to proceed on that. I am not asking for any code to be written, but some references and guidelines which I can follow and learn on that. Please advice.</p>
",0,0,0,2025-10-31T04:56:19+00:00,7,197,True
79805848,3737704,,postgresql,create unlogged table automatically on postgres startup or atleast not lose them due to a restart,"<p>I tried postgres unlogged table for using them as a cache to speed up queries and it works (50% query time got avoided). I can really recommend that to everyone.</p>
<p>But I restarted the server (I mean the hardware server using reboot) and now the unlogged tables are gone completely - not only the data.
So I could add them after every startup manually - but that should be possible somehow automatically.</p>
<p>Or does anyone have a clue how to make the table structure permanently including unlogged tables as well as normal tables?</p>
<p>versions: postgres 18.0, debian 13.2 on a physical server.</p>
<p>To reboot I only did type 'reboot' in the terminal.</p>
<p>systemctl stop/start postgresql doesn't delete the unlogged tables, only 'reboot'</p>
<p>Postgres is not running in anything like docker container, but only got updated from version 9 doing a new install.</p>
",4,4,0,2025-10-31T13:35:30+00:00,1,139,True
79806315,21102187,Brazil,postgresql,Postgres JSON or JSONB to store game&#39;s character customization data,"<p>I'm building the backend for a platform that works similarly to a game. It includes a simple character customization system with a few properties that aren't particularly relevant to the product (things like hair color, shirt type, etc).</p>
<p>We've decided to use PostgreSQL for the database, and I believe this character customization data can be stored as JSON. I'm just not entirely sure whether we should use plain JSON or JSONB, since JSONB allows for indexing and our database has the potential to scale quickly (selling to a single client could bring in hundreds or even thousands of users at once).</p>
<p>Should we go with JSONB, considering the potential scalability? Or should we stick with plain JSON, since the data isn’t critical?</p>
",2,0,0,2025-11-01T03:10:56+00:00,2,65,True
79806471,24826231,,postgresql,System cache constantly grows when stress testing backend app,"<p>I've been stress testing my .NET application that uses PostgreSQL to find memory leaks. The app is very stable memory-wise — memory usage only depends on the number of concurrent connections.</p>
<p>However, I’ve noticed that the RAM usage marked as cache is slowly growing over time until it fills up, causing a RAM shortage (the cache is not being released).</p>
<p>I’m testing my app by making 1000 concurrent POST requests to the backend, which is configured with a pool of up to 100 PostgreSQL connections. PostgreSQL has shared_buffers set to 128MB and a maximum of 100 connections.</p>
<p>I haven’t found any high memory usage from the .NET process or from my simple Node app that I’m using for the tests.</p>
<p>The test simply makes many concurrent HTTPS connections:
The test is just a simple https client trying to make many concurrent connections.</p>
<pre><code>while (true) {
  await stressREST(token, url, msg, 1000);
}
</code></pre>
<pre><code>const httpsAgent = new https.Agent({
  keepAlive: true,
  maxSockets: 100,
  maxTotalSockets: 100,
});

const client = axios.create({
  httpsAgent,
  timeout: 10_000,
});

export default async function stressREST(
  token: string,
  url: string,
  content: object,
  times: number
) {
  const promises = [];

  for (let i = 0; i &lt; times; i++) {
    promises.push(
      client.post(url, content, {
        headers: { Authorization: `Bearer ${token}` },
      })
    );
  }

  await Promise.all(promises);
  console.log(times, &quot;requests done&quot;);
}
</code></pre>
<p>I added the HTTPS client because I thought the backend wasn’t keeping up with the connections and Node or the server was caching the requests, but it didn’t help. When I stop the Node process, the high cache usage still persists in RAM even after stopping my backend. Using SetInterval() instead of while true loop didn't help either.</p>
<p>I also set the kernel shmmax value to 8GB, but it didn’t make any difference.</p>
<p>I use Fedora 43 with 32GB of RAM.</p>
",2,2,0,2025-11-01T10:28:48+00:00,0,98,False
79806877,19659058,,postgresql,Save and get large objects (BLOB) to postgresql using hibernate and Spring Data JPA,"<p>experts!</p>
<p>I encountered the following problem: when trying to read a large file from a PostgreSQL database in a Spring Boot application using the Spring Data JPA framework, all the data is loaded into memory, even though the Blob class specification says <a href=""https://docs.oracle.com/en/java/javase/17/docs/api/java.sql/java/sql/Blob.html"" rel=""nofollow noreferrer"">otherwise</a>:</p>
<p><code>By default drivers, implement Blob using an SQL locator (BLOB), which means that a Blob object contains a logical pointer to the SQL BLOB data rather than the data itself.</code></p>
<p>Details below:</p>
<pre class=""lang-kotlin prettyprint-override""><code>@RestController
class LargeFileController(
    val fileRepo: FileContentRepo
) {
    @PostMapping(&quot;/file&quot;, consumes = [MediaType.MULTIPART_FORM_DATA_VALUE])
    fun saveFile(@RequestBody file: MultipartFile) {
        fileRepo.save(LargeFileContent(file))
    }

    @GetMapping(&quot;/file/{id}&quot;, produces = [MediaType.APPLICATION_OCTET_STREAM_VALUE])
    @Transactional
    fun getFileById(@PathVariable(&quot;id&quot;) id: Long): InputStreamResource {
        val file = fileRepo.findById(id).orElseThrow()
        // Here I see that all the data is stored in memory (regardless of file size) 
        // in the InputStream buffer (in the screenshot below) 
        // If I turn off the database at this point, I still have access to the entire contents of the file.
        val content = file.content

        return InputStreamResource {
            file.content.binaryStream
        }
    }
}

@Entity
@Table(name = &quot;file_content&quot;)
class LargeFileContent (
    @Column(nullable = false)
    @Lob
    @JdbcTypeCode(java.sql.Types.BINARY)
    val content: Blob,

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    val id: Long?=null,
) {
    constructor(file: MultipartFile) : this(
        content = BlobProxy.generateProxy(file.inputStream, file.size)
    )
}

@Repository
interface FileContentRepo: JpaRepository&lt;LargeFileContent, Long&gt; {
}
</code></pre>
<p><a href=""https://i.sstatic.net/QsYPp1gn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QsYPp1gn.png"" alt=""All data saved in buf"" /></a></p>
<p>My idea was to save the InputStream from the database to a temporary file on disk and then return the InputStream from that file to the client (so as not to hold up the database connection while the client reads). Is it possible to read the data from the Blob in chunks, rather than loading the entire contents into memory at once?</p>
<p>Thanks in advance for your answers!</p>
",3,3,0,2025-11-02T00:02:50+00:00,3,254,True
79807577,1159764,"Melbourne, Australia",postgresql,Sort aggregated query results by two methods simultaneously,"<p>I need to sort a query's results by two methods at the same time.
I want the first 3 records (returned) to be based on their prevalence in another table
And then I want the rest of the results sorted alphabetically.</p>
<p>Assuming I have 6 records in a query result set....</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>empl_Type</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>10</td>
</tr>
<tr>
<td>B</td>
<td>5</td>
</tr>
<tr>
<td>C</td>
<td>2</td>
</tr>
<tr>
<td>D</td>
<td>1</td>
</tr>
<tr>
<td>E</td>
<td>1</td>
</tr>
<tr>
<td>F</td>
<td>20</td>
</tr>
</tbody>
</table></div>
<p>then results should be
<code>F</code>,<code>A</code>,<code>B</code>,<code>C</code>,<code>D</code>,<code>E</code></p>
<ul>
<li><code>F</code>,<code>A</code>,<code>B</code> sorted by their count in another table</li>
<li><code>C</code>,<code>D</code>,<code>E</code> (remainder of rows) sorted alphabetically</li>
</ul>
<p>I have the first part working in the following contrived example:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
    et.id,
    et.employeetype_description,
    count(e.employeetypeid) as thesortorder
FROM 
    employeetype et
LEFT JOIN
    employee e ON e.employeetypeid = et.id
GROUP BY
    et.id,
    et.employeetype_description
ORDER BY thesortorder DESC
</code></pre>
<p>And this is where (hopefully you come in...)
How do I meet the rest of the requirements?
Thanks</p>
",5,5,0,2025-11-03T06:12:30+00:00,4,249,True
79807640,19358412,,postgresql,EDB Postgres on OpenShift – Unable to Read Content from postgres.json Log File,"<p>I've deployed an EDB cluster on OpenShift, but I'm unable to read the contents of the postgres.json file. Do you happen to know why this might be the case, or if there's a workaround to access the log data?</p>
<pre><code>➜  oc get cluster
NAME   AGE   INSTANCES   READY   STATUS                     PRIMARY
smt   4d    3           3       Cluster in healthy state   smt-1
➜  
➜  oc rsh -c postgres smt-1
sh-4.4$ psql -U postgres
psql (14.19 (EDB Postgres Extended Server 14.19.0))
Type &quot;help&quot; for help.

postgres=# show log_line_prefix;
                   log_line_prefix                   
-----------------------------------------------------
 %m [%p][%x]: [%l-1] user=%u,db=%d,app=%a,client=%h 
(1 row)

postgres=# show log_directory;
  log_directory  
-----------------
 /controller/log
(1 row)

postgres=# \q
sh-4.4$ 
sh-4.4$ cd /controller/log
sh-4.4$ ls -l
total 0
prw-------. 1 1000890000 1000890000 0 Nov  3 02:36 edb_audit.csv
prw-------. 1 1000890000 1000890000 0 Nov  3 02:36 postgres
prw-------. 1 1000890000 1000890000 0 Nov  3 02:54 postgres.csv
prw-------. 1 1000890000 1000890000 0 Nov  3 02:44 postgres.json
sh-4.4$ 
sh-4.4$ more postgres

sh-4.4$ more edb_audit.csv
</code></pre>
<p>But I can read file postgres.csv with command:</p>
<pre><code>sh-4.4$ more postgres.csv
2025-11-03 03:50:15.970 CET,,,918,&quot;[local]&quot;,69081867.396,1,&quot;&quot;,2025-11-03 03:50:15 CET,,0,LOG,00000,&quot;connection received: host=[local]&quot;,,,,,,,,,&quot;&quot;,&quot;not initialized&quot;,,0
2025-11-03 03:50:15.971 CET,&quot;postgres&quot;,&quot;postgres&quot;,918,&quot;[local]&quot;,69081867.396,2,&quot;authentication&quot;,2025-11-03 03:50:15 CET,6/462,0,LOG,00000,&quot;connection authenticated: identity=&quot;&quot;1000890000&quot;&quot; method=
peer (/var/lib/postgresql/data/pgdata/pg_hba.conf:7)&quot;,,,,,,,,,&quot;&quot;,&quot;client backend&quot;,,0
2025-11-03 03:50:15.971 CET,&quot;postgres&quot;,&quot;postgres&quot;,918,&quot;[local]&quot;,69081867.396,3,&quot;authentication&quot;,2025-11-03 03:50:15 CET,6/462,0,LOG,00000,&quot;connection authorized: user=postgres database=postgres ap
plication_name=pg_isready&quot;,,,,,,,,,&quot;&quot;,&quot;client backend&quot;,,0
2025-11-03 03:50:15.973 CET,&quot;postgres&quot;,&quot;postgres&quot;,918,&quot;[local]&quot;,69081867.396,4,&quot;idle&quot;,2025-11-03 03:50:15 CET,,0,LOG,00000,&quot;disconnection: session time: 0:00:00.002 user=postgres database=postgres
 host=[local]&quot;,,,,,,,,,&quot;pg_isready&quot;,&quot;client backend&quot;,,0
2025-11-03 03:50:19.277 CET,,,920,&quot;10.130.3.15:37828&quot;,6908186b.398,1,&quot;&quot;,2025-11-03 03:50:19 CET,,0,LOG,00000,&quot;connection received: host=10.130.3.15 port=37828&quot;,,,,,,,,,&quot;&quot;,&quot;not initialized&quot;,,0
2025-11-03 03:50:19.283 CET,&quot;streaming_replica&quot;,&quot;postgres&quot;,920,&quot;10.130.3.15:37828&quot;,6908186b.398,2,&quot;authentication&quot;,2025-11-03 03:50:19 CET,6/465,0,LOG,00000,&quot;connection authenticated: identity=&quot;&quot;C
N=streaming_replica&quot;&quot; method=cert (/var/lib/postgresql/data/pgdata/pg_hba.conf:10)&quot;,,,,,,,,,&quot;&quot;,&quot;client backend&quot;,,0
</code></pre>
<p>Another odd thing is that although I've configured log_line_prefix in PostgreSQL, the format of entries in postgres.csv doesn't match the expected structure.</p>
",2,2,0,2025-11-03T08:08:46+00:00,1,60,True
79807659,7740888,,postgresql,What is the difference between session_authorization and set role,"<p>In Postgres, you can set <code>current_user</code> by:</p>
<pre class=""lang-sql prettyprint-override""><code>set session_authorization = 'new_user';
</code></pre>
<p>and</p>
<pre class=""lang-sql prettyprint-override""><code>set role 'new_user';
</code></pre>
<p>While I like the shorter syntax, I'm asking if there is a difference in functionality between the two?</p>
",1,1,0,2025-11-03T08:45:00+00:00,1,94,True
79808016,31766313,,postgresql,PostgreSQL UUID query issue,"<p>I have the following in a TS script that I'm running with the npm <code>tsx</code> package (slicing to 1 item to make it shorter, but same error when using full array):</p>
<pre><code>console.log(psql(`SELECT c.id lead_uuid, part.id storefront_id FROM conversations c CROSS JOIN LATERAL (SELECT parts-&gt;&gt;'id' id FROM jsonb_path_query(c.data,'$.participants.*') parts WHERE parts-&gt;'tags'@&gt;'[&quot;booked&quot;]') part WHERE EXISTS (SELECT 1 FROM jsonb_each(data -&gt; 'participants') AS participants_item WHERE participants_item.value -&gt; 'tags' IS NOT NULL AND jsonb_typeof(participants_item.value -&gt; 'tags') = 'array' AND participants_item.value -&gt; 'tags' @&gt; '[&quot;booked&quot;]') AND c.id IN ('${conversationIds.slice(0,1).join(&quot;','&quot;)}');`));

return await psql`SELECT c.id lead_uuid, part.id storefront_id FROM conversations c CROSS JOIN LATERAL (SELECT parts-&gt;&gt;'id' id FROM jsonb_path_query(c.data,'$.participants.*') parts WHERE parts-&gt;'tags'@&gt;'[&quot;booked&quot;]') part WHERE EXISTS (SELECT 1 FROM jsonb_each(data -&gt; 'participants') AS participants_item WHERE participants_item.value -&gt; 'tags' IS NOT NULL AND jsonb_typeof(participants_item.value -&gt; 'tags') = 'array' AND participants_item.value -&gt; 'tags' @&gt; '[&quot;booked&quot;]') AND c.id IN ('${conversationIds.slice(0,1).join(&quot;','&quot;)}');`;
</code></pre>
<p>The console log is this:</p>
<pre><code>Identifier {
  value: `&quot;SELECT c&quot;.&quot;id lead_uuid, part&quot;.&quot;id storefront_id FROM conversations c CROSS JOIN LATERAL (SELECT parts-&gt;&gt;'id' id FROM jsonb_path_query(c&quot;.&quot;data,'$&quot;.&quot;participants&quot;.&quot;*') parts WHERE parts-&gt;'tags'@&gt;'[&quot;&quot;booked&quot;&quot;]') part WHERE EXISTS (SELECT 1 FROM jsonb_each(data -&gt; 'participants') AS participants_item WHERE participants_item&quot;.&quot;value -&gt; 'tags' IS NOT NULL AND jsonb_typeof(participants_item&quot;.&quot;value -&gt; 'tags') = 'array' AND participants_item&quot;.&quot;value -&gt; 'tags' @&gt; '[&quot;&quot;booked&quot;&quot;]') AND c&quot;.&quot;id IN ('91174f9d-b65e-440a-ad3b-7ec18b26b7db');&quot;`
}
</code></pre>
<p>But I'm getting this error on the <code>return</code> line:</p>
<pre><code>Fatal error PostgresError: invalid input syntax for type uuid: &quot;$1&quot;
</code></pre>
<p>New to PostgreSQL in NodeJS so I'm hoping it's an obvious error that can be pointed out.</p>
",1,1,0,2025-11-03T15:20:11+00:00,1,93,False
79808711,20377292,,postgresql,PostgreSQL Docker Container Permission Issues with NFS Mount and Domain User,"<h2>Problem Summary</h2>
<p>I'm experiencing permission issues when deploying a PostgreSQL 9.6 container in Docker Swarm that uses an NFS-mounted volume for data storage. The container fails to start with the error:</p>
<pre><code>FATAL: data directory &quot;/var/lib/postgresql/data/pg_data&quot; has wrong ownership
HINT: The server must be started by the user that owns the data directory.
</code></pre>
<h2>Environment Details</h2>
<ul>
<li><strong>Platform</strong>: Docker Swarm</li>
<li><strong>PostgreSQL Version</strong>: 9.6.24</li>
<li><strong>NFS Server</strong>: re0srt10003.eresz03.com</li>
<li><strong>NFS Mount</strong>: <code>/vol/re0srt10003_vol011/NFS_customer_data_02/Production_data/nfs01/</code></li>
<li><strong>Note</strong>:
→ only AD valid users (unix/windows) can be used to access a file share
→ local users access is not permitted (users that only exists on the local linux system)
→ Please be aware of that new file shares will be on new Server re0srt10003.eresz03.com and as mentioned in our docupedia page local users (Example UID 1000 / GID 1000) are not working. No exceptions possible!
Please use your Domain Users and Groups (UID 188044 / GID 806642981) to connect to the share.</li>
</ul>
<h2>Current Configuration</h2>
<h3>Dockerfile used to create image</h3>
<pre class=""lang-none prettyprint-override""><code>FROM postgres:9.6.24

ENV http_proxy=http://proxy.com:8686
ENV https_proxy=http://proxy.com:8686

RUN sed -i '/stretch-updates/d' /etc/apt/sources.list &amp;&amp; \
    sed -i 's|http://deb.debian.org/debian|http://archive.debian.org/debian|g' /etc/apt/sources.list &amp;&amp; \
    sed -i 's|http://security.debian.org/debian-security|http://archive.debian.org/debian-security|g' /etc/apt/sources.list &amp;&amp; \
    rm -f /etc/apt/sources.list.d/pgdg.list &amp;&amp; \
    echo 'Acquire::Check-Valid-Until &quot;false&quot;;' &gt; /etc/apt/apt.conf.d/10-no-check-valid-until &amp;&amp; \
    apt-get update &amp;&amp; \
    apt-get install -y --allow-unauthenticated postgresql-contrib &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

RUN usermod -u 188044 postgres
RUN groupmod -g 806642981 postgres

RUN su root -c &quot;chown -R postgres:postgres /var/lib/postgresql&quot;

USER postgres
STOPSIGNAL SIGINT
</code></pre>
<h3>Docker Compose Service</h3>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.7'
services:
  fossology-scheduler:
    command: scheduler
    image: fossology:$VERSION
    environment:
      - FOSSOLOGY_DB_HOST=fossology-db
      - FOSSOLOGY_DB_NAME=fossology
      - FOSSOLOGY_DB_USER=fossy
      - FOSSOLOGY_INSTANCE=$INSTANCE
    networks:
      - fossology-net
    volumes:
      - fossy_repo:/srv/fossology/repository
    deploy:
      placement:
        constraints:
          - node.labels.fossology == true
    secrets:
      - source: fossology-db-pwd
        target: fossology.pwd
        uid: '188044'
        gid: '806642981'
        mode: 0400
    hostname: fossology-scheduler.localhost

  fossology-web:
    command: web
    image: fossology:$VERSION
    environment:
      - FOSSOLOGY_DB_HOST=fossology
      - FOSSOLOGY_DB_NAME=fossology
      - FOSSOLOGY_DB_USER=fossui
      - FOSSOLOGY_SCHEDULER_HOST=fossology-scheduler
      - FOSSOLOGY_INSTANCE=$INSTANCE
    user: fossui
    networks:
      - fossology-net
      - nginx-net
    volumes:
     - fossy_repo:/srv/fossology/repository
    deploy:
      placement:
        constraints:
          - node.labels.fossology == true
    secrets:
      - source: fossology-db-pwd
        target: fossology.pwd
        uid: '188044'
        gid: '806642981'
        mode: 0400

  fossology-db:
    image: postgres_9.6:01
    environment:
      - POSTGRES_DB=fossology
      - POSTGRES_USER=fossui
      - POSTGRES_PASSWORD=password
      - POSTGRES_INITDB_ARGS='-E UTF8'
      - PGDATA=/var/lib/postgresql/data
    ports:
      - target: 5432
        published: 9999
        protocol: tcp
        mode: ingress
    networks:
      - fossology-net
    volumes:
      - fossy_pg_data:/var/lib/postgresql/data
    deploy:
      placement:
        constraints:
          - node.labels.fossology == true

networks:
  fossology-net:
    name : fossology-net-$INSTANCE
    driver: overlay
    external: true
  nginx-net:
    external: true

volumes:
  fossy_repo:
    driver: local
    driver_opts:
      type: nfs
      o: &quot;addr=re0srt10003.eresz03.com,rw,nfsvers=4&quot;
      device: &quot;:/vol/re0srt10003_vol011/NFS_customer_data_02/Production_data/nfs01/repo&quot;
  fossy_pg_data:
    driver: local
    driver_opts:
      type: nfs
      o: &quot;addr=re0srt10003.eresz03.com,rw,nfsvers=4&quot;
      device: &quot;:/vol/re0srt10003_vol011/NFS_customer_data_02/Production_data/nfs01/db&quot;
secrets:
  fossology-db-pwd:
    external: true
</code></pre>
<h2>What I've Tried</h2>
<ul>
<li>Modified the Dockerfile to change postgres user UID/GID to match domain requirements</li>
<li>Verified the user mapping inside the container shows correct IDs
id postgres
uid=188044(postgres) gid=806642981(postgres) groups=806642981(postgres),101(ssl-cert)</li>
<li>Confirmed NFS mount is accessible by ownership shows as below,
drwx--S--- 8     188044  806642981 db</li>
<li>Tried to run the service without NFS volume - working as expected.</li>
</ul>
<h2>Additional Context</h2>
<p>This setup worked previously with local user/root user, but fails when migrating to the new NFS-based storage system that enforces domain users.</p>
<p>Any suggestions regarding the possible root causes or alternative ways to address this issue would be highly appreciated!.</p>
",0,0,0,2025-11-04T09:04:38+00:00,0,67,False
79809217,22000063,,postgresql,Equivalent to replace input date from User/API In PostgreSQL from Oracle,"<p>I am in the process of transitioning Oracle queries to PostgreSQL and I am trying to pass a date range to pull back the results.</p>
<p>The original Oracle query is below:</p>
<pre><code>select 
    iss_id
   ,date_created
from issues i 
where 
   trunc(i.date_created) 
   BETWEEN trunc(TO_DATE(:StartDt ,'mmddyyyy'))
    --                    ^^^^^^^
        AND trunc(TO_DATE(:EndDt ,'mmddyyyy'))  
    --                    ^^^^^^
</code></pre>
<p>My problem is with the <code>:StartDt</code> &amp; <code>:EndDt</code>, when this is run in Oravle Developer it will produce a popup box that will allow the user to input a date value for these two variables.  What I am looking for is an equivalent to this in PostgreSQL.</p>
<p>Any assistance to this would be greatly appreciated.</p>
",0,1,1,2025-11-04T17:35:52+00:00,1,93,True
79809822,13278813,,postgresql,Jooq multiset and Postgres max 100 args limit,"<p>I am using multiset successfully. But as soon as I request more than 100 columns then the postgres throws an exception.
It seems that Jason aggregate functions that jooq uses have limits on number of arguments.</p>
<p>Is it possible to work around this?
Could jooq pack the columns in array thus avoiding the limitation?</p>
<p>This is a <a href=""https://www.reddit.com/r/jOOQ/s/yaf4mAo7FR"" rel=""nofollow noreferrer"">cross post</a> from reddit's jooq community.</p>
<p>Adding a reproduction example:</p>
<pre><code>import org.jooq.DSLContext;
import org.jooq.Field;
import org.jooq.Record;
import org.jooq.Record1;
import org.jooq.Result;
import org.jooq.SQLDialect;
import org.jooq.SelectSelectStep;
import org.jooq.conf.ParamType;
import org.jooq.impl.DSL;

import java.util.ArrayList;
import java.util.List;

class Scratch {
    public static void main(String[] args) {
        List&lt;Field&gt; fields  = new ArrayList&lt;&gt;();
        for(int i=0; i&lt;101; i++){
            fields.add(DSL.inline(i));
        }
        SelectSelectStep&lt;Record1&lt;Result&lt;Record&gt;&gt;&gt; select = DSL.select(DSL.multiset(DSL.select(fields)));
        DSLContext context = DSL.using(SQLDialect.POSTGRES);
        context.attach(select);
        System.out.println(select.getSQL(ParamType.INLINED));
    }
}
</code></pre>
<p>This will generate following sql:</p>
<pre><code>select (select coalesce(jsonb_agg(jsonb_build_array( &lt;fields&gt; ) )), jsonb_build_array()) from ( &lt;select the values&gt; )
</code></pre>
<p>and will result in error:</p>
<pre><code>[2025-11-05 15:33:45] [54023] ERROR: cannot pass more than 100 arguments to a function
[2025-11-05 15:33:45] Position: 35
</code></pre>
",2,2,0,2025-11-05T08:52:33+00:00,1,81,True
79810530,14914853,,postgresql,"Sequelize transaction stays open too long during API calls, causing connection pool exhaustion","<p>I'm facing an issue with Sequelize(postgres) transactions in a high-traffic Node.js/Express app. I have a flow like this:</p>
<pre class=""lang-js prettyprint-override""><code>async function createPayment(data, trnx) {
  // Save in DB within transaction
  await Payment.create({ ...data }, { transaction: trnx });

  // Call external API
  await callThirdPartyAPI(data);

  // Update DB within transaction
  await Payment.update(
    { status: 'SUCCESS' },
    { where: { id: data.id }, transaction: trnx }
  );
}

async function processPayment(data) {
  const trnx = await db.sequelize.transaction();

  await createPayment(data, trnx);

  await publishToQueue(data);

  await Payment.update(
    { queued: true },
    { where: { id: data.id }, transaction: trnx }
  );

  await trnx.commit();
}
</code></pre>
<p><strong>Problem:</strong></p>
<ul>
<li><p>The transaction <code>trnx</code> stays open for a long time because of the API
call and queue publishing, as well as other operations before and after this one.</p>
</li>
<li><p>Under high load, this leads to connection pool exhaustion and<br />
<code>SequelizeConnectionAcquireTimeoutError</code>.</p>
</li>
<li><p>I cannot open the transaction after the API call, because if the API
fails, I need to rollback all DB changes, including ones that took place before the code above. The <code>trnx</code> arrives here already open by something else, earlier in the workflow.</p>
</li>
<li><p>Breaking it into multiple transactions is not ideal because rollback
of the entire flow would no longer be possible.</p>
</li>
</ul>
<p><strong>Question:</strong></p>
<p>How can I structure this flow so that:</p>
<ol>
<li>I can call an external API without holding a DB transaction open for
a long time.</li>
<li>I can still rollback all DB changes if something fails.</li>
<li>I avoid exhausting the Sequelize connection pool under high traffic.</li>
</ol>
<p>I’m open to patterns like storing operations, in-memory database, or other best practices in Node.js with Sequelize.</p>
",1,1,0,2025-11-05T19:31:43+00:00,2,130,False
79810659,3607041,,postgresql,Make Postgres query fast without UNION,"<p>The ORDMBS is <em>PostgreSQL 17.5 on x86_64-suse-linux-gnu, compiled by gcc (SUSE Linux) 7.5.0, 64-bit</em>.</p>
<p>I have big table (about 150 GB), partitioned.</p>
<p>Full table description:</p>
<pre><code>CREATE TABLE table_partition 
(
    id int4,
    &quot;key&quot; text, 
    value_type int4, 
    value jsonb, 
    device_time timestamptz, 
    ts timestamptz
) PARTITION BY RANGE (ts);

CREATE INDEX table_partition_ts_id_index ON table_partition (id, ts);
CREATE INDEX table_partition_ts_id_index_2 ON table_partition (ts DESC, id);
CREATE INDEX table_partition_key_idx ON table_partition (key);
CREATE INDEX table_partition_key_ts_idx ON table_partition (key, ts);
CREATE INDEX table_partition_ts_key_idx ON table_partition (ts, key);

CREATE TABLE table_partition202506 PARTITION OF table_partition  FOR VALUES FROM ('2025-06-01') TO ('2025-07-01'); -- 30 GB
CREATE TABLE table_partition202507 PARTITION OF table_partition  FOR VALUES FROM ('2025-07-01') TO ('2025-08-01'); -- 31 GB
CREATE TABLE table_partition202508 PARTITION OF table_partition  FOR VALUES FROM ('2025-08-01') TO ('2025-09-01'); -- 66 GB
CREATE TABLE table_partition202509 PARTITION OF table_partition  FOR VALUES FROM ('2025-09-01') TO ('2025-10-01'); -- 1.3 GB
CREATE TABLE table_partition202510 PARTITION OF table_partition  FOR VALUES FROM ('2025-10-01') TO ('2025-11-01'); -- 22 GB
CREATE TABLE table_partition202511 PARTITION OF table_partition  FOR VALUES FROM ('2025-11-01') TO ('2025-12-01'); -- 160K
</code></pre>
<p>Two of the most interesting columns are:</p>
<pre><code>key text NOT NULL,
ts timestamptz NOT NULL
</code></pre>
<p>The first, slow query is:</p>
<pre><code>    SELECT csh.key, csh.ts
    FROM table_partition202508 csh
    WHERE csh.key IN ('string1', 'string2')
    ORDER BY csh.ts
    LIMIT 10;
</code></pre>
<p>The execution plan with SET track_io_timing = on is</p>
<pre><code>Limit  (cost=0.57..48.31 rows=10 width=109) (actual time=292447.149..292582.363 rows=10 loops=1)
  Output: key, ts
  Buffers: shared hit=8 read=1104471
  I/O Timings: shared read=275637.044
  -&gt;  Index Scan Backward using table_partition202508_ts_key_idx on table_partition202508 csh  (cost=0.57..11776409.95 rows=2466994 width=109) (actual time=292447.147..292582.353 rows=10 loops=1)
        Output: key, ts
        Index Cond: (csh.key = ANY ('{string1,string2}'::text[]))
        Buffers: shared hit=8 read=1104471
        I/O Timings: shared read=275637.044
Settings: effective_cache_size = '9592GB', work_mem = '32MB', search_path = 'public, public, &quot;$user&quot;'
Planning:
  Buffers: shared hit=232 read=6 dirtied=1
  I/O Timings: shared read=2.570
Planning Time: 3.980 ms
Execution Time: 292582.395 ms
</code></pre>
<p>The second, fast query is:</p>
<pre><code>SELECT csh.key, csh.ts
FROM table_partition202508 csh
WHERE csh.key IN ('string1', 'string2')

UNION ALL

SELECT csh.key, csh.ts
FROM table_partition202508 csh
WHERE csh.key = 'rubbish'  -- there is no such a key value in the dB!
ORDER BY csh.ts
LIMIT 10;
</code></pre>
<p>The execution plan (fast one) is</p>
<pre><code>Limit  (cost=7365718.00..7365719.17 rows=10 width=109) (actual time=78.605..82.522 rows=10 loops=1)
  Output: csh.key,  csh.ts
  Buffers: shared hit=12582
  -&gt;  Gather Merge  (cost=7365718.00..7725513.02 rows=3083742 width=109) (actual time=78.604..82.519 rows=10 loops=1)
        Output: csh.key,  csh.ts
        Workers Planned: 2
        Workers Launched: 2
        Buffers: shared hit=12582
        -&gt;  Sort  (cost=7364717.98..7368572.66 rows=1541871 width=109) (actual time=74.467..74.470 rows=8 loops=3)
              Output: csh.key,  csh.ts
              Sort Key: csh.ts DESC
              Sort Method: top-N heapsort  Memory: 27kB
              Buffers: shared hit=12582
              Worker 0:  actual time=73.081..73.084 rows=10 loops=1
                Sort Method: top-N heapsort  Memory: 27kB
                Buffers: shared hit=2488
              Worker 1:  actual time=73.079..73.082 rows=10 loops=1
                Sort Method: top-N heapsort  Memory: 27kB
                Buffers: shared hit=4877
              -&gt;  Parallel Append  (cost=13868.17..7331398.70 rows=1541871 width=109) (actual time=1.458..73.329 rows=4359 loops=3)
                    Buffers: shared hit=12566
                    Worker 0:  actual time=0.167..72.114 rows=2577 loops=1
                      Buffers: shared hit=2480
                    Worker 1:  actual time=0.180..71.840 rows=5076 loops=1
                      Buffers: shared hit=4869
                    -&gt;  Parallel Bitmap Heap Scan on table_partition202508 csh  (cost=27696.34..4072318.22 rows=1027914 width=109) (actual time=1.361..72.909 rows=4359 loops=3)
                          Output: csh.key,  csh.ts
                          Recheck Cond: (csh.key = ANY ('{string1,string2}'::text[]))
                          Heap Blocks: exact=5194
                          Buffers: shared hit=12562
                          Worker 0:  actual time=0.166..71.894 rows=2577 loops=1
                            Buffers: shared hit=2480
                          Worker 1:  actual time=0.179..71.465 rows=5076 loops=1
                            Buffers: shared hit=4869
                          -&gt;  Bitmap Index Scan on table_partition202508_key_idx  (cost=0.00..27079.59 rows=2466994 width=0) (actual time=2.156..2.156 rows=13076 loops=1)
                                Index Cond: (csh.key = ANY ('{string1,string2}'::text[]))
                                Buffers: shared hit=19
                    -&gt;  Parallel Bitmap Heap Scan on table_partition202508 csh_1  (cost=13868.17..3251371.12 rows=513957 width=109) (actual time=0.288..0.288 rows=0 loops=1)
                          Output: csh_1.key,  csh_1.ts
                          Recheck Cond: (csh_1.key = 'rubbish'::text)
                          Buffers: shared hit=4
                          -&gt;  Bitmap Index Scan on table_partition202508_key_idx  (cost=0.00..13559.80 rows=1233497 width=0) (actual time=0.038..0.039 rows=0 loops=1)
                                Index Cond: (csh_1.key = 'rubbish'::text)
                                Buffers: shared hit=4
Settings: effective_cache_size = '9592GB', work_mem = '32MB', search_path = 'public, public, &quot;$user&quot;'
Planning:
  Buffers: shared hit=101
Planning Time: 0.371 ms
Execution Time: 82.582 ms
</code></pre>
<p>You could see the time difference: 275759.240 ms v.s. 82.582 ms ms!</p>
<p>Is it possible to convince Postgres to use the second execution plan w/o &quot;magic&quot; unions?</p>
",0,2,2,2025-11-05T22:19:15+00:00,1,216,True
79810742,409976,,postgresql,Understanding Debezium Snapshot,"<p>Debezium’s PostgreSQL <a href=""https://debezium.io/documentation/reference/stable/connectors/postgresql.html"" rel=""nofollow noreferrer"">snapshot docs</a> note :</p>
<blockquote>
<p>always
The connector performs a snapshot every time that it starts. The snapshot includes the structure and data of the captured tables. Specify this value to populate topics with a complete representation of the data from the captured tables every time that the connector starts. After the snapshot completes, the connector begins to stream event records for subsequent database changes.</p>
</blockquote>
<p>Let’s say the following happens:</p>
<ol>
<li>Snapshot begins.</li>
<li>New row is inserted</li>
<li>Snapshot ends</li>
<li>Stream from latest</li>
</ol>
<p>Will row 2’s message still be handled by Debezium or it may be skipped?</p>
",1,1,0,2025-11-06T00:49:28+00:00,1,96,True
79811486,1413691,,postgresql,How do I fix the error psql: /lib64/libpq.so.5: no version information available (required by psql)?,"<p>I'm working on a Zabbix installation on Centos 8 Stream, and have run into a situation where a failed installation of postgresql is interfering with a new installation. I am getting the error psql: /lib64/libpq.so.5: no version information available (required by psql) after installation when I go to use postgre to set up users and initialize.</p>
<p>I have a few different postgre options  available in the appstream, and have changed the default version to no avail.</p>
<p>I have looked at various solutions on SO and DBA such as:</p>
<p><a href=""https://stackoverflow.com/questions/67476407/how-do-i-fix-this-error-psql-usr-pgsql-13-lib-libpq-so-5-no-version-informat"">how do I fix this error: psql: /usr/pgsql-13/lib/libpq.so.5: no version information available (required by psql)?</a></p>
<p>and</p>
<p><a href=""https://dba.stackexchange.com/questions/269473/what-does-psql-usr-pgsql-11-lib-libpq-so-5-no-version-information-available"">https://dba.stackexchange.com/questions/269473/what-does-psql-usr-pgsql-11-lib-libpq-so-5-no-version-information-available</a></p>
<p>How can I fix this error?</p>
",2,2,0,2025-11-06T15:42:02+00:00,1,124,True
79811492,1375442,"Dublin, Ireland",postgresql,DBT Postgres. Locally run postgress Adapter error,"<p>I’m about to run initial dbt setup across a local instance of Postgres 18.</p>
<p>Having quite a basic <code>dbt init</code> structure with one test table CARS.</p>
<p>Doing <code>dbt seed</code> witch csv like:</p>
<pre class=""lang-sql prettyprint-override""><code>brand,model,year
audi,q3,2014
</code></pre>
<p>Then <code>dbt run</code> gives me this (weird):</p>
<pre><code>warning: dbt0255: Cannot register databases or schemas in the remote.
Adapter ‘postgres’ does not support metadata operations. error:
dbt1000: Failed to receive render result for
model.jaffle_shop.bronze_cars
 
Execution Summary
Finished ‘run’ with 2 warnings and 1 error for target ‘dev’ [648ms]
Processed: 1 model Summary: 1 total | 1 error
</code></pre>
<p>I’m quite sure I have all correct in my Postgres installation and settings in my <code>profile.yml</code>.</p>
<p>My model SQL is simple as:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM {{ ref(‘raw_cars’) }}
</code></pre>
<p>What is the correct way to refer to or set up schemas for a local Postgres instance? And what metadata is supposed to be in use for the dbt-postgres adapter?</p>
",0,0,0,2025-11-06T15:52:15+00:00,2,102,False
79811688,31829620,,postgresql,How to squash multiple DDL migrations into one script in Flyway Community Edition?,"<p>I’m using <strong>Flyway Community Edition</strong> and want to <strong>squash a long chain of DDL migrations into a single script</strong> to simplify migration history.</p>
<p>I’d like to <strong>remove the old DDL files</strong>, but we already have <strong>multiple environments</strong> that are up to date with the current DB version. We also need to <strong>support those existing environments</strong> while still being able to <strong>spin up new environments</strong> using the squashed DDL script.</p>
<p>Since the Community Edition doesn’t support baseline “B” files, what’s the recommended way to handle this scenario safely?</p>
",0,0,0,2025-11-06T19:15:31+00:00,1,74,False
79812024,2291357,Ga&#239;a,postgresql,copying and deleting a collection of records from one postgresql table to a second table,"<p>Two tables with identical attributes exist:  <code>itemised_origin_rows</code> and the target second table <code>itemised_rows</code> .</p>
<p>They exist because archival requirements are different: based on certain transactions they will remain in the former table or be copied to a second table and deleted from the former.</p>
<p>All attributes are to be transferred, <strong>except</strong> ID, which should be managed autonomously by the DB (another primary key <code>document_id</code> will ensure data coherence).</p>
<p>The following are methods defined, with a fallback to ensure data integrity of number of records.<br />
However, I remain uncertain as to<br />
• whether this approach is sound, and;<br />
• what will happen to the ids in the new table.</p>
<pre class=""lang-rb prettyprint-override""><code># parent method
rows = ItemisedOriginRow.where( document_id: processed_document.id).all
row_ids = rows.pluck(:id)
document_id = processed_document.id
transfer_from_origin(row_ids)
validate_records_quantity(row_ids, document_id, rows)


def transfer_from_origin(row_ids)
  ActiveRecord::Base.connection.execute(&quot;INSERT INTO itemised_rows SELECT * FROM itemised_origin_rows WHERE id IN ?&quot;, row_ids)
end

def validate_records_quantity(row_ids, document_id, rows)
  destination_records = ItemisedRow.where( document_id: document_id ).all.size
  if destination_records == row_ids.size
    delete_from_origin(rows)
  else
    log_transfer_errors(row_ids, document_id, destination_records)
  end
end

def origin_row_transfer_error_logger
  @@origin_row_transfer_error_logger ||= Logger.new(&quot;#{Rails.root}/log/origin_row_transfer_error.log&quot;)
end

def delete_from_origin(rows)
  rows.destroy_all
end

def log_transfer_errors(row_ids, document_id)
  origin_row_transfer_error_logger.info(&quot;#{row_ids} for #{document_id} mismatch.  #{row_ids} sent, #{destination_records} registered&quot;)
end
</code></pre>
",2,0,0,2025-11-07T07:04:23+00:00,2,120,True
79812429,22510933,,postgresql,TypeORM @CreateDateColumn() returns wrong time compared to PostgreSQL,"<p>I’m using TypeORM with PostgreSQL. My entity has:</p>
<pre class=""lang-ts prettyprint-override""><code>@Entity()
export class AuditLog extends BaseEntity {
  @PrimaryGeneratedColumn()
  id: number;

  @CreateDateColumn()
  date_action: Date;
}
</code></pre>
<ul>
<li><p>When I insert a record, PostgreSQL shows the correct time:</p>
<pre><code>2025-11-07 14:42:37.490469
</code></pre>
</li>
<li><p>But when I fetch it with TypeORM:</p>
<pre class=""lang-ts prettyprint-override""><code>const log = await auditLogRepository.findOne({ where: { id: 1 } });
console.log(log.date_action);
</code></pre>
<p>I get:</p>
<pre><code>2025-11-07 13:42:37.490469
</code></pre>
</li>
</ul>
<p>My database timezone and also nodejs timezone is <code>Africa/Casablanca</code>, and  date_action have type of <code>timestamp</code> in table. I want the fetched date to <strong>match exactly what’s stored in PostgreSQL</strong>, without a one-hour shift.</p>
<p>This query <code>select name, setting, source from pg_settings where name = 'TimeZone'</code> return <code>Africa/Casablanca</code> as a result;</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Why does TypeORM / Node.js shift the time when fetching <code>@CreateDateColumn()</code>?</li>
<li>How can I make TypeORM return <strong>exactly the same timestamp</strong> as stored in PostgreSQL?</li>
</ol>
",1,1,0,2025-11-07T14:08:39+00:00,0,72,False
79812481,9625918,,postgresql,How to get a deletion trigger to fire on a logical replica when a record is deleted on the source db,"<p>Due to performance concerns by the system devs and internal politics we cannot add triggers to the prod tables to capture deletions (long story, we need to to implement pseudo-CDC to a third party system). Updates and Inserts we're covering by changes to write_date.</p>
<p>We therefore have a new logical replica that is read/write.</p>
<p>On this replica I have added a deletion trigger to one table (lots more to add once it's working):</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION public.capture_deletions() 
  RETURNS trigger LANGUAGE 'plpgsql' COST 100 
  VOLATILE NOT LEAKPROOF AS 
$BODY$ 
begin 
insert into bi_deleted_records(table_name, alt_table_name, id, deletion_timestamp) 
values (TG_TABLE_NAME::regclass::text, TG_TABLE_NAME, OLD.id, current_timestamp); 
return new; 
end; 
$BODY$; 

ALTER FUNCTION public.capture_deletions() OWNER TO XXXX;

CREATE OR REPLACE TRIGGER table_name_deletions 
  AFTER DELETE ON public.table_name 
  FOR EACH ROW EXECUTE FUNCTION public.capture_deletions(); 
ALTER TABLE public.table_name 
  ENABLE ALWAYS TRIGGER table_name_deletions;
</code></pre>
<p>It captures the change when I delete a record directly from the replicated table, but we need it to fire when a record is deleted from the source table.</p>
<p>We have tried changing the <code>session_replication_role</code> along with:</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE table_name ENABLE ALWAYS TRIGGER trigger_name;
</code></pre>
<p>but we're seeing the same results - captures deletions made on the replica but not when the deletion is made on the source.</p>
<p>Is there a simple solution?</p>
",0,0,0,2025-11-07T14:57:11+00:00,0,79,False
79812747,5072692,"Boston, MA, USA",postgresql,How to apply value for a date series based on effective date in postgres,"<p>I have these two tables. Table 1 has item and the bucket it's supposed be in and the ratio and the effective date/start date. Table two has the value of the items for a date series. I want to join the two tables based on item and then apply the ratio and bucket based on effective date. I tried a few approaches but haven't been able to produce anything promising to share.</p>
<p><strong>Table1:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Item</th>
<th>Bucket</th>
<th>Effective Date</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-01</td>
<td>1</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-03</td>
<td>0.7</td>
</tr>
<tr>
<td>First</td>
<td>B</td>
<td>2025-01-03</td>
<td>0.3</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-05</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p><strong>Table2:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Item</th>
<th>Date</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>First</td>
<td>2025-01-01</td>
<td>100</td>
</tr>
<tr>
<td>First</td>
<td>2025-01-02</td>
<td>101</td>
</tr>
<tr>
<td>First</td>
<td>2025-01-03</td>
<td>102</td>
</tr>
<tr>
<td>First</td>
<td>2025-01-04</td>
<td>103</td>
</tr>
<tr>
<td>First</td>
<td>2025-01-05</td>
<td>104</td>
</tr>
<tr>
<td>First</td>
<td>2025-01-06</td>
<td>105</td>
</tr>
</tbody>
</table></div>
<p><strong>Result:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Item</th>
<th>Bucket</th>
<th>Date</th>
<th>Value</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-01</td>
<td>100</td>
<td>1</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-02</td>
<td>101</td>
<td>1</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-03</td>
<td>102</td>
<td>0.7</td>
</tr>
<tr>
<td>First</td>
<td>B</td>
<td>2025-01-03</td>
<td>102</td>
<td>0.3</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-04</td>
<td>103</td>
<td>0.7</td>
</tr>
<tr>
<td>First</td>
<td>B</td>
<td>2025-01-04</td>
<td>103</td>
<td>0.3</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-05</td>
<td>104</td>
<td>1</td>
</tr>
<tr>
<td>First</td>
<td>A</td>
<td>2025-01-06</td>
<td>105</td>
<td>1</td>
</tr>
</tbody>
</table></div>
",0,0,0,2025-11-07T19:59:10+00:00,3,49,True
79812842,31836437,,postgresql,UPDATE with LEFT JOIN and condition IS NULL,"<p>I have the following table definitions:</p>
<p>Table <code>public.messages</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column</th>
<th>Type</th>
<th>Collation</th>
<th>Nullable</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>ip</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>msg</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ignore</td>
<td>boolean</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Table <code>public.host</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Column</th>
<th>Type</th>
<th>Collation</th>
<th>Nullable</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>ip</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>text</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Yes, I could set type as IP for the ip, but for this example it doesn't matter.</p>
<p>I run this query:</p>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip;
</code></pre>
<p>And get this result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>Test1</td>
<td></td>
<td>1.1.1.1</td>
<td>host1</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Sample data can be found here: <a href=""https://dbfiddle.uk/3LTo9shO"" rel=""nofollow noreferrer"">https://dbfiddle.uk/3LTo9shO</a></p>
<p>I want to update all rows where there is no host name to set ignore to true.</p>
<p>So basically, just the ignore column for this result:</p>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip 
WHERE host.name IS NULL;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I can do an update for the row that <strong>does</strong> have a host name. When I set the value, everything works:</p>
<pre><code>UPDATE messages 
SET ignore = FALSE 
FROM host 
WHERE messages.ip = host.ip AND host.name IS NOT NULL;
</code></pre>
<blockquote>
<p>UPDATE 1</p>
</blockquote>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>Test1</td>
<td>f</td>
<td>1.1.1.1</td>
<td>host1</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>However, setting true for those that don't have a host name doesn't work.</p>
<pre><code>UPDATE messages 
SET ignore = TRUE 
FROM host 
WHERE messages.ip = host.ip AND host.name IS NULL;
</code></pre>
<blockquote>
<p>UPDATE 0</p>
</blockquote>
<p>I tried with a <code>LEFT JOIN</code>, but then <em>everything</em> was updated:</p>
<pre><code>UPDATE messages 
SET ignore = TRUE 
FROM messages m 
LEFT JOIN host h ON m.ip = h.ip 
WHERE h.name IS NULL;
</code></pre>
<blockquote>
<p>UPDATE 2</p>
</blockquote>
<pre><code>SELECT * 
FROM messages 
LEFT JOIN host ON messages.ip = host.ip;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ip</th>
<th>msg</th>
<th>ignore</th>
<th>ip</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.1</td>
<td>Test1</td>
<td>t</td>
<td>1.1.1.1</td>
<td>host1</td>
</tr>
<tr>
<td>1.1.1.2</td>
<td>Test2</td>
<td>t</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p><a href=""https://dbfiddle.uk/fOJ55JBM"" rel=""nofollow noreferrer"">https://dbfiddle.uk/fOJ55JBM</a></p>
<p>How can I update just rows without a host name?</p>
",5,5,0,2025-11-07T22:39:37+00:00,4,227,True
79813060,26676608,,postgresql,Property &#39;VECTOR&#39; does not exist on &#39;DataTypes&#39; in Sequelize with pgvector,"<p>I am building a RAG application using Node.js, Sequelize, and PostgreSQL with the <code>pgvector</code> extension. I have successfully enabled the <code>vector</code> extension in my database.</p>
<p>I am now trying to create a Sequelize model for my <code>knowledge_base</code> table, but I'm getting a TypeScript error when I try to use <code>DataTypes.VECTOR</code>.</p>
<p>What I'm trying to do :</p>
<p>I want to define a <code>KnowledgeBase</code> model that includes an <code>embedding</code> field of type <code>vector(768)</code>.</p>
<h4><strong>What I have done</strong></h4>
<h4>I have followed the <code>pgvector/sequelize</code> documentation:</h4>
<ol>
<li><p>Installed <code>pgvector</code>: <code>npm install pgvector</code></p>
</li>
<li><p>I am registering the types in my main database connection file <em>before</em> any models are defined.</p>
</li>
</ol>
<p>This is my db.config.ts file :</p>
<pre class=""lang-js prettyprint-override""><code>const sequelize = new Sequelize(process.env.DATABASE_URL as string, {
  dialect: &quot;postgres&quot;,
  logging: false,
  dialectOptions: {
    ssl: getSslConfig(),
  },
});

const initialize = async () =&gt; {
  try {
    // Create vector extension if it doesn't exist
    await sequelize.query(&quot;CREATE EXTENSION IF NOT EXISTS vector&quot;);

    pgvector.registerTypes(Sequelize);

    console.log(&quot;Vector extension and type registered successfully&quot;);
  } catch (error) {
    console.error(&quot;Error initializing vector support:&quot;, error);
  }
};

// Initialize vector support
initialize();

export default sequelize;
</code></pre>
<p>This is my KnowledgeBase.model.ts file :</p>
<pre><code>KnowledgeBase.init(
 {
   id: {
    type: DataTypes.UUID,
    defaultValue: DataTypes.UUIDV4,
    primaryKey: true,
   },
  courseId: {
    type: DataTypes.UUID,
    allowNull: false,
    references: {
      model: Course,
      key: &quot;id&quot;,
      },
    },
    embedding: {
      type: DataTypes.VECTOR(786),
      allowNull: false,
    },
    content: {
      type: DataTypes.TEXT,
      allowNull: false,
    },....
</code></pre>
<p>When I define the <code>embedding</code> field, TypeScript gives me the following error:</p>
<blockquote>
<p><strong>Property 'VECTOR' does not exist on type 'typeof import(&quot;sequelize/types/data-types&quot;)'. ts(2551)</strong></p>
</blockquote>
<p>It seems like TypeScript doesn't know about the <code>VECTOR</code> type, even though I've called <code>pgvector.registerTypes(sequelize)</code>.</p>
<p>Thanks in Advance !</p>
",0,0,0,2025-11-08T08:50:18+00:00,1,83,False
79814599,31749517,,postgresql,How to efficiently calculate an exponential moving average in postgres?,"<p>I'm trying to calculate the <a href=""https://en.wikipedia.org/wiki/Average_true_range#:%7E:text=%5B4%5D-,Calculation,-%5Bedit%5D"" rel=""nofollow noreferrer"">average true range</a> on some time series dataset stored in postgres. Its calculation requires a 14 period exponential moving average of true range which based on the <a href=""https://stackoverflow.com/questions/60024643/sql-calculate-exponential-moving-average-with-ctes-or-aggregates"">answer</a> here is obtained using:</p>
<pre><code>with recursive p as (
    select *,
           greatest(
               high - low,
               abs(high - lag(close) over (order by timestamp)),
               abs(low - lag(close) over (order by timestamp))
           ) as true_range,
           row_number() over (order by timestamp) as seqnum
    from core_price
    where timestamp &gt; 1751329927
      and symbol_id = 1
    limit 20000
),
cte as (
    select seqnum,
           high,
           low,
           close,
           timestamp,
           true_range,
           true_range::float as average_true_range
    from p
    where seqnum = 1

    union all

    select p.seqnum,
           p.high,
           p.low,
           p.close,
           p.timestamp,
           p.true_range,
           (cte.average_true_range * 13.0 / 14 + p.true_range / 14) as average_true_range
    from cte
    join p on p.seqnum = cte.seqnum + 1
)
select *
from cte
order by seqnum;
</code></pre>
<p>It calculates the correct values however, it's very slow (takes 3s on m4 max mbp for 20,000 rows) and there's millions of rows so it's not by any means efficient or acceptable. The same calculation takes milliseconds in <code>pandas</code> using <code>df.ewm</code> for the entire period of 7M rows. How can it be optimized?</p>
<p>One possible way of optimization is to replace <code>ema</code> with <code>sma</code> which can be calculated quickly in postgres without recursion, using window functions but it doesn't calculate the standard ATR and is less sensitive to recent changes in price, so I'm not interested in that approach.</p>
",1,3,2,2025-11-09T04:53:32+00:00,2,182,True
79814928,14673556,,postgresql,Django ORM: Add integer days to a DateField to annotate next_service and filter it (PostgreSQL),"<p>I am trying to annotate a queryset with <code>next_service = last_service + verification_periodicity_in_days</code> and then filter by that date. I am on Django 5.2.6 with PostgreSQL. <code>last_service</code> is a <code>DateField</code>. <code>verification_periodicity</code> lives on a related <code>SubCategory</code> and is the number of days (integer).</p>
<p><strong>Models (minimal):</strong></p>
<pre class=""lang-py prettyprint-override""><code># main/models.py

class Category(models.Model):
    name = models.CharField(max_length=100)

class SubCategory(models.Model):
    category = models.ForeignKey(Category, on_delete=models.CASCADE, related_name='subcategories')
    name = models.CharField(max_length=100)
    verification_periodicity = models.IntegerField()  # days

class Asset(models.Model):
    sub_category = models.ForeignKey(SubCategory, on_delete=models.PROTECT)
    last_service = models.DateField(null=True, blank=True)
</code></pre>
<p><strong>Goal:</strong>
Compute <code>next_service = last_service + verification_periodicity days</code> in the database, expose it in the API, and support filtering like <code>?next_date__gte=2025-12-06</code>.</p>
<p><strong>What I tried:</strong></p>
<ol>
<li>Simple cast and multiply:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from django.db.models import ExpressionWrapper, F, DateField, IntegerField
from django.db.models.functions import Cast

qs = qs.annotate(
    next_service = ExpressionWrapper(
        F('last_service') + Cast(F('sub_category__verification_periodicity'), IntegerField()) * 1,
        output_field=DateField()
    )
)
</code></pre>
<p>This does not shift by days and later caused type issues. Filtering by the annotated date also did not work as expected.</p>
<ol start=""2"">
<li>Using a Python <code>timedelta</code>:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from datetime import timedelta

qs = qs.annotate(
    next_service = F('last_service') + timedelta(days=1) * F('sub_category__verification_periodicity')
)
</code></pre>
<p>This produced a duration in seconds in the serialized output. Example: <code>&quot;next_service&quot;: &quot;86400.0&quot;</code> for one day, rather than a proper date. I need a date.</p>
<p><strong>Errors seen along the way:</strong></p>
<ul>
<li><code>ProgrammingError: operator does not exist: interval * interval</code> when I ended up multiplying two intervals in SQL.</li>
<li><code>&quot;next_service&quot;: &quot;86400.0&quot;</code> which suggests I created a duration rather than a date.</li>
</ul>
<p><strong>Environment:</strong></p>
<ul>
<li>Django 5.2.6</li>
<li>Python 3.11.5</li>
<li>PostgreSQL (AWS RDS and local)</li>
<li>URL example that should work:
<code>GET /api/main/assets/?next_date__gte=2025-12-06</code></li>
</ul>
<p><strong>Questions:</strong></p>
<ol>
<li>What is the correct, idiomatic Django ORM expression to compute <code>last_service + verification_periodicity days</code> as a <strong>date</strong> on PostgreSQL?</li>
<li><strong>How should</strong> I define <code>output_field</code> so the annotation is a date, not a duration?</li>
<li>What is the proper way to filter by this annotated date in the same query, for example <code>next_service__gte=&lt;some date&gt;</code>?</li>
</ol>
",0,0,0,2025-11-09T17:19:21+00:00,0,76,False
79815133,43615,"Munich, Bavaria, Germany",postgresql,How to avoid &quot;ON CONFLICT DO UPDATE command cannot affect row a second time&quot; error in WITH statement,"<p>I have two tables: <a href=""https://dbfiddle.uk/q5pH246i"" rel=""nofollow noreferrer""><sub>demo at db&lt;&gt;fiddle</sub></a></p>
<ol>
<li>Table <code>keywords</code> has two columns <code>id</code> and <code>v</code> (which hold the keyword's value)
<pre class=""lang-sql prettyprint-override""><code>create table keywords(
  id int generated always as identity primary key
 ,v text unique);
</code></pre>
</li>
<li>Table <code>main</code> has, among others, two columns that are each a foreign key into the <code>keywords</code> table's <code>id</code>, named <code>key1_id</code> and <code>key2_id</code>
<pre class=""lang-sql prettyprint-override""><code>create table main(
  id int generated always as identity primary key
 ,key1_id int references keywords(id)
 ,key2_id int references keywords(id));
</code></pre>
</li>
</ol>
<p>Now, I want to insert pairs of keywords (key1 and key2 as <code>$1</code> and <code>$2</code>) into the main table, like this:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH key1 AS (INSERT INTO keywords (v) 
              VALUES ($1) 
              ON CONFLICT (v) DO UPDATE
              SET v = EXCLUDED.v
              RETURNING id),
     key2 AS (INSERT INTO keywords (v) 
              VALUES ($2) 
              ON CONFLICT (v) DO UPDATE
              SET v = EXCLUDED.v
              RETURNING id)
INSERT INTO main (key1_id, key2_id)
SELECT key1.id, key2.id
FROM key1, key2
RETURNING id
</code></pre>
<p>Basically, the two keys often have recurring values, so I use the keywords table to keep a unique set of them (mostly for storage optimization, as I have millions of rows with them).</p>
<p>But if <code>$1</code> and <code>$2</code> have identical values, I get this error, whereas there's no issue if they're different:</p>
<blockquote>
<p>pg_query_params(): Query failed: ERROR:  ON CONFLICT DO UPDATE command cannot affect row a second time<br />
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.</p>
</blockquote>
<p>The goal is that both <code>key1_id</code> and <code>key2_id</code> point to the correct row in <code>keywords</code> based on the values passed as <code>$1</code> and <code>$2</code>, even if they're both the same.</p>
<p>How do I modify the SQL statement (ideally, it should remain a single one) so that I can insert these keys without getting the error?</p>
<p>I am using Postgresql 16.9.</p>
",2,2,0,2025-11-10T02:02:26+00:00,2,229,True
79816071,3903479,Montana,postgresql,Extract jsonb value with square bracket notation,"<p>I've got a jsonb column in my table that I can access through <code>-&gt;&gt;'key'</code> or <code>['key']</code>. When I use the former it returns the unquoted value, but when I use the latter it behaves like <code>-&gt;</code> and returns a <code>&quot;quoted&quot;</code> value:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH values AS (
  SELECT '{&quot;foo&quot;: &quot;bar&quot;}'::jsonb, 
  ('{&quot;foo&quot;: &quot;bar&quot;}'::jsonb)-&gt;&gt;'foo' val1, 
  (('{&quot;foo&quot;: &quot;bar&quot;}'::jsonb)['foo'])::text val2,
  trim(both '&quot;' from (('{&quot;foo&quot;: &quot;bar&quot;}'::jsonb)['foo'])::text) val3
)
SELECT *, 
  val1 = 'bar' check1, 
  val2 = 'bar' check2, 
  val3 = 'bar' check3
FROM values
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>jsonb</th>
<th>val</th>
<th>val2</th>
<th>val3</th>
<th>check1</th>
<th>check2</th>
<th>check3</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>{&quot;foo&quot;: &quot;bar&quot;}</code></td>
<td>bar</td>
<td>&quot;bar&quot;</td>
<td>bar</td>
<td>1</td>
<td></td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>Is there any shorthand to get the value from the jsonb column while using square brackets without having to trim the quotes from the value? I think it might have something to do with converting val2 from jsonb with <code>::text</code>, but am not sure</p>
",2,2,0,2025-11-10T21:39:19+00:00,2,112,True
79816352,17783052,,postgresql,AWS How to identify/recommend the instance type for an RDS?,"<p>I am currently working on recommendation for aws rds instance type recommendation. We have identified the instances which are underutilised.</p>
<p>I want to know how to recommend  a better version of rds instance type? I'm looking for the logic to find the appropriate  instance type</p>
<p>Thanks in advance</p>
",1,0,0,2025-11-11T07:38:40+00:00,2,77,True
79817729,14616522,,postgresql,Slow TimescaleDB lookup on indexed column (event_key) without a time filter,"<p>I'm using a self-hosted TimescaleDB instance to store logs. I have a hypertable <code>logs</code> that is partitioned by the <code>timestamp</code> column.</p>
<p>I need to perform a fast lookup query based on the <code>event_key</code> column, which is <code>bytea</code> and has an index. However, the query is extremely slow, and it appears to be scanning every chunk.</p>
<h4>Explain sample output</h4>
<pre><code>Append (cost=0.04..176086370.50 rows=11693899004 width=31)
  -&gt;  Custom Scan (ColumnarScan) on _hyper_3_130_chunk (cost=0.04..1229478.19 rows=122519000 width=32)
          Filter: (event_key = '\x30626538...')
          -&gt;  Seq Scan on compress_hyper_4_198_chunk (cost=0.00..4288.19 rows=122519 width=180)
Custom Scan (ColumnarScan) on _hyper_3_177_chunk (cost=0.14..626711.51 rows=61800000 width=32)
          Filter: (event_key = '\x30626538...')
          -&gt;  Seq Scan on compress_hyper_4_323_chunk (cost=0.00..8711.51 rows=61800 width=180)
                Filter: _timescaledb_functions.bloom1_contains(_ts_meta_v2_bloom1_event_key, '\x30626538...'::bytea)
</code></pre>
<h4>Explain Analyze Output With Time Range</h4>
<pre><code>[
  {
    &quot;QUERY PLAN&quot;: &quot;Index Scan using _hyper_3_371_chunk_idx_logs_event_key_timestamp on _hyper_3_371_chunk  (cost=0.70..2.92 rows=1 width=32) (actual time=0.027..0.028 rows=1 loops=1)&quot;
  },
  {
    &quot;QUERY PLAN&quot;: &quot;  Index Cond: ((event_key = '\\x30626538383634353639373266653330633439336237623166363132343433333435363664326161363231316463663031343138336561323531333736636265'::bytea) AND (\&quot;timestamp\&quot; &gt;= '2025-11-10 00:00:00+00'::timestamp with time zone) AND (\&quot;timestamp\&quot; &lt; '2025-11-11 00:00:00+00'::timestamp with time zone))&quot;
  },
  {
    &quot;QUERY PLAN&quot;: &quot;  Buffers: shared hit=6&quot;
  },
  {
    &quot;QUERY PLAN&quot;: &quot;Planning:&quot;
  },
  {
    &quot;QUERY PLAN&quot;: &quot;  Buffers: shared hit=28&quot;
  },
  {
    &quot;QUERY PLAN&quot;: &quot;Planning Time: 0.404 ms&quot;
  },
  {
    &quot;QUERY PLAN&quot;: &quot;Execution Time: 0.039 ms&quot;
  }
]
</code></pre>
<h3><strong>The Question</strong></h3>
<p>For this specific use case, <strong>my application cannot provide a time filter.</strong> I need to be able to look up an <code>event_key</code> quickly across the entire dataset.</p>
<p>How can I achieve fast key-based lookups on a TimescaleDB hypertable <em>without</em> providing a time filter? Note that it works fine on a regular postgresql table.</p>
<p>Any suggestions what causes the issue and what I can do?</p>
<p>Example slow query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT agent_name
FROM logs
WHERE event_key = '&lt;event_key&gt;';
</code></pre>
<h3><strong>Configuration</strong></h3>
<p>RAM : 128GB</p>
<p>CPU (32cores)</p>
<p>TimescaleDB version affected :2.22.1</p>
<p>PostgreSQL version used: 16.10</p>
<p>Operating System: Ubuntu 24.04.3 LTS</p>
<h3>Table Schema</h3>
<pre class=""lang-sql prettyprint-override""><code>Table &quot;public.logs&quot;
       Column        |           Type           | Collation | Nullable | Default
-----------------------+--------------------------+-----------+----------+---------
 agent_name            | text                     |           |          |
 agent_id              | text                     |           |          |
 mitreid               | text                     |           |          |
 location              | text                     |           |          |
 predecoder_hostname   | text                     |           |          |
 decoder_name          | text                     |           |          |
 timestamp             | timestamp with time zone |           | not null |
 id                    | numeric(25,10)           |           |          |
 rule_level            | smallint                 |           |          |
 decoded_logs          | jsonb                    |           |          |
 decoded_logs_compress | bytea                    |           |          |
 event_key             | bytea                    |           |          |
Indexes:
    &quot;idx_logs_event_key_timestamp&quot; btree (event_key, &quot;timestamp&quot; DESC) WHERE event_key IS NOT NULL
    &quot;idx_logs_rule_level&quot; btree (rule_level) WHERE rule_level IS NOT NULL
    &quot;idx_logs_timestamp&quot; btree (&quot;timestamp&quot; DESC)
Triggers:
    ts_insert_blocker BEFORE INSERT ON logs FOR EACH ROW EXECUTE FUNCTION _timescaledb_functions.insert_blocker()
Number of child tables: 91 (Use \d+ to list them.)
</code></pre>
",1,1,0,2025-11-12T12:24:53+00:00,1,95,True
79817779,27593330,,postgresql,Spark JDBC reading wrong character encoding from PostgreSQL with server_encoding = SQL_ASCII,"<p>I'm reading data from a PostgreSQL 8.4 database into PySpark using the JDBC connector.
The database's server_encoding is SQL_ASCII.</p>
<p>When I query the table directly in pgAdmin, names like <strong>SÉRGIO</strong> or <strong>AURÉLIO</strong> display correctly.
However, when I load the same data in Spark, I get broken characters such as:</p>
<pre><code>S�RGIO MARTINS DOS SANTOS
</code></pre>
<p>Here’s how I’m connecting:</p>
<pre><code>conn = (spark.read
    .format(&quot;jdbc&quot;)
    .option(&quot;url&quot;, &quot;jdbc:postgresql://host:5432/dbname&quot;)
    .option(&quot;dbtable&quot;, &quot;public.my_table&quot;)
    .option(&quot;user&quot;, user)
    .option(&quot;password&quot;, pw)
    .option(&quot;driver&quot;, &quot;org.postgresql.Driver&quot;)
    .load()
)
</code></pre>
<p>I’ve tried adding:</p>
<pre><code>.option(&quot;sessionInitStatement&quot;, &quot;SET client_encoding TO 'WIN1252'&quot;) # or Latin1
.option(&quot;url&quot;, &quot;jdbc:postgresql://host:5432/dbname?charSet=WIN1252&quot;)
</code></pre>
<p>but the characters are still garbled.</p>
<p>Question:</p>
<p>How can I force Spark (via JDBC) to decode the text correctly when the PostgreSQL server_encoding is SQL_ASCII?</p>
",1,1,0,2025-11-12T13:15:14+00:00,1,76,True
79818073,43615,"Munich, Bavaria, Germany",postgresql,"Grouping rows, and then deleting only a sub range (based on their dates) from each of those groups","<p>I use Postgres on my web server in order to record incoming queries into a table <code>calls2</code>, basically writing a single row each time with lots of repeating information, such as a date field (<code>&quot;when&quot;</code>) of when the recording was made plus lots of other statistical information (ip address, query parameters etc). One field also identifies each event's caller (<code>uid</code>).</p>
<p>Now the table has gotten way too large, and I like to thin it out, by removing all but the oldest and newest (based on the date field, which I called <code>&quot;when&quot;</code>) entry, because that's all I really need.</p>
<p>So, assuming I group the rows by their <code>uid</code> (integer), how do I either select only the oldest and newest row (which could be the same) of all my records? And how do select all the others so that I can delete them all at once, for all <code>uid</code>s?</p>
<p>I've created a <a href=""https://dbfiddle.uk/SZ5849jK"" rel=""nofollow noreferrer"">db fiddle</a> with an example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">id</th>
<th style=""text-align: right;"">uid</th>
<th style=""text-align: left;"">when</th>
<th style=""text-align: left;"">other</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">a1</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-02</td>
<td style=""text-align: left;"">a2</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-03</td>
<td style=""text-align: left;"">a3</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">11</td>
<td style=""text-align: left;"">2010-01-04</td>
<td style=""text-align: left;"">a4</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: right;"">22</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">b1</td>
</tr>
<tr>
<td style=""text-align: right;"">6</td>
<td style=""text-align: right;"">33</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">c1</td>
</tr>
<tr>
<td style=""text-align: right;"">7</td>
<td style=""text-align: right;"">33</td>
<td style=""text-align: left;"">2010-01-02</td>
<td style=""text-align: left;"">c2</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: left;"">2010-01-01</td>
<td style=""text-align: left;"">d1</td>
</tr>
<tr>
<td style=""text-align: right;"">9</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: left;"">2010-01-02</td>
<td style=""text-align: left;"">d2</td>
</tr>
<tr>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">44</td>
<td style=""text-align: left;"">2010-01-03</td>
<td style=""text-align: left;"">d3</td>
</tr>
</tbody>
</table></div>
<p>The goal would be to identify <code>id</code>s <code>2</code>, <code>3</code> and <code>9</code> for deletion.</p>
<p>It would also be helpful if I could be shown how I select the other IDs in a way that they list a single row for each <code>uid</code>, showing the <code>uid</code> plus the values of oldest and latest pairs for <code>&quot;when&quot;</code> and <code>other</code>. Or at least just list even row that has the lowest and highest entry for each <code>uid</code> (i.e. the inverse of what I want to delete).</p>
<p>I need help with this because anything involving grouping or clustering in Psql breaks my head.</p>
",1,1,0,2025-11-12T17:52:00+00:00,5,102,True
79818707,23392679,,postgresql,"Postgresql: server does not support ssl, but it was required","<p>I was learning postgres with flutter, and my app kept crashing when I tried to connect to the database. The database is a postgres db running on a docker container.</p>
<p>Here is the Dockerfile</p>
<pre><code>FROM postgres:18.0-alpine3.22

WORKDIR /app

VOLUME [ &quot;/var/lib/postgresql/data&quot; ]

EXPOSE 5432
</code></pre>
<p>I then tried connecting to the db in a simple cli program instead of my app</p>
<pre><code>import 'package:postgres/postgres.dart';

void connectToDB() async {
  final dbConn = await Connection.open(
    Endpoint(
      host: &quot;localhost&quot;,
      port: 5432,
      username: &quot;cat&quot;,
      password: &quot;meow&quot;,
      database: &quot;auth_app&quot;,
    ),
  );

  final result = await dbConn.execute(&quot;SELECT * FROM users;&quot;);

  print(result);
}

void main(List&lt;String&gt; arguments) {
  connectToDB();
}
</code></pre>
<p>but then I get this</p>
<pre><code>dart run


Building package executable...
Built db_postgres:db_postgres.
Unhandled exception:
Severity.error Server does not support SSL, but it was required (default configuration). To disable secure connections, use `ConnectionSettings(sslMode: SslMode.disable)`.
#0      PgConnectionImplementation._connect (package:postgres/src/v3/connection.dart:372:9)
&lt;asynchronous suspension&gt;
#1      PgConnectionImplementation.connect (package:postgres/src/v3/connection.dart:238:29)
&lt;asynchronous suspension&gt;
#2      connectToDB (file:///home/tamara/Developer/DART/db_postgres/bin/db_postgres.dart:4:18)
&lt;asynchronous suspension&gt;
</code></pre>
<p>Setting <code>sslMode</code> to <code>SslMode.disable</code> works fine but does it matter if the connection is secure, what's at stake and how do people do it in production ?</p>
",0,1,1,2025-11-13T10:04:31+00:00,3,94,True
79819582,17257184,,postgresql,PostgreSQL: connection was terminated due to conflict with recovery on primary node when running SELECT … FOR UPDATE,"<p>I have a PostgreSQL cluster with 1 primary and 1 streaming-replica standby. Occasionally I get the following JDBC exception on the primary node, not the standby:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT \* FROM orders WHERE id IN (?) FOR UPDATE
</code></pre>
<blockquote>
<p><code>FATAL: connection was terminated due to conflict with recovery</code><br />
<code>Detail: User was holding a relation lock for too long.</code><br />
<code>Hint: In a moment, you should be able to reconnect to the database and repeat your command.</code></p>
</blockquote>
<p>The strange part:</p>
<ul>
<li>The query is executed only on the primary</li>
<li>At the time of failure, no user queries were running on the standby</li>
<li><code>max_standby_streaming_delay</code> and <code>max_standby_archive_delay</code> are both set to 30s</li>
</ul>
<p>I also checked the pg-pool logs and confirmed that the exceptions occur when multiple transactions run this query concurrently:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT \* FROM orders WHERE id IN (?) FOR UPDATE;
</code></pre>
<p>There is nothing unusual running on the standby, yet the primary still reports a recovery conflict and terminates the session.</p>
<p>Any suggestion to resolve? Thanks.</p>
",1,1,0,2025-11-14T01:51:04+00:00,2,83,True
79820341,979784,"Atlanta, GA, United States",postgresql,PG::InsufficientPrivilege error using Digital Ocean&#39;s App Platform and a connected dev database for a Rails 8 app,"<p>I have a Rails 8 application using an attached dev database and I am deploying to the Digital Ocean App Platform. I can successfully build the application and when I get to the deploy step I see this error in the logs</p>
<pre class=""lang-none prettyprint-override""><code>Nov 14 18:09:59  ActiveRecord::StatementInvalid: PG::InsufficientPrivilege: ERROR:  permission denied for schema public (ActiveRecord::StatementInvalid)
</code></pre>
<p>I am still trying to get the first successful deploy, so I cannot access the console. I am currently setting the <code>DATABASE_URL</code> to be the connection string given in the database component's settings. It looks like this:</p>
<pre class=""lang-bash prettyprint-override""><code>DATABASE_URL = postgresql://staging-db-name:XXXX_password@app-xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx-do-user-xxxxxxxx-0.m.db.ondigitalocean.com:25060/staging-db-name?sslmode=require
</code></pre>
",-4,0,4,2025-11-14T18:26:01+00:00,1,61,True
79820463,,,postgresql,Compare fingerprints with a set of 10*10^6 other audio fingerprints in postgres,"<p>So I have a function that I used to compare audio fingerprints with a few thousand audio fingerprints stored in a postgresql. What I did basically was:</p>
<pre><code>    def my_function(cur: Cursor, threshold: float = 0.8) -&gt; tuple:

        if not 0.0 &lt;= threshold &lt;= 1.0:
            raise ValueError(&quot;threshold must be between 0.0 and 1.0&quot;)

        # Get fingerprint and duration
        duration, fprint = fingerprint_file(self.file_path, maxlength=AUDIO_MAX_LENGTH)

        cur.execute(&quot;SELECT duration, fprint FROM fingerprints&quot;)
        # Loop through db fingerprints and compare for similarity
        for db_duration, db_fprint in cur:
            # `similarity_score` is between 0.0 and 1.0
            similarity_score = compare_fingerprints(
                (duration, fprint),
                (db_duration, db_fprint),
            )
            if similarity_score &gt;= threshold:
                return 0, 0, 0

        return 1, duration, fprint
</code></pre>
<p>But after amount of fingerprints became more than 100 000, it started taking a considerable amount of time, so I can no longer use this approach. Amount of fingerprints is expected to grow to 10*10^6 entries, so I need an approach that can perform a similarity search under 30 seconds or so. I don't need 100% accurancy and I can loosen it up to 85% (that is, it should show correct results in at least 85% of comparisons).</p>
<p>Fingerprints look like this: `\x4151414263496f34533475495773467a346f3847356...`</p>
<p>Honestly, I have no idea how to make it work properly, I thought about comparing just 1000 random/last entries or using md5 hash (but I don't have access to audio and I can recalculate hash for it, I don't know it makes sense to calculate hash for fingerprints instead). My current approach has O(n) complexity, and I suspect that I can't take less steps without sacrificing accuraccy.</p>
<p>So here goes my question: what are the most optimal ways to compare such amount of fingerprints in under 30 seconds, and, also, what would you recommend to read to build a mental model for understanding this kind of situations?</p>
",0,0,0,2025-11-14T21:54:21+00:00,3,63,True
79820764,3362530,,postgresql,How to retrieve a sub-array from result of array_agg?,"<p>I have a SQL table in postgres 14 that looks something like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>f_key</th>
<th>data1</th>
<th>data2</th>
<th>fit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><code>{'a1', 'a2'}</code></td>
<td><code>null</code></td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td><code>{'b1', 'b2'}</code></td>
<td><code>{'b3'}</code></td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td><code>{'c1', 'c2'}</code></td>
<td><code>null</code></td>
<td>3</td>
</tr>
</tbody>
</table></div>
<p>Note that data1 and data2 are arrays.</p>
<p>I need to query this so that I will get data1 and data2 that have best (highest) fit, but are not null (where possible), grouped by f_key.</p>
<p>So the result would look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>f_key</th>
<th>data1</th>
<th>data2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><code>{'a1', 'a2'}</code></td>
<td><code>{'b3'}</code></td>
</tr>
<tr>
<td>2</td>
<td><code>{'c1', 'c2'}</code></td>
<td><code>null</code></td>
</tr>
</tbody>
</table></div>
<p>My current approach is to use <code>array_agg</code> in this fashion:</p>
<pre class=""lang-sql prettyprint-override""><code>select 
    tt.f_key,
    (array_agg(tt.data1) filter (where tt.data1 is not null))[1] as d1,
    (array_agg(tt.data2) filter (where tt.data2 is not null))[1] as d2
from
   (select * from items order by f_key, fit desc) as tt
group by f_key;
</code></pre>
<p>, but <code>d1</code> and <code>d2</code> return <code>null</code> in this case. What leaves me completely puzzled, is that:</p>
<ul>
<li><code>(array_agg(tt.data1) filter (where tt.data1 is not null)) as d1,</code> returns an array of arrays, as expected</li>
<li><code>(array_agg(tt.data1) filter (where tt.data1 is not null))[1][1] as d1,</code> returns first element of first sub-array, as expected.</li>
</ul>
<p>How do I retrieve the first sub-array from the result of <code>array_agg</code>?</p>
",3,3,0,2025-11-15T11:01:18+00:00,3,164,True
79821332,345657,,postgresql,PowerSync sync rules - functions and aliases,"<p>Can sync rules query Postgres functions and can PowerSync get the name of the table to sync from the AS clause of a query and not just the WHERE clause?</p>
<p>The idea is to use a function to do a more sophisticated query than PS can normally handle, return a table that exactly matches the columns of a real table., and then in the sync rule SQL do something like <code>SELECT * FROM my_function(bucket.project_id) AS &quot;my_real_table_name&quot;</code>.</p>
",-3,0,3,2025-11-16T07:48:40+00:00,1,62,True
79821379,893254,"London, UK",postgresql,Slow database table insert (upload) with Pandas to_sql. What is the fastest method?,"<h1>End Objective</h1>
<ul>
<li><p>Download some data from the internet (about 5 GB in size)</p>
</li>
<li><p>Possibly convert some strings/datetimes</p>
</li>
<li><p>Upload to Postgres database</p>
</li>
</ul>
<hr />
<p>I have written some code which uploads some data to a Postgres database. &quot;Upload&quot; here meaning &quot;replace all existing data in the table and insert new data&quot;. Of course, there is no SQL &quot;upload&quot; statement, so what this really does is run a sequence of <code>INSERT</code> statements.</p>
<p>I have tried several variations, searching for improved performance. The data is read from a csv file. The file size is 5 GB.</p>
<p>The original version of my code took about 90 minutes to complete. That is 5120 MB in 90 minutes, or 57 MB/minute, 1MB/s. The data is on the same host as the database. There is no network connection between the data source and sink, except for the Linux kernel. (Everything is via localhost.)</p>
<p>The first version uses the following code.</p>
<pre class=""lang-py prettyprint-override""><code>df.to_sql(
    name='table',
    schema='schema',
    con=postgres_engine,
    if_exists='replace',
    index=False,
)
</code></pre>
<p>I tried some variations, including adding</p>
<pre class=""lang-py prettyprint-override""><code>chunksize=1000,
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>method='multi',
</code></pre>
<p>I was using <code>psycopg2</code> and <code>psycopg2-binary</code>.</p>
<p>When I enabled <code>method='multi'</code> the process never completed. It took longer than 5 hours, then I killed it.</p>
<p>I did not notice any particular improvement using <code>chunksize</code>. I tried values <code>1000</code>, and <code>100000</code>.</p>
<p>psycopg2 does offer something closer to the form of an &quot;upload&quot; statement in the form of <code>copy_expert</code>. I found this took about 15 minutes to complete.</p>
<pre><code>connection = psycopg2.connect(
    host=postgres_host,
    user=postgres_user,
    password=postgres_password,
    dbname=postgres_database,
    port=5432,
)

buffer = io.StringIO()
df.to_csv(buffer, index=False, header=False)
buffer.seek(0)

cursor = connection.cursor()
cursor.copy_expert(
    'COPY schema.table FROM STDIN WITH (FORMAT csv)',
    buffer,
)
connection.commit()
</code></pre>
<p>A significant improvement but it is only about 5 MB/s. That still seems very slow.</p>
<p>The sequence of operations may seem a bit weird. Firstly a file is read using <code>pandas.read_csv</code>. The data is then written to a <code>StringIO</code> object using <code>DataFrame.to_csv</code>. It is outside of the scope of this question, but I need some method of translating a datetime string from one (weird) format to an ISO format which Postgres/Pandas understands.</p>
<p>The data is sourced from a call to <code>requests</code>. In other words, downloaded from some remote server.</p>
<hr />
<p>Aside: Datetime format</p>
<p>If you wanted to know more detail, the source provides data with dates in the following format, which is not ISO and therefore I assume not readable by Postgres.</p>
<pre><code>format='%Y-%m-%d %H:%M'
</code></pre>
<hr />
<p>My conclusion thus far from trying a few different methods with variations on parameters is that I have so far not been able to achieve better performance than about 5 MB/s.</p>
<p>It seems that there are a large number of possible methods, accounting for all the variations in arguments which can be supplied to each function call.</p>
<p>At the end of the day, all I really need to do is download some data from the internet, and upload it to Postgres, possibly with some string manipulation to account for differences in recognized datetime formats.</p>
<p>What method should I use?</p>
<ul>
<li><p>psycopg (psycopg3) or psycopg2?</p>
</li>
<li><p>sqlalchemy or not?</p>
</li>
<li><p>What packages need to be installed with <code>pip3</code>?</p>
</li>
<li><p>Should I use <code>pandas.to_sql</code> or some other method such as <code>copy_expert</code> or something else?</p>
</li>
</ul>
<hr />
<h1>Solution</h1>
<p>I found the following <code>psycopg3</code> solution works well and gives good performance.</p>
<pre class=""lang-bash prettyprint-override""><code>pip3 install psycopg[binary]
</code></pre>
<p>And yes the above is correct. <code>psycopg2</code> = psycopg2. <code>psycopg</code> = psycopg3, somewhat confusingly.</p>
<p>Example code is provided below. Here I assume that <code>some_data</code> is a <code>bytes</code> object, returned by something such as <code>response = requests.get</code>, <code>some_data = response.content</code>.</p>
<p>If you are sourcing data from some other location, such as a local file, change this accordingly.</p>
<p>Note also that you may be able to obtain improved performance by not using <code>pandas</code> at all. In my particular case, I need to read the data, modify it and then write it back to a <code>StringIO</code> object. However, you may be able to read your data directly to a <code>StringIO</code> object, bypassing at least two expensive operations. (<code>read_csv</code> and <code>to_csv</code>.)</p>
<pre class=""lang-py prettyprint-override""><code>import pandas
import psycopg
import io

postgres_connection_string = f'user={}, password={}, host={}, dbname={}, port={}'

df = pandas.read_csv(io.BytesIO(some_data), header=None)
buffer = io.StringIO()
df.to_csv(buffer, index=False, header=False)
buffer.seek(0)
columns = '(column_1, column_2)'
with psycopg.connect(postgres_connection_string) as connection:
    with connection.cursor() as cursor:
        with cursor.copy(f'COPY some_schema.some_table {columns} FROM STDIN WITH (FORMAT csv)') as copy:
            copy.write(file.read())
    conn.commit()
</code></pre>
<p>I see performance figures of about 51 MB/s for this.</p>
",0,0,0,2025-11-16T09:54:13+00:00,6,127,True
79821391,29071226,,postgresql,DB query execution hanging in golang,"<p>I don’t understand why this is happening. I’m using the Go database package to connect to a PostgreSQL database. I loop through <code>fullmsg</code> (88 messages), but after processing about 27 messages, <code>UpdateSyncId</code> stops returning anything and my code hangs</p>
<p><code>sampleservice.go</code></p>
<pre class=""lang-golang prettyprint-override""><code>func (s *SampleService) SomeFunction(
    ctx context.Context,
    userId uuid.UUID,

) error {

    for _, msg := range fullMsgs {
        /*
        some operations here
    */
    
        syncId := strconv.FormatUint(msg.SyncId, 10)
    if err := s.UserService.UpdateSyncId(ctx, userId, syncId); err != nil {
        return fmt.Errorf(&quot;failed to update syncid: %v&quot;, msg.ID, err)
    }
    }
    return nil
    }
</code></pre>
<p><code>userservice.go:</code></p>
<pre><code>func (s *Userservice) UpdateSyncId(ctx context.Context, userID uuid.UUID, syncId string) error {
    return s.UserRepo.UpdateSyncId(ctx, userID, syncId)
}
</code></pre>
<p><code>userrepo.go</code></p>
<pre><code>
func (r *userRepo) UpdateSyncId(ctx context.Context, userID uuid.UUID, maxHistoryId string) error {
    // The table structure is assumed to be a 'users' table or a dedicated 'sync_state' table.
    // We will assume a 'users' table which has a column for the last processed SyncId.
    query := `
        UPDATE 
            users
        SET 
            sync_id = $1
        WHERE 
            id = $2
    `

    // Execute the update query
    result, err := r.DB.ExecContext(ctx, query, maxHistoryId, userID)
    if err != nil {
        return fmt.Errorf(&quot;failed to execute update for user sync id: %w&quot;, err)
    }

    // Optional: Check if exactly one row was updated
    rowsAffected, err := result.RowsAffected()
    if err != nil {
        return fmt.Errorf(&quot;could not retrieve rows affected after sync id update: %w&quot;, err)
    }

    if rowsAffected == 0 {
        return fmt.Errorf(&quot;theres no user with that id %s&quot;, err)
    }

    return nil
}
</code></pre>
<p>i dont know why it is happening, i've created custom interface for db operation, but i dont think that is the cause.<br />
here's my custom interface implementation for <code>sql/db</code> operations that i use.</p>
<pre class=""lang-golang prettyprint-override""><code>type IPsqlDb interface {
    
QueryRowContext(ctx context.Context, query string, args ...interface{}) *sql.Row
    
QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error)
    

ExecContext(ctx context.Context, query string, args ...interface{}) (sql.Result, error)
    
QueryRow(query string, args ...interface{}) *sql.Row
    
Query(query string, args ...interface{}) (*sql.Rows, error)
    
Exec(query string, args ...interface{}) (sql.Result, error)
}


var (
    _ IPsqlDb = (*sql.DB)(nil)
    _ IPsqlDb = (*sql.Tx)(nil)
)
</code></pre>
",1,1,0,2025-11-16T10:31:52+00:00,0,111,False
79821435,893254,"London, UK",postgresql,Minimal Python program to upload data to Postgres using psygopg3 COPY does not appear to run the COPY operation,"<p>I have written a minimal test example to try and debug some issue I have been having where <code>COPY</code> does not appear to do anything when used to copy data to a postgres database.</p>
<p>This is my Python code:</p>
<pre class=""lang-py prettyprint-override""><code>postgres_connection_string = 'user=postgres password=example host=192.168.x.y dbname=postgres port=5432'

import pandas
df = pandas.read_csv('test_file.csv', header=None)
import psycopg
import io
file = io.StringIO()
df.to_csv(file, index=False, header=False)
file.seek(0)
columns = '(string_column, int_column)'
with psycopg.connect(postgres_connection_string) as conn:
    with conn.cursor() as cur:
        cur.copy(f'COPY example_schema.test_table {columns} FROM STDIN WITH (FORMAT csv)', file)
    conn.commit()
</code></pre>
<p>The contents of <code>test_file.csv</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>$ cat test_file.csv 
hello world,1
goodbye world,2
</code></pre>
<p>The DDL for the Postgres table:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE example_schema.test_table (
    test_table_id serial4 NOT NULL,
    string_column varchar NOT NULL,
    int_column int4 NOT NULL,
    CONSTRAINT test_table_pkey PRIMARY KEY (test_table_id)
);
</code></pre>
<p>When I run this it completes without error, however no data is uploaded to the postgres database.</p>
<p>For the &quot;real&quot; program, the quantity of data to upload is large. (Several gigabytes.) However the final block containing <code>cur.copy</code> completes in much less than 1 second, suggesting that it does not do anything.</p>
<p>Why might this be?</p>
",-1,0,1,2025-11-16T11:49:31+00:00,1,52,True
79821446,3109904,"Vancouver, BC",postgresql,Safe to run &quot;VACUUM FREEZE&quot; on template1 or template0 databases in postgres?,"<p>I run periodic VACUUM FREEZE on my main database to avoid aggressive vacuuming during busy times, wondering if it's safe to run freeze on template databases too before they reach the threshold (200M in my case).</p>
",1,0,0,2025-11-16T12:13:59+00:00,3,50,True
79821519,31888092,,postgresql,Dbeaver error - &#39;invalid column reference&#39;,"<p>I was trying to delete a row from the table by writing the following SQL script in DBeaver</p>
<pre class=""lang-sql prettyprint-override""><code>delete from products where id = 11;
select id from products;
</code></pre>
<p>in the first line id is showing an error - 'invalid column reference'. But in the second line id is fine.</p>
<p>And executing the program, delete is working. Then what's the problem? Why is DBeaver showing the following error/warning?</p>
<p><img src=""https://i.sstatic.net/V02IdHBt.png"" alt=""image"" /></p>
",1,1,0,2025-11-16T14:14:01+00:00,1,213,True
79821823,31890544,,postgresql,EF Core + Npgsql: &quot;column &#39;status&#39; is of type application_status but expression is of type text&quot;,"<p>I've been struggling for two days with a PostgreSQL enum type issue in Entity Framework Core. Despite all my configuration, EF Core keeps trying to send a string (`text`) to an enum column.</p>
<p><strong>The Problem</strong></p>
<p>When calling <code>SaveChangesAsync()</code>, I get this error:</p>
<blockquote>
<p>Npgsql.PostgresException (0x80004005): 42804: column &quot;status&quot; is of type application_status but expression is of type text</p>
</blockquote>
<p>This is the code in my repository:</p>
<pre class=""lang-cs prettyprint-override""><code>public async Task UpdateJobApplicationStatusAndReviewDate(
    JobApplication jobApplication,
    ApplicationStatus status,
    DateTime reviewedAt)
{
    if (jobApplication == null) 
        return;     

    jobApplication.ApplicationStatus = status;
    jobApplication.ReviewedAt = reviewedAt;

    await _context.SaveChangesAsync(); // ← FAILS HERE
}
</code></pre>
<p><strong>Generated SQL</strong> (from EF Core logs)</p>
<pre><code> (Microsoft.EntityFrameworkCore.Database.Command)
  Executed DbCommand (20ms) [Parameters=[@p0='1', @p1='6', @p2='2025-11-17T00:00:00.0000000+03:00' (Nullable = true) (DbType = Date), @p3='1', @p6='1', @p4='approved' (Nullable = false), @p5='2025-11-17T00:00:00.0000000+03:00' (Nullable = true) (DbType = Date)], CommandType='Text', CommandTimeout='30']
  INSERT INTO employees (car_rental_id, client_id, hire_date, position_id)
  VALUES (@p0, @p1, @p2, @p3)
  RETURNING employee_id;
  UPDATE job_applications SET status = @p4, reviewed_at = @p5
  WHERE application_id = @p6;
</code></pre>
<p>It doesn't take into account <code>INSERT INTO</code>; there's no <code>::application_status</code> cast → PostgreSQL rejects it.</p>
<p>Entity class:</p>
<pre class=""lang-cs prettyprint-override""><code>public class JobApplication
{
    public int Id { get; set; }
    public ApplicationStatus ApplicationStatus { get; set; } = ApplicationStatus.pending;
    public DateTime? ReviewedAt { get; set; }
    // ...other props
}
</code></pre>
<p>Enum</p>
<pre class=""lang-cs prettyprint-override""><code>public enum ApplicationStatus
{
    pending,
    approved,
    rejected
}
</code></pre>
<p><strong>EF Core configuration</strong> (<code>IEntityTypeConfiguration&lt;JobApplication&gt;</code>)</p>
<pre class=""lang-cs prettyprint-override""><code>builder.Property(ja =&gt; ja.ApplicationStatus)
       .HasColumnName(&quot;status&quot;)
       .HasColumnType(&quot;application_status&quot;)
       .HasConversion(
              v =&gt; v.ToString(),
              v =&gt; Enum.Parse(v) 
        )  // ← tried with and without, even tried just HasConvertion&lt;string&gt;() 
       .HasDefaultValue(ApplicationStatus.pending)
       .IsRequired();
</code></pre>
<p><code>OnModelCreating</code> method:</p>
<pre class=""lang-cs prettyprint-override""><code>protected override void OnModelCreating(ModelBuilder modelBuilder)
{
    base.OnModelCreating(modelBuilder);
    // Tried both:
    modelBuilder.HasPostgresEnum&lt;ApplicationStatus&gt;();
    modelBuilder.HasPostgresEnum&lt;ApplicationStatus&gt;(name: &quot;application_status&quot;);
    modelBuilder.ApplyConfigurationsFromAssembly(Assembly.GetExecutingAssembly());
}
</code></pre>
<p><strong>PostgreSQL schema</strong></p>
<p>Enum values match C# enum</p>
<pre><code>SELECT enum_range(null::application_status);  

    → { pending, approved, rejected }
</code></pre>
<p>Column in <code>job_applications</code> table in PostgreSQL:</p>
<pre><code>ColumnTypeNullableDefaultstatusapplication_statusnot null'pending '::application_status
</code></pre>
<p>All configs matches with my Postgres database.</p>
<p>Additional note: querying works perfectly — no issues at all</p>
<p>This operation executes smoothly with zero errors:</p>
<pre class=""lang-cs prettyprint-override""><code>public async Task&lt;List&lt;JobApplication&gt;&gt; GetPendingByRentalAsync(int carRentalId)
{
    return await _context.JobApplications
        .Where(ja =&gt; ja.CarRentalId == carRentalId &amp;&amp; ja.ApplicationStatus == ApplicationStatus.pending)
        .ToListAsync(); 
}
</code></pre>
",1,1,0,2025-11-17T00:38:09+00:00,0,62,False
79822122,4934150,,postgresql,"Calculate difference between two values, including those only appearing once within the partition","<p><strong><a href=""https://dbfiddle.uk/Fea5xW31"" rel=""nofollow noreferrer"">DB&lt;&gt;Fiddle</a></strong></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE inventory (
    id SERIAL PRIMARY KEY,
    stock_date DATE,
    product VARCHAR,
    stock_balance INT
);
INSERT INTO inventory
(stock_date, product, stock_balance)VALUES 
('2025-10-01','prod_01','100'),
('2025-10-01','prod_02','500'),
('2025-10-31','prod_01','800'),
('2025-10-31','prod_02','600'),
('2025-10-31','prod_03','700'),
('2025-10-31','prod_04','400');
</code></pre>
<p><strong>Expected Result:</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>product</th>
<th>stock_date</th>
<th>stock_balance</th>
<th>change</th>
</tr>
</thead>
<tbody>
<tr>
<td>prod_01</td>
<td>2025-10-01</td>
<td>100</td>
<td>700</td>
</tr>
<tr>
<td>prod_01</td>
<td>2025-10-31</td>
<td>800</td>
<td>700</td>
</tr>
<tr>
<td>prod_02</td>
<td>2025-10-01</td>
<td>500</td>
<td>100</td>
</tr>
<tr>
<td>prod_02</td>
<td>2025-10-31</td>
<td>600</td>
<td>100</td>
</tr>
<tr>
<td>prod_03</td>
<td>2025-10-01</td>
<td>0</td>
<td>700</td>
</tr>
<tr>
<td>prod_03</td>
<td>2025-10-31</td>
<td>700</td>
<td>700</td>
</tr>
<tr>
<td>prod_04</td>
<td>2025-10-01</td>
<td>0</td>
<td>400</td>
</tr>
<tr>
<td>prod_04</td>
<td>2025-10-31</td>
<td>400</td>
<td>400</td>
</tr>
</tbody>
</table></div>
<p>I want to display for each <code>product</code> in the table the <code>stock_balance</code> on <code>2025-10-01</code> and <code>2025-10-31</code> and calculate the <code>change</code> of the <code>stock_balance</code> between these two dates in a separate column.</p>
<p>So for I have been able to develop this query:</p>
<pre class=""lang-sql prettyprint-override""><code>select t1.product as product
      ,t1.stock_date as stock_date
      ,t1.stock_balance as stock_balance
      ,last_value(t1.stock_balance)over(partition by t1.product 
                                        order by stock_date 
                                        rows between unbounded preceding 
                                                 and unbounded following)
      -first_value(t1.stock_balance)over(partition by t1.product 
                                         order by stock_date
                                         rows between unbounded preceding 
                                                  and unbounded following) as change
from(select product as product
           ,stock_date as stock_date
           ,sum(stock_balance) as stock_balance
     from inventory
     group by 1,2
    ) as t1
group by 1,2,3
order by 1,2,3;
</code></pre>
<p>The query provides the result correctly for <code>prod_01</code> and <code>prod_02</code>. <br>
However, for <code>prod_03</code> and <code>prod_04</code> it is not correct. I assume this is because they appear only on <code>stock_date = 2025-10-31</code>. <br></p>
<p>How do I need to modify the query to also get these products displayed as in the expected results? <br>
<em>(I guess somehow I need to insert an empty row for these products for <code>stock_date = 2025-10-01</code>)</em></p>
",2,2,0,2025-11-17T09:49:41+00:00,3,132,True
79822251,31893715,,postgresql,How to inject dynamic PostgreSQL SET parameters (tenant_id) before every Superset query for multi-tenancy?,"<p>I am integrating a Superset dashboard into my web application and need multi-tenant behavior where every client sees the same dashboard layout but only the data that belongs to them. My backend already uses PostgreSQL RLS policies, and each client has a unique <code>tenant_id</code>.</p>
<p>From what I’ve found, Superset supports adding database-level session parameters using PostgreSQL’s <code>SET</code> command. This would allow me to set a dynamic value—such as <code>SET my.tenant_id = 'A'</code> or <code>'B'</code>—before every query Superset runs. The goal is that when a user logs into my application, I determine their tenant (e.g., <code>'A'</code> or <code>'B'</code>) and then inject:</p>
<pre><code>SET my.tenant_id = 'A';
</code></pre>
<p>or</p>
<pre><code>SET my.tenant_id = 'B';
</code></pre>
<p>before all SQL statements hitting the database.</p>
<p>To do this, I understand that I need to modify <code>superset_config.py</code> to configure <code>SQLALCHEMY_ENGINE_OPTIONS</code> or implement a custom <code>before_execute</code> hook that dynamically sets these session variables each time Superset executes a query. Something like:</p>
<pre><code>SQLALCHEMY_ENGINE_OPTIONS = {
    &quot;execution_options&quot;: {
        &quot;schema_translate_map&quot;: {&quot;tenant_id&quot;: &quot;&lt;dynamic value&gt;&quot;}
    }
}
</code></pre>
<p>or a custom event:</p>
<pre><code>from sqlalchemy import event

@event.listens_for(engine, &quot;before_cursor_execute&quot;)
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.execute(f&quot;SET my.tenant_id = '{tenant_value}'&quot;)
</code></pre>
<p>However, I have not found any complete example or official solution describing how to correctly implement this inside <code>superset_config.py</code>, especially since the tenant value must be determined dynamically per logged-in user.</p>
<p>I am looking for proper guidance or an example of the recommended way to inject dynamic <code>SET</code> statements into every Superset query to support multi-tenant dashboards with PostgreSQL RLS.</p>
",1,1,0,2025-11-17T11:48:14+00:00,0,88,False
79822323,17446939,,postgresql,Environment variable is not seen in schema.prisma file,"<p><strong>prisma/schema.prisma:</strong></p>
<pre><code>generator client {
  provider = &quot;prisma-client&quot;
  output   = &quot;../generated/prisma&quot;
}

datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}

model Flower {
  id        Int      @id @default(autoincrement())
  name      String
  color     String
  price     Float
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt
}
</code></pre>
<p><strong>.env:</strong></p>
<pre><code>DATABASE_URL=&quot;postgresql://postgres:qwerty@localhost:5432/bot_users?schema=public&quot;
</code></pre>
<p><strong>prisma.config.ts:</strong></p>
<pre><code>import path from 'node:path';
import { defineConfig, env } from 'prisma/config';
import 'dotenv/config';

type Env = {
  DATABASE_URL: string;
};

export default defineConfig({
  engine: 'classic',
  datasource: {
    url: env&lt;Env&gt;('DATABASE_URL'),
  },
  schema: path.join('prisma', 'schema.prisma'),
  migrations: {
    path: 'prisma/migrations',
  },
});
</code></pre>
<p>So the .env variable is seen in prisma.config.ts but not in scema.prisma file.</p>
<p>That happened after I regenerated prizma to get rid of commonjs syntax (&quot;type&quot;: &quot;module&quot; in package.json).</p>
",0,0,0,2025-11-17T13:05:29+00:00,0,27,False
79822511,9540123,,postgresql,"Avoid invalid cast inside CASE expression, in a query plan not respecting JOIN","<p>I'm trying to get partitioned tables from a postgres DB. Our partitions have the date in them in various formats which I'm extracting like this</p>
<pre class=""lang-sql prettyprint-override""><code>  SELECT tab.relname AS table_name,
        (CASE
          WHEN relname ~ '(\d{8})$' THEN to_date(right(relname, 8), 'YYYYMMDD')
          WHEN relname ~ '(\d{6})$' THEN to_date(right(relname, 6), 'YYYYMM')
          WHEN relname ~ '(\d{6})_' THEN  to_date(substring(relname from '(\d{6})'), 'YYYYMMDD')
          WHEN relname ~ '(\d{8})_' THEN  to_date(substring(relname from '(\d{8})'), 'YYYYMMDD')
          ELSE NULL
      END)::date AS extracted_date 
    FROM pg_inherits AS inh
    JOIN pg_class AS tab
    ON inh.inhrelid = tab.oid
  WHERE tab.relkind IN ('r','p');
</code></pre>
<p>This works fine. But if I move the CASE expression into the where clause I get an error 'ERROR: date/time field value out of range: &quot;81468193&quot;'</p>
<p>I tried creating a temp table and inserting the data and then querying and it works fine.</p>
<p>I then tried creating a CTE like this</p>
<pre class=""lang-sql prettyprint-override""><code>WITH partitioned_tabs AS
  (
  SELECT
  tab.relname AS table_name
        ,(CASE
          WHEN relname ~ '(\d{8})$' THEN to_date(right(relname, 8), 'YYYYMMDD')
          WHEN relname ~ '(\d{6})$' THEN to_date(right(relname, 6), 'YYYYMM')
          WHEN relname ~ '(\d{6})_' THEN  to_date(substring(relname from '(\d{6})'), 'YYYYMMDD')
          WHEN relname ~ '(\d{8})_' THEN  to_date(substring(relname from '(\d{8})'), 'YYYYMMDD')
          ELSE NULL
      END)::date AS extracted_date 
    FROM pg_inherits AS inh
    JOIN pg_class AS tab
    ON inh.inhrelid = tab.oid
  WHERE tab.relkind IN ('r','p')
  )
SELECT *
FROM partitioned_tabs  AS td 
WHERE td.extracted_date = '2025-01-01';
</code></pre>
<p>I get the same error. There is a table in the DB which matches the pattern in the error, but that table is not partitioned, so if I run this
<code>SELECT * FROM pg_class WHERE relname LIKE '%81468193%';</code> it returns the table but if I run this</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
    FROM pg_inherits AS inh
    JOIN pg_class AS tab
    ON inh.inhrelid = tab.oid
WHERE relname LIKE '%81468193%';
</code></pre>
<p>It returns nothing.
So I got the query plan which is</p>
<pre><code>Gather  (cost=500.43..49454.41 rows=394 width=68)
  Workers Planned: 4
  -&gt;  Nested Loop  (cost=0.42..48950.47 rows=98 width=68)
        -&gt;  Parallel Seq Scan on pg_class tab  (cost=0.00..48760.92 rows=130 width=68)
              Filter: ((relkind = ANY ('{r,p}'::&quot;char&quot;[])) AND (CASE 
                        WHEN (relname ~ '(\d{8})$'::text) THEN to_date((regexp_match((relname)::text, '(\d{8})$'::text))[1], 'YYYYMMDD'::text)
                        WHEN (relname ~ '(\d{6})$'::text) THEN to_date((regexp_match((relname)::text, '(\d{6})$'::text))[1], 'YYYYMM'::text)
                        WHEN (relname ~ '_(\d{6})_'::text) THEN to_date((regexp_match((relname)::text, '_(\d{6})_'::text))[1], 'YYYYMMDD'::text)
                        WHEN (relname ~ '_(\d{8})_'::text) THEN to_date((regexp_match((relname)::text, '_(\d{8})_'::text))[1], 'YYYYMMDD'::text)
                        ELSE NULL::date END = '2025-01-01'::date)
                      )
        -&gt;  Index Only Scan using pg_inherits_relid_seqno_index on pg_inherits inh  (cost=0.42..1.42 rows=1 width=4)
              Index Cond: (inhrelid = tab.oid)
</code></pre>
<p>It seems to me that the parallel worker is trying to query only pg_class and then running into a date error. Is this possible? Am I doing something stupid?
This is postgres 15</p>
",1,1,0,2025-11-17T15:58:48+00:00,1,115,False
79822762,12817776,,postgresql,"Regex function to extract numbers from string-type values, separated by &quot;-&quot;, to get the difference of each two values","<p>I'm 100% new to Regex so I've been floundering about on regex101 trying to figure out how to get my desired output. I am using PostgreSQL to write a query to extract a set of values from the string. Once extracted, I need to convert them to int types and then take the difference between the two values.</p>
<p>A sample of the data I am working with can be found here:<br />
<a href=""https://regex101.com/r/Twkphj/3"" rel=""noreferrer"">https://regex101.com/r/Twkphj/3</a> (Each line break is a new record/value of the data.)
<a href=""https://dbfiddle.uk/ELitHDni"" rel=""noreferrer"">https://dbfiddle.uk/ELitHDni</a></p>
<pre class=""lang-sql prettyprint-override""><code>create table t(id int generated always as identity primary key, data text);
insert into t(data)values
('01-08,24-32')
,('38-70')
,('01-25, 27-38')
,('1-6,13-20,25-32')
,('1-4, 7-8, 11-12')
,('1-83,85-112')
,(NULL)
,('NULL')
,('162-169')
,('145-167, 169-214, 217-218, 247-254, 256-257, 382')
,('01-17, 23-27')
,('73-120, 145-192, 217-264, 289-336, 361-408, 433-480, 505-552, 577-624, 649-696, 721-768')
,('1-33, 37-45');
</code></pre>
<p>The end goal is to get an output like this:</p>
<pre><code>SELECT
data, 
regex(difference of &quot;data&quot; points)
FROM table
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>data</th>
<th>difference (inclusive)</th>
</tr>
</thead>
<tbody>
<tr>
<td>01-08,24-32</td>
<td>8,9</td>
</tr>
<tr>
<td>145-167, 169-214, 217-218, 247-254, 256-257, 382</td>
<td>23, 46, 2, 8, 2, 1</td>
</tr>
</tbody>
</table></div>
<p>From here, I can just use split() if the stakeholder needs to break it down further.</p>
<p>Again I'm not to familiar with regex so regex101 has been great to breakdown and understand why certain &quot;tokens&quot;(?) are used. I think I need to stick to PCRE2 if that matters.</p>
<p>TIA</p>
",5,5,0,2025-11-17T20:44:52+00:00,3,181,True
79822777,13115896,,postgresql,Postgres Recursive Query Behavior,"<p>I have a table <code>test</code> with two columns, each referencing the other</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID1</th>
<th>ID2</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>4</td>
</tr>
</tbody>
</table></div>
<p>Given an <code>id</code>, I need to traverse the chain to return all the rows traversing the chain. Below is the query I have written:</p>
<pre><code>WITH RECURSIVE 
-- backward 
    backward_loop as (
    SELECT id1, id2 
    FROM test t
    WHERE test.id2 = '&lt;id&gt;'
    UNION
    SELECT id1, id2 
    FROM
        test t
        INNER JOIN backward_loop b ON (b.id1 = t.id2)
    ),
     -- forward 
    forward_loop as (
    SELECT id1, id2
    FROM test t
    WHERE test.id1 = '&lt;id&gt;'
    UNION
    SELECT id1, id2
    FROM
        test t
        INNER JOIN forward_loop f ON (f.id2 = t.id1)
)   
SELECT id1, id2
FROM backward_loop bl
UNION ALL
SELECT id1, id2
FROM forward_loop fl;
</code></pre>
<p>E.g. if given 2, I need to return all the rows (2,1), (3,2), (4,3) and (5,4). The query works fine and returns all the rows correctly.</p>
<p>However, when I have the below data, where one of the <code>id</code>s (50) has multiple mappings:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ID1</th>
<th>ID2</th>
</tr>
</thead>
<tbody>
<tr>
<td>20</td>
<td>10</td>
</tr>
<tr>
<td>30</td>
<td>20</td>
</tr>
<tr>
<td>40</td>
<td>30</td>
</tr>
<tr>
<td>50</td>
<td>40</td>
</tr>
<tr>
<td><strong>50</strong></td>
<td><strong>60</strong></td>
</tr>
</tbody>
</table></div>
<p>and when I run the query passing 20, I get only (20,10), (30,20), (40,30) and (50,40), but not the row (50,60). When I send in 50, I get back all the rows, but it's an issue when I input any other id.</p>
<p>Any idea how I can get the row (50,60) as well?</p>
",1,2,1,2025-11-17T21:02:47+00:00,1,121,True
79822878,656388,,postgresql,Creating Materialized view in PG 17 ignores data types in SELECT,"<p>I have the following SQL to create a materialized view and the SQL seems reasonable to me but it's unclear from Postgres docs what should happen in this case.</p>
<p>I am listing the columns when creating the matview and casting to the correct types in the <code>SELECT</code> and all columns work except one, while it's cast to <code>numeric(9, 2)</code> Postgres returns that column as <code>numeric</code> with no precision. The values actually appear to be cast correctly, although the column type is wrong. Here's an excerpt:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE MATERIALIZED VIEW &quot;plus&quot;.&quot;quote_exposure_chars&quot; (... &quot;distance_to_coast_ft&quot;, ...) 
AS 
    SELECT 
        ..., 
        MAX(CASE
                WHEN (qexpcf.field_name = 'distance_to_coast_ft'::text) 
                    THEN (qexpcf.field_value)::numeric(9,2)
                ELSE NULL::numeric
            END) AS ..., 
        distance_to_coast_ft, ...
</code></pre>
<p>Is it something about casting the <em><code>null</code></em> to <code>numeric</code> or not specifying the precision of the new alias column name? The PG doc on matviews seems to gloss over all of this.</p>
",1,2,1,2025-11-17T23:47:16+00:00,1,90,True
79823228,9298910,,postgresql,Failing healthcheck of GitLab CI PostgreSQL service,"<p>I get this error in my pipelines:</p>
<pre class=""lang-none prettyprint-override""><code>Health check container logs:
2025-11-18T09:45:19.064500182Z FATAL: No HOST or PORT found                      
Service container logs:
2025-11-18T09:45:19.061855973Z The files belonging to this database system will be owned by user &quot;postgres&quot;.
2025-11-18T09:45:19.061917474Z This user must also own the server process.
2025-11-18T09:45:19.061925574Z 
2025-11-18T09:45:19.063865280Z The database cluster will be initialized with locale &quot;en_US.utf8&quot;.
2025-11-18T09:45:19.063913380Z The default database encoding has accordingly been set to &quot;UTF8&quot;.
2025-11-18T09:45:19.063924080Z The default text search configuration will be set to &quot;english&quot;.
2025-11-18T09:45:19.063932680Z 
2025-11-18T09:45:19.063940380Z Data page checksums are enabled.
2025-11-18T09:45:19.063947880Z 
2025-11-18T09:45:19.063955780Z fixing permissions on existing directory /var/lib/postgresql/data ... ok
2025-11-18T09:45:19.063967380Z creating subdirectories ... ok
2025-11-18T09:45:19.063978780Z selecting dynamic shared memory implementation ... posix
2025-11-18T09:45:19.146643049Z selecting default &quot;max_connections&quot; ... 100
2025-11-18T09:45:19.172384232Z selecting default &quot;shared_buffers&quot; ... 128MB
2025-11-18T09:45:19.209953854Z selecting default time zone ... Etc/UTC
2025-11-18T09:45:19.211827860Z creating configuration files ... ok
</code></pre>
<p>As you can see, the logs start out by stating that there was an error, but the database still starts up correctly (as shown by the subsequent logs).</p>
<p>I've done a little research and most people seem to run into this issue when they use a custom <code>network_mode</code> setup. However, I am using the default <code>bridge</code> mode.</p>
<p>My best guess is that either GitLab tries the wrong port or the database is still starting when the healthcheck is performed.</p>
<p>Does anyone know how to fix this? It is not an error <em>per se</em> (since the database starts up correctly and the rest of the CI stage runs perfectly fine), but it is still annoying.</p>
",0,0,0,2025-11-18T10:06:30+00:00,0,131,False
79823770,5788795,,postgresql,Using pg_try_advisory_xact_lock for events processing with strict ordering,"<p>Suppose I am implementing a local Event-Driven service with internal events to decouple logic.</p>
<p>For database we use Postgres 17 and PgBouncer with transactional mode. It's just a single instance.</p>
<pre class=""lang-sql prettyprint-override""><code>create table entity(id bigint generated by default as identity primary key);
create table events(
   event_id bigint generated by default as identity primary key
  ,event_key bigint references entity(id)
  ,event_body jsonb
  ,create_date timestamptz);
</code></pre>
<p>Let's suppose my service runs in multiple replicas <strong>(app replicas, not db, db instance is single)</strong> in cloud environment. Each app instance has a poller thread that scans unprocessed events and invokes logic for processing. I want to avoid situation where events with same event_key are processed in a wrong order. Let me give you an example <strong>what I want to avoid</strong>:</p>
<ul>
<li><p>Entity(id='xxx') is created, an event SomeEntityEvent(id='1') is published with (event_key='xxx', create_date='01-01-2026T00:00:00')</p>
</li>
<li><p>The same Entity(id='xxx') is updated and one more instance of SomeEntityEvent(id='2') is published with (event_key='xxx', create_date='01-01-2026T00:00:01')</p>
</li>
<li><p>Poller from Service(instance='1') queries for events and locks(?) them, receives SomeEntityEvent(id='1'). Poller from Service(instance='2') queries for events as well at the same time and receives SomeEntityEvent(id='2')</p>
</li>
<li><p>Now let's suppose Service(instance='2') performs his part of the job much faster and processes his event(id='2') much faster than another instance. This might lead to inconsistent behaviour.</p>
</li>
</ul>
<p>I have read Postgres documentation and I found out about advisory locks, <code>pg_try_advisory_xact_lock()</code> to be more specific.</p>
<p><strong>My question is: Can I implement my service in the following way (with a help of <code>pg_try_advisory_xact_lock()</code>)</strong>:</p>
<ol>
<li><p>Poller scans for events_keys for which unprocessed events exist;</p>
</li>
<li><p>For each unique event_key, in a loop invoke <code>pg_try_advisory_xact_lock(event_key)</code></p>
<ol>
<li><p>if result is <code>false</code>, interrupt this event_key processing</p>
</li>
<li><p>If result is <code>true</code>, select all unprocessed events for this event_key, sort them by create_date and process each one in the correct order.</p>
</li>
</ol>
</li>
</ol>
<p>Of course I can add <code>LIMIT</code> clause and interrupt earlier to avoid long running processing.</p>
<p>A group of events with same event_key must be processed sequentially (by create_date). Events of different groups (with different event_key) can be processed however they want, ordering applies only to a group of events.</p>
<p><strong>I am new to advisory locks and have a relatively small experience as a developer with Postgres, PgBouncer cause a few confusions as well and I really want to now whether this combination of pg_try_advisory_xact_lock and PgBouncer is usable.</strong></p>
<p>Please also help me to understand the performance impact. From what I have read, it will not result in a lot of locks holded in a buffer, but still I may miss some details.</p>
",2,2,0,2025-11-18T19:47:10+00:00,1,113,True
79824820,2183431,Germany,postgresql,Symfony/Doctrine: PostgreSQL Migration Issue – Undefined column error only in HTTP Kernel (CLI works),"<p>Symfony/Doctrine: PostgreSQL Migration Issue – Undefined column error only in HTTP Kernel (CLI works)</p>
<p>I'm working on a <strong>Symfony 7.3</strong> project running on <strong>PHP 8.2</strong>. I recently migrated the database from <strong>MySQL</strong> to <strong>PostgreSQL</strong>, and since then, I've encountered a highly specific issue with Doctrine's ORM that only manifests in the HTTP context.</p>
<h3>The Problem</h3>
<p>When I try to fetch data from the <code>user</code> table via an HTTP request, I receive the following SQL error:</p>
<pre><code>SQLSTATE[42703]: Undefined column: 7
ERROR:  column t0.id does not exist
LINE 1: SELECT t0.id AS id_1, t0.email AS email_2, t0.password AS pa...
</code></pre>
<p>This error is <strong>exclusive</strong> to any code executed within the HTTP Kernel (e.g., Controllers, Listeners).</p>
<h3>Factual Observations (Crucial)</h3>
<h4>1. Command Line Interface (CLI) Works Fine ✔️</h4>
<p>When executing queries directly via the Symfony console (which uses the exact same Doctrine configuration and database connection), everything works perfectly. This confirms the table <code>public.user</code> and the column <code>id</code> exist.</p>
<h4>2. Isolated Debug Controller Fails ❌</h4>
<p>A minimal debug controller fetching the <code>User</code> entity triggers the error.</p>
<h4>3. Working Counter-Example (The Clue) 💡</h4>
<p>A structurally identical controller for the <strong><code>Customer</code></strong> entity (table name <code>customer</code>, which is <strong>not</strong> a reserved PostgreSQL keyword) works without error. This strongly points to a conflict with the reserved keyword <strong><code>user</code></strong>.</p>
<h3>Entity and Database Details</h3>
<h4><strong>User Entity (Abridged)</strong></h4>
<p>The entity uses standard Doctrine annotations:</p>
<pre class=""lang-php prettyprint-override""><code>// src/Entity/User.php
#[ORM\Entity(repositoryClass: 'App\Repository\UserRepository')]
#[ORM\Table(name: 'user')] // Problematic line
class User {
    // ... ORM\Id and ORM\Column definitions
}
</code></pre>
<h4><strong>Database Structure</strong></h4>
<ul>
<li><strong>DBMS:</strong> PostgreSQL</li>
<li><strong>Table Name in DB:</strong> <code>user</code> (lowercase, public schema)</li>
<li><strong>PostgreSQL Fact:</strong> The word <code>user</code> is a reserved SQL keyword.</li>
</ul>
<hr />
<h3>The Temporary Fix &amp; The Core Question ⚠️</h3>
<p>I discovered that the error is resolved by manually <strong>forcing Doctrine to quote</strong> the table name using <strong>backticks</strong> in the Entity mapping:</p>
<pre class=""lang-php prettyprint-override""><code>// Temporary Fix
#[ORM\Table(name: '`user`')]
</code></pre>
<p><strong>While this solution works, I reject it as a permanent fix.</strong> It introduces an unsightly exception into my clean entity code, only addresses this single reserved word, and does not provide a future-proof, global guarantee that Doctrine will correctly handle <strong>other</strong> reserved PostgreSQL keywords (e.g., <code>group</code>, <code>order</code>, etc.) without manual intervention.</p>
<hr />
<h3>The Question</h3>
<p>Based on the confirmed root cause (PostgreSQL reserved keyword <code>user</code> not being reliably quoted in the HTTP context) and the desire for a clean, global solution:</p>
<p><strong>What is the underlying Doctrine DBAL or Symfony configuration setting (e.g., in <code>doctrine.yaml</code>) that must be applied to the PostgreSQL connection to <em>globally</em> enforce the correct quoting behavior for <em>all</em> reserved SQL keywords, thereby allowing the Entity mapping to remain clean (<code>#[ORM\Table(name: 'user')]</code>) and guaranteeing future compatibility with PostgreSQL?</strong></p>
",2,3,1,2025-11-19T18:38:58+00:00,1,94,True
79825199,17965846,,postgresql,Jooq get sequence increment,"<p>Maybe there's an easier way to get the Increment with JOOQ?
This one looks more like a crutch:</p>
<pre><code>int increment =
    Optional.ofNullable(CUSTOMER_ID_SEQ.getIncrementBy())
        .map(Field::getName)
        .map(Integer::valueOf)
        .orElse(1);
</code></pre>
",2,2,0,2025-11-20T07:18:05+00:00,1,49,True
79825480,21314431,,postgresql,Adding CHECK Constraint Without Error and Duplication,"<p>I have implemented this code to add a <code>check</code> constraint after creating the table</p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE recruitment_data.candidate_experiences 
  ADD CONSTRAINT chk_experience_dates 
    CHECK (   experience_end_date IS NULL 
           OR experience_end_date &gt;= experience_start_date);
</code></pre>
<p>but it gives an error when I run it more than once:</p>
<blockquote>
<p><code>ERROR:  constraint &quot;chk_experience_dates&quot; for relation &quot;candidate_experiences&quot; already exists</code></p>
</blockquote>
<p>Since it is impossible to run <code>ADD CONSTRAINT</code> <strong><code>IF NOT EXISTS</code></strong>, is there any other way to run it twice without creating duplicate constraints, and without having to catch and handle or ignore the error?</p>
<p>What should I do to make this code <strong>re</strong>-runnable?</p>
",-2,1,3,2025-11-20T11:52:45+00:00,1,87,True
79825741,1830895,"Munich, Germany",postgresql,PostgreSQL SQLAlchemy index NULLS NOT DISTINCT,"<p>I have a table (TA) on which I want to have a composite unique index on two columns (col1 and col2).</p>
<p>col2 can be NULL.</p>
<p>I know Postgres treats NULLs as distinct.</p>
<p>In SQL I would therefore do:</p>
<pre><code>CREATE UNIQUE INDEX index_name ON TA (col1, col2) NULLS NOT DISTINCT
</code></pre>
<p>How can I achieve the same in a SQLAlchemy model?</p>
<p>Something like:</p>
<pre><code>class TherapeuticArea(Base):
    __tablename__ = &quot;TA&quot;
    __table_args__ = (
        sa.Index(
            &quot;index_name&quot;,
            &quot;col1&quot;,
            &quot;col2&quot;,
            unique=True,
            # postgresql_ option here? Which one? Or..?
        ),
    )
</code></pre>
<p>Note: so far I have added such SQL statement as custom <code>op.execute()</code> to the migration generated by Alembic and it works fine.
But then Alembic keeps removing the index on subsequent migrations.
Therefore I'd like to have it as a plain declaration in the model</p>
",1,1,0,2025-11-20T16:41:19+00:00,1,93,True
79825809,20265378,,postgresql,Self-signed certificate error when forcing SSL connecting to RDS PSQL,"<p>Hoping to get yalls help on the issue I am now sort of desperate about</p>
<p>Context:
I am deploying a docker container on EC2 Ubuntu, where it is used by two images - one for my Prisma-NestJs-based API and another is a side-worker used to send emails, based on Pgboss.
The db is PSQL17, deployed on RDS in an isolated subnet.</p>
<p>Problem: While the API entrypoint connects to the RDS just fine, the Pgboss worker is very reluctant about doing so. I also tested connection via psql in EC2's bash, and it works. There are different errors I've encoutered with pgboss, but the bottom line is the following -</p>
<ul>
<li><p>When I try to use ?sslmode=verify or verify-ca - Error: self-signed certificate in certificate chain</p>
</li>
<li><p>When I do not specify any ssl mode - error: no pg_hba.conf entry for host &quot;10.0.<em>.</em>.*&quot;, user &quot;user&quot;, database &quot;database&quot;, no encryption (keep in mind that it's definitely not a VPC issue, since the other image works but I also quadruple-checked everything)</p>
</li>
</ul>
<p>Additonally, I tried puttin ca-cert related lines to my dockerfile, assuming it may be the root.</p>
<pre><code># 1) Base builder
FROM node:20-alpine AS builder
WORKDIR /app

# Install OS deps for prisma engine + CA certificates
RUN apk add --no-cache openssl ca-certificates

// -- more dockerfile lines -- //


# Install OS dependencies including CA certificates
RUN apk add --no-cache openssl ca-certificates

# Update CA certificates (important!)
RUN update-ca-certificates

...
</code></pre>
<p>Thanks for everyone's input on the issue.</p>
",3,3,0,2025-11-20T17:51:42+00:00,1,290,True
79825835,31916523,,postgresql,Storing store time of day ranges where timezone matters,"<p>I need to store some store hours that exist in different time zones and may be interacted with in different timezones through an admin portal where it will display store time and local time.</p>
<p>I know <code>timetz</code> is terrible to use so I'm trying to get an idea of what the right way to do this is. It seems like one of those edge cases because time isnt meant to be used with timezones and I don't need the <code>date</code> of <code>timestamp</code>.</p>
<ul>
<li>I can store the hours in UTC in <code>time</code> with a <code>timezone</code> column</li>
<li>or should I use <code>timestamptz</code> while ignoring the <code>date</code>, which is
entirely unneeded.</li>
</ul>
<p>Here is my current expectation of the table:</p>
<pre class=""lang-sql prettyprint-override""><code>create table site_operation_hours(
    site_operation_hours_id serial primary key,
    site_id int not null references sites,
    days_id smallint not null references days,
    timezone_id smallint not null references timezones,
    start_time_utc time not null,
    end_time_utc time not null
);
</code></pre>
<p>These will only be listed by day. So <code>Mon 6am-8pm</code>, <code>Tue 8am-4pm</code>. I did hear there may be a case for split hours though where they can close for a period during the day.</p>
",2,2,0,2025-11-20T18:28:17+00:00,1,80,True
79825887,17824494,,postgresql,Why does `.WithInitScripts` not run my SQL files?,"<h2>Description</h2>
<p>I am using a test Postgres container (from <a href=""https://golang.testcontainers.org/modules/postgres/#postgres"" rel=""nofollow noreferrer"">testcontainers</a>) for more accurate integration tests. To prevent repeating code, I want to use my existing SQL migration files to setup the container, but I am encountering this error:</p>
<pre><code>init.go:59: failed to add fixture 2db86489-a1e0-4364-8954-a6215f68b99a: ERROR: relation &quot;public.account&quot; does not exist (SQLSTATE 42P01)
</code></pre>
<h2>Background</h2>
<p>My code structure looks like this:</p>
<pre><code>.
└── database/
    ├── migrations/
    │   ├── 000001_create_account_table.up.sql
    │   └── 000001_create_account_table.down.sql
    └── testcontainers/
        └── init.go
</code></pre>
<p>In <code>init.go</code>, I have a <code>InitTestContainer()</code> func that gets the <code>*.up.sql</code> files like so:</p>
<pre><code>migrationFiles, err := filepath.Glob(&quot;../migrations/*.up.sql&quot;)
</code></pre>
<p>Then I pass it to the test container creation:</p>
<pre><code>cntr, err := postgres.Run(ctx,
    &quot;postgres:17.5-alpine&quot;,
    postgres.WithInitScripts(migrationFiles...),
    postgres.WithDatabase(&quot;test&quot;),
    postgres.WithUsername(&quot;user&quot;),
    postgres.WithPassword(&quot;pass&quot;),
    postgres.WithSQLDriver(&quot;pgx&quot;),
    postgres.BasicWaitStrategies(),
)
</code></pre>
<p>This step doesn't error, but when I run my populate function (below), the error appears.</p>
<pre><code>_, err := conn.Exec(ctx, queries.CreateAccount, &lt;args here&gt;)
</code></pre>
<p>Why is this happening?</p>
<hr />
<h2>Update</h2>
<p>After more debugging, <code>filepath.Glob</code> wasn't returning any files. Now I am trying to point to the folder like this:</p>
<pre><code>migrationDir := filepath.Join(&quot;..&quot;, &quot;migrations&quot;, &quot;000001_create_account_table.up.sql&quot;)
    fmt.Printf(&quot;migrations dir: %s\n&quot;, migrationDir)
</code></pre>
<p>That is producing this log:</p>
<pre><code>migrations dir: ../migrations/000001_create_account_table.up.sql
</code></pre>
<p>And no this error:</p>
<pre><code>init.go:31: failed to create test Postgres container: generic container: create container: created hook: can't copy ../migrations/000001_create_account_table.up.sql to container: open ../migrations/000001_create_account_table.up.sql: no such file or directory
</code></pre>
<p>From the docs, I can see that the files are mount under <code>/docker-entrypoint-initdb.d</code>, I'm assuming the whole directory is mounted - that could be wrong tho.</p>
",0,1,1,2025-11-20T19:18:33+00:00,1,56,True
79826234,14124751,,postgresql,Spring native query (declare..begin..end) with parameters,"<p>I have trouble passing parameters for a native query with <code>declare..begin..end</code> style.
This code doesn't work:</p>
<pre><code>@Test
@Transactional
void foo() {
    em.createNativeQuery(&quot;&quot;&quot;
        do
        $$
            declare
                v_i integer := :id;
                v_j integer;
            begin
                select v_i + 1
                into v_j;
                raise notice '%', v_j;
            end;
         $$
        &quot;&quot;&quot;)
        .setParameter(&quot;id&quot;, 1)
        .executeUpdate();
}
</code></pre>
<p>The exception is <code>org.hibernate.exception.DataException: JDBC exception executing SQL [The column index is out of range: 1, number of columns: 0.]</code>.</p>
<p>But the code without parameters works:</p>
<pre><code>@Test
@Transactional
void foo() {
    em.createNativeQuery(&quot;&quot;&quot;
        do
           $$
            declare
                v_i integer := 1;
                v_j integer;
            begin
                select v_i + 1
                into v_j;
                raise notice '%', v_j;
            end;
        $$
        &quot;&quot;&quot;)
        .executeUpdate();
}
</code></pre>
<p>I suppose it's related to passing parameters but I can't figure out how to make it work.</p>
<p>Could anyone know how to solve this? And whether it's possible.</p>
",0,0,0,2025-11-21T07:41:17+00:00,1,56,False
79826268,624884,"Melbourne, Australia",postgresql,Inserting into text array column when using unnest(),"<p>I have a table with the follow definition:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE &quot;test&quot; (
    id UUID PRIMARY KEY,
    created TIMESTAMPTZ NOT NULL,
    description TEXT,
    permissions TEXT[]
);
</code></pre>
<p>I want to bulk insert into the table using <code>unnest()</code> and be able to ignore the update if there is a conflict:</p>
<pre class=""lang-sql prettyprint-override""><code>INSERT INTO test (id, created, description, permissions)
SELECT unnest($1::uuid[]), unnest($2::timestamptz[]), unnest($3::text[]), unnest($4::text[][])
ON CONFLICT (user_id, session_id, confirmation_code_hash) DO NOTHING
</code></pre>
<p>The arguments passed in are:</p>
<ul>
<li>Array of UUIDs</li>
<li>Array of timestamps</li>
<li>Array of texts</li>
<li>Array of array of texts</li>
</ul>
<p>The problem is that <code>unnest()</code> flattens the array of array of texts completely, where as I would just like to flatten the first dimension and insert an array of text into each column.</p>
<p>Is there any way to use <code>unnest()</code> for bulk insertion of array columns?</p>
",1,1,0,2025-11-21T08:29:54+00:00,0,71,False
79826372,31721459,,postgresql,Cannot connect to PostgreSQL over Cloudflare Tunnel,"<p>I’m running into a problem exposing a PostgreSQL database through a Cloudflare Tunnel, and I’m hoping someone with deeper Cloudflare Zero Trust/WARP experience can help me understand what’s going on.</p>
<p>Setup:</p>
<p>I have a Cloudflare Tunnel running on my server, and all HTTP services behind it work without any issues.</p>
<p>I created a data.mydomain.com hostname in the tunnel and mapped it to a TCP service pointing at the database.</p>
<p>DNS is a clean CNAME to the tunnel, and Cloudflare Access is enabled.</p>
<p>WARP is enabled on my local machine and enrolled in my Zero Trust organization.</p>
<p>I can resolve and reach other services (including Grafana) through the tunnel without any problems.</p>
<p>The issue:</p>
<p>Whenever I try to connect to my database through the tunnel hostname, PostgreSQL clients fail instantly with the usual “is the server running on that host and accepting TCP/IP connections?” message.</p>
",0,0,0,2025-11-21T10:07:29+00:00,0,83,False
79826891,435159,"Fort Collins, CO, United States",postgresql,"Unallowed GRANT does not raise an exception and doesn&#39;t grant the privilege either, if user already has other privileges on the object","<p>I have a regression test for a PostgreSQL database that checks to make sure a particular user cannot self-assign additional privileges to a particular schema. I do this by logging in as the user &quot;testuser&quot;, then executing this SQL: <a href=""https://dbfiddle.uk/6cKsmzED"" rel=""nofollow noreferrer"">fiddle</a></p>
<pre class=""lang-sql prettyprint-override""><code>GRANT CREATE ON SCHEMA testschema TO testuser;
</code></pre>
<p>In past PostgreSQL versions, this would raise an error (again, this user should not be able to grant permissions on the schema.) With version 17, the SQL <em>does not</em> raise an error (but no additional privilege is granted.)</p>
<p>Using <a href=""https://stackoverflow.com/users/1319998/michal-charemza"">@Michal Charemza</a>'s <a href=""https://stackoverflow.com/a/78466268/5298879"">excellent query</a> from <a href=""https://stackoverflow.com/questions/40759177/postgresql-show-all-the-privileges-for-a-concrete-user"">this question</a> shows that testuser's privileges for <code>testschema</code> to be <code>USAGE</code>, for the database to be <code>CONNECT</code>, and to be inheriting the role <code>users</code>.</p>
<p>Running the same privilege query on the <code>users</code> role shows it has no privileges.</p>
<p>So: under v17, why doesn't <code>testuser</code>'s attempt to <code>GRANT</code> additional privileges on a schema raise an error if <code>testuser</code> only has <code>USAGE</code> privilege on the schema?</p>
",1,1,0,2025-11-21T19:30:52+00:00,1,122,True
79826986,542981,"Hong Kong, Hong Kong",postgresql,Can I remove the pg_try_advisory_lock() call for function run_maintenance_proc() generated by pg_partman extension?,"<p>Why does the PostgreSQL <code>run_maintenance_proc</code> need <code>pg_try_advisory_lock</code>? Can I remove that? It keeps creating table deadlocks.</p>
<p>Its probably because I am also using dbt to generate a few materialized tables, but anyway, can we remove that locking?</p>
",0,0,0,2025-11-21T22:32:11+00:00,1,57,False
79827087,17039707,,postgresql,How do I make an authenticated request from my Next.js API on Supabase with RLS enabled?,"<p>In my case, I only want authenticated users to perform a <code>SELECT</code> query on my database, so I set up my RLS Policy for this scenario. This is where I am hitting an issue. With RLS enabled, I need to let the server know that I am authenticated when making a request, but it hits unauthorized all the time.</p>
<p><strong>My RLS Policy:</strong></p>
<pre><code>alter policy &quot;Read all locations&quot;

on &quot;public&quot;.&quot;sampling_location&quot;

to public

using (

  (auth.role() = 'authenticated'::text)

);
</code></pre>
<p>T<strong>his is how my frontend makes a request on my API endpoint:</strong></p>
<pre><code>const response = await fetch('/api/locations');
</code></pre>
<p><strong>This is my code block on making a fetch request on Supabase:</strong></p>
<pre><code>

export async function GET(request: NextRequest) {
 const supabase = createSupabaseServerClient();

  // Check if user is authenticated
  const {
    data: { user },
    error
  } = await supabase.auth.getUser();

  if (!user) {
    return NextResponse.json({ error: &quot;Unauthorized&quot; }, { status: 401 });
  }
  try {
    const { searchParams } = new URL(request.url);
    const region = searchParams.get('region');

    let query = supabase
      .from('sampling_location')
      .select('*')
      .order('created_at', { ascending: false });

    if (region) {
      query = query.ilike('region', `%${region}%`);
    }

    const { data, error} = await query;

    if (error) throw error;

    return NextResponse.json(data.map(normalizeLocation), { status: 200 });
  } catch (error) {
    return NextResponse.json({ error: getErrorMessage(error) }, { status: 500 });
  }
}
</code></pre>
",1,0,0,2025-11-22T03:20:49+00:00,1,83,False
79827540,15945535,,postgresql,"Supabase RLS: Anonymous INSERT into reviews returns 401 while authenticated INSERT works (campaign_id active, policies correct)","<p>I’m facing a strange Supabase RLS issue that I can’t resolve after days of debugging.
I have a public review flow:
Users should be able to submit reviews without logging in (anonymous users) and also submit reviews while logged in. The behavior is inconsistent:</p>
<h1>The core problem</h1>
<p><strong>When logged-in (authenticated):</strong> I can insert a review successfully.</p>
<p><strong>When not logged-in (anonymous user):</strong> I get:</p>
<pre><code>401 Unauthorized
new row violates row-level security policy for table &quot;reviews&quot;
code: 42501
</code></pre>
<p><strong>Even worse:</strong>
With certain policy configurations, authenticated users succeed while anonymous users always fail - even though I explicitly allow both roles.</p>
<p><strong>My Policies on public.reviews</strong></p>
<pre><code>policyname                    | permissive | roles                | cmd    | qual | with_check
-------------------------------------------------------------------------------------------------------
campaign_owner_select_reviews | PERMISSIVE | {authenticated}      | SELECT | EXISTS(...) | null
campaign_owner_update_reviews | PERMISSIVE | {authenticated}      | UPDATE | EXISTS(...) | null
anonymous_reviews             | PERMISSIVE | {anon,authenticated} | INSERT | null        | EXISTS(SELECT 1 FROM campaigns c WHERE c.id = reviews.campaign_id AND c.active = true)
</code></pre>
<ul>
<li><code>campaign_id</code> definitely exists and is active.</li>
<li>No triggers are present.</li>
<li>RLS is enabled on both <code>reviews</code> and <code>campaigns</code>.</li>
</ul>
<p><strong>My Policies on public.campaigns</strong></p>
<pre><code>policyname                   | roles           | cmd    | qual
---------------------------------------------------------------------------
anon_select_active_campaigns | {anon}          | SELECT | (active = true)
select_campaigns_policy      | {authenticated} | SELECT | (auth.uid() = user_id)
update_campaigns_policy      | {authenticated} | UPDATE | (auth.uid() = user_id)
insert_campaigns_policy      | {authenticated} | INSERT | null
delete_campaigns_policy      | {authenticated} | DELETE | (auth.uid() = user_id)
</code></pre>
<p>So anonymous users are allowed to read only active campaigns, which should make the <code>WITH CHECK</code> condition valid.</p>
<p><strong>The request headers during the failing anonymous insert</strong></p>
<pre><code>apikey:
&lt;anonKeyHere&gt;

authorization:
Bearer &lt;same anonKeyHere&gt;   &lt;-- same token in both headers
</code></pre>
<p>So the request is correctly using the anon key.</p>
<p><strong>Additional important details</strong></p>
<p>Campaign record exists and is active</p>
<pre><code>campaign_id = 5dea87a0-ff1b-450e-aede-bb735c59baa7
active = true
</code></pre>
<p>I'm not selecting on reviews
I only do:</p>
<pre><code>supabase
  .from(&quot;reviews&quot;)
  .insert(reviewData)
  .select(&quot;id&quot;)
  .single();
</code></pre>
<p>But removing <code>.select(&quot;id&quot;).single()</code> still produces the same 401 error.</p>
<p><strong>Supabase client code</strong></p>
<p>Client used for reviews:</p>
<pre><code>export const supabaseReview = createClient(
  SUPABASE_URL,
  SUPABASE_ANON_KEY,
  {
    auth: {
      persistSession: false,
      autoRefreshToken: false,
      detectSessionInUrl: false,
      storageKey: &quot;supabase-review-anon&quot;,
    },
    global: {
      headers: {
        apikey: SUPABASE_ANON_KEY,
        &quot;X-Client-Info&quot;: &quot;supabase-review-anon&quot;,
      }
    }
  }
);
</code></pre>
<p>The same anon key appears in both <code>apikey</code> and <code>authorization</code> headers — no session token.</p>
<p><strong>What I already tried</strong></p>
<p>✔ Dropping and recreating all INSERT policies on reviews</p>
<p>✔ Dropping and recreating all SELECT policies on campaigns</p>
<p>✔ Explicitly adding USING (true) to INSERT policy</p>
<p>✔ Adding a full permissive policy TO anon</p>
<p>✔ Disabling .select(&quot;id&quot;)</p>
<p>✔ Testing with PostgREST debug</p>
<p>✔ Verified campaign exists and is active</p>
<p>✔ Verified there is no conflicting restrictive RLS policy</p>
<p>✔ Confirmed anonymous requests use the correct anon JWT</p>
<p>✔ Verified the client is NOT authenticated in Incognito mode</p>
<p>✔ Reviewed all migrations</p>
<p>Still, anonymous requests fail with 401 while authenticated ones succeed.</p>
<p><strong>My Question</strong></p>
<p>Why does Supabase return 401 for anonymous inserts even though:</p>
<ul>
<li>The anon role is explicitly allowed in the INSERT policy</li>
<li>The WITH CHECK condition is satisfied</li>
<li>The apikey and authorization headers contain a valid anon JWT</li>
<li>The target campaign record exists and is active</li>
<li>The same INSERT succeeds when authenticated</li>
<li>RLS on campaigns allows anon SELECT on active campaigns</li>
</ul>
<p>What else could cause anonymous RLS INSERT to fail even when policies appear correct?</p>
<p>Any insight would be greatly appreciated.</p>
",0,0,0,2025-11-22T20:18:26+00:00,1,72,True
79827833,14823472,,postgresql,Stuck with Prisma error after updating to v7,"<p>I was working with Prisma and NestJS and it is fine but when I updated Prisma to version 7 I started getting errors, fixed them, but I am stuck with this one.</p>
<p>Part of schema:</p>
<pre class=""lang-none prettyprint-override""><code>datasource db {
  provider = &quot;postgresql&quot;
  
}

generator client {  
  provider = env(&quot;CLIENT_PROVIDER&quot;)
}
    
model User {
  userId       Int       @id @default(autoincrement()) @map(&quot;user_id&quot;)
  name         String
  email        String    @unique
  password     String
  status       String    @default(&quot;ACTIVE&quot;)
  isSuperAdmin Boolean   @default(false) @map(&quot;is_super_admin&quot;)
  createdAt    DateTime  @default(now()) @map(&quot;created_at&quot;)

  otps         Otp[]
  auditLogs    AuditLog[]

  @@map(&quot;users&quot;)
  @@index([name])
}
</code></pre>
<p>Prisma service:</p>
<pre class=""lang-typescript prettyprint-override""><code>import { Injectable, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { PrismaClient } from '@prisma/client';

@Injectable()
export class PrismaService extends PrismaClient implements OnModuleInit, OnModuleDestroy {
  async onModuleInit() {
    await this.$connect();
  }

  async onModuleDestroy() {
    await this.$disconnect();
  }
}
</code></pre>
<p>Prisma module:</p>
<pre class=""lang-typescript prettyprint-override""><code>import { Global, Module } from '@nestjs/common';
import { PrismaService } from './prisma.service';

@Global()
@Module({
  providers: [PrismaService],
  exports: [PrismaService],
})
export class PrismaModule {}
</code></pre>
<p>Prisma config:</p>
<pre class=""lang-typescript prettyprint-override""><code>import { defineConfig, env } from &quot;@prisma/config&quot;;
import dotenv from &quot;dotenv&quot;;
import path from &quot;path&quot;;

dotenv.config({ path: path.resolve(process.cwd(), &quot;.env&quot;) });

export default defineConfig({
  schema: &quot;src/shared/database/prisma/schema.prisma&quot;,
  datasource: {
    url: env(&quot;DATABASE_URL&quot;),
  },
});
</code></pre>
<p>and the error:</p>
<pre class=""lang-none prettyprint-override""><code>npm run start

&gt; servers-monitor@0.0.1 start
&gt; nest start

(node:127213) [DEP0190] DeprecationWarning: Passing args to a child process with shell option true can lead to security vulnerabilities, as the arguments are not escaped, only concatenated.
(Use `node --trace-deprecation ...` to show where the warning was created)
[Nest] 127247  - 11/23/2025, 1:56:22 PM     LOG [NestFactory] Starting Nest application...
[Nest] 127247  - 11/23/2025, 1:56:22 PM   ERROR [ExceptionHandler] TypeError [ERR_INVALID_ARG_TYPE]: The &quot;paths[1]&quot; argument must be of type string. Received undefined
</code></pre>
",4,4,0,2025-11-23T11:20:38+00:00,2,2420,False
79828051,20025580,,postgresql,Supabase upsert in React Native profile table not updating onboarding_completed,"<p>I'm building a mobile app with React Native (Expo) and using Supabase for authentication and  user profiles.</p>
<h2>This is my profiles table</h2>
<pre><code>create table public.profiles (
id uuid primary key references auth.users(id),
full_name text,
birth_date date,
gender text,
city text,
postal_code text,
leisure_preferences text[],
notify_events boolean default false,
notify_recommendations boolean default false,
notify_news boolean default false,
onboarding_completed boolean default false
);

</code></pre>
<p>My RLS policies:</p>
<pre><code>-- Insert
alter policy &quot;Enable insert for users based on user_id&quot;
on public.profiles
to public
with check (auth.uid() = id);

-- Update
alter policy &quot;Enable update for users&quot;
on public.profiles
to public
using (id = auth.uid())
with check (auth.uid() = id);

-- Select
alter policy &quot;select profile&quot;
on public.profiles
to public
using (auth.uid() = id);

</code></pre>
<p>In my OnboardingScreen, I try to save the user profile when pressing Save:</p>
<pre><code>const { data: { session }, error: sessionError } = await supabase.auth.getSession();
const user = session?.user;

const { data, error } = await supabase
  .from('profiles')
  .upsert([{
    id: user.id,
    full_name: nombre,
    birth_date: fechaNacimiento,
    gender: genero,
    city: ciudad,
    postal_code: codigoPostal,
    leisure_preferences: preferencias,
    notify_events: alertasEventos,
    notify_recommendations: recomendaciones,
    notify_news: novedades,
    onboarding_completed: true,
  }], { onConflict: 'id' })
  .select();

console.log('Result:', { data, error });

</code></pre>
<p>My App.tsx</p>
<pre><code>const [session, setSession] = useState&lt;any&gt;(null);
  const [loading, setLoading] = useState&lt;boolean&gt;(true);
  const [onboardingCompleted, setOnboardingCompleted] = useState&lt;boolean&gt;(false);


  useEffect(() =&gt; {
    const getSession = async () =&gt; {
      try {
        const { data, error } = await supabase.auth.getSession();
        if (error) throw error;
        setSession(data?.session ?? null);

        if (data?.session?.user) {
          await checkOnboardingStatus(data.session.user.id);
        }
      } catch (error) {
        console.error('Error al obtener la sesión:', error);
      } finally {
        setLoading(false);
      }
    };

    const checkOnboardingStatus = async (userId: string) =&gt; {
      try {
        const { data, error } = await supabase
          .from('profiles')
          .select('onboarding_completed')
          .eq('id', userId)
          .single();

        if (error) throw error;
        console.log('Estado de onboarding obtenido:', data?.onboarding_completed);
        setOnboardingCompleted(data?.onboarding_completed ?? false);
      } catch (error) {
        console.error('Error al verificar el estado de Onboarding:', error);
      }
    };

    getSession();


    const { data: authListener } = supabase.auth.onAuthStateChange(async (_event: any, currentSession: any) =&gt; {
      setSession(currentSession);
      if (currentSession?.user) {
        await checkOnboardingStatus(currentSession.user.id);
      }
    });

    return () =&gt; {
      authListener?.subscription?.unsubscribe(); // Asegúrate de acceder a `subscription` antes de llamar a `unsubscribe`
    };
  }, []);

 return (
    &lt;SafeAreaProvider&gt;
      &lt;View style={{ flex: 1, backgroundColor: '#1B1A1A' }}&gt;
        &lt;EventProvider&gt;
          &lt;NavigationContainer&gt;
            &lt;Stack.Navigator screenOptions={{ headerShown: false, animation: 'slide_from_right' }}&gt;
              {!session ? (
                &lt;Stack.Screen name=&quot;Login&quot; component={LoginScreen} /&gt;
              ) : onboardingCompleted ? (
                &lt;&gt;
                  &lt;Stack.Screen name=&quot;MainApp&quot; component={MainAppTabs} /&gt;
                  &lt;Stack.Screen name=&quot;EventDetailsScreen&quot; component={EventDetailsScreen} /&gt;
                &lt;/&gt;
              ) : (
                &lt;Stack.Screen name=&quot;Onboarding&quot;&gt;
                  {props =&gt; (
                    &lt;OnboardingScreen
                      {...props}
                      setOnboardingCompleted={setOnboardingCompleted}
                    /&gt;
                  )}
                &lt;/Stack.Screen&gt;

              )}
            &lt;/Stack.Navigator&gt;
          &lt;/NavigationContainer&gt;
        &lt;/EventProvider&gt;
      &lt;/View&gt;
    &lt;/SafeAreaProvider&gt;
  );
}
</code></pre>
<p>The button press works (handleSave runs, I see logs)
But the upsert never seems to update the row
In Supabase, the onboarding_completed field stays false.
Because of that, my App.tsx logic never switches to MainApp</p>
",0,1,1,2025-11-23T18:48:29+00:00,0,38,False
79828237,30959676,,postgresql,Large joins between three tables in Postgres and potential improvement,"<p>Below is my postgres db settings:</p>
<p><strong>VERSION</strong>: PostgreSQL 15.14 on x86_64-pc-linux-gnu, compiled by Debian clang version 12.0.1, 64-bit'.</p>
<p><strong>SHOW work_mem</strong>: 4MB</p>
<p>Now suppose I have three tables below in postgres:</p>
<p><strong>1, players table</strong> (records basic information about each game player): ~4000000 rows.</p>
<ul>
<li><p><strong>player_id</strong> (bigint, unique): indexed with <code>CREATE INDEX players__idx1 ON players USING btree (player_id) TABLESPACE pg_default;</code></p>
</li>
<li><p>business_name (char var, a rather trivial column, no index, just need to select it, no filter on it)</p>
</li>
<li><p>last_name (char var, no filter on it)</p>
</li>
<li><p>first_name (char var, no filter on it)</p>
</li>
</ul>
<p><strong>2, player_performance table</strong> (records the yearly performance &quot;score&quot; for certain game types for each player): ~35000000 rows.</p>
<ul>
<li><p><strong>player_id</strong> (bigint, duplicated since this table contains performance for each player from different years): indexed with <code>CREATE INDEX IF NOT EXISTS player_performance_idx0 ON player_performance USING btree (player_id ASC NULLS LAST) TABLESPACE pg_default;</code></p>
</li>
<li><p>hashed_player_id (char var, it is the &quot;hashed version&quot; of the player_id, but also included other sensitive information based on target year): indexed with <code>CREATE INDEX IF NOT EXISTS player_performance_idx2 ON player_performance USING btree (hashed_player_id COLLATE pg_catalog.&quot;default&quot; ASC NULLS LAST) TABLESPACE pg_default;</code></p>
</li>
<li><p>fps_score (double, the score for player's fps game for that year)</p>
</li>
<li><p>year (bigint, record what year the performance scores are based on, mainly 7 years are considered: 2007, 2010, 2013, 2016, 2019, 2022, 2025): indexed with <code>CREATE INDEX IF NOT EXISTS player_performance_idx1 ON player_performance USING btree (year ASC NULLS LAST) TABLESPACE pg_default;</code></p>
</li>
<li><p>and all other kinds of scores from around 190 categories, subcategories, and genres (this is only a point to illustrate how &quot;wide&quot; this table is).</p>
</li>
</ul>
<p>year, player_id, hashed_player_id combined are unique, meaning every year, each player_id will only show up once (hashed_player_id is really just another form of player_id for that year).</p>
<p><strong>3, org_and_players table</strong> (maps what organization or team is this player from): ~13000000 rows.</p>
<ul>
<li><p>id (primary key of this table, not directly used, since this table maps an N to N relationship between two columns below)</p>
</li>
<li><p>org_id (big int, id for organization or team that players come from): indexed with <code>CREATE INDEX IF NOT EXISTS org_and_players_x0 ON org_and_players USING btree (org_id ASC NULLS LAST) TABLESPACE pg_default;</code></p>
</li>
<li><p><strong>player_id</strong> (big int, id, id for players): indexed with <code>CREATE INDEX IF NOT EXISTS org_and_players_idx1 ON org_and_players USING btree (player_id ASC NULLS LAST) TABLESPACE pg_default;</code></p>
</li>
</ul>
<p>one org_id can contain multiple player_ids, one player_id can work for different org_ids.</p>
<p><strong>QUERY GOAL:</strong></p>
<p>Given a list of <strong>org_ids</strong> (currently ranging from 1-2000) and a <strong>target year</strong>, what are the fps_score, business name, last and first name, and hashed_player_id for each unique players being queried out. Here &quot;unique&quot; is mainly defined by unique hashed_player_id, since the context is that we cannot expose player_id directly, hence we use hashed_player_id.</p>
<p>Also, the result might be really long, so currently I plan to paginate the query with a cursor based pagination, hence the result will be sorted on hashed_player_id, which will act as cursor. Finally, a limit is applied as page size.</p>
<p><strong>My query:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
DISTINCT players.business_name, 
player_performance.fps_score,
player_performance.hashed_player_id, 
players.first_name, 
players.last_name 
FROM player_performance 
JOIN players ON player_performance.player_id = players.player_id 
JOIN org_and_players ON player_performance.player_id = org_and_players.player_id 
WHERE player_performance.year = 2019 
AND org_and_players.org_id IN (7979014, 129425416 ... A 1800 LENGTH ORGID LIST) 
ORDER BY player_performance.hashed_player_id 
 LIMIT 5000
</code></pre>
<p>I kind of have to add distinct to filter out duplicated player info who worked for multiple team. Checked that no duplicates on hashed_player_id in the results, but this query is really unstable. First time running it can take 1-2 minutes, running it again later takes 5-6 seconds (I want it to be faster like within 2 seconds, since it is just one paginated call).</p>
<p><strong>My first query plan after EXPLAIN ANALYZE: took 2:26 minutes.</strong></p>
<pre><code>Limit  (cost=6527172.35..6527817.18 rows=5000 width=90) (actual time=143124.887..143671.954 rows=5000 loops=1)&quot;
  -&gt;  Unique  (cost=6527172.35..6795216.15 rows=2078399 width=90) (actual time=143124.886..143671.565 rows=5000 loops=1)&quot;
        -&gt;  Gather Merge  (cost=6527172.35..6769236.16 rows=2078399 width=90) (actual time=143124.883..143670.271 rows=5464 loops=1)&quot;
              Workers Planned: 2&quot;
              Workers Launched: 2&quot;
              -&gt;  Sort  (cost=6526172.32..6528337.32 rows=866000 width=90) (actual time=142991.780..142994.927 rows=2148 loops=3)&quot;
                    Sort Key: player_performance.hashed_player_id, players.business_name, player_performance.fps_score, players.&quot;first_name&quot;, players.&quot;last_name&quot;&quot;
                    Sort Method: external merge  Disk: 29352kB&quot;
                    Worker 0:  Sort Method: external merge  Disk: 29336kB&quot;
                    Worker 1:  Sort Method: external merge  Disk: 29248kB&quot;
                    -&gt;  Nested Loop  (cost=225555.20..6351965.37 rows=866000 width=90) (actual time=1064.386..141456.323 rows=280533 loops=3)&quot;
                          -&gt;  Parallel Hash Join  (cost=225554.64..409451.50 rows=815502 width=65) (actual time=1064.251..2399.889 rows=288925 loops=3)&quot;
                                Hash Cond: (org_and_players.player_id = players.player_id)&quot;
                                -&gt;  Parallel Bitmap Heap Scan on org_and_players  (cost=22432.97..175331.40 rows=815502 width=8) (actual time=28.601..96.129 rows=325598 loops=3)&quot;
                                      Recheck Cond: (org_id = ANY ('{7979014, 129425416 ... A 1800 LENGTH ORGID LIST }'))&quot;                                      Heap Blocks: exact=7004&quot;
                                      -&gt;  Bitmap Index Scan on org_and_players_idx0  (cost=0.00..21939.30 rows=1957204 width=0) (actual time=25.567..25.568 rows=976793 loops=1)&quot;
                                            Index Cond: (org_id = ANY ('{7979014, 129425416 ... A 1800 LENGTH ORGID LIST }'))&quot;                                -&gt;  Parallel Hash  (cost=166013.63..166013.63 rows=1596563 width=57) (actual time=877.377..877.379 rows=1276741 loops=3)&quot;
                                      Buckets: 131072  Batches: 64  Memory Usage: 6528kB&quot;
                                      -&gt;  Parallel Seq Scan on players  (cost=0.00..166013.63 rows=1596563 width=57) (actual time=0.012..511.014 rows=1276741 loops=3)&quot;
                          -&gt;  Index Scan using player_performance_key on player_performance  (cost=0.56..7.29 rows=1 width=49) (actual time=0.478..0.478 rows=1 loops=866774)&quot;
                                Index Cond: ((player_id = players.player_id) AND (year = 2019))&quot;
</code></pre>
<p><strong>My second query plan after EXPLAIN ANALYZE: 4-5 seconds.</strong></p>
<pre><code>Limit  (cost=6527172.35..6527817.18 rows=5000 width=90) (actual time=4465.932..4915.135 rows=5000 loops=1)&quot;
  -&gt;  Unique  (cost=6527172.35..6795216.15 rows=2078399 width=90) (actual time=4465.931..4914.791 rows=5000 loops=1)&quot;
        -&gt;  Gather Merge  (cost=6527172.35..6769236.16 rows=2078399 width=90) (actual time=4465.929..4913.950 rows=5464 loops=1)&quot;
              Workers Planned: 2&quot;
              Workers Launched: 2&quot;
              -&gt;  Sort  (cost=6526172.32..6528337.32 rows=866000 width=90) (actual time=4411.999..4414.418 rows=2093 loops=3)&quot;
                    Sort Key: player_performance.hashed_player_id, players.business_name, player_performance.fps_score, players.&quot;first_name&quot;, players.&quot;last_name&quot;&quot;
                    Sort Method: external merge  Disk: 29256kB&quot;
                    Worker 0:  Sort Method: external merge  Disk: 29120kB&quot;
                    Worker 1:  Sort Method: external merge  Disk: 29544kB&quot;
                    -&gt;  Nested Loop  (cost=225555.20..6351965.37 rows=866000 width=90) (actual time=1548.699..3545.096 rows=280533 loops=3)&quot;
                          -&gt;  Parallel Hash Join  (cost=225554.64..409451.50 rows=815502 width=65) (actual time=1548.575..1915.568 rows=288925 loops=3)&quot;
                                Hash Cond: (org_and_players.player_id = players.player_id)&quot;
                                -&gt;  Parallel Bitmap Heap Scan on org_and_players  (cost=22432.97..175331.40 rows=815502 width=8) (actual time=40.478..102.287 rows=325598 loops=3)&quot;
                                      Recheck Cond: (org_id = ANY ('{7979014, 129425416 ... A 1800 LENGTH ORGID LIST }'))&quot;                                      Heap Blocks: exact=7183&quot;
                                      -&gt;  Bitmap Index Scan on org_and_players_idx0  (cost=0.00..21939.30 rows=1957204 width=0) (actual time=37.347..37.348 rows=976793 loops=1)&quot;
                                            Index Cond: (org_id = ANY ('{7979014, 129425416 ... A 1800 LENGTH ORGID LIST }'))&quot;                                -&gt;  Parallel Hash  (cost=166013.63..166013.63 rows=1596563 width=57) (actual time=1324.274..1324.276 rows=1276741 loops=3)&quot;
                                      Buckets: 131072  Batches: 64  Memory Usage: 6560kB&quot;
                                      -&gt;  Parallel Seq Scan on players  (cost=0.00..166013.63 rows=1596563 width=57) (actual time=0.010..533.905 rows=1276741 loops=3)&quot;
                          -&gt;  Index Scan using player_performance_key on player_performance  (cost=0.56..7.29 rows=1 width=49) (actual time=0.005..0.005 rows=1 loops=866774)&quot;
                                Index Cond: ((player_id = players.player_id) AND (year = 2019))&quot;
</code></pre>
<p><strong>Questions:</strong></p>
<p>1, Why it can take this long to run the query? is it because of the memory limit? Feels like later queries do speed up due to &quot;warm cache&quot;.</p>
<p>2, is there any way to improve the query and make it stable between runs and potentially faster?</p>
<p><strong>My attempt:</strong></p>
<p>I tried to materialize one or all the join, but the query to create the materialized view between even two of those tables cannot even finish for me (waited for 20 minutes), probably because:</p>
<ul>
<li><p>there are duplication in <strong>player_performance</strong> table across years.</p>
</li>
<li><p>there are duplicated in <strong>org_and_players</strong> table due to n to n relationship.</p>
</li>
</ul>
<p>Also, fps_score is just ONE of the score concerned. User may query different scores, hence when creating materialized view, I need to carry over those 190 scores from the performance table as well, which is another issue.</p>
<p>Any help is greatly appreciated. If I missed any information or context, kindly mention it, and I will add it. Thanks.</p>
",1,1,0,2025-11-24T02:48:22+00:00,0,115,False
79828496,24968068,,postgresql,Frequent update of records in database,"<p>I’m building a real-time chat application using NestJS, Postgresql (Main DB), Redis and Socket.IO.</p>
<p>My database schema (simplified) looks like this:</p>
<pre class=""lang-none prettyprint-override""><code>-- Chats
table chats (
  id   serial primary key,
  name varchar(255) not null
  -- other fields...
);

-- Users
table users (
  id    serial primary key,
  login varchar(255) not null
  -- other fields...
);

-- Many-to-many relation between users and chats
table chat_members (
  id                     serial primary key,
  user_id                int not null references users(id),
  chat_id                int not null references chats(id),
  unread_messages_count  int not null default 0
);
</code></pre>
<p>For unread messages, every time someone sends a message in a chat, I increment <code>unread_messages_count</code> for all chat members who are not currently in that chat.</p>
<p>This works functionally, but it feels very inefficient and resource-heavy, especially when a chat has many members (e.g., group chats). Each message triggers multiple updates to <code>chat_members</code>.</p>
<p>Is this a good approach for implementing unread message counters, or is there a more efficient / standard pattern for this kind of feature?</p>
<p>Specifically, how can I avoid updating many rows on every new message while still being able to show an accurate unread counter like other chat apps do?</p>
",1,1,0,2025-11-24T10:13:26+00:00,2,119,True
79828677,13929339,,postgresql,Error in PostgreSQL when running Rails and PostgreSQL using Docker Compose,"<p>When I run the command on docker container backend:<br />
<code>psql  -h localhost -p 5432 -U postgres</code></p>
<p>I receive the following error:</p>
<pre><code>psql: error: connection to server at &quot;localhost&quot; (::1), port 5432 failed: Connection refused
        Is the server running on that host and accepting TCP/IP connections?
connection to server at &quot;localhost&quot; (127.0.0.1), port 5432 failed: Connection refused
        Is the server running on that host and accepting TCP/IP connections?
</code></pre>
<p>My <a href=""https://github.com/andbri321/streaming/blob/docker_backend/streaming_api/docker-compose.yml"" rel=""nofollow noreferrer"">docker-compose</a>, <a href=""https://github.com/andbri321/streaming/blob/docker_backend/streaming_api/Dockerfile.dev"" rel=""nofollow noreferrer"">dockerfile</a>, <a href=""https://github.com/andbri321/streaming/blob/docker_backend/streaming_api/config/database.yml"" rel=""nofollow noreferrer"">database.yml</a>.</p>
<p>I'm running my application via Docker Compose on Ubuntu 24.</p>
",-3,0,3,2025-11-24T13:11:29+00:00,0,105,False
79829003,347877,,postgresql,CREATE OR REPLACE FUNCTION in concurrently running transactions,"<p>How does <code>create or replace function</code> in PostgreSQL <strong>14</strong> behave with respect to transactions? If there are multiple concurrent transactions running at <a href=""https://www.postgresql.org/docs/14/transaction-iso.html#XACT-READ-COMMITTED"" rel=""nofollow noreferrer"">read committed</a> isolation level, and trying to <code>create or replace</code> the same function, will it cause any errors?</p>
<p>Is there any chance of the <code>tuple concurrently updated</code> error occurring in this scenario?</p>
",1,1,0,2025-11-24T18:41:02+00:00,2,127,True
79829103,27366805,,postgresql,SQLAlchemy and partitioning table,"<p>I have tables which are already in Postgres and partitioned. How can I write a query with partioned <code>birth_year</code></p>
<pre><code>stmt = select(UserOrm)
session.execute(stmt)`
</code></pre>
",0,1,1,2025-11-24T21:06:24+00:00,1,66,True
79829334,824624,,postgresql,insert sample data into a postgresql in docker container not working,"<p>I am running a postgresql in docker container and trying to insert some sample data into this database.</p>
<p>prepare_source_data.sql exists in the host environment.</p>
<p>here I tried</p>
<pre><code>docker exec -it postgres bash psql &quot;postgres://postgres:postgres@localhost/test&quot;  -f ./prepare_source_data.sql
</code></pre>
<p>but got some errors</p>
<pre><code>/usr/bin/psql: line 19: use: command not found
/usr/bin/psql: line 20: use: command not found
/usr/bin/psql: line 21: use: command not found
/usr/bin/psql: line 22: use: command not found
/usr/bin/psql: psql: line 24: syntax error near unexpected token `$version,'
/usr/bin/psql: psql: line 24: `my ($version, $cluster);'
</code></pre>
<p>However executing the commands work well</p>
<pre><code>  docker exec -it postgres bash and then psql -U postgres 
</code></pre>
<p>any idea how would I insert the data into the container in the host environment</p>
",0,0,0,2025-11-25T05:56:14+00:00,1,57,True
79829377,1251549,,postgresql,How apply airflow db migration and save them into postgress docker image?,"<p>I use docker-compose to run airflow. Postgres section is</p>
<pre><code>postgres:
  image: postgres:12.16
  environment:
    - POSTGRES_USER
    - POSTGRES_PASSWORD
    - POSTGRES_DB
  healthcheck:
    test: [ &quot;CMD&quot;, &quot;pg_isready&quot;, &quot;-U&quot;, &quot;airflow&quot; ]
    interval: 5s
    retries: 5
</code></pre>
<p>Every time when airflow starts it runs db migration. How to automate this process? At first glance I can mount local folder and keep database state but in this case I always have previous airflow runs that is not the case for development.</p>
<p>May be somebody knows how to apply airflow migrations during postgres build? So I can build my own postges with schema and speed up docker-compose starts. Any ideas?</p>
",0,0,0,2025-11-25T07:06:21+00:00,0,55,False
79831774,13743804,,postgresql,cloudsql gcp PGAUDIT,"<p>I activated pgaudit on my Cloud SQL instance by adding the two flags enable and pgaudit.log, and I immediately saw audit logs in Cloud Logging without having to create the extension. When I connect to the database and look at the installed extensions, I can't find it, but I can find it in the preloaded libraries. How is that possible?</p>
",0,0,0,2025-11-27T14:09:50+00:00,0,40,False
79831791,3421066,,postgresql,"How to run Production PostgreSQL on a VPS (Hetzner/Digital Ocean,etc) - best practices etc?","<p>I am getting into the world of self-hosted applications and I am trying to run a Production PostgreSQL on a VPS - Hetzner.
So far I have been using AWS RDS and everything has been working great - never had any issues. This being the case, they are doing a lot of stuff under the hood and I am trying to understand what would be the best practices to run it on my Hetzner VPS.
Here is my current setup:</p>
<ol>
<li>Hetzner Server (running Docker CE) running on a Private Subnet where I have installed and setup PostgreSQL with the  following two commands below:
mkdir -p ~/pg-data ~/pg-conf
docker run -d  --name postgres -e POSTGRES_USER=demo-user -e POSTGRES_PASSWORD=demo-password -e POSTGRES_DB=postgres  --restart unless-stopped -v ~/pg-data:/var/lib/postgresql/data -p 5432:5432 postgres:17.7</li>
<li>I have the Application Servers (in the same Private Subnet) accessing the DB Server via Private IP.</li>
<li>The DB is not exposed publicly and the DB Server has a daily backup of the disk.</li>
<li>By having the volume mount in the docker command (-v ~/pg-data:/var/lib/postgresql/data), there is a daily backup of the database</li>
</ol>
<p>Reading online and asking different LLM's - they have quite different opinions on whether my setup is Production ready or not - in general the consensus they have is that if the Disk Snapshot happened while the DB is writing to a disk - the DB can get corrupted.<br />
Is that the case?<br />
What would be additional things that I can do to have the backups working correctly and not hitting those edge cases (if hit ever).<br />
Also any other Production readiness hints/tips that I could use?
Read Replicas are not on my mind/not needed for the time being.<br />
UPDATE with clarifications:<br />
Scalability is not needed - the instance is big enough and able to handle the traffic<br />
There can be downtime for updating the database - our customers do not work during the weekends<br />
There is no strict RTO, for RPO - we are fine with losing the data from the last 1 hour</p>
<p>Thanks a lot!</p>
",0,0,0,2025-11-27T14:26:10+00:00,2,89,False
79831844,31955087,,postgresql,Find maximal subsets,"<p>I have sets identified by a <code>set_id</code>, say:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE sets (
  INTEGER set_id,
  INTEGER element_id  
); 
</code></pre>
<p>I need a <code>SELECT</code> statement that finds the sets that are maximal with respect to the subset relation. Sets are partially ordered by the &quot;is subset of&quot; relation. So I am trying to find the sets that are not subsets of any other set in the table.</p>
<p>For</p>
<pre><code>set_id | element_id
     1 | 1 
     1 | 2 
     1 | 3 
     2 | 1
     2 | 2 
     3 | 1 
     3 | 2 
     3 | 4 
</code></pre>
<p>the query should return</p>
<pre><code>set_id 
     1 
     3 
</code></pre>
<p>since the set identified by 2 is a subset of the set identified by 1.</p>
<p>Note that the table above can be imitated by prefixing the actual query with</p>
<pre><code>WITH sets AS (
  SELECT *
  FROM UNNEST(ARRAY [1,1,1,2,2,3,3,3],
              ARRAY [1,2,3,1,2,1,2,4]) AS sets(set_id, element_id)
)
</code></pre>
<p>which is easier to handle than explicit insert statements.</p>
",2,2,0,2025-11-27T15:25:43+00:00,3,136,True
79832501,241552,"Moscow, Russia",postgresql,PostgreSQL: primary key based on timestamp,"<p>I am working on a highload system, which needs to process quite a lot of events, but not an insanely huge amount. The data ingested has to be editable during a short period of time, and then the need for editing diminishes. I want to store that data in a cache while it's &quot;hot&quot;, and then save it to PG when the data &quot;cools down&quot;.  I want to be able to partition my tables based on the dates, and drop partitions as they expire. This way I try to avoid update and delete related vacuum. Thus I need to be able to assign record ids in my cache, but such that they don't conflict in the db.</p>
<p>PG's <code>bigserial</code> type is so big that I can use nanosecond precision timestamps and have the last three digits to use as random &quot;tie brake&quot;, and that will last me till 2262. I will be able to partition my main table on its primary key and the related table on their foreign keys, thus keeping my related data in the same partitions.</p>
<p>Is it a good idea?</p>
",0,0,0,2025-11-28T11:24:25+00:00,3,74,True
79832565,23075832,,postgresql,What is the practical difference between static and dynamic background workers in PostgreSQL?,"<p>I’m trying to understand when to use a static background worker vs a dynamic one in PostgreSQL.</p>
<p>From what I know so far:</p>
<ul>
<li>Static workers are registered during postmaster startup</li>
<li>Dynamic workers can be started at runtime by a backend</li>
</ul>
<p>I am building an extension that may need a bgw in the future. I am confused whether the extension must be listed in shared_preload_libraries or not. Since bgw can be registered on extension load, is there still a need to preload it, The bgw can also be created at runtime anyway.</p>
<p>So I want to know:</p>
<ul>
<li><p>What is the exact difference between static and dynamic bgw in behavior and lifecycle</p>
</li>
<li><p>What can static bgw do that dynamic bgw cannot</p>
</li>
<li><p>In which cases should an extension use static bgw and require shared_preload_libraries</p>
</li>
</ul>
<p>Any guidance or examples for deciding between the two</p>
<p>Thanks in advance.</p>
",0,1,1,2025-11-28T13:02:36+00:00,1,64,True
79832676,31955087,,postgresql,Determine Connectedness Components for a Graph with a RECURSIVE Cte,"<p>I have a graph given by 2-element sets of points, say in a table</p>
<pre><code>CREATE TABLE edge_set (
  INTEGER edge_set_id,
  INTEGER point_id  
)  
</code></pre>
<p>I want to determine the graphs connectedness components. These are the unions of all sets that have a non empty intersection.</p>
<p>I am looking at</p>
<pre><code>1     5
 \   /
  3-4
 /   \
2     6

7-8-9
</code></pre>
<p>as an example.</p>
<p>I have a working query for the example.</p>
<pre><code>  WITH
      edge AS (SELECT *
                 FROM UNNEST(ARRAY [1,2,3,4,4,7,7],
                             ARRAY [3,3,4,5,6,8,9]) AS edge(id1, id2)),
      -- edge_set_ids are generated with primes
      edge_set AS (SELECT 99991 * id1 + 99971 * id2 AS edge_set_id,
                          id1                       AS point_id
                     FROM edge
                    UNION ALL
                   SELECT 99991 * id1 + 99971 * id2 AS edge_set_id,
                          id2                       AS point_id
                     FROM edge),
      map_1 AS (
              SELECT edge_set_id,
                     MIN(edge_set_id)
                         OVER (PARTITION BY point_id)
                             AS new_edge_set_id
                FROM edge_set
          ),
      unions_of_intersecting_1 AS (
          SELECT  edge_set_id,
                  point_id
            FROM (
          SELECT COALESCE(m.new_edge_set_id, es.edge_set_id) AS edge_set_id,
                 es.point_id
            FROM edge_set es
            LEFT JOIN map_1 m
              ON es.edge_set_id = m.edge_set_id
             AND m.edge_set_id &lt;&gt; m.new_edge_set_id)
           GROUP BY edge_set_id, point_id
            ),
      map_2 AS (
          SELECT edge_set_id,
                 MIN(edge_set_id)
                 OVER (PARTITION BY point_id)
                     AS new_edge_set_id
            FROM unions_of_intersecting_1
      ),
      unions_of_intersecting_2 AS (
          SELECT  edge_set_id,
                  point_id
            FROM (
                SELECT COALESCE(m.new_edge_set_id, es.edge_set_id) AS edge_set_id,
                       es.point_id
                  FROM unions_of_intersecting_1 es
                  LEFT JOIN map_2 m
                            ON es.edge_set_id = m.edge_set_id
                                AND m.edge_set_id &lt;&gt; m.new_edge_set_id)
           GROUP BY edge_set_id, point_id
      )
SELECT * FROM unions_of_intersecting_2
  ORDER BY edge_set_id
;
</code></pre>
<p>The query returns</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">edge_set_id</th>
<th style=""text-align: left;"">point_id</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">399904</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: right;"">399904</td>
<td style=""text-align: left;"">2</td>
</tr>
<tr>
<td style=""text-align: right;"">399904</td>
<td style=""text-align: left;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">399904</td>
<td style=""text-align: left;"">4</td>
</tr>
<tr>
<td style=""text-align: right;"">399904</td>
<td style=""text-align: left;"">5</td>
</tr>
<tr>
<td style=""text-align: right;"">399904</td>
<td style=""text-align: left;"">6</td>
</tr>
<tr>
<td style=""text-align: right;"">1499705</td>
<td style=""text-align: left;"">7</td>
</tr>
<tr>
<td style=""text-align: right;"">1499705</td>
<td style=""text-align: left;"">8</td>
</tr>
<tr>
<td style=""text-align: right;"">1499705</td>
<td style=""text-align: left;"">9</td>
</tr>
</tbody>
</table></div>
<p>as it should.</p>
<p>Unfortunately all my attempts to write a recursive query to solve the general problem have failed so far. I always encountered PostgreSQL restrictions that forbid using the recursive query twice or ended up with an unmanageable complexity.</p>
<p>For an example this is the obvious one:</p>
<pre><code>  WITH RECURSIVE
      edge AS (SELECT *
                 FROM UNNEST(ARRAY [1,2,3,4,4,7,7],
                             ARRAY [3,3,4,5,6,8,9]) AS edge(id1, id2)),
      edge_set AS (SELECT 99991 * id1 + 99971 * id2 AS edge_set_id,
                          id1                       AS point_id
                     FROM edge
                    UNION ALL
                   SELECT 99991 * id1 + 99971 * id2 AS edge_set_id,
                          id2                       AS point_id
                     FROM edge),
      unions_of_intersecting AS (
          SELECT edge_set_id,
                 point_id
            FROM edge_set
           UNION ALL
          SELECT edge_set_id,
                  point_id
            FROM (
          SELECT COALESCE(m.new_edge_set_id, uoi.edge_set_id)
                        AS edge_set_id,
                 uoi.point_id,
                 m.new_edge_set_id
            FROM unions_of_intersecting uoi
            LEFT JOIN (
          SELECT edge_set_id,
                 MIN(edge_set_id)
                    OVER (PARTITION BY point_id)
                        AS new_edge_set_id
            FROM unions_of_intersecting) m
              ON uoi.edge_set_id = m.edge_set_id
             AND m.edge_set_id &lt;&gt; m.new_edge_set_id)
           GROUP BY edge_set_id, point_id
          HAVING BOOL_OR(new_edge_set_id IS NOT NULL)
      )
  SELECT * FROM unions_of_intersecting
   ORDER BY edge_set_id
;

</code></pre>
<p>This fails to execute with an [42P19].</p>
<p>I am aware that I could start walking the graph but I prefer the flood fill approach that I am using above because it is simpler.</p>
<p>Solving this would give a recipe for grouping by a given equivalence relation. As an example you could group points in a vector space by being closer to each other than a given distance.</p>
<p>Any hints on how to solve this are welcome.</p>
",1,0,0,2025-11-28T14:53:12+00:00,5,63,True
79832861,8538706,"Montr&#233;al, QC, Canada",postgresql,Connection to remote PostgreSQL server with hashtag in password with node-postgres,"<p>I have a password for a remote PostgreSQL server that I cannot change. This password contains hashtags (#). From searching, I know that this can be a problem.</p>
<p>I can connect to the server from the command line:
<code>psql -h host -U username -d dbname -p 5432</code> (and then manually entering the ##pw##).
I can connect to it with pgAdmin, with VSCode, and so on.</p>
<p>I cannot manage to connect to the server with node-postgres, whichever of the following connection methods I attempt, I get two types of errors:</p>
<pre class=""lang-js prettyprint-override""><code>// Error 1
// error: no pg_hba.conf entry for host &quot;IP&quot;, user &quot;username&quot;, database &quot;dbname&quot;, no encryption
//     at C:\repos\perfo\node_modules\pg-pool\index.js:45:11
//     at process.processTicksAndRejections (node:internal/process/task_queues:105:5)  
//     at async C:\repos\perfo\server.js:118:24 {
//   length: 160,
//   severity: 'FATAL',
//   code: '28000',
//   detail: undefined,
//   hint: undefined,
//   position: undefined,
//   internalPosition: undefined,
//   internalQuery: undefined,
//   where: undefined,
//   schema: undefined,
//   table: undefined,
//   column: undefined,
//   dataType: undefined,
//   constraint: undefined,
//   file: 'auth.c',
//   line: '646',
//   routine: 'ClientAuthentication'
// }

// Error 2
//TypeError: Cannot read properties of undefined (reading 'searchParams')
    // at parse (C:\repos\perfo\node_modules\pg-connection-string\index.js:39:30)      
    // at new ConnectionParameters (C:\repos\perfo\node_modules\pg\lib\connection-parameters.js:56:42)
    // at new Client (C:\repos\perfo\node_modules\pg\lib\client.js:18:33)
    // at BoundPool.newClient (C:\repos\perfo\node_modules\pg-pool\index.js:233:20)    
    // at BoundPool.connect (C:\repos\perfo\node_modules\pg-pool\index.js:227:10)      
    // at BoundPool.query (C:\repos\perfo\node_modules\pg-pool\index.js:411:10)        
    // at C:\repos\perfo\server.js:117:35
    // at Layer.handleRequest (C:\repos\perfo\node_modules\router\lib\layer.js:152:17) 
    // at next (C:\repos\perfo\node_modules\router\lib\route.js:157:13)
    // at Route.dispatch (C:\repos\perfo\node_modules\router\lib\route.js:117:3) 
</code></pre>
<p>Here is what I tried:</p>
<pre class=""lang-js prettyprint-override""><code>// #1
const pool = new Pool({connectionString: 'postgresql://username:##pw##@host:5432/dbname'})
// #2 - replacing hashtag with '%23'
const pool = new Pool({connectionString: 'postgresql://username:%23%23pw%23%23@host:5432/dbname'})

I have tried setting the password in a variable and then adding it to the connectionString and also tried encoding it.
// #3
var pw = &quot;##pw##&quot;
const pool = new Pool({connectionString: `postgresql://username:${pw}@host:5432/dbname`})
// or
const pool = new Pool({connectionString: &quot;postgresql://username:&quot;+pw+&quot;@host:5432/dbname&quot;})

// #4
var pw = encodeURIComponent('##pw##')
const pool = new Pool({connectionString: `postgresql://username:${pw}@host:5432/dbname`})
// or
const pool = new Pool({connectionString: &quot;postgresql://username:&quot;+pw+&quot;@host:5432/dbname&quot;})

Tried encoding the whole connection string:
// #5
var connstring = encodeURIComponent(&quot;postgres://username:##pw##@host:5432/dbname&quot;)
const pool = new Pool({connectionString: connstring})
// here I get Error: getaddrinfo ENOTFOUND base

Also tried to set the config not in a connection string:
// #6
const pool = new Pool({
    user: 'username',
    password: '##pw##', // or '%23%23pw%23%23' or encodeURIComponent('##pw##'),
    host: 'host',
    port: '5432',
    database: 'dbname'
})

// or
var config = {
    user: 'username',
    password: '##pw##', // or '%23%23pw%23%23' or encodeURIComponent('##pw##'),
    host: 'host',
    port: '5432',
    database: 'dbname'
}
const pool = new Pool(config)

</code></pre>
<p>Any idea on what I'm doing wrong?</p>
<p>Thank you!</p>
",0,0,0,2025-11-28T19:24:00+00:00,1,54,False
79832904,30872427,,postgresql,How to write test code for Prisma version 7,"<p>For testing Prisma, I used <code>prismock</code> or <code>prisma-mock</code> . But those packages are compatible with Prisma version 6 or earlier. I visit the Prisma Official website, and they are suggesting using <code>jest-mock-extended</code> or <code>vitest-mock-extended</code>.</p>
<p>This is fully manual and time-consuming. Do you have any soluation for this?</p>
",0,0,0,2025-11-28T20:42:42+00:00,1,61,True
79832942,23332429,,postgresql,How to call Postgres function from .NET?,"<p>I tried this code:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE OR REPLACE FUNCTION public.t4(in iid int, OUT n1 text, OUT n2 text)  
LANGUAGE plpgsql
AS $function$
    BEGIN 
            SELECT DISTINCT n1=l.name, n2=l.comment from public.languages l where l.id=iid;
    END;        
$function$;
</code></pre>
<pre class=""lang-cs prettyprint-override""><code> NpgsqlConnection connection = new NpgsqlConnection(ConString);
 connection.Open();
 NpgsqlCommand cmd = new NpgsqlCommand(&quot;SELECT * from public.t4()&quot;, connection);
 cmd.Parameters.Add(new NpgsqlParameter(&quot;iid&quot;, DbType.Int32) { Direction = ParameterDirection.Input });
 cmd.Parameters[0].Value = 18;
 cmd.Parameters.Add(new NpgsqlParameter(&quot;n1&quot;, DbType.String) { Direction = ParameterDirection.Output });
 cmd.Parameters.Add(new NpgsqlParameter(&quot;n2&quot;, DbType.String) { Direction = ParameterDirection.Output });
 cmd.ExecuteNonQuery();
 t1.Text = cmd.Parameters[1].Value.ToString();
 t2.Text = cmd.Parameters[2].Value.ToString();
</code></pre>
<p>And I got an error:</p>
<blockquote>
<p>Npgsql.PostgresException: '42883: function public.t4() does not exist.</p>
</blockquote>
<p>What am I doing wrong?</p>
",1,2,1,2025-11-28T22:10:04+00:00,2,131,True
79832965,742402,,postgresql,How to use index in simple select,"<p>Table has index on name column:</p>
<pre><code>    CREATE TABLE firma2.klient
    (
        kood character(12) primary key,
         nimi character(100),
       ...
    );
    
   CREATE INDEX IF NOT EXISTS klient_nimi_idx
    ON firma2.klient USING btree
    (nimi COLLATE pg_catalog.&quot;default&quot; ASC NULLS LAST)
    TABLESPACE pg_default;
</code></pre>
<p>Database settings have default values:</p>
<pre><code>enable_indexonlyscan       on
enable_indexscan           on
enable_indexonlyscan       on
enable_indexscan           on
cpu_index_tuple_cost       0.005                                                       
cpu_tuple_cost             0.01                                                    
</code></pre>
<p>Query</p>
<pre><code>SELECT * FROM firma2.klient WHERE nimi='John';
</code></pre>
<p>Runs slowly.</p>
<pre><code>analyze firma2.klient; 
explain analyze select * from firma2.klient where nimi='John'
</code></pre>
<p>Shows that index is not used:</p>
<pre><code>&quot;Seq Scan on klient  (cost=0.00..2287976.20 rows=1 width=4002) (actual time=12769.987..12769.988 rows=0 loops=1)&quot;
&quot;  Filter: (nimi = 'John'::bpchar)&quot;
&quot;  Rows Removed by Filter: 849971&quot;
&quot;Planning Time: 4.751 ms&quot;
&quot;Execution Time: 12770.029 ms&quot;
</code></pre>
<p>How to force Postgres to use index? It probably worked long time but suddenly stopped working today.
Re-started whole windows server but problem persists.</p>
<p>Using</p>
<p>PostgreSQL 17.5 on x86_64-windows, compiled by msvc-19.43.34808, 64-bit</p>
<p>in Windows Server 2022 vers 21H2</p>
",0,1,1,2025-11-28T22:55:14+00:00,0,172,False
79833204,15220892,,postgresql,ordering whole query by the field of multiset in jooq,"<p>in spring boot, i wanna have nested records, because if that i need to use multiset in my selects. then i need to order the whole query by fields in my multiset. but the problem is that i get a error that the table alias can not be found.<br />
so for that i use joins with multiset, and that makes the execution of my query take so long.<br />
and if i dont use multiset, i will not get nested record result at all and i have to make it nested on my own, and i dont want that.</p>
<p>now what is the best way of having the both sides of my requirement, having the nested record as i have it with multiset and also have ordering functionality as i do with join. but have the best performance and not making a redundant commands in my query.</p>
",1,0,0,2025-11-29T11:36:37+00:00,2,50,False
79833573,742402,,postgresql,How to upgrade cluster from Postgres 17 Windows to Postgres 18 Debian,"<p>Postgres 17 cluster is in Windows server in Estonian locale. Databases are defined like</p>
<pre><code>CREATE DATABASE mydb
    WITH
    OWNER = mydb_owner
    ENCODING = 'UTF8'
    LC_COLLATE = 'et-EE'
    LC_CTYPE = 'et-EE'
    LOCALE_PROVIDER = 'libc'
    TABLESPACE = pg_default
    CONNECTION LIMIT = -1
    IS_TEMPLATE = False;
</code></pre>
<p>Trying to move this to Postgres 18 running in Debian</p>
<pre><code>pg_dumpall --clean -h windowsserver.com -U postgres | psql --echo-errors -h localhost -U postgres
</code></pre>
<p>Throws</p>
<pre><code> ERROR:  invalid LC_COLLATE locale name: &quot;et-EE&quot; 
 STATEMENT:  CREATE DATABASE template1 WITH TEMPLATE = template0 ENCODING = 'UTF8'
 LOCALE_PROVIDER = libc LOCALE = 'et-EE';
</code></pre>
<p>locale -a shows that Estonian locale is present:</p>
<pre><code>C
C.utf8
en_US.utf8
et_EE.utf8
POSIX
</code></pre>
<p>How to move cluster to Postgres 18 ?</p>
",-1,0,1,2025-11-29T22:15:16+00:00,0,30,False
79833901,24872728,,postgresql,Trigger on PostgreSQL updating salary column,"<p>I am having some troubles setting a trigger in PostgreSQL. I have a table <code>Employees</code> with columns such as <code>employee_id</code>, <code>salary</code> and <code>hourly_pay</code>. The <code>salary</code> is annual, so I'd like to change the <code>salary</code> when I change the <code>hourly pay</code>.</p>
<p>For instance, by setting <code>hourly pay</code> a value, the <code>salary</code> must be updated to 2080 times that value. But it seems that I can't make the trigger at all. What is wrong with the following code?</p>
<pre><code>create or replace function salary_update_function()
returns trigger as $salary_update$
begin
    update employees 
    set new.salary = new.hourly_pay*2080;

    return new;
end;
$salary_update$ language plpgsql;

create trigger salary_update_trigger
before update of hourly_pay on employees
for each row execute function salary_update_function();
</code></pre>
",1,1,0,2025-11-30T12:55:13+00:00,2,90,True
79834214,1216939,,postgresql,How to connect to a Postgres accessible under a pathname via JDBC?,"<p>I have a Postgres on a server which is available under <code>https://somethig.my.domain/postgres</code> on port 5432.</p>
<p>I'm trying to connect via <a href=""https://www.jetbrains.com/datagrip/"" rel=""nofollow noreferrer""><em>DataGrip</em></a> which uses a JDBC driver. But I cannot figure out the connection string.</p>
<p><a href=""https://jdbc.postgresql.org/documentation/use/#connecting-to-the-database"" rel=""nofollow noreferrer"">Official docs</a> doesn't have an example with a subpath:</p>
<ul>
<li>jdbc:postgresql:database</li>
<li>jdbc:postgresql:/</li>
<li>jdbc:postgresql://host/database</li>
<li>jdbc:postgresql://host/</li>
<li>jdbc:postgresql://host:port/database</li>
<li>jdbc:postgresql://host:port/</li>
</ul>
<p>And I cannot find a way for a subpath there.</p>
<p><s>Please note that at least 2 different ORMs for Node.js connect to the Postgres without any issue, so it's definitely possible. The question is how to form JDBC connection string.</s> Nope, I'm just connecting to Docker containers here instead of reverse-proxied Postgres under a subpath. Please ignore.</p>
",0,1,1,2025-11-30T22:09:45+00:00,3,84,True
79834760,5878883,,postgresql,"PostgreSQL, unique on combination of two columns","<p>In PostgreSQL, it is easy to set a uniqueness constraint for two columns using CONSTRAINT UNIQUE.<br />
For example</p>
<pre><code>CREATE TABLE someTable (
    id serial PRIMARY KEY,
    col1 int NOT NULL,
    col2 int NOT NULL,
    UNIQUE (col1, col2)
)
</code></pre>
<p>in this case, col1, col2 will be acceptable:</p>
<pre><code>1 1
1 2
2 1
2 2
</code></pre>
<p>But what if I need a pair of values to be unique? i.e. if there is a pair</p>
<pre><code>1 2
</code></pre>
<p>then pairs</p>
<pre><code>2 1
</code></pre>
<p>will be already unacceptable.<br />
How to achieve this?</p>
",0,0,0,2025-12-01T11:41:13+00:00,3,86,True
79835440,6711573,,postgresql,Trigger Function to check record uniqueness,"<p>I have a trigger in Postgresql 13 like this to prevent duplicate entry on one of the column.</p>
<p>This trigger is used in Arcgis Field Map apps.</p>
<pre><code>CREATE OR REPLACE FUNCTION xxx_check_unique()
                           RETURNS TRIGGER
AS
$$
BEGIN
IF (SELECT EXISTS(SELECT FROM schema1.chamber where id = new.id)) = 'false' THEN   //check 
    RETURN new;   //if no duplicate exist, enter this value
    else
    RAISE INFO '% DUP', new.id;
    return null;  //if there is duplicate, do nothing       
end if;
END;
$$
LANGUAGE plpgsql;
</code></pre>
<pre><code>CREATE OR REPLACE TRIGGER xx_check_unique
BEFORE update of id ON schema1.chamber
FOR EACH ROW EXECUTE PROCEDURE xxx_check_unique();
CREATE OR REPLACE TRIGGER xx_check_unique
BEFORE insert ON schema1.chamber
FOR EACH ROW EXECUTE PROCEDURE xxx_check_unique();
</code></pre>
<p>Questions:</p>
<ol>
<li><p>Is the part <code>RETURN NULL</code> correct? This part is causing the Field Map to return an error if a duplicate is entered. In the context of Field Map, after the error, all the text boxes have to be emptied out. I have to re-enter the data again from the very beginning with another unique ID. This is pain.</p>
</li>
<li><p>Is there any other argument for <code>RETURN</code> that does not clear out the already entered data in the form?</p>
</li>
</ol>
<p>Many thanks</p>
",0,0,0,2025-12-02T02:54:47+00:00,3,54,True
79835521,5693776,,postgresql,What is the method to combine multiple sheets when there are no common fields?,"<p>I am uncertain about how to merge data from multiple sheets using only the file name, especially when the file names vary in case.</p>
<p>C:\Report\overview.csv:</p>
<pre><code>overview.csv.

Issue Name, Issue Type, Issue Priority
Validation: Missing &lt;head&gt; Tag,Issue,High
Validation: Multiple &lt;body&gt; Tags,Issue,High
</code></pre>
<p>C:\Report\validation_missing_head_tag.csv</p>
<pre><code>validation_missing_head_tag.csv

Address,Content Type,Status Code,Status,Indexability
https://g2.com/view ,text/html,200,OK,Indexable
https://g2.com/view1 ,text/html,200,OK,Indexable

validation_multiple_body_tags.csv

Address,Content Type,Status Code,Status,Indexability
https://g2.com/sampleview, text/html,200,OK,Indexable
</code></pre>
<p>Now, I want to join overview.csv with validation_missing_head_tag and validation_multiple_body_tags.</p>
<p>The purpose of this is to merge the columns Issue Type and Issue Priority.</p>
<p>Expected Output:</p>
<pre><code>Issue Name, Issue Type, Issue Priority, Address, Content Type, Status Code,Status, Indeability
validation: Missing &lt;head&gt; Tag, Issue, High, https://g2.com/view ,text/html,200,OK,Indexable
Validation: Missing &lt;head&gt; Tag,Issue,High,https://g2.com/view1 ,text/html,200,OK,Indexable
Validation: Multiple &lt;body&gt; Tags,Issue,High,https://g2.com/sampleview, text/html,200,OK,Indexable
</code></pre>
<p>It would be preferable to combine this using the C# method or SQL to combine those tables.</p>
",-8,0,8,2025-12-02T05:48:24+00:00,1,134,True
79835758,29004102,,postgresql,Prisma V7 Error: Client Password must be a string,"<p>I am sorry if it's asked already, I just migrated to Prisma V7 and am stuck with an unsolvable error.</p>
<pre><code>SCRAM-SERVER-FIRST-MESSAGE: client password must be a string
</code></pre>
<p>The server runs fine, Though the error only occurs when i change the database using Prisma on API calls. I have verified the version of prisma client, Prisma and Prisma adapter, All are V 7.01. This is how I initialize prisma:</p>
<pre><code>import { PrismaClient } from &quot;../../generated/prisma/client.js&quot;;
import { PrismaPg } from &quot;@prisma/adapter-pg&quot;;
import dotenv from &quot;dotenv&quot;;

const adapter = new PrismaPg({
    connectionString: process.env.DATABASE_URL
});

const prismaClientSingleton = () =&gt; {
    dotenv.config();
    console.log(&quot;DATABASE_URL =&quot;, process.env.DATABASE_URL);
    return new PrismaClient({
        adapter: adapter
    });
};
declare const globalThis: {
    prismaGlobal: ReturnType&lt;typeof prismaClientSingleton&gt;
} &amp; typeof global;

const prisma: PrismaClient = globalThis.prismaGlobal ?? prismaClientSingleton();

if (process.env.NODE_ENV !== &quot;production&quot;) {
    globalThis.prismaGlobal = prisma;
};

export default prisma;
</code></pre>
<p>My Prisma.config:</p>
<pre><code>import { defineConfig, env } from &quot;prisma/config&quot;;
import &quot;dotenv/config&quot;;

export default defineConfig({
    schema: &quot;prisma/schema.prisma&quot;,
    migrations: {
        path: &quot;prisma/migrations&quot;
    },
    datasource: {
        url: env(&quot;DATABASE_URL&quot;),
        shadowDatabaseUrl: env(&quot;DIRECT_URL&quot;)
    }
});
</code></pre>
<p>I have made sure My DB URL isnt null:</p>
<pre><code>&gt; server@1.0.0 dev
&gt; nodemon start

[nodemon] 3.1.10
[nodemon] to restart at any time, enter `rs`
[nodemon] watching path(s): *.*
[nodemon] watching extensions: js,mjs,cjs,json
[nodemon] starting `node start dist/index.js`
[dotenv@17.2.2] injecting env (9) from .env -- tip: 📡 version env with Radar: https://dotenvx.com/radar
DATABASE_URL = postgresql://postgres:EducatorSQLSchool@localhost:5432/authentication_fullstack
</code></pre>
<p>All help would be appreciated!</p>
",0,0,0,2025-12-02T10:44:00+00:00,1,160,False
79836803,7700150,"Hamburg, Germany",postgresql,Drizzle update custom type with magic sql operator,"<p>I have a PostgreSQL database that I connect to with Drizzle. I want to do a bulk update of a JSON field with a custom type. The field is specified <a href=""https://github.com/openSenseMap/frontend/blob/fix/deadlock-possible-when-posting-box-locations-via-measurement/app/schema/sensor.ts#L48"" rel=""nofollow noreferrer"">like this</a>:</p>
<pre class=""lang-js prettyprint-override""><code>lastMeasurement: json(&quot;lastMeasurement&quot;).$type&lt;LastMeasurement&gt;(),
</code></pre>
<p>The bulk update then looks <a href=""https://github.com/openSenseMap/frontend/blob/fix/deadlock-possible-when-posting-box-locations-via-measurement/app/models/measurement.server.ts#L526"" rel=""nofollow noreferrer"">like this</a>:</p>
<pre class=""lang-js prettyprint-override""><code>async function updateLastMeasurements(lastMeasurements: Record&lt;string, NonNullable&lt;LastMeasurement&gt;&gt;, tx: any) {
  const sqlChunks: SQL[] = [
    sql`(case`,
    ...Object.entries(lastMeasurements).map(([sensorId, lastMeasurement]) =&gt;
      [sql`when ${sensor.id} = ${sensorId} then`, sql&lt;LastMeasurement&gt;`${JSON.stringify(lastMeasurement)}`]
    ).flat(),
    sql`end)`
  ];

  const finalSql: SQL = sql.join(sqlChunks, sql.raw(' '));

  await tx
    .update(sensor)
    .set({ lastMeasurement: finalSql })
    .where(inArray(sensor.id, Object.keys(lastMeasurements)));
}
</code></pre>
<p>Notice how I need to use the <a href=""https://orm.drizzle.team/docs/sql"" rel=""nofollow noreferrer"">magic</a> <code>sql</code> operator to have a bulk update with different values. This is inspired by <a href=""https://orm.drizzle.team/docs/guides/update-many-with-different-value"" rel=""nofollow noreferrer"">this example</a>. This would work fine if the type of the field to be updated would be <code>string</code>; however, it's of type <code>LastMeasurement</code> (or <code>json</code>?). Thus an error is thrown:</p>
<pre><code>Error: Failed query: update &quot;sensor&quot; set &quot;lastMeasurement&quot; = (case when &quot;sensor&quot;.&quot;id&quot; = $1 then $2 end) where &quot;sensor&quot;.&quot;id&quot; in ($3)
params: c3e481d40550d771523b1f94,{&quot;value&quot;:0,&quot;createdAt&quot;:&quot;2000-05-25T11:11:11.000Z&quot;,&quot;sensorId&quot;:&quot;c3e481d40550d771523b1f94&quot;},c3e481d40550d771523b1f94
 ❯ PostgresJsPreparedQuery.queryWithCache node_modules/src/pg-core/session.ts:73:11
 ❯ updateLastMeasurements app/models/measurement.server.ts:537:3
    535|   const finalSql: SQL = sql.join(sqlChunks, sql.raw(' '));
    536| 
    537|   await tx
       |   ^
    538|     .update(sensor)
    539|     .set({ lastMeasurement: finalSql })
 ❯ app/models/measurement.server.ts:303:5
 ❯ scope node_modules/postgres/src/index.js:260:18
 ❯ sql.begin node_modules/postgres/src/index.js:243:14
 ❯ saveMeasurements app/models/measurement.server.ts:299:3
 ❯ tests/routes/api.boxes.$deviceId.locations.spec.ts:96:5

Caused by: PostgresError: column &quot;lastMeasurement&quot; is of type json but expression is of type text
 ❯ ErrorResponse node_modules/postgres/src/connection.js:794:26
 ❯ handle node_modules/postgres/src/connection.js:480:6
 ❯ TLSSocket.data node_modules/postgres/src/connection.js:315:9

⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
Serialized Error: { severity_local: 'ERROR', severity: 'ERROR', code: '42804', hint: 'You will need to rewrite or cast the expression.', position: '42', file: 'parse_target.c', routine: 'transformAssignedExpr', query: 'update &quot;sensor&quot; set &quot;lastMeasurement&quot; = (case when &quot;sensor&quot;.&quot;id&quot; = $1 then $2 end) where &quot;sensor&quot;.&quot;id&quot; in ($3)', parameters: [ 'c3e481d40550d771523b1f94', '{&quot;value&quot;:0,&quot;createdAt&quot;:&quot;2000-05-25T11:11:11.000Z&quot;,&quot;sensorId&quot;:&quot;c3e481d40550d771523b1f94&quot;}', 'c3e481d40550d771523b1f94' ], args: [ 'c3e481d40550d771523b1f94', '{&quot;value&quot;:0,&quot;createdAt&quot;:&quot;2000-05-25T11:11:11.000Z&quot;,&quot;sensorId&quot;:&quot;c3e481d40550d771523b1f94&quot;}', 'c3e481d40550d771523b1f94' ], types: [ +0, +0, +0 ] }
</code></pre>
<p>So I guess the question is, how can I cast the expression within the magic <code>sql</code> operator? The whole project is <a href=""https://github.com/openSenseMap/frontend/tree/fix/deadlock-possible-when-posting-box-locations-via-measurement"" rel=""nofollow noreferrer"">open source</a>, if you feel like taking a closer look</p>
",0,0,0,2025-12-03T11:19:14+00:00,1,56,False
79837537,2727670,"Copenhagen, DK",postgresql,Postgres logical replication to restored snapshot,"<p>I am attempting to logically replicate a production postgres instance to facilitate a blue/green major version upgrade from 13 -&gt; 18 (don't ask).</p>
<p>First, I start a replication slot on the blue (production) instance to begin WAL retention and take note of the LSN.</p>
<pre><code>CREATE PUBLICATION all_rep FOR ALL TABLES;
SELECT * FROM pg_create_logical_replication_slot('all_rep_sub', 'pgoutput');
</code></pre>
<p>I take a snapshot of the blue environment, and bring up the green from it immediately, which takes 1-2 hours or so.</p>
<p>I advance the slot's LSN to the subscriber's</p>
<pre><code>SELECT pg_current_wal_lsn();
</code></pre>
<p>I subscribe from green to blue:</p>
<pre><code>CREATE SUBSCRIPTION all_rep_sub CONNECTION 'connstring' PUBLICATION all_rep 
  WITH (slot_name = 'all_rep_sub', create_slot = false, copy_data = false);
</code></pre>
<p>I see two behaviors I don't understand: firstly, despite advancing the LSN on the slot successfully, I see default uuid primary key conflicts on green on busy timeseries tables. I would expect the WAL replay to &quot;pick up where it left of&quot; wrt the data on the green instance. Am I picking the wrong LSN? Is there any way I can reliably see the &quot;last&quot; LSN that the snapshot had consumed before it was taken?</p>
<p>Second and more importantly, the slot never advances, at all. I had suspected that maybe the volume was too high and the single slot simply couldn't keep up, but in that case I would expect to see the slot advancing at least some, just not keeping pace. But I basically see it staying completely frozen.</p>
<p>I have tried to eliminate long running transactions on blue. There are wraparound vacuums but as far as I know they are not long lived.</p>
<p>I have 5 large (billions) timeseries tables that are partitioned by day.</p>
<p>To add to the mystery, if I create 4 distinct slots for the timeseries tables and 1 for &quot;everything else,&quot; the replication seems to work fine!</p>
<p>This is managed on aws rds, and the stuck <code>all tables</code> replication is preventing us from using the aws blue/green deployment. I have a suspicion that, since I'm able to &quot;replicate&quot; the issue (lol) manually by creating my own all tables publication, the partitions are maybe not playing well with it, and that aws managed b/g on rds is using that under the hood, which is the show stopper, but I have limited visibility into what is stalling the slot.</p>
<p>EDIT: to add a small amount of context, I am able to see the slot not advance if I advance the LSN past a <em>known</em> point of overlap, in other words, I am guaranteed data loss in that scenario but no overlap. So these are two different issues kind of, but I understand that if the replication slot hits a conflict, it will stop streaming.</p>
",2,2,0,2025-12-04T05:21:55+00:00,0,40,False
79837688,2908017,"Bloemfontein, Free-State, South Africa",postgresql,How do I create a &quot;UUIDv7&quot; column using the pgAdmin GUI in PostgreSQL?,"<p>I'm trying to figure out how I can add a new primary key column using the new <code>UUIDv7</code> from PostgreSQL v18.</p>
<p>From reading online. It seems I can add them like this with code:</p>
<pre class=""lang-sql prettyprint-override""><code>profile_id UUID PRIMARY KEY DEFAULT uuidv7()
</code></pre>
<p>But I'm not finding anything that explains how to add them using pgAdmin GUI. From what I can see, I'm able to add a <code>uuid</code> column like this:</p>
<p><a href=""https://i.sstatic.net/67rUcaBM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/67rUcaBM.png"" alt=""uuid in pgAdmin"" /></a></p>
<p>But I'm not seeing where to specify v7 for it. If I check the created code for the above, then it looks like this:</p>
<pre class=""lang-sql prettyprint-override""><code>profile_id uuid NOT NULL,
CONSTRAINT profile_pkey PRIMARY KEY (profile_id)
</code></pre>
<p>So it doesn't seem like it's creating v7.</p>
<p>How can I do v7 using the GUI?</p>
",1,1,0,2025-12-04T08:26:28+00:00,1,68,True
79838006,8772319,,postgresql,Why does this `select` work in PostgreSQL?,"<pre class=""lang-sql prettyprint-override""><code>select unnest(array[1, 2, 3]), unnest(array[4, 5, 6])
</code></pre>
<p>It produces the following</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>unnest</th>
<th>unnest</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>3</td>
<td>6</td>
</tr>
</tbody>
</table></div>
<p>The docs on <a href=""https://www.postgresql.org/docs/current/queries-table-expressions.html#QUERIES-TABLEFUNCTIONS"" rel=""nofollow noreferrer"">table functions</a> indicate that the proper syntax should be</p>
<pre class=""lang-sql prettyprint-override""><code>select * from unnest(array[1, 2, 3], array[4, 5, 6])
</code></pre>
<p>Is this one of those cases where PostgreSQL is being forgiving, or is there an actual mechanism that's being exploited here?</p>
",3,3,0,2025-12-04T14:11:00+00:00,1,103,True
79838116,31997781,,postgresql,Why does SET p1 = $person in Apache AGE Cypher return “SET clause expects a map” when using a prepared statement?,"<p>I'm using <a href=""https://github.com/apache/age/tree/release/PG17/1.6.0"" rel=""nofollow noreferrer"">Apache AGE with PostgreSQL 17</a>. I’m trying to create a node and set all of its properties from a parameter map using a prepared statement, as mentioned here: <a href=""https://age.apache.org/age-manual/master/advanced/prepared_statements.html"" rel=""nofollow noreferrer"">https://age.apache.org/age-manual/master/advanced/prepared_statements.html</a>.</p>
<pre class=""lang-sql prettyprint-override""><code>PREPARE ag_create_person(agtype) AG 
SELECT * FROM cypher(
    'graph', 
    $$ CREATE (p1:Person) SET p1 = $person $$, 
    $1) 
AS (result agtype);
</code></pre>
<p>However, PostgreSQL returns this error:</p>
<pre class=""lang-bash prettyprint-override""><code>ERROR:  SET clause expects a map
LINE 2: ... * FROM cypher('graph', $$ create (p1:Person) set p1 = $pers...
</code></pre>
<p>Using <code>SET p1.name = $name</code> works, but it requires manually listing all property keys and values.</p>
<p>My question is: Does AGE support using <code>SET p1 = $param</code> with <code>agtype</code>? If not, what is the correct syntax for parameterized property maps?</p>
",1,1,0,2025-12-04T15:53:52+00:00,0,40,False
79838406,5202246,"Kyiv, Ukraine",postgresql,pgAdmin 4 Query Tool Workspace connection failed FATAL: database &quot;NNNNN&quot; does not exist,"<p>When I try connecting to my database from the &quot;Query Tool Workspace&quot; tab, it refuses with an error like this:</p>
<blockquote>
<p>connection failed: connection to server at &quot;127.0.0.1&quot;, port 5432 failed: FATAL: database &quot;16388&quot; does not exist</p>
</blockquote>
<p><a href=""https://i.sstatic.net/WfS3lwXK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WfS3lwXK.png"" alt=""connection failederror"" /></a></p>
<p>That database exists, and I can connect to it using psql, like this:</p>
<pre><code>$ psql -h localhost -p 5432 -U postgres -d testdb
Password for user postgres: 
psql (18.1 (Ubuntu 18.1-1.pgdg24.04+2), server 17.5 (Debian 17.5-1.pgdg120+1))
Type &quot;help&quot; for help.

testdb=# \l
                                                    List of databases
   Name    |  Owner   | Encoding | Locale Provider |  Collate   |   Ctype    | Locale | ICU Rules |   Access privileges   
-----------+----------+----------+-----------------+------------+------------+--------+-----------+-----------------------
 postgres  | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | 
 template0 | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/postgres          +
           |          |          |                 |            |            |        |           | postgres=CTc/postgres
 template1 | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/postgres          +
           |          |          |                 |            |            |        |           | postgres=CTc/postgres
 testdb    | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | 
(4 rows)
</code></pre>
<p>Here are my connection parameters from the &quot;Query Tool Workspace&quot; tab:
<a href=""https://i.sstatic.net/IYfyVJRW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IYfyVJRW.png"" alt=""connection parameters from QT Workspace window"" /></a></p>
<p>Here's my working Query Tool from the Default Workspace tab:
<a href=""https://i.sstatic.net/jOQwd0Fd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jOQwd0Fd.png"" alt=""working QT with the same connection params in Default Workspace"" /></a></p>
<p><strong>Have you encountered this issue? Do you know how to fix it?</strong></p>
<p>Version info (from &quot;About pgAdmin 4&quot;):</p>
<pre><code>Version - 9.10
Application Mode - Desktop
Commit - 60c56b328584c0d7f3a4b28601d01adde4ba2882 2025-11-10
Python Version - 3.12.3
Current User - pgadmin4@pgadmin.org
Electron Version - 39.1.1
Browser - Chrome 142.0.7444.59
Operating System - Ubuntu 24.04.3 LTS, Linux-6.8.0-88-generic-x86_64-with-glibc2.39
</code></pre>
<p><a href=""https://i.sstatic.net/3GYWCnPl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3GYWCnPl.png"" alt=""About pgAdmin 4"" /></a></p>
",2,2,0,2025-12-04T22:36:47+00:00,1,144,True
79838482,4759176,,postgresql,"Testcontainer: H2 is still used in test, Domain &quot;TEXT&quot; not found;","<p>I have a very simple example: the entity uses <code>TEXT</code> column and the test fails because of it, despite using PostgreSQL testcontainer:</p>
<p><strong>application-test.properties</strong></p>
<pre><code>spring.datasource.url=jdbc:postgresql://localhost:5432/testdb   
spring.datasource.username=postgres
spring.datasource.password=postgres
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.jdbc.batch_size=0
</code></pre>
<p><strong>Dog.java</strong>:</p>
<pre class=""lang-java prettyprint-override""><code>@Entity
@Getter
@Setter
public class Dog {
    @Id
    @SequenceGenerator(name = &quot;dog_sequence&quot;,
            sequenceName = &quot;dog_sequence&quot;,
            allocationSize = 1)
    @GeneratedValue(strategy = GenerationType.SEQUENCE,
            generator = &quot;dog_sequence&quot;)
    private Long id;

    @Column
    private String name;
    @Column(columnDefinition = &quot;TEXT&quot;)
    private String description;

}
</code></pre>
<p><strong>DogRepository.java</strong>:</p>
<pre class=""lang-java prettyprint-override""><code>public interface DogRepository extends JpaRepository&lt;Dog, Long&gt; {}
</code></pre>
<p><strong>DogTest.java</strong>:</p>
<pre class=""lang-java prettyprint-override""><code>@Testcontainers
@DataJpaTest
@ActiveProfiles(&quot;test&quot;)
public class DogTest {
    @Container
    static PostgreSQLContainer&lt;?&gt; postgres = new PostgreSQLContainer&lt;&gt;(&quot;postgres:15.4&quot;)
            .withDatabaseName(&quot;testdb&quot;)
            .withUsername(&quot;postgres&quot;)
            .withPassword(&quot;postgres&quot;);

    @DynamicPropertySource
    static void overrideProperties(DynamicPropertyRegistry registry) {
        registry.add(&quot;spring.datasource.url&quot;, postgres::getJdbcUrl);
        registry.add(&quot;spring.datasource.username&quot;, postgres::getUsername);
        registry.add(&quot;spring.datasource.password&quot;, postgres::getPassword);
        registry.add(&quot;spring.datasource.driver-class-name&quot;, postgres::getDriverClassName);
        registry.add(&quot;spring.jpa.hibernate.ddl-auto&quot;, () -&gt; &quot;create-drop&quot;);
        registry.add(&quot;spring.jpa.database-platform&quot;, () -&gt; &quot;org.hibernate.dialect.PostgreSQLDialect&quot;);
    }


    @Autowired
    private DogRepository repository;

    @Autowired
    private TestEntityManager entityManager;


    @Before
    public void setUp() {
        for (int i = 0; i &lt; 10; i++) {
            Dog dog = new Dog();
            entityManager.persist(dog);
        }

        entityManager.flush();
    }


    @Test
    public void testFindDogs() {
        List&lt;Dog&gt; dogs = repository.findAll();
        Assertions.assertEquals(1, dogs.size());

    }
}
</code></pre>
<p>When I run the test I get the error:</p>
<pre><code>Hibernate: 
    create table &quot;dog&quot; (
        &quot;id&quot; bigint not null,
        &quot;description&quot; &quot;TEXT&quot;,
        &quot;name&quot; varchar(255),
        primary key (&quot;id&quot;)
    )
2025-12-04T20:27:04.096-05:00  WARN 560907 --- [           main] o.h.t.s.i.ExceptionHandlerLoggedImpl     : GenerationTarget encountered exception accepting command : Error executing DDL &quot;
    create table &quot;dog&quot; (
        &quot;id&quot; bigint not null,
        &quot;description&quot; &quot;TEXT&quot;,
        &quot;name&quot; varchar(255),
        primary key (&quot;id&quot;)
    )&quot; via JDBC [Domain &quot;TEXT&quot; not found;]

org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL &quot;
    create table &quot;dog&quot; (
        &quot;id&quot; bigint not null,
        &quot;description&quot; &quot;TEXT&quot;,
        &quot;name&quot; varchar(255),
        primary key (&quot;id&quot;)
    )&quot; via JDBC [Domain &quot;TEXT&quot; not found;]
</code></pre>
<p>From what I understand, <code>TEXT</code> is not used by <code>H2</code>, however works with <code>PostgreSQL</code>. It should work with PostgreSQL testcontainer, yet it doesn't, so it seems like the test is still using <code>H2</code>, and I don't see why.</p>
<p>How do I make the test run without removing <code>TEXT</code> column type?</p>
",2,2,0,2025-12-05T01:30:27+00:00,2,81,True
79838612,32001765,,postgresql,Risks of using Java 25 LTS versus Java 21 LTS in a new Spring Boot backend,"<p>I’m starting an inventory system: multi-branch, reporting, invoicing, which I plan to maintain for several years.</p>
<p>Tentative stack:
Backend: Java + Spring Boot
DB: PostgreSQL
Interface: Angular 21</p>
<p>But I have some doubts about versions:
Java:</p>
<p>Option A: Java 21 LTS + Spring Boot 3.5
Option B: Java 25 LTS + Spring Boot 3.5 or 4</p>
<p>PostgreSQL:
16, 17 or even 18 for this new project.</p>
<p>And I’d like to clarify a few questions about the following:</p>
<p>For a new backend with Spring Boot, what risks do you see in using Java 25 LTS instead of Java 21 (library compatibility, tooling, stability) as of today?</p>
<p>For PostgreSQL, what criteria would you use to choose between 16, 17 or 18 in a CRUD-intensive inventory system? Is there any strong reason to avoid any of these versions in production right now?</p>
",0,0,0,2025-12-05T07:05:07+00:00,2,136,True
79838816,31475874,,postgresql,"Error when running docker compose up command in terminal (Powershell, Windows 11)","<p>I'm following along with this tutorial: <a href=""https://www.youtube.com/watch?v=PHsC_t0j1dU&amp;t=740s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=PHsC_t0j1dU&amp;t=740s</a></p>
<p>As shown in the video at timestamp 1:29:09, when the person runs <code>docker compose up</code> in their terminal, they don't get an error such as &quot;elt-elt_script-1 exited with code 0&quot;.</p>
<p>However, when I run the same command in my own terminal, I get this:</p>
<pre><code>elt_script-1 exited with code 1
</code></pre>
<p>The following error comes from my Python script:</p>
<pre><code>Traceback (most recent call last):  
elt_script-1            |   File &quot;elt_script.py&quot;, line 54, in \&lt;module\&gt;  
elt_script-1            |     subprocess.run(dump_command, env=subprocess_env, check=True)  
elt_script-1            |   File &quot;/usr/local/lib/python3.8/subprocess.py&quot;, line 516, in run  
elt_script-1            |     raise CalledProcessError(retcode, process.args,  
elt_script-1            | subprocess.CalledProcessError: Command '\['pg_dump', '-h', 'source_postgres', '-U', 'postgres', '-d', 'source_db', '-f', 'data_dump.sql', '-w'\]' returned non-zero exit status 1
</code></pre>
<p>I've been trying to figure this out for a long time, and can't find a solution. I'm going to keep trying, but I really need your help. I'm new to this, and it's doing my head in right now.</p>
<p>As we can see from this error, the problem happens when running the command in my Python script (<code>elt_script.py</code>). Here is the script:</p>
<pre class=""lang-py prettyprint-override""><code>import subprocess
import time


def wait_for_postgres(host, max_retries=5, delay_seconds=5):
    retries = 0
    while retries &lt; max_retries:
        try:
            result = subprocess.run(
                [&quot;pg_isready&quot;, &quot;-h&quot;, host], check=True, capture_output=True, text=True)
            if &quot;accepting connections&quot; in result.stdout:
                print(&quot;Successfully connected to Postgres&quot;)
            return True
        except subprocess.CalledProcessError as e:
            print(f&quot;Error connecting to Postgres: {e}&quot;)
            retries += 1
            print(
                f&quot;Retrying in {delay_seconds} seconds... (Attempt {retries}/{max_retries})&quot;)
            time.sleep(delay_seconds)
    print(&quot;Max retries reached. Exiting.&quot;)
    return False


if not wait_for_postgres(host=&quot;source_postgres&quot;):
    exit(1)

print(&quot;Starting ELT script...&quot;)

source_config = {
  'dbname': 'source_db',
  'user': 'postgres',
  'password': 'secret',
  'host': 'source_postgres'
}

destination_config = {
  'dbname': 'destination_db',
  'user': 'postgres',
  'password': 'secret',
  'host': 'destination_postgres'
}

dump_command = [
  'pg_dump',
  '-h', source_config['host'],
  '-U', source_config['user'],
  '-d', source_config['dbname'],
  '-f', 'data_dump.sql',
  '-w'
]

subprocess_env = dict(PGPASSWORD=source_config['password'])

subprocess.run(dump_command, env=subprocess_env, check=True)

load_command = [
  'psql',
  '-h', destination_config['host'],
  '-U', destination_config['user'],
  '-d', destination_config['dbname'],
  '-a', '-f', 'data_dump.sql',
]

subprocess_env = dict(PGPASSWORD=destination_config['password'])

subprocess.run(load_command, env=subprocess_env, check=True)

print(&quot;Ending ELT Script...&quot;)
</code></pre>
<p>Here is my Dockerfile as well, in case it helps:</p>
<pre class=""lang-none prettyprint-override""><code>FROM python:3.8-slim

RUN apt-get update &amp;&amp; apt-get install -y postgresql-client

COPY elt_script.py .

CMD [ &quot;python&quot;, &quot;elt_script.py&quot; ]
</code></pre>
<p>And here is my docker compose file:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3'

services:
  source_postgres:
    image: postgres:18
    ports:
      - &quot;5433:5432&quot;
    networks:
      - elt_network
    environment:
      POSTGRES_DB: source_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    volumes:
      - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/init.sql

  destination_postgres:
    image: postgres:18
    ports:
      - &quot;5434:5432&quot;
    networks:
      - elt_network
    environment:
      POSTGRES_DB: destination_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    volumes:
      - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/init.sql
    
  elt_script:
    build:
      context: ./elt
      dockerfile: Dockerfile
    command: [&quot;python&quot;, &quot;elt_script.py&quot;]  
    networks:
      - elt_network
    depends_on:
      - source_postgres
      - destination_postgres
  
networks:
  elt_network:
    driver: bridge
</code></pre>
<p>I appreciate you sticking around this long. Your help would be greatly appreciated. Constructive feedback is also welcome, as I want to grow and continue learning. If you need anymore information, please don't hesitate to ask.</p>
",1,1,0,2025-12-05T11:17:44+00:00,0,96,False
79838897,32003520,,postgresql,PostgreSQL: Should I store timestamps as UNIX epoch or timestamptz?,"<p>I'm dealing with a timezone issue in a PostgreSQL project and would like some advice.</p>
<p>I'm currently choosing between two ways to store timestamps:</p>
<h3><strong>1. UNIX timestamp (bigint)</strong></h3>
<ul>
<li><p>Very safe in terms of timezone handling (it's always UTC)</p>
</li>
<li><p>Easy to work with in application code</p>
</li>
<li><p>But not human-readable in the database</p>
</li>
</ul>
<h3><strong>2. <code>timestamptz</code></strong></h3>
<ul>
<li><p>Much more readable in PostgreSQL</p>
</li>
<li><p>Internally stored as UTC</p>
</li>
<li><p>But inserting values requires more discipline:<br />
if the application accidentally inserts a local time (or the session timezone isn't what you expect), it can lead to unexpected results.</p>
<p><strong>Given this tradeoff, what is the recommended approach?<br />
Is there a good way to get both safety and readability without risking incorrect inserts?</strong></p>
</li>
</ul>
<p>Thanks for the explanation! I have a follow-up question:</p>
<p>What if I store the real value as a UNIX timestamp (bigint), but create a PostgreSQL VIEW that converts it to a readable timestamptz using to_timestamp()?
I also store the original IANA timezone (e.g., &quot;Asia/Tokyo&quot;) in a separate column for reconstructing local times.</p>
<p>This gives me:</p>
<ul>
<li><p>safe storage (always UTC)</p>
</li>
<li><p>readable timestamps in SQL</p>
</li>
<li><p>access to PostgreSQL’s timestamp functions</p>
</li>
<li><p>preserved timezone information for converting back to local time</p>
</li>
</ul>
<p>Is this an acceptable approach, or is there a downside I'm missing?</p>
",3,0,0,2025-12-05T12:48:52+00:00,4,100,True
79839460,20376176,,postgresql,UPDATE using jsonb_delete doesn&#39;t remove the target keys,"<p>I'm learning the <code>jsonb</code> data type in PostgreSQL. I executed the following statements:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE my_table (
    id SERIAL PRIMARY KEY,
    data JSONB
);
INSERT INTO my_table (data)
VALUES ('{&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30, &quot;address&quot;: {&quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;}}');
</code></pre>
<p>Then:</p>
<pre class=""lang-sql prettyprint-override""><code>UPDATE my_table SET data = jsonb_delete(data, '{address, state}') 
 WHERE id = 1
 RETURNING *;
</code></pre>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>UPDATE 1
</code></pre>
</blockquote>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>data</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>{&quot;age&quot;: 30, &quot;name&quot;: &quot;John&quot;, &quot;address&quot;: {&quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;}}</td>
</tr>
</tbody>
</table></div>
<p>You can see that <code>address.state</code> field is not removed. Why?</p>
<pre class=""lang-sql prettyprint-override""><code>select version();
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>version</th>
</tr>
</thead>
<tbody>
<tr>
<td>PostgreSQL 18.1 on x86_64-windows, compiled by msvc-19.44.35219, 64-bit</td>
</tr>
</tbody>
</table></div>
",1,1,0,2025-12-06T02:59:10+00:00,2,83,True
79839469,23592847,,postgresql,Filter for distinct values on the union of multiple columns rather than the intersection,"<p>I'm looking to filter data like the following <a href=""https://dbfiddle.uk/hLmLIDiW"" rel=""nofollow noreferrer"">(fiddle)</a></p>
<p><a href=""https://i.sstatic.net/M2UIJTpB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M2UIJTpB.png"" alt=""neo4jsandbox graph visualisation of example input data"" /></a></p>
<pre class=""lang-sql prettyprint-override""><code>create table t(col1,col2)as values
 ('a',  1)
,('f',  3)
,('d',  4)
,('a',  5)
,('a',  9)
,('a',  8)
,('b',  1)
,('g',  3)
,('a',  1)
,('b',  2)
,('b',  2)
,('b',  3)
,('d',  1)
,('c',  1)
,('g',  null);
</code></pre>
<p>...into a result set like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td>f</td>
<td>3</td>
</tr>
<tr>
<td>d</td>
<td>4</td>
</tr>
<tr>
<td>b</td>
<td>2</td>
</tr>
<tr>
<td>g</td>
<td><em><code>null</code></em></td>
</tr>
</tbody>
</table></div>
<p>Notice in this example that <code>(g, 5)</code> is not included because it does not exist in the original set. It would be included by just matching distinct values of each column arbitrarily, which is not what I'm looking for.</p>
<p>Standard <code>DISTINCT</code> behavior on multiple columns returns rows with a distinct combination of values, which would return all rows in the original set because no rows have the same value of col1 and col2 simultaneously.</p>
<p>Nested <code>DISTINCT</code> filters on each column yields only the first row unless the data can be cleverly sorted.</p>
<p>Instead I'm looking for a filter that yields rows that are unique on all of the chosen columns individually - i.e. a &quot;union&quot; distinct operation rather than an &quot;intersection&quot; distinct operation. Is there an idiomatic or at least scalable way to do this?</p>
<hr />
<h3>Edit</h3>
<p>To clear up confusion, this is the key requirement:</p>
<blockquote>
<p>a filter that yields rows that are unique on all of the chosen columns individually - i.e. a &quot;union&quot; distinct operation rather than an &quot;intersection&quot; distinct operation.</p>
</blockquote>
<p>As some have pointed out, such an operation is dependent on the ordering of the input rows. This is expected and not an issue.</p>
<p>I suspect it may be easier to understand the problem conceptually than by example, but I will attempt to address more edge cases below. Regardless, a valid solution should solve the quoted problem with no additional assumptions based on the example data. To address a few specifically:</p>
<ul>
<li>The number of distinct values in col1 might not equal the number of distinct values in col2.</li>
<li>Values in col1 and col2 might not be co-ordered.</li>
<li>Ideally, null values in either column should be handled as a unique value. But if this makes the problem significantly harder, I'd also be interested in a solution that does not handle this edge case.</li>
<li>Data may not be ordered, but the output may vary depending on the order.</li>
</ul>
<h4>Example Use Case</h4>
<p>Suppose we want to match people from population 1 to people from mutually exclusive population 2. Matching is done by a bidirectional eligibility check that may yield 0-many matches for any given person. Now suppose we want a list of matches (population 1 person ID, population 2 person ID) that contains no duplicate entries for any person of either population, matched if possible or with a null match if not. It's ok that the order of people matters and that the result set may not be optimized for greatest or fewest possible matches or any other condition.</p>
<br>
<p><em><strong>Edit :</strong></em> Important quote from comments below.</p>
<blockquote>
<p>If a value in either column is present in a previous row, eliminate it from the solution set. (a, 5) is eliminated by a in row 1 col 1. (c, 1) is eliminated by 1 in row 1 col 2.</p>
</blockquote>
",0,1,1,2025-12-06T03:30:27+00:00,5,350,True
79839909,13459947,,postgresql,Is it possible to avoid a deadlock in an update without using select ... for update?,"<p>I have a table in <code>postgres</code> with 3 columns (<code>id</code>, <code>status</code>, <code>work_time</code>). The application that works with the table inserts, updates, and deletes data at a rate of 10,000 tps for each operation.Deletions are performed by <code>id</code>, and updates are based on the <code>status</code> column.</p>
<p>I am trying to write a query that can help me avoid deadlocks.
I have written the following query:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH tmp AS (
 SELECT w.id FROM work_queue AS w
 WHERE w.status = 'ready'
 ORDER BY w.id
 LIMIT 100
)
UPDATE work_queue SET status = 'processing'
 FROM tmp 
 WHERE work_queue.id = tmp.id
 returning *;
</code></pre>
<p>Is this query still susceptible to deadlocks? I read that postgres does not guarantee the order of rows in an update, but on the other hand, some sources suggest that order by in select helps to specify the order for update.</p>
<p>I know that it is recommended to use select ... for update skip locked, but it seems that under high data volume and frequent updates/deletions, it can lead to performance degradation. Is this concern valid?</p>
",3,3,0,2025-12-06T19:51:01+00:00,2,113,True
79841070,2691248,"Davao City, Philippines",postgresql,postgres &quot;&lt;schema.function&gt;&quot; does not exist,"<p>I have a script in postgres to create test data and error <code>Error: relation &quot;schema&quot; does not exist</code> is being thrown. I have verified that the schema and function exists and that I can run the function manually from the psql client but running it from a script gives me that error.</p>
<p>Also the weird thing is that if I login from psql and run the proc:</p>
<pre class=""lang-sql prettyprint-override""><code>select * from schema.function_id_from_name('test');
</code></pre>
<p>at first it will throw that error. But if I set the schema to the correct schema and run the proc it works. Then I can set the schema to <code>public</code>, I can then run the function normally from there. If I put the set schema statements in the script, it will not work but running it manually from the client will it work.</p>
",0,0,0,2025-12-08T14:29:46+00:00,1,65,False
79841251,2221574,"Lino Lakes, MN, USA",postgresql,Autovacuum causing deadlocks with ALTER TABLE RENAME on Aurora PostgreSQL 14,"<p>I'm working with COTS software, so changing the software is difficult.</p>
<p>The software has a process:</p>
<ol>
<li>Creates a temporary table</li>
<li>Adds a bunch of data</li>
<li>When finished, renames the table
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE old_table_name RENAME TO new_table_name;
</code></pre>
</li>
</ol>
<p>On Aurora PostgreSQL 14.17 we are getting deadlocks when running the alter table rename and an autovacuum analyze is kicked off on the same table.</p>
<p>Here is an example of the deadlock message:</p>
<pre class=""lang-none prettyprint-override""><code>Process 15466 waits for AccessExclusiveLock on relation 110111533 of database 16403; blocked by process 27051.  
Process 27051 waits for ShareLock on transaction 1886441802; blocked by process 15466.  
Process 15466: ALTER TABLE OLD_TABLE_NAME RENAME TO NEW_TABLE_NAME  
Process 27051: autovacuum: ANALYZE user.OLD_TABLE_NAME
</code></pre>
<p>I attempted to reduce the chances of collision by increasing <code>autovacuum_naptime</code> from 5 to 30 seconds, but the issue still occurs.</p>
<p>Should PostgreSQL be expected to avoid this kind of deadlock automatically?</p>
",3,3,0,2025-12-08T18:17:18+00:00,2,76,True
79841501,10146441,,postgresql,Update a value in a mapping table using SQL,"<p>I have tables like below,</p>
<p>fathers</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Jeff</td>
</tr>
<tr>
<td>2</td>
<td>John</td>
</tr>
</tbody>
</table></div>
<p>mothers</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Emy</td>
</tr>
<tr>
<td>2</td>
<td>Sarah</td>
</tr>
</tbody>
</table></div>
<p>siblings</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Andy</td>
</tr>
<tr>
<td>2</td>
<td>Robert</td>
</tr>
</tbody>
</table></div>
<p>people</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Susan</td>
</tr>
<tr>
<td>2</td>
<td>Mark</td>
</tr>
</tbody>
</table></div>
<p>and a mapping table</p>
<p>person_map</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>father_id</th>
<th>mother_id</th>
<th>sibling_id</th>
<th>person_id</th>
<th>active</th>
<th>updated_at</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>t</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>t</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I want to write a SQL <code>update</code> query such as <code>person_map</code>'s <code>active</code> column change to <code>f</code> based on the other table values and I have tried the following in <code>postgres</code></p>
<pre><code>update person_map
set active = false::boolean, updated_at = NOW()
from person_map pm

join fathers f on pm.father_id = f.id
join mothers m on pm.mother_id = m.id
join siblings s on pm.sibling_id = s.id
join people p on pm.person_id = p.id

where
  f.name = 'Jeff'
  m.name = 'Emy'
  s.name = 'Andy'
  p.name = 'Susan'
</code></pre>
<p>Above query should only match the first row <code>id = 1</code> of the <code>person_map</code> table however, it matches all.
Could someone let me know what am I missing?</p>
<p>Any help would be highly appreciated</p>
",0,0,0,2025-12-09T02:30:05+00:00,8,130,True
79841768,556958,"Moscow, Russia",postgresql,Can PostgreSQL optimizer carry `ORDER BY` out of `JOIN` subquery?,"<p>There is a &quot;theoretical&quot; query:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM a
JOIN (
    SELECT b.pk, b.not_pk
    FROM b
    ORDER BY b.not_pk
) AS b2
USING (pk)
</code></pre>
<p>and <code>EXPLAIN</code> shows <code>Sort</code> on the <em>whole</em> <code>b</code>.</p>
<p>Can it be carried out of the join (by the optimizer), theoretically?
If so, why it isn't?</p>
",3,3,0,2025-12-09T09:53:12+00:00,2,151,True
79842533,31884740,,postgresql,Hibernate: How to keep PostgreSQL enum type up-to-date with Java enum?,"<p>I'm building a Spring server program which uses a PostgreSQL database. For one of the table columns, I want to restrict the possible values to a static pre-defined set, so I created a PostgreSQL enum type. Then, when beginning to implement the the Spring project, I created a corresponding Java enum and added it to the respective class. With the usual annotations, the conversion between PostgreSQL and Java enum values works perfectly fine.</p>
<p>However, today, I added a value to the Java enum, but to my surprise, <code>spring.jpa.hibernate.ddl-auto=update</code> did not add the new Java enum value to the PostgreSQL enum. Of course, I could just edit the PostgreSQL enum type <code>brand_type</code> manually all the time, but that would be tedious.</p>
<p><strong>How can I ensure that my database's <code>brand_type</code> enum is always up-to-date with the Java enum <code>Brand</code>?</strong></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TYPE brand_type AS ENUM (
    'BOEING',
    'AIRBUS',
    'LOCKHEED',
    'BOMBARDIER'
    -- This PostgreSQL enum shall be updated according to the Java enum “Brand”.
)
</code></pre>
<pre class=""lang-java prettyprint-override""><code>@Entity
@Table(name = &quot;planes&quot;)
public class Airplane {
    @Column(name = &quot;brand&quot;)
    @JdbcType(value = PostgreSQLEnumJdbcType.class) // conversion works fine
    private Brand brand;
}
</code></pre>
<pre class=""lang-java prettyprint-override""><code>public enum Brand {
    BOEING,
    AIRBUS,
    LOCKHEED,
    BOMBARDIER
    // When I add DOUGLAS here, the database won't accept this value.
}
</code></pre>
<p>On Stack Overflow and elsewhere, all of the questions/answers either focussed on how to map the PostgreSQL enum values to Java enum values or suggested to use the now deprecated <code>@Type</code> and <code>@TypeDef</code> annotations.</p>
<p>Skimming the Hibernate documentation (for the first time), I quickly found the setting <a href=""https://docs.hibernate.org/orm/7.1/userguide/html_single/#settings-hibernate.type.prefer_native_enum_types"" rel=""nofollow noreferrer""><code>hibernate.type.prefer_native_enum_types</code></a> and thus wrote <code>hibernate.type.prefer_native_enum_types=true</code> to the file <code>hibernate.properties</code> file, but that had no effect.</p>
<p>Since the Hibernate documentation also mentions <code>@JdbcTypeCode(SqlTypes.NAMED_ENUM)</code>, I added this annotation to the <code>Airplane#brand</code> field, but that didn't do anything either.</p>
",1,1,0,2025-12-10T03:41:37+00:00,1,108,True
79842655,13157036,USA,postgresql,"Server-side autocomplete for ~1200 schools in Supabase/Postgres — need typo/acronym-tolerant, low resource usage","<p><strong>Problem:</strong></p>
<p>I have a React + TypeScript site with ~1200 schools stored in Supabase (PostgreSQL). The page only loads the first ~20 schools to reduce API usage, so full client-side matching isn’t possible in the school search.</p>
<p><strong>My search bar needs to include the following</strong></p>
<ul>
<li><p>Autocomplete suggestions as the user types (within 300-500ms to keep engagement)</p>
</li>
<li><p>Typo tolerance (e.g., 'uaburn' should still return 'Auburn University' in autocomplete suggestions)</p>
</li>
<li><p>Acronym matching (e.g. 'UCLA' shows 'University of California, Los Angeles' in autocomplete suggestions)</p>
</li>
<li><p>Minimal API calls / low cost (avoid excessive requests)</p>
</li>
</ul>
<p><strong>Request:</strong></p>
<p>I am not looking for exact code, just some guidance from the community for a good approach or tools/libraries to implement the above. If this is not feasible, a why-not explanation works too!</p>
",1,0,0,2025-12-10T07:43:27+00:00,2,56,True
79843291,4927940,,postgresql,PostgreSQL / TimeScaleDB issue after reimporting a huge set of data,"<p>I have a PostgresQL 17 DB with a quite simple table (no outer relations) but with a lot of data (around &gt; 446 000 000 records) storing some events on a day basis representing more than 2 years of records.
I initially started with a PostgreSQL classic table approach, but have converted this table to an TimeScaleDB hypertable based on a day-basis index segmentation.
This table have also some other btree, brin, gist and gin indexes.
It was working quite well.</p>
<p>But, we discovered a bug in our tool that injects such data.
Its fixing have required to drop everything, and to reinsert each data through a dedicated batch insert.
Since this reinsertion each of my query seems to still use hyperchunk indexes (based on explain plan) but is very slow compared to before this &quot;reimport&quot; scenario.
I have tried may ANALYZE or REINDEX commands. without success.
Any tips to diagnose it and eventually to solve it ?</p>
",0,0,0,2025-12-10T17:49:04+00:00,0,36,False
79843666,24887874,,postgresql,Solr stores PostgreSQL date fields as epoch milliseconds (1667952000000) — how to keep pipe-separated date string like 2017-07-17 | 2018-02-20?,"<p>I'm migrating data from PostgreSQL into Solr. In PostgreSQL my priority dates are stored as two dates and I want them indexed in Solr as a single pipe-separated string:</p>
<p>2017-07-17 | 2018-02-20</p>
<p>I tried declaring the Solr field as a <code>string</code> in <code>schema.xml</code> but after indexing I see values like:</p>
<p>1667952000000&quot;,&quot;1636416000000</p>
<p>What I'm doing:</p>
<ul>
<li><p>Source: PostgreSQL date columns (e.g. <code>priority_date1</code>, <code>priority_date2</code>).</p>
</li>
<li><p>Solr: I added a field in <code>schema.xml</code> and set <code>type=&quot;string&quot;</code>.</p>
</li>
<li><p>In the index I see epoch milliseconds instead of the date strings or the pipe-separated form.</p>
</li>
</ul>
<p>What I want: either</p>
<ul>
<li><p>keep the original pipe-separated string <code>YYYY-MM-DD | YYYY-MM-DD</code> in Solr as a string field, <strong>or</strong></p>
</li>
<li><p>if Solr converts the dates, have it return the ISO date strings rather than epoch millis.</p>
</li>
</ul>
<p>What I tried:</p>
<ul>
<li><p>Declaring field as <code>&lt;field name=&quot;priority_date&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;</code>.</p>
</li>
<li><p>Sending data via my ETL (custom script / DIH / JSON) — still getting epoch millis in Solr.</p>
</li>
</ul>
<p>Questions:</p>
<ol>
<li><p>Why am I getting epoch milliseconds instead of the date strings?</p>
</li>
<li><p>How can I preserve the pipe-separated literal string in Solr?</p>
</li>
<li><p>If I want Solr to store proper date strings, what is the recommended approach (field type / data transformation)?</p>
</li>
</ol>
<p><strong>Example snippets (if useful):</strong></p>
<p><code>schema.xml</code> (current attempt)</p>
<pre><code>&lt;field name=&quot;priority_date&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;
</code></pre>
<p>Doc sent to Solr (example; my actual ingest may vary)</p>
<pre><code>{
  &quot;id&quot;:&quot;123&quot;,
  &quot;priority_date&quot;:&quot;2017-07-17 | 2018-02-20&quot;
}
</code></pre>
<p>But what ends up in Solr for that field is <code>1667952000000&quot;,&quot;1636416000000</code>.</p>
<p>Any hints on where conversion is happening and how to stop it (or how to convert it back on the Solr side) would be appreciated. Thanks!</p>
",0,1,1,2025-12-11T06:25:16+00:00,0,64,False
79843814,24968068,,postgresql,WhatsApp style for deleting messages for me/for everyone,"<p>I have a real time chat app that uses PostgreSQL database.App has 2 different type of chats One-to-One(conversation between 2 people) and group chats(up to 1000 members), at this moment feature of deleting message looks like this: Only sender can delete message and delete it for everyone, but the feature request of implementing deletion <strong>&quot;just for me&quot;</strong> where user can delete other people message in one-to-one chat or groups is a little struggling.</p>
<p>Messages table looks like this:</p>
<p>The <code>messages</code> table has the following structure:</p>
<ul>
<li><p><code>id</code>: <code>INT</code>, auto-increment primary key, indexed</p>
</li>
<li><p><code>user_id</code>: <code>INT</code>, foreign key to the <code>users</code> table, indexed</p>
</li>
<li><p><code>chat_id</code>: <code>INT</code>, foreign key to the <code>chats</code> table, indexed</p>
</li>
<li><p>additional columns for message content, timestamps, and other metadata</p>
</li>
</ul>
<p>At the moment, I’m considering two approaches:</p>
<h3><strong>1. Use a many-to-many relation</strong></h3>
<p>This involves creating an additional table (e.g., <code>deleted_messages</code>) that stores <code>user_id</code> and <code>message_id</code>.<br />
When fetching messages, I would join this table to exclude rows where a matching record exists.</p>
<p><strong>Pros:</strong></p>
<ul>
<li><p>Fully normalized design following SQL best practices.</p>
</li>
<li><p>Clear relational structure.</p>
</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><p>The join table may grow very large over time.</p>
</li>
<li><p>Requires additional joins when querying messages, which may affect performance.</p>
</li>
</ul>
<hr />
<h3><strong>2. Add an nullable <code>int[]</code> array column to the <code>messages</code> table</strong></h3>
<p>This array would store the IDs of users who deleted the message for themselves.<br />
Based on real-world usage, this array should remain relatively small because “delete for me” is not a frequently used feature and as i said above group chats can have up to 1000 users and it is very small chance that even 500 people will delete same message for themselves .</p>
<p><strong>Pros:</strong></p>
<ul>
<li><p>Simpler queries—just check whether the array exists and whether it contains the requesting user’s ID.</p>
</li>
<li><p>No extra join table required.</p>
</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><p>Array columns are considered non-normalized.</p>
</li>
<li><p>Slightly breaks traditional relational modeling.</p>
</li>
</ul>
<p>If anyone has experience dealing with this design choice, I’d appreciate your guidance. Feel free to suggest which approach is more suitable—or propose an even better solution. Thanks!</p>
",2,0,0,2025-12-11T09:53:19+00:00,2,94,True
79844911,28098810,,postgresql,Postgres 23505 duplicate key exception in EF Core AddOrUpdate (race condition on initial insert),"<p>I am encountering a duplicate key value violates unique constraint error in my ASP.NET Core application, and while I understand <em>why</em> it happens, I am unsure about the best practice to handle it using EF Core and PostgreSQL (Npgsql).</p>
<p>I have a high-frequency endpoint that receives GPS coordinates from mobile apps.</p>
<pre><code>[HttpPost(&quot;{courierId:guid}/locations&quot;)]
public async Task&lt;IActionResult&gt; ReceiveLocations(Guid courierId, [FromBody] LocationsDto locationsDto)
{
</code></pre>
<p>I store the <em>latest</em> location in a table called <code>LastLocations</code>. The Primary Key is <code>CourierId</code>, so there is a strict 1-to-1 relationship (one record per courier). The logic is simple: if the record exists, update it; otherwise, insert it.</p>
<p>Here is my repository method:</p>
<pre><code>
public async Task AddOrUpdate(LastLocationEntity lastLocationEntity)
{
    var existingCourierLocation = await applicationDbContext.LastLocations
        .FindAsync(lastLocationEntity.CourierId);
    
    if (existingCourierLocation is null)
        applicationDbContext.LastLocations.Add(lastLocationEntity);
    else
        applicationDbContext.Entry(existingCourierLocation).CurrentValues.SetValues(lastLocationEntity);
        
    await applicationDbContext.SaveChangesAsync();
}
</code></pre>
<p>Rarely, I see the following exception in the logs:</p>
<pre><code>Microsoft.EntityFrameworkCore.DbUpdateException: An error occurred while saving the entity changes.
 ---&gt; Npgsql.PostgresException (0x80004005): 23505: duplicate key value violates unique constraint &quot;PK_LastLocations&quot;
 ...
 Detail: Key (&quot;CourierId&quot;)=(...) already exists.
</code></pre>
<p><strong>My Analysis</strong><br />
I successfully reproduced this using <strong>k6</strong> by sending a burst of concurrent requests for the same <code>CourierId</code> into an empty database.</p>
<p>I understand this is a <strong>Race Condition</strong> during the initial insertion:</p>
<ol>
<li><p>Two (or more) requests execute FindAsync simultaneously.</p>
</li>
<li><p>Both receive <code>null</code> because the transaction hasn't committed yet.</p>
</li>
<li><p>Both try to <code>Add</code> a new entity.</p>
</li>
<li><p><code>SaveChangesAsync</code> is called. The first one succeeds, the second one fails with error 23505.</p>
</li>
</ol>
<p>If the record already exists in the database, the error never happens because FindAsync correctly returns the entity, and the code proceeds to the Update path.</p>
<p>What is the standard way to handle this <code>&quot;Insert or Update&quot; (Upsert)</code> scenario in EF Core to avoid race conditions?</p>
<p>Should I wrap this in a try-catch block with a retry policy, or is there a way to perform an atomic <code>UPSERT (like INSERT ... ON CONFLICT)</code> natively in EF Core without dropping down to raw SQL?</p>
<p>Thanks!</p>
",2,3,1,2025-12-12T13:05:31+00:00,1,113,True
79845075,25956179,,postgresql,"PrismaClientInitializationError: `PrismaClient` needs to be constructed with a non-empty, valid `PrismaClientOptions`:","<p>I created <code>schema.prisma</code> and <code>schema.prisma</code></p>
<p><code>schema.prisma:</code></p>
<pre><code>generator client {
  provider = &quot;prisma-client&quot;
  engineType = &quot;client&quot;
  output   = &quot;./generated/main&quot;
}

datasource db {
  provider = &quot;postgresql&quot;
}
</code></pre>
<p><code>schemaSA.prisma:</code></p>
<pre><code>generator client {
    provider = &quot;prisma-client&quot;
    engineType = &quot;client&quot;
    output   = &quot;./generated/superadmin&quot;
}

datasource db {
    provider  = &quot;postgresql&quot;
}
</code></pre>
<p>Then I created <code>schema.config.ts</code> and <code>schemaSA.config.ts</code></p>
<p><code>schema.config.ts</code></p>
<pre><code>/** @format */

import 'dotenv/config';
import { defineConfig } from 'prisma/config';

// Main Admin Database Configuration
export default defineConfig({
  schema: 'prisma/schema.prisma',
  migrations: {
    path: 'prisma/migrations',
  },
  datasource: {
    url: process.env.DATABASE_URL as string,
  }
});
</code></pre>
<p><code>schemaSA.config.ts</code></p>
<pre><code>/** @format */

import 'dotenv/config';
import { defineConfig } from 'prisma/config';

// Super Admin Database Configuration
export default defineConfig({
  schema: 'prisma/schemaSA.prisma',
  migrations: {
    path: 'prisma/migrationsSA',
  },
  datasource: {
    url: process.env.DATABASE_URL_SUPER_ADMIN as string,
  },
});

</code></pre>
<p>Then I created <code>main-prisma.service.ts</code> and <code>superadmin-prisma.service.ts</code></p>
<p><code>main-prisma.service.ts</code></p>
<pre><code>import { Injectable, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { PrismaClient } from 'prisma/generated/main';
import { PrismaPg } from '@prisma/adapter-pg';
import { readReplicas } from '@prisma/extension-read-replicas';

@Injectable()
export class MainPrismaService extends PrismaClient implements OnModuleInit, OnModuleDestroy {
  constructor() {
    const adapter = new PrismaPg({
      connectionString: process.env.DATABASE_URL_ADMIN_POOLING as string,
    });

    super({ adapter });

    // Create replica clients with adapters
    const replicaAdapter1 = new PrismaPg({
      connectionString: process.env.DATABASE_REPLICA_ADMIN_URL_1 as string,
    });

    const replicaAdapter2 = new PrismaPg({
      connectionString: process.env.DATABASE_REPLICA_ADMIN_URL_2 as string,
    });

    const replicaClient1 = new PrismaClient({ adapter: replicaAdapter1 });
    const replicaClient2 = new PrismaClient({ adapter: replicaAdapter2 });

    // Extend with read replicas
    this.$extends(
      readReplicas({
        replicas: [replicaClient1, replicaClient2],
      }),
    );
  }

  async onModuleInit() {
    await this.$connect();
  }

  async onModuleDestroy() {
    await this.$disconnect();
  }
}
</code></pre>
<p><code>superadmin-prisma.service.ts</code></p>
<pre><code>import { Injectable, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { PrismaClient } from 'prisma/generated/superadmin';
import { PrismaPg } from '@prisma/adapter-pg';
import { readReplicas } from '@prisma/extension-read-replicas';

@Injectable()
export class SuperAdminPrismaService extends PrismaClient implements OnModuleInit, OnModuleDestroy {
  constructor() {
    const adapter = new PrismaPg({
      connectionString: process.env.DATABASE_URL_SUPER_ADMIN_POOLING as string,
    });

    super({ adapter });

    // Create replica clients with adapters
    const replicaAdapter1 = new PrismaPg({
      connectionString: process.env.DATABASE_REPLICA_SUPER_ADMIN_URL_1 as string,
    });

    const replicaAdapter2 = new PrismaPg({
      connectionString: process.env.DATABASE_REPLICA_SUPER_ADMIN_URL_2 as string,
    });

    const replicaClient1 = new PrismaClient({ adapter: replicaAdapter1 });
    const replicaClient2 = new PrismaClient({ adapter: replicaAdapter2 });

    // Extend with read replicas
    this.$extends(
      readReplicas({
        replicas: [replicaClient1, replicaClient2],
      }),
    );
  }

  async onModuleInit() {
    await this.$connect();
  }

  async onModuleDestroy() {
    await this.$disconnect();
  }
}
</code></pre>
<p>THen I generated <code>prisma client</code> using commands:</p>
<p><code>npx prisma generate</code></p>
<p><code>npx prisma generate --schema=./prisma/schemaSA.prisma --config=./schemaSA.config.ts</code></p>
<p>Here, I have pasted the picture of the error in terminal:</p>
<p><img src=""https://i.sstatic.net/5MZztjHO.png"" alt=""enter image description here"" title=""PrismaClient Error"" /></p>
",2,3,1,2025-12-12T16:47:31+00:00,3,1702,True
79845222,2283954,"Norman, Ok",postgresql,How do I select a list of all the values of a specific field name in a recursive set of jsonb data in Postgres?,"<p>I have a column named layout in a table named pages which contains JSON.</p>
<p>In this case, the JSON actually represents the HTML of a page. Information is infinitely nested, just as an HTML page would be. I want to search for all &quot;href&quot; fields in the page and return a list of what I found.</p>
<p>(Simplified example of data stored in the layout jsonb field)</p>
<pre><code>{
  &quot;tagName&quot;: &quot;html&quot;,
  &quot;children&quot;: [
    {
      &quot;tagName&quot;: &quot;head&quot;,
      &quot;children&quot;: []
    },
    {
      &quot;tagName&quot;: &quot;body&quot;,
      &quot;children&quot;: [
        {
          &quot;tagName&quot;: &quot;a&quot;,
          &quot;href&quot;: &quot;https://google.com&quot;
        },
        {
          &quot;tagName&quot;: &quot;div&quot;,
          &quot;children&quot;: [
            {
              &quot;tagName&quot;: &quot;a&quot;,
              &quot;href&quot;: &quot;https://yahoo.com&quot;
            }
          ]
        }
      ]
    }
  ]
}
</code></pre>
<p>I'd like to query one row of the page table and return all the values of &quot;href&quot;.</p>
<p>For example</p>
<p>select ... from page where page_id=123</p>
<p>and return
<a href=""https://google.com"" rel=""nofollow noreferrer"">https://google.com</a> , <a href=""https://yahoo.com"" rel=""nofollow noreferrer"">https://yahoo.com</a></p>
<p>I know I can brute force this with a regex of the text, however the data is quite big and I believe querying it based on JSONB would be faster/more efficient. I also could probably write a recursive CTE to pull this off (as the one constant in this structure is the &quot;children&quot; array), but again, I imagine there's a better way of doing this with json path, which I'm largely unfamiliar with how to use for this case.</p>
",0,0,0,2025-12-12T20:22:30+00:00,1,74,True
79845225,12149993,,postgresql,Postgres dump works with python os.system but not subprocess,"<p>This runs successfully:</p>
<pre><code>os.system('pg_dump sdtests &gt; /home/bret/dump.custom')
</code></pre>
<p>But doing it  with subprocess gives an error:</p>
<pre><code>command = ['pg_dump', 'sdtests', '&gt;', '/home/bret/dump.custom']
subPopenTry(command)
</code></pre>
<p>method:</p>
<pre><code>def subPopenTry(cmd):
    try:
        proc = subprocess.Popen(cmd,stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True,shell=True)
        output, error = proc.communicate()
        if proc.returncode != 0:
            raise subprocess.CalledProcessError(proc.returncode, proc.args, output=output, stderr=error)
        outputLines = output.splitlines()
        return outputLines
    except subprocess.CalledProcessError as e:
        sys.exit('Stop: Error output: {} on cmd &quot;{}&quot;'.format(e.stderr, ' '.join(cmd)))
        return e.stderr
</code></pre>
<p>The error is</p>
<pre><code>FATAL:  database &quot;bret&quot; does not exist
</code></pre>
<p>which implies that using subprocess means it can't find the database <code>sdtests</code> and is looking for a default database with the username <code>bret</code>.</p>
<p>Why don't these have the same behavior?</p>
",1,1,0,2025-12-12T20:28:52+00:00,2,115,True
79846354,17304375,,postgresql,"How to map PostgreSQL EXPLAIN ANALYZE output to Execution Time, CPU, Memory, and I/O?","<p>I’m trying to standardize how I report <strong>Execution Time, CPU, Memory, and I/O</strong> from PostgreSQL queries using EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT). Here’s an example of a query plan I get:</p>
<pre><code>Aggregate  (cost=80.48..80.49 rows=1 width=8) (actual time=0.530..0.532 rows=1 loops=1)
  Buffers: shared hit=22
  -&gt;  Hash Join  (cost=70.41..80.05 rows=172 width=8) (actual time=0.395..0.507 rows=110 loops=1)
        Hash Cond: (sl.company_id = uc.id)
        Buffers: shared hit=22
        -&gt;  Hash Join  (cost=14.74..23.88 rows=188 width=16) (actual time=0.098..0.185 rows=187 loops=1)
              Hash Cond: (scu.location_id = sl.id)
              Buffers: shared hit=15
              -&gt;  Seq Scan on storage_coolingunit scu  (cost=0.00..8.64 rows=188 width=16) (actual time=0.009..0.062 rows=187 loops=1)
                    Filter: ((name)::text !~~ '%test%'::text)
                    Rows Removed by Filter: 24
                    Buffers: shared hit=6
              -&gt;  Hash  (cost=11.55..11.55 rows=255 width=16) (actual time=0.084..0.084 rows=267 loops=1)
                    Buckets: 1024  Batches: 1  Memory Usage: 21kB
                    Buffers: shared hit=9
                    -&gt;  Seq Scan on storage_location sl  (cost=0.00..11.55 rows=255 width=16) (actual time=0.003..0.054 rows=267 loops=1)
                          Buffers: shared hit=9
        -&gt;  Hash  (cost=49.05..49.05 rows=530 width=8) (actual time=0.293..0.293 rows=542 loops=1)
              Buckets: 1024  Batches: 1  Memory Usage: 30kB
              Buffers: shared hit=7
              -&gt;  Seq Scan on user_company uc  (cost=0.00..49.05 rows=530 width=8) (actual time=0.004..0.237 rows=542 loops=1)
                    Filter: (id &lt;&gt; ALL ('{161,162,163,...}'::bigint[]))
                    Rows Removed by Filter: 50
                    Buffers: shared hit=7
Planning:
  Buffers: shared hit=29
Planning Time: 0.393 ms
Execution Time: 0.563 ms
</code></pre>
<p>I want to consistently extract the following metrics from this output:</p>
<ul>
<li><p><strong>Execution Time</strong></p>
</li>
<li><p><strong>CPU</strong></p>
</li>
<li><p><strong>Memory Usage</strong></p>
</li>
<li><p><strong>I/O (Buffers)</strong></p>
</li>
</ul>
<p>So far, I’ve been interpreting them like this:</p>
<ul>
<li><p>CPU / Execution Time = actual time of the <strong>top-most node</strong> or the Execution Time at the bottom.</p>
</li>
<li><p>Memory = largest Memory Usage among hash nodes.</p>
</li>
<li><p>I/O = Buffers in the main nodes (top or Hash nodes).</p>
</li>
</ul>
<p><strong>My questions:</strong></p>
<ol>
<li><p>Is it correct to take <strong>CPU / execution time from the top-most node</strong>?</p>
</li>
<li><p>For memory, should I <strong>sum all hash node memory usage</strong>, or just report the <strong>largest node</strong>?</p>
</li>
<li><p>For I/O, should I <strong>sum all buffer usage</strong> or only look at the <strong>top node</strong>?</p>
</li>
</ol>
<p>Any guidance or examples of how to map the full EXPLAIN ANALYZE output to these metrics would be very helpful.</p>
",0,0,0,2025-12-13T02:23:39+00:00,1,136,True
79847871,567389,Singapore,postgresql,Testcontainer error: Could not find a valid Docker environment,"<p>Why am I suddenly encountering this error when running the repository tests locally?</p>
<pre class=""lang-none prettyprint-override""><code>2025-12-15 21:59:28 [ZScheduler-Worker-11] ERROR o.t.d.DockerClientProviderStrategy - Could not find a valid Docker environment. Please check configuration. Attempted configurations were:
2025-12-15 21:59:28 [ZScheduler-Worker-11] ERROR o.t.d.DockerClientProviderStrategy -     EnvironmentAndSystemPropertyClientProviderStrategy: failed with exception DockerException (Status 503: Embedded runtime VM is not started
)
2025-12-15 21:59:28 [ZScheduler-Worker-11] ERROR o.t.d.DockerClientProviderStrategy -     UnixSocketClientProviderStrategy: failed with exception BadRequestException (Status 400: {&quot;ID&quot;:&quot;&quot;,&quot;Containers&quot;:0,&quot;ContainersRunning&quot;:0,&quot;ContainersPaused&quot;:0,&quot;ContainersStopped&quot;:0,&quot;Images&quot;:0,&quot;Driver&quot;:&quot;&quot;,&quot;DriverStatus&quot;:null,&quot;Plugins&quot;:{&quot;Volume&quot;:null,&quot;Network&quot;:null,&quot;Authorization&quot;:null,&quot;Log&quot;:null},&quot;MemoryLimit&quot;:false,&quot;SwapLimit&quot;:false,&quot;CpuCfsPeriod&quot;:false,&quot;CpuCfsQuota&quot;:false,&quot;CPUShares&quot;:false,&quot;CPUSet&quot;:false,&quot;PidsLimit&quot;:false,&quot;IPv4Forwarding&quot;:false,&quot;Debug&quot;:false,&quot;NFd&quot;:0,&quot;OomKillDisable&quot;:false,&quot;NGoroutines&quot;:0,&quot;SystemTime&quot;:&quot;&quot;,&quot;LoggingDriver&quot;:&quot;&quot;,&quot;CgroupDriver&quot;:&quot;&quot;,&quot;NEventsListener&quot;:0,&quot;KernelVersion&quot;:&quot;&quot;,&quot;OperatingSystem&quot;:&quot;&quot;,&quot;OSVersion&quot;:&quot;&quot;,&quot;OSType&quot;:&quot;&quot;,&quot;Architecture&quot;:&quot;&quot;,&quot;IndexServerAddress&quot;:&quot;&quot;,&quot;RegistryConfig&quot;:null,&quot;NCPU&quot;:0,&quot;MemTotal&quot;:0,&quot;GenericResources&quot;:null,&quot;DockerRootDir&quot;:&quot;&quot;,&quot;HttpProxy&quot;:&quot;&quot;,&quot;HttpsProxy&quot;:&quot;&quot;,&quot;NoProxy&quot;:&quot;&quot;,&quot;Name&quot;:&quot;&quot;,&quot;Labels&quot;:[&quot;com.docker.desktop.address=unix:///Users/ujjaldey/Library/Containers/com.docker.docker/Data/docker-cli.sock&quot;],&quot;ExperimentalBuild&quot;:false,&quot;ServerVersion&quot;:&quot;&quot;,&quot;Runtimes&quot;:null,&quot;DefaultRuntime&quot;:&quot;&quot;,&quot;Swarm&quot;:{&quot;NodeID&quot;:&quot;&quot;,&quot;NodeAddr&quot;:&quot;&quot;,&quot;LocalNodeState&quot;:&quot;&quot;,&quot;ControlAvailable&quot;:false,&quot;Error&quot;:&quot;&quot;,&quot;RemoteManagers&quot;:null},&quot;LiveRestoreEnabled&quot;:false,&quot;Isolation&quot;:&quot;&quot;,&quot;InitBinary&quot;:&quot;&quot;,&quot;ContainerdCommit&quot;:{&quot;ID&quot;:&quot;&quot;},&quot;RuncCommit&quot;:{&quot;ID&quot;:&quot;&quot;},&quot;InitCommit&quot;:{&quot;ID&quot;:&quot;&quot;},&quot;SecurityOptions&quot;:null,&quot;CDISpecDirs&quot;:null,&quot;Warnings&quot;:null})
2025-12-15 21:59:28 [ZScheduler-Worker-11] ERROR o.t.d.DockerClientProviderStrategy -     DockerMachineClientProviderStrategy: failed with exception ShellCommandException (Exception when executing docker-machine status ). Root cause InvalidExitValueException (Unexpected exit value: 1, allowed exit values: [0], executed command [docker-machine, status, ], output was 122 bytes:
Docker machine &quot;&quot; does not exist. Use &quot;docker-machine ls&quot; to list machines. Use &quot;docker-machine create&quot; to add a new one.)
2025-12-15 21:59:28 [ZScheduler-Worker-11] ERROR o.t.d.DockerClientProviderStrategy - As no valid configuration was found, execution cannot continue

</code></pre>
<p>I am using in build.sbt:</p>
<pre class=""lang-none prettyprint-override""><code>&quot;io.github.scottweaver&quot;       %% &quot;zio-2-0-testcontainers-postgresql&quot; % &quot;0.10.0&quot; % Test,

</code></pre>
<p>My trait definition:</p>
<pre class=""lang-scala prettyprint-override""><code>trait RepositorySpec {
  val initScript: String = &quot;db/migration.sql&quot;

  private def createContainer() = {
    val container: PostgreSQLContainer[Nothing] =
      PostgreSQLContainer(&quot;postgres&quot;)
        .withInitScript(initScript)

    container.start()
    container
  }

  private def createDataSource(container: PostgreSQLContainer[Nothing]): DataSource = {
    val dataSource = new PGSimpleDataSource()
    dataSource.setUrl(container.getJdbcUrl)
    dataSource.setUser(container.getUsername)
    dataSource.setPassword(container.getPassword)
    dataSource
  }

  val dataSourceLayer: ZLayer[Any &amp; Scope, Throwable, DataSource] = ZLayer.scoped {
    for {
      container  &lt;- ZIO.acquireRelease(ZIO.attempt(createContainer()))(c =&gt; ZIO.attempt(c.stop()).orDie)
      dataSource &lt;- ZIO.attempt(createDataSource(container))
    } yield dataSource
  }
}

</code></pre>
<p>This setup was working earlier, but it has stopped working unexpectedly. I have already tried reinstalling Docker Desktop, but the issue persists.</p>
<p>Notably, the problem occurs only on my local machine; the same tests run successfully in GitHub Actions.</p>
",1,1,0,2025-12-15T17:01:33+00:00,1,217,True
79848239,31209924,,postgresql,mock feature in SeaORM v1.1.19 makes DatabaseConnection lose the trait to clone it,"<p>Im doing a Actix Web rust backend with SeaORM, I finished and it works fine, so I'm doing some tests, SeaORM has a mock feature, which you can enable in the Cargo.toml file, but enabling it makes the type DatabaseConnection lose the trait to clone it, which I'm using to clone the connections, seems like this happens because without mock, DatabaseConnection is wrapped inside an ARC (Atomic Reference Counter), and when I enable mock, it's implementation eliminates the ARC, is there any workaround to use mock feature and still been able to clone the connection?</p>
<p>This is my current pool implementation for SeaORM connection:</p>
<pre class=""lang-rust prettyprint-override""><code>impl BaseDatos {
    pub async fn new() -&gt; Self {
        let url = &quot;my_secret_url&quot;
        let mut opt = ConnectOptions::new(url.to_owned());
        opt.max_connections(50)
            .min_connections(5)
            .connect_timeout(std::time::Duration::from_secs(8))
            .idle_timeout(std::time::Duration::from_secs(300))
            .max_lifetime(std::time::Duration::from_secs(1800))
            .sqlx_logging(true)
            .sqlx_logging_level(log::LevelFilter::Info);

        let pool = Database::connect(opt)
            .await
            .expect(&quot;Error when connecting to the DB&quot;);

        Self { pool }
    }

    pub fn obtener_conexion(&amp;self) -&gt; DatabaseConnection {
        self.pool.clone()
    }

    pub fn _sqlx_pool(&amp;self) -&gt; &amp;sqlx::Pool&lt;sqlx::Postgres&gt; {
        self.pool.get_postgres_connection_pool()
    }
}
</code></pre>
",1,1,0,2025-12-16T06:04:05+00:00,0,90,False
79848391,16967557,,postgresql,why is using indexes not helping my query in any way?,"<p>I have a problem. I have a query</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT department.&quot;name&quot; AS &quot;division&quot;, 
  COUNT(DISTINCT equipment.&quot;inventory_number&quot;) AS &quot;count of equipment&quot; 
FROM department 
JOIN auditory ON department.&quot;ID_department&quot; = auditory.&quot;ID_department&quot; 
JOIN equipment ON auditory.&quot;number_auditory&quot; = equipment.&quot;number_auditory&quot;
LEFT JOIN inventorylist ON equipment.&quot;inventory_number&quot; = inventorylist.&quot;inventory_number&quot; 
      AND inventorylist.&quot;ID_total&quot; IN (2, 3) 
GROUP BY department.&quot;ID_department&quot;, department.&quot;name&quot;
HAVING COUNT(inventorylist.&quot;inventory_number&quot;) = 0 
ORDER BY department.&quot;name&quot; ASC;
</code></pre>
<p>and this database contains about 1 million rows for each table that is in the query. My task is to use indexes to speed up the execution of this query.  I tried to use different indexes, but none of them helps.</p>
<p>An example of the indexes that I tried to use:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE INDEX idx_auditory_id_department 
         ON auditory (&quot;ID_department&quot;) INCLUDE (&quot;number_auditory&quot;);
CREATE INDEX idx_equipment_number_auditory 
         ON equipment (number_auditory) INCLUDE (&quot;inventory_number&quot;);
CREATE INDEX idx_inventorylist_id_total_inventory
         ON inventorylist (ID_total, inventory_number) INCLUDE (&quot;ID_condition&quot;);
CREATE INDEX idx_division_div_name on division (&quot;ID_department&quot;) include (name);
</code></pre>
<p>This an query plan on test db:</p>
<pre><code>&quot;Sort  (cost=179037.05..179043.30 rows=2500 width=56) (actual time=3764.971..3810.081 rows=699 loops=1)&quot;
&quot;  Sort Key: department.name&quot;
&quot;  Sort Method: quicksort  Memory: 79kB&quot;
&quot;  -&gt;  GroupAggregate  (cost=85869.32..178895.95 rows=2500 width=56) (actual time=3179.260..3803.851 rows=699 loops=1)&quot;
&quot;        Group Key: department.&quot;&quot;ID_department&quot;&quot;&quot;
&quot;        Filter: (count(inventorylist.inventory_number) = 0)&quot;
&quot;        -&gt;  Gather Merge  (cost=85869.32..167395.85 rows=700000 width=70) (actual time=3178.613..3630.801 rows=700000 loops=1)&quot;
&quot;              Workers Planned: 2&quot;
&quot;              Workers Launched: 2&quot;
&quot;              -&gt;  Sort  (cost=84869.29..85598.46 rows=291667 width=70) (actual time=3096.904..3249.819 rows=233333 loops=3)&quot;
&quot;                    Sort Key: department.&quot;&quot;ID_department&quot;&quot;, equipment.inventory_number&quot;
&quot;                    Sort Method: external merge  Disk: 16176kB&quot;
&quot;                    Worker 0:  Sort Method: external merge  Disk: 16168kB&quot;
&quot;                    Worker 1:  Sort Method: external merge  Disk: 15816kB&quot;
&quot;                    -&gt;  Hash Left Join  (cost=11963.83..46431.73 rows=291667 width=70) (actual time=1143.016..1303.165 rows=233333 loops=3)&quot;
&quot;                          Hash Cond: ((equipment.inventory_number)::text = (inventorylist.inventory_number)::text)&quot;
&quot;                          -&gt;  Parallel Hash Join  (cost=11720.44..45094.58 rows=291667 width=59) (actual time=1140.499..1256.968 rows=233333 loops=3)&quot;
&quot;                                Hash Cond: (auditory.&quot;&quot;ID_department&quot;&quot; = department.&quot;&quot;ID_department&quot;&quot;)&quot;
&quot;                                -&gt;  Nested Loop  (cost=0.86..27927.36 rows=291667 width=15) (actual time=1.099..739.685 rows=233333 loops=3)&quot;
&quot;                                      -&gt;  Parallel Index Only Scan using idx_equipment_id_auditorium on equipment  (cost=0.42..20309.09 rows=291667 width=20) (actual time=0.885..334.672 rows=233333 loops=3)&quot;
&quot;                                            Heap Fetches: 0&quot;
&quot;                                      -&gt;  Memoize  (cost=0.43..0.48 rows=1 width=13) (actual time=0.001..0.001 rows=1 loops=700000)&quot;
&quot;                                            Cache Key: equipment.number_auditory&quot;
&quot;                                            Cache Mode: logical&quot;
&quot;                                            Hits: 232141  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            Worker 0:  Hits: 231735  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            Worker 1:  Hits: 234024  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            -&gt;  Index Scan using u_nom on auditory  (cost=0.42..0.47 rows=1 width=13) (actual time=0.406..0.406 rows=1 loops=2100)&quot;
&quot;                                                  Index Cond: ((number_auditory)::text = (equipment.number_auditory)::text)&quot;
&quot;                                -&gt;  Parallel Hash  (cost=7283.37..7283.37 rows=208337 width=48) (actual time=321.385..321.386 rows=166669 loops=3)&quot;
&quot;                                      Buckets: 131072  Batches: 8  Memory Usage: 6240kB&quot;
&quot;                                      -&gt;  Parallel Seq Scan on department  (cost=0.00..7283.37 rows=208337 width=48) (actual time=0.416..249.114 rows=166669 loops=3)&quot;
&quot;                          -&gt;  Hash  (cost=243.38..243.38 rows=1 width=11) (actual time=2.482..2.483 rows=0 loops=3)&quot;
&quot;                                Buckets: 1024  Batches: 1  Memory Usage: 8kB&quot;
&quot;                                -&gt;  Index Only Scan using ix_inventorylist_inv_num_id_total on inventorylist  (cost=0.29..243.38 rows=1 width=11) (actual time=2.481..2.481 rows=0 loops=3)&quot;
&quot;                                      Index Cond: (&quot;&quot;ID_total&quot;&quot; = ANY ('{2,3}'::integer[]))&quot;
&quot;                                      Heap Fetches: 0&quot;
&quot;Planning Time: 44.228 ms&quot;
&quot;Execution Time: 3813.560 ms&quot;
</code></pre>
<p>So, how do I create indexes for this query to improve its execution time?/what could be the problem with these indexes? Thanks.</p>
",0,0,0,2025-12-16T09:38:14+00:00,1,100,True
79848632,26631237,Viet Nam,postgresql,Celery crashes when PgBouncer closes idle connections (idle timeouts enabled),"<p>I’m encountering an issue when running Celery with PgBouncer and PostgreSQL after enabling idle connection timeouts.</p>
<p>My stack includes:</p>
<ul>
<li><p>Django (served via Tornado)</p>
</li>
<li><p>Celery (workers + beat)</p>
</li>
<li><p>PostgreSQL</p>
</li>
<li><p>PgBouncer (in front of PostgreSQL)</p>
</li>
</ul>
<p>Due to a large number of <strong>idle database connections</strong> caused by Tornado + Django, I introduced idle timeout settings to protect PostgreSQL from running out of connections.</p>
<p>PgBouncer</p>
<pre><code>idle_transaction_timeout=240 (4mins)
client_idle_timeout=240
</code></pre>
<p>PostgreSQL</p>
<pre><code>idle_in_transaction_session_timeout=300000 (5mins)
idle_session_timeout=300000 (5mins)
</code></pre>
<p><strong>Problem:</strong></p>
<p>After applying these settings, Celery occasionally crashes with the following error:</p>
<pre><code>[2025-12-16 06:12:01,578: ERROR/MainProcess] Unrecoverable error: DatabaseError('client_idle_timeout\nserver closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n',)
Traceback (most recent call last):
  File &quot;/usr/local/lib/python2.7/site-packages/celery/worker/__init__.py&quot;, line 351, in start
    component.start()
  File &quot;/usr/local/lib/python2.7/site-packages/celery/worker/consumer.py&quot;, line 393, in start
    self.consume_messages()
  File &quot;/usr/local/lib/python2.7/site-packages/celery/worker/consumer.py&quot;, line 885, in consume_messages
    self.connection.drain_events(timeout=10.0)
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/connection.py&quot;, line 276, in drain_events
    return self.transport.drain_events(self.connection, **kwargs)
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/transport/virtual/__init__.py&quot;, line 760, in drain_events
    item, channel = get(timeout=timeout)
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/transport/virtual/scheduling.py&quot;, line 39, in get
    return self.fun(resource, **kwargs), resource
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/transport/virtual/__init__.py&quot;, line 780, in _drain_channel
    return channel.drain_events(timeout=timeout)
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/transport/virtual/__init__.py&quot;, line 578, in drain_events
    return self._poll(self.cycle, timeout=timeout)
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/transport/virtual/__init__.py&quot;, line 287, in _poll
    return cycle.get()
  File &quot;/usr/local/lib/python2.7/site-packages/kombu/transport/virtual/scheduling.py&quot;, line 39, in get
    return self.fun(resource, **kwargs), resource
  File &quot;/usr/local/lib/python2.7/site-packages/djkombu/transport.py&quot;, line 31, in _get
    m = Queue.objects.fetch(queue)
  File &quot;/usr/local/lib/python2.7/site-packages/djkombu/managers.py&quot;, line 18, in fetch
    queue = self.get(name=queue_name)
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/models/manager.py&quot;, line 132, in get
    return self.get_query_set().get(*args, **kwargs)
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/models/query.py&quot;, line 344, in get
    num = len(clone)
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/models/query.py&quot;, line 82, in __len__
    self._result_cache = list(self.iterator())
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/models/query.py&quot;, line 273, in iterator
    for row in compiler.results_iter():
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/models/sql/compiler.py&quot;, line 680, in results_iter
    for rows in self.execute_sql(MULTI):
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/models/sql/compiler.py&quot;, line 735, in execute_sql
    cursor.execute(sql, params)
  File &quot;/usr/local/lib/python2.7/site-packages/django/db/backends/postgresql_psycopg2/base.py&quot;, line 44, in execute
    return self.cursor.execute(query, args)
DatabaseError: client_idle_timeout
server closed the connection unexpectedly
    This probably means the server terminated abnormally
    before or while processing the request.

[2025-12-16 06:12:02,291: INFO/MainProcess] Celerybeat: Shutting down...
</code></pre>
<p>Questions:</p>
<ul>
<li><p>Is this a known issue when using Celery with PgBouncer idle timeouts?</p>
</li>
<li><p>Are these timeout values incompatible with long-running Celery workers?</p>
</li>
<li><p>What is the recommended way to configure PgBouncer/PostgreSQL idle timeouts when Celery is involved?</p>
</li>
</ul>
<p>Any guidance or best practices would be greatly appreciated. Thanks in advance!</p>
",1,1,0,2025-12-16T14:51:59+00:00,0,143,False
79848896,16967557,,postgresql,Why indexes don&#39;t improve performance?,"<p>This query uses my indexes according to the plan, however, they do not give any increase in query execution speed. Absolutely.</p>
<p>I've tried a wide variety of indexes, but none have helped.   I've already been told that the query must have &quot;where&quot; and/or &quot;order by&quot; in order for the indexes to work correctly, but even with them, nothing has changed. There is also a problem with sorting in this query, it takes quite a long time to complete, I don't know what can be done about it.</p>
<p>Indexes:</p>
<pre><code>CREATE INDEX idx_dept_name_id 
ON department (&quot;name&quot;, &quot;ID_department&quot;);

CREATE INDEX idx_inventory_faulty_partial 
ON inventorylist (&quot;inventory_number&quot;) 
WHERE &quot;ID_total&quot; IN (2, 3);

CREATE INDEX idx_auditory_dept_num 
ON auditory (&quot;ID_department&quot;, &quot;number_auditory&quot;);

CREATE INDEX idx_equipment_audit_inv 
ON equipment (&quot;number_auditory&quot;, &quot;inventory_number&quot;);

CREATE INDEX idx_equipment_inv_audit 
ON equipment (&quot;inventory_number&quot;, &quot;number_auditory&quot;);
</code></pre>
<p>The query:</p>
<pre><code>WITH BadDepartments AS (
    SELECT DISTINCT a.&quot;ID_department&quot;
    FROM auditory a
    JOIN equipment e ON a.&quot;number_auditory&quot; = e.&quot;number_auditory&quot;
    JOIN inventorylist il ON e.&quot;inventory_number&quot; = il.&quot;inventory_number&quot;
    WHERE il.&quot;ID_total&quot; IN (2, 3)
),
DeptStats AS (
    SELECT 
        a.&quot;ID_department&quot;,
        COUNT(e.&quot;inventory_number&quot;) as cnt
    FROM auditory a
    JOIN equipment e ON a.&quot;number_auditory&quot; = e.&quot;number_auditory&quot;
    GROUP BY a.&quot;ID_department&quot;
)
SELECT 
    d.&quot;name&quot; AS &quot;division&quot;,
    COALESCE(ds.cnt, 0) AS &quot;num of equip&quot;
FROM department d
LEFT JOIN BadDepartments bd ON d.&quot;ID_department&quot; = bd.&quot;ID_department&quot;
LEFT JOIN DeptStats ds ON d.&quot;ID_department&quot; = ds.&quot;ID_department&quot;
WHERE bd.&quot;ID_department&quot; IS NULL
ORDER BY d.&quot;name&quot; ASC;
</code></pre>
<p>The query plan of my query:</p>
<pre><code>&quot;Gather Merge  (cost=144266.05..192881.18 rows=416672 width=52) (actual time=3277.979..4597.799 rows=500008 loops=1)&quot;
&quot;  Workers Planned: 2&quot;
&quot;  Workers Launched: 2&quot;
&quot;  -&gt;  Sort  (cost=143266.03..143786.87 rows=208336 width=52) (actual time=3155.924..3507.905 rows=166669 loops=3)&quot;
&quot;        Sort Key: d.name&quot;
&quot;        Sort Method: external merge  Disk: 11040kB&quot;
&quot;        Worker 0:  Sort Method: external merge  Disk: 11088kB&quot;
&quot;        Worker 1:  Sort Method: external merge  Disk: 10112kB&quot;
&quot;        -&gt;  Hash Left Join  (cost=102493.05..117738.55 rows=208336 width=52) (actual time=632.838..754.065 rows=166669 loops=3)&quot;
&quot;              Hash Cond: (d.&quot;&quot;ID_department&quot;&quot; = ds.&quot;&quot;ID_department&quot;&quot;)&quot;
&quot;              -&gt;  Hash Anti Join  (cost=13.08..9926.70 rows=208336 width=48) (actual time=0.063..42.340 rows=166669 loops=3)&quot;
&quot;                    Hash Cond: (d.&quot;&quot;ID_department&quot;&quot; = a.&quot;&quot;ID_department&quot;&quot;)&quot;
&quot;                    -&gt;  Parallel Seq Scan on department d  (cost=0.00..7283.37 rows=208337 width=48) (actual time=0.011..11.846 rows=166669 loops=3)&quot;
&quot;                    -&gt;  Hash  (cost=13.07..13.07 rows=1 width=4) (actual time=0.033..0.036 rows=0 loops=3)&quot;
&quot;                          Buckets: 1024  Batches: 1  Memory Usage: 8kB&quot;
&quot;                          -&gt;  Unique  (cost=13.06..13.07 rows=1 width=4) (actual time=0.033..0.035 rows=0 loops=3)&quot;
&quot;                                -&gt;  Sort  (cost=13.06..13.06 rows=1 width=4) (actual time=0.032..0.034 rows=0 loops=3)&quot;
&quot;                                      Sort Key: a.&quot;&quot;ID_department&quot;&quot;&quot;
&quot;                                      Sort Method: quicksort  Memory: 25kB&quot;
&quot;                                      Worker 0:  Sort Method: quicksort  Memory: 25kB&quot;
&quot;                                      Worker 1:  Sort Method: quicksort  Memory: 25kB&quot;
&quot;                                      -&gt;  Nested Loop  (cost=0.97..13.05 rows=1 width=4) (actual time=0.015..0.016 rows=0 loops=3)&quot;
&quot;                                            -&gt;  Nested Loop  (cost=0.55..12.58 rows=1 width=9) (actual time=0.015..0.016 rows=0 loops=3)&quot;
&quot;                                                  -&gt;  Index Only Scan using idx_inventory_faulty_partial on inventorylist il  (cost=0.12..8.14 rows=1 width=11) (actual time=0.015..0.015 rows=0 loops=3)&quot;
&quot;                                                        Heap Fetches: 0&quot;
&quot;                                                  -&gt;  Index Only Scan using idx_equipment_inv_audit on equipment e  (cost=0.42..4.44 rows=1 width=21) (never executed)&quot;
&quot;                                                        Index Cond: (inventory_number = (il.inventory_number)::text)&quot;
&quot;                                                        Heap Fetches: 0&quot;
&quot;                                            -&gt;  Index Scan using u_nom on auditory a  (cost=0.42..0.47 rows=1 width=13) (never executed)&quot;
&quot;                                                  Index Cond: ((number_auditory)::text = (e.number_auditory)::text)&quot;
&quot;              -&gt;  Hash  (cost=98489.63..98489.63 rows=229547 width=12) (actual time=632.183..632.185 rows=699 loops=3)&quot;
&quot;                    Buckets: 262144  Batches: 2  Memory Usage: 2063kB&quot;
&quot;                    -&gt;  Subquery Scan on ds  (cost=87062.75..98489.63 rows=229547 width=12) (actual time=631.126..631.556 rows=699 loops=3)&quot;
&quot;                          -&gt;  HashAggregate  (cost=87062.75..96194.16 rows=229547 width=12) (actual time=631.125..631.472 rows=699 loops=3)&quot;
&quot;                                Group Key: a_1.&quot;&quot;ID_department&quot;&quot;&quot;
&quot;                                Planned Partitions: 4  Batches: 1  Memory Usage: 1617kB&quot;
&quot;                                Worker 0:  Batches: 1  Memory Usage: 1617kB&quot;
&quot;                                Worker 1:  Batches: 1  Memory Usage: 1617kB&quot;
&quot;                                -&gt;  Nested Loop  (cost=0.86..42219.00 rows=700000 width=16) (actual time=0.402..480.119 rows=700000 loops=3)&quot;
&quot;                                      -&gt;  Index Only Scan using idx_equipment_audit_inv on equipment e_1  (cost=0.42..24392.42 rows=700000 width=21) (actual time=0.331..113.335 rows=700000 loops=3)&quot;
&quot;                                            Heap Fetches: 0&quot;
&quot;                                      -&gt;  Memoize  (cost=0.43..0.48 rows=1 width=13) (actual time=0.000..0.000 rows=1 loops=2100000)&quot;
&quot;                                            Cache Key: e_1.number_auditory&quot;
&quot;                                            Cache Mode: logical&quot;
&quot;                                            Hits: 699300  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            Worker 0:  Hits: 699300  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            Worker 1:  Hits: 699300  Misses: 700  Evictions: 0  Overflows: 0  Memory Usage: 82kB&quot;
&quot;                                            -&gt;  Index Scan using u_nom on auditory a_1  (cost=0.42..0.47 rows=1 width=13) (actual time=0.019..0.019 rows=1 loops=2100)&quot;
&quot;                                                  Index Cond: ((number_auditory)::text = (e_1.number_auditory)::text)&quot;
&quot;Planning Time: 6.017 ms&quot;
&quot;Execution Time: 4618.617 ms&quot;
</code></pre>
",2,2,0,2025-12-16T20:58:02+00:00,2,166,True
79849124,32065523,,postgresql,PostgreSQL Connection Error via DBeaver: FATAL: invalid value for parameter &quot;TimeZone&quot;: &quot;Asia/Calcutta&quot;,"<p>I have a PostgreSQL database server running on my VPS. I am currently working from my local machine using an SSH connection and would like to view the database tables and data graphically for better analysis. For this purpose, I am using the DBeaver Desktop application.</p>
<p>In DBeaver, I have configured all the required connection details, including the SSH tunnel settings. When I test the SSH tunnel configuration, the connection is established successfully. However, when I attempt to test or establish the actual database connection, I encounter the following error message:</p>
<p><strong>FATAL: invalid value for parameter &quot;TimeZone&quot;: &quot;Asia/Calcutta&quot;</strong></p>
<p>I have tried multiple configuration changes on both the client and server sides, but the issue remains unresolved. The SSH tunnel works correctly, but the PostgreSQL connection fails during initialization due to the timezone error.</p>
",1,1,0,2025-12-17T06:35:34+00:00,2,154,True
79849582,9456807,,postgresql,TimescaleDB Hypertable Performance Slower Than Normal Table for Time-Series Queries – Why?,"<p>I created two tables—one normal table and one TimescaleDB hypertable—and inserted 50 million records into each. However, I’m not seeing a significant performance difference between them. In fact, the normal table often performs better in my queries.</p>
<p>Here are the table definitions:</p>
<pre class=""lang-sql prettyprint-override""><code>-- Hypertable
CREATE TABLE stat_flow
(
    time         TIMESTAMPTZ NOT NULL,
    dev_id       BIGINT      NOT NULL,
    stat_info    JSONB,
    setting_info JSONB,
    run_info     JSONB
);
SELECT create_hypertable('stat_flow', by_range('time'));

-- Added an extra dimension
SELECT add_dimension(
               'stat_flow',
               'dev_id',
               number_partitions =&gt; 50
       );

-- Enabled compression
ALTER TABLE stat_flow
    SET (
        timescaledb.compress,
        timescaledb.compress_segmentby = 'dev_id',
        timescaledb.compress_orderby = 'time DESC'
        );
-- Chunk size set to 7 days

-- Normal table
CREATE TABLE stat_flow_normal
(
    time         TIMESTAMPTZ NOT NULL,
    dev_id       BIGINT      NOT NULL,
    stat_info    JSONB,
    setting_info JSONB,
    run_info     JSONB
);

-- Created indexes
create index stat_flow_normal_dev_id_time_index
    on stat_flow_normal (dev_id, time);

create index stat_flow_normal_time_dev_id_index
    on stat_flow_normal (time, dev_id);
</code></pre>
<p>I then used a Python script to insert 50 million rows into both tables.</p>
<p>Despite this, query performance doesn’t show the expected improvements with TimescaleDB. For example:</p>
<p><strong>Example 1: Aggregation over several weeks</strong></p>
<pre class=""lang-sql prettyprint-override""><code>-- 57 ms with hypertable
SELECT time_bucket('1 hour', time)                    as hour,
       AVG((stat_info -&gt;&gt; 'temperature')::numeric)    as avg_temperature,
       AVG((stat_info -&gt;&gt; 'humidity')::numeric)       as avg_humidity,
       AVG((stat_info -&gt;&gt; 'carbon_dioxide')::numeric) as avg_carbon_dioxide
FROM stat_flow
WHERE true
  and dev_id = 26
  and time &lt; now() - interval '1 week'
  and time &gt; now() - interval '5 week'
GROUP BY time_bucket('1 hour', time)
ORDER BY hour;

-- 15 ms with normal table
SELECT date_trunc('hour', time)                    as hour,
       AVG((stat_info -&gt;&gt; 'temperature')::numeric) as avg_temperature,
       AVG((stat_info -&gt;&gt; 'humidity')::numeric)       as avg_humidity,
       AVG((stat_info -&gt;&gt; 'carbon_dioxide')::numeric) as avg_carbon_dioxide
FROM stat_flow_normal
WHERE true
  and dev_id = 26
  and time &lt; now() - interval '1 week'
  and time &gt; now() - interval '5 week'
GROUP BY date_trunc('hour', time)
ORDER BY hour;
</code></pre>
<p><strong>Example 2: Time-ordered fetch for a device</strong></p>
<pre class=""lang-sql prettyprint-override""><code>-- 35 ms with hypertable
SELECT (stat_info -&gt;&gt; 'temperature')::numeric    as temperature,
       (stat_info -&gt;&gt; 'humidity')::numeric       as humidity,
       (stat_info -&gt;&gt; 'carbon_dioxide')::numeric as carbon_dioxide,
       time
from stat_flow
where dev_id = 20
  and time &lt; now() - interval '1 week'
  and time &gt; now() - interval '2 week'
order by time desc
limit 100;

-- 7 ms with normal table
SELECT (stat_info -&gt;&gt; 'temperature')::numeric    as temperature,
       (stat_info -&gt;&gt; 'humidity')::numeric       as humidity,
       (stat_info -&gt;&gt; 'carbon_dioxide')::numeric as carbon_dioxide,
       time
from stat_flow_normal
where dev_id = 20
  and time &lt; now() - interval '1 week'
  and time &gt; now() - interval '2 week'
order by time desc
limit 100;
</code></pre>
<p>I also tested queries over longer time ranges, but the trend remains:</p>
<pre class=""lang-sql prettyprint-override""><code>-- 899 ms with hypertable
SELECT time_bucket('1 month', time)                    as hour,
       AVG((stat_info -&gt;&gt; 'temperature')::numeric)    as avg_temperature,
       AVG((stat_info -&gt;&gt; 'humidity')::numeric)       as avg_humidity,
       AVG((stat_info -&gt;&gt; 'carbon_dioxide')::numeric) as avg_carbon_dioxide
FROM stat_flow
WHERE true
  and dev_id = 26
  and time &lt; now() - interval '1 year'
  and time &gt; now() - interval '5 year'
GROUP BY time_bucket('1 month', time)
ORDER BY hour;

-- 254 ms with normal table
SELECT date_trunc('month', time)                    as hour,
       AVG((stat_info -&gt;&gt; 'temperature')::numeric) as avg_temperature,
       AVG((stat_info -&gt;&gt; 'humidity')::numeric)       as avg_humidity,
       AVG((stat_info -&gt;&gt; 'carbon_dioxide')::numeric) as avg_carbon_dioxide
FROM stat_flow_normal
WHERE true
  and dev_id = 26
  and time &lt; now() - interval '1 year'
  and time &gt; now() - interval '5 year'
GROUP BY date_trunc('month', time)
ORDER BY hour;
</code></pre>
<p>Why is the hypertable underperforming compared to the normal indexed table in these cases? Are there configuration improvements I should consider, or is this expected for my query patterns?</p>
<p><strong>Appendix: Data Generation Script</strong></p>
<p>To help with reproduction, here's the Python script I used to generate and insert the 50 million records:</p>
<pre class=""lang-py prettyprint-override""><code>import psycopg2
import json
import random
import time
import threading
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import sys
import psycopg2.extras

# 数据库配置
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'postgres',
    'user': 'admin',
    'password': '123456',
    'connect_timeout': 5
}


class DatabaseInserter:
    &quot;&quot;&quot;专门负责数据库插入的类，使用多个连接&quot;&quot;&quot;

    def __init__(self, db_config, num_connections=5):
        self.db_config = db_config
        self.num_connections = num_connections
        self.connections = []
        self.cursors = []
        self.data_queue = Queue(maxsize=10000)  # 控制队列大小防止内存爆炸
        self.inserted_count = 0
        self.lock = threading.Lock()

    def create_connections(self):
        &quot;&quot;&quot;创建多个数据库连接&quot;&quot;&quot;
        for i in range(self.num_connections):
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            self.connections.append(conn)
            self.cursors.append(cursor)
        print(f&quot;创建了 {self.num_connections} 个数据库连接&quot;)

    def close_connections(self):
        &quot;&quot;&quot;关闭所有连接&quot;&quot;&quot;
        for cursor in self.cursors:
            cursor.close()
        for conn in self.connections:
            conn.close()

    def start_inserters(self):
        &quot;&quot;&quot;启动多个插入线程&quot;&quot;&quot;
        self.inserter_threads = []
        for i in range(self.num_connections):
            thread = threading.Thread(target=self._inserter_worker, args=(i,))
            thread.daemon = True
            thread.start()
            self.inserter_threads.append(thread)

    def _inserter_worker(self, worker_id):
        &quot;&quot;&quot;插入工作线程&quot;&quot;&quot;
        cursor = self.cursors[worker_id]
        conn = self.connections[worker_id]
        batch_size = 1000
        batch_data = []

        while True:
            try:
                # 从队列获取数据，设置超时避免永久阻塞
                data = self.data_queue.get(timeout=5)
                if data is None:  # 结束信号
                    break

                batch_data.append(data)

                # 批次达到大小时插入
                if len(batch_data) &gt;= batch_size:
                    self._insert_batch(cursor, conn, batch_data, worker_id)
                    batch_data = []

            except Exception as e:
                # 队列超时或其他错误
                if batch_data:  # 插入剩余数据
                    self._insert_batch(cursor, conn, batch_data, worker_id)
                    batch_data = []
                continue

        # 插入最后一批数据
        if batch_data:
            self._insert_batch(cursor, conn, batch_data, worker_id)

    def _insert_batch(self, cursor, conn, batch_data, worker_id):
        &quot;&quot;&quot;执行批量插入&quot;&quot;&quot;
        try:
            insert_sql = &quot;&quot;&quot;
                         INSERT INTO stat_flow (time, dev_id, stat_info, setting_info, run_info)
                         VALUES %s \
                         &quot;&quot;&quot;
            psycopg2.extras.execute_values(cursor, insert_sql, batch_data)
            conn.commit()

            with self.lock:
                self.inserted_count += len(batch_data)

            # 每10万条打印一次进度
            if self.inserted_count % 100000 == 0:
                print(f&quot;已插入: {self.inserted_count:,} 条数据 (Worker {worker_id})&quot;)

        except Exception as e:
            print(f&quot;插入错误 (Worker {worker_id}): {e}&quot;)
            conn.rollback()

    def add_data(self, data):
        &quot;&quot;&quot;向队列添加数据&quot;&quot;&quot;
        self.data_queue.put(data)

    def wait_completion(self):
        &quot;&quot;&quot;等待所有数据插入完成&quot;&quot;&quot;
        # 发送结束信号
        for _ in range(self.num_connections):
            self.data_queue.put(None)

        # 等待队列清空
        while not self.data_queue.empty():
            time.sleep(1)

        print(f&quot;最终插入记录数: {self.inserted_count:,}&quot;)


def generate_device_config(total_devices=100):
    &quot;&quot;&quot;生成设备配置&quot;&quot;&quot;
    devices = {}

    for dev_id in range(1, total_devices + 1):
        # 随机分配5或10分钟间隔
        interval = random.choice([5, 10])
        # 随机起始时间偏移（0-59分钟）
        start_offset = random.randint(0, 59)
        start_time = datetime(2024, 1, 1) + timedelta(minutes=start_offset)

        devices[dev_id] = {
            'interval': interval,
            'start_time': start_time,
            'base_temp': 20 + random.uniform(5, 10),  # 每个设备有不同的基准温度
            'base_humidity': 0.6 + random.uniform(0.1, 0.3)  # 不同的基准湿度
        }

    return devices


def generate_sensor_data(dev_id, base_temp=25.0, base_humidity=0.7):
    &quot;&quot;&quot;生成模拟传感器数据&quot;&quot;&quot;
    # 温度：在基准值上下波动 ±2.0度
    temp_variation = random.uniform(-2.0, 2.0)
    temperature = round(base_temp + temp_variation, 1)

    # 湿度：在基准值上下波动 ±0.1
    humidity_variation = random.uniform(-0.1, 0.1)
    humidity = round(max(0.3, min(0.9, base_humidity + humidity_variation)), 2)

    # CO2：在一定范围内波动
    carbon_dioxide = random.randint(40000, 60000)

    return {
        &quot;temperature&quot;: temperature,
        &quot;humidity&quot;: humidity,
        &quot;carbon_dioxide&quot;: carbon_dioxide
    }


def generate_setting_info(dev_id):
    &quot;&quot;&quot;生成设备设置信息&quot;&quot;&quot;
    return {
        &quot;temperature&quot;: round(22 + random.uniform(2, 5), 1),
        &quot;humidity&quot;: round(0.6 + random.uniform(0.1, 0.2), 2)
    }


def generate_run_info(dev_id):
    &quot;&quot;&quot;生成设备运行信息&quot;&quot;&quot;
    status = random.choice([&quot;normal&quot;, &quot;warning&quot;, &quot;error&quot;])

    return {
        &quot;status&quot;: status,
        &quot;operating_hours&quot;: random.randint(0, 10000),
        &quot;last_maintenance&quot;: f&quot;2023-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}&quot;
    }


def generate_device_data(dev_id, config, target_records=500000):
    &quot;&quot;&quot;生成单个设备的数据流&quot;&quot;&quot;
    interval = config['interval']
    base_temp = config['base_temp']
    base_humidity = config['base_humidity']

    # 计算需要的时间范围
    total_minutes = target_records * interval
    start_time = config['start_time']

    current_time = start_time
    generated_count = 0

    while generated_count &lt; target_records:
        # 生成数据
        stat_info = json.dumps(generate_sensor_data(dev_id, base_temp, base_humidity))
        setting_info = json.dumps(generate_setting_info(dev_id))
        run_info = json.dumps(generate_run_info(dev_id))

        yield (
            current_time,
            dev_id,
            stat_info,
            setting_info,
            run_info
        )

        current_time += timedelta(minutes=interval)
        generated_count += 1


def main_optimized():
    &quot;&quot;&quot;更优化的版本：流式生成，避免内存爆炸&quot;&quot;&quot;
    print(&quot;开始生成5000万条时序数据（流式优化版）...&quot;)
    start_time = time.time()

    # 创建数据库插入器
    inserter = DatabaseInserter(DB_CONFIG, num_connections=16)
    inserter.create_connections()
    inserter.start_inserters()

    print(&quot;设备配置:&quot;)
    # 生成设备配置
    device_configs = generate_device_config(100)
    for config in device_configs:
        print(config)

    target_total_records = 50000000
    records_per_device = target_total_records // 100

    print(f&quot;目标总记录数: {target_total_records:,}&quot;)
    print(f&quot;开始流式生成数据...&quot;)

    # 直接并行生成并插入，避免存储所有数据在内存中
    def generate_and_insert(dev_id, config, records_to_generate):
        &quot;&quot;&quot;生成并立即插入数据&quot;&quot;&quot;
        interval = config['interval']
        base_temp = config['base_temp']
        base_humidity = config['base_humidity']

        current_time = config['start_time']
        generated = 0

        while generated &lt; records_to_generate:
            # 生成单条数据并立即插入
            stat_info = json.dumps(generate_sensor_data(dev_id, base_temp, base_humidity))
            setting_info = json.dumps(generate_setting_info(dev_id))
            run_info = json.dumps(generate_run_info(dev_id))

            data = (current_time, dev_id, stat_info, setting_info, run_info)
            inserter.add_data(data)

            current_time += timedelta(minutes=interval)
            generated += 1

        return generated

    # 使用线程池并行生成
    total_generated = 0
    with ThreadPoolExecutor(max_workers=16) as executor:
        futures = {
            executor.submit(generate_and_insert, dev_id, config, records_per_device): dev_id
            for dev_id, config in device_configs.items()
        }

        for future in as_completed(futures):
            dev_id = futures[future]
            try:
                count = future.result()
                total_generated += count
                print(f&quot;设备 {dev_id} 完成: {count:,} 条&quot;)
            except Exception as e:
                print(f&quot;设备 {dev_id} 失败: {e}&quot;)

    # 等待插入完成
    inserter.wait_completion()
    inserter.close_connections()

    end_time = time.time()
    total_time = end_time - start_time

    print(f&quot;\n完成！总耗时: {total_time:.2f} 秒&quot;)
    if total_time &gt; 0:
        print(f&quot;平均速度: {target_total_records / total_time:.0f} 条/秒&quot;)


if __name__ == &quot;__main__&quot;:
    # 使用优化版本
    main_optimized()

</code></pre>
",0,0,0,2025-12-17T15:32:17+00:00,0,56,False
79850341,24487968,,postgresql,"How to design profile view email notifications using Node.js, Bull queue and Redis?","<p>I have a social networking site where people connect, meet and build network. User created their profile based on there interest. Users can visit others people profile and send connection request to them. I wanted that user who visited other profile that user will recevied the email that hey this user is viewed your profile go check it out.<br />
I want high level plan for the project in three steps</p>
<p>1.⁠ ⁠Router that invokes the members pulling from the DB</p>
<p>2.⁠ ⁠Operate on the records and pull list of viewers, build the template based listing of members and store into the Bull or Redis</p>
<p>3.⁠ ⁠A consumer that reads from the Bull queue and send email using google SMTP.</p>
<p>guide me to how to approach for this project.</p>
",0,0,0,2025-12-18T13:12:18+00:00,1,33,False
79850577,17629737,,postgresql,Django Can&#39;t find table during insert: LINE 1: SELECT 1 AS &quot;a&quot; FROM &quot;qmgr_def_rules&quot; WHERE &quot;qmgr_def_rules&quot;,"<p>I'm trying to add a row to my model, but it keeps coming back with:</p>
<blockquote>
<p>django.db.utils.ProgrammingError: relation &quot;qmgr_def_rules&quot; does not exist
LINE 1: SELECT 1 AS &quot;a&quot; FROM &quot;qmgr_def_rules&quot; WHERE &quot;qmgr_def_rules&quot;...</p>
</blockquote>
<p>Some context:
I have 3 different applications under the same django project. I have 2 different postgres schemas: rules and django.
The django schema is set to default where all the django specific tables go. The rules schema is for internal data where I can set interval timers, object definitions, etc. I have multiple tables in the rules schema for different types of data.</p>
<p>When I use the admin panel to insert/update/delete on any other table in that schema, it works. Just not def_rules for some odd reason.</p>
<p>A few really odd things:</p>
<ul>
<li>Updates work</li>
<li>Deletions work</li>
<li>Inserts break with error above through admin panel</li>
<li>Inserts work if done through django shell</li>
</ul>
<p>(this is the shell that I used)</p>
<pre><code>from admin_page.models import QmgrDefRules
from django.utils import timezone

# Try to create a test entry with all required fields
try:
    test_entry = QmgrDefRules(
        # Primary key - required
        qmgr='TEST',
        
        # Required boolean fields (NOT NULL in DDL)
        moni_core_enabled=False,
        moni_inventory_enabled=False,
        qreset=True,  # Has default=true in DDL
        dbinsert_0_value=False,  # Has default=false in DDL
        qsg_monitor=False,  # Has default=false in DDL
        dlqh=False,  # Has default=false in DDL
        smds_monitor=False,  # Has default=false in DDL
        monitor_queuestats=False,  # Has default=false in DDL
        monitor_smds=False,  # Has default=false in DDL
        monitor_rba=False,  # Has default=false in DDL
        monitor_psid_bp=False,  # Has default=false in DDL
        monitor_qhandles=False,  # Has default=false in DDL
        monitor_chstatus_p2p=False,  # Has default=false in DDL
        monitor_chstatus_svrconn=False,  # Has default=false in DDL
        monitor_cfstatus=False,  # Has default=false in DDL
        monitor_chstatus_cluschls=False,  # Has default=false in DDL
        
        # Required integer fields (NOT NULL in DDL)
        rule_refresh_interval_in_sec=60,
        restapi_timeout=5,
        restapi_max_retry=5,
        
        # Optional fields (can be NULL in DDL)
        long_retry_wait_in_sec=600,
        bu='bu',  
        location='test',  
        qsg=None,
        day_alert_from=None,
        day_alert_to=None,
        time_alert_from=None,
        time_alert_to=None,
        email_groups='mail@mail.com',
        page_groups=None,
        mq_rest_url='http://url:port/',
        opsmvs_rest_url=None,
        dlqh_qdepth=None,
        dlqh_msgage=None,
        dlq_name=None,
        last_update=timezone.now(),
        date_created=timezone.now()
    )
    
    # Save to the 'rules' database
    test_entry.save(using='rules')
    print(f&quot; Success! Created QMGR: {test_entry.qmgr}&quot;)
    
except Exception as e:
    print(f&quot; Error: {type(e).__name__}: {e}&quot;)
    import traceback
    traceback.print_exc()
</code></pre>
<p>Here's a model and its admin definition that works:</p>
<pre><code>class QdepthRules(models.Model):
    rule_id = models.AutoField(db_column='id', primary_key=True)
    enabled = models.BooleanField(db_column='enabled')  # Checkbox
    qmgr = models.CharField(db_column='qmgr', max_length=4, validators=[qmgrValidator], help_text='All caps (ex: QQQQ)')  # Field name made lowercase.
    queue = models.CharField(db_column='queue', max_length=45, help_text=&quot;Case sensitive. Wildcards allowed.&quot;)  # Field name made lowercase.
    threshold = models.IntegerField(db_column='threshold',default=1, validators=[MinValueValidator(1)], help_text=&quot;How many messages until MONI alerts.&quot;)  # Field name made lowercase.
    day_alert_from = models.IntegerField(db_column='day_alert_from', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    day_alert_to = models.IntegerField(db_column='day_alert_to', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    time_alert_from = models.TimeField(db_column='time_alert_from', blank=True, null=True)  # Field name made lowercase.
    time_alert_to = models.TimeField(db_column='time_alert_to', blank=True, null=True)  # Field name made lowercase.
    email_groups = models.TextField(db_column='email_groups', blank=True, null=True, validators=[emailValidator], help_text=&quot;Format: mail@mail.com&quot;)  # Field name made lowercase.
    page_groups = models.TextField(db_column='page_groups', blank=True, null=True, validators=[pageValidator]&quot;)  # Field name made lowercase.
    repeat_alert_val = models.IntegerField(db_column='repeat_alert_val', default=1, validators=[MinValueValidator(1), MaxValueValidator(240)], help_text=&quot;How many times the alert needs to trip before sending an alert&quot;)  # Field name made lowercase.
    email_page_both = models.CharField(db_column='email_page_both', max_length=45, blank=True, null=True)  # Field name made lowercase.
    last_update = models.DateTimeField(db_column='last_update', blank=True, null=True)  # Field name made lowercase.
    date_created = models.DateTimeField(db_column='date_created', blank=True, null=True)  # Field name made lowercase.

    def __str__(self):
        return f&quot;QMGR: {self.qmgr} - QUEUE: {self.queue} - THRESH: {self.threshold} - ENABLED: {bool(self.enabled)}&quot;

    class Meta:
        managed = False
        db_table = 'qdepth_rules'
        verbose_name_plural = &quot;QDepth_Rules&quot;



---------------------------

from django.contrib import admin

class QDRAdmin(admin.ModelAdmin):
    # fields = (&quot;Title&quot;,)

    field_list = []
    for each in QdepthRules._meta.get_fields():
        field_list.append(each.name)

    list_display = field_list
    list_filter = ('date_created',)
    search_fields = ['qmgr','queue']
    readonly_fields = ('date_created','last_update')
    list_display_links = ['rule_id','qmgr','queue']
    
    def get_queryset(self, request):
        return super().get_queryset(request).using('rules')
    
    def save_model(self, request, obj, form, change):
        obj.save(using='rules')
    
    def delete_model(self, request, obj):
        obj.delete(using='rules')
    
    def save_related(self, request, form, formsets, change):
        form.save_m2m()
</code></pre>
<p>And heres the one that doesn't:</p>
<pre><code>class QmgrDefRules(models.Model):
    qmgr = models.CharField(primary_key=True,db_column='qmgr', max_length=4, validators=[qmgrValidator], help_text='All caps (ex: QQQQ)')  # Field name made lowercase.
    moni_core_enabled = models.BooleanField(db_column='moni_core_enabled')  # Checkbox
    moni_inventory_enabled = models.BooleanField(db_column='moni_inventory_enabled') # Checkbox
    rule_refresh_interval_in_sec = models.IntegerField(db_column='rule_refresh_interval_in_sec', default=60, help_text=&quot;How often (in seconds) MONI checks data against the QMGR.&quot;)  # Field name made lowercase.
    long_retry_wait_in_sec = models.IntegerField(db_column='long_retry_wait_in_sec', default=600)  # Field name made lowercase.
    bu = models.CharField(max_length=4)
    location = models.CharField(max_length=10)
    qsg = models.CharField(db_column='qsg', max_length=10, blank=True, null=True)  # Field name made lowercase.
    qsg_monitor = models.BooleanField(help_text='If QMGR is full repository, enable this to collect shared data. Only enable one QMGR per QSG')
    day_alert_from = models.IntegerField(db_column='day_alert_from', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    day_alert_to = models.IntegerField(db_column='day_alert_to', blank=True, null=True, choices=[(0,&quot;Monday&quot;),(1,&quot;Tuesday&quot;),(2,&quot;Wednesday&quot;),(3,&quot;Thursday&quot;),(4,&quot;Friday&quot;),(5,&quot;Saturday&quot;),(6,&quot;Sunday&quot;)])  # Field name made lowercase.
    time_alert_from = models.TimeField(db_column='time_alert_from', blank=True, null=True)  # Field name made lowercase.
    time_alert_to = models.TimeField(db_column='time_alert_to', blank=True, null=True)  # Field name made lowercase.
    email_groups = models.TextField(db_column='email_groups', blank=True, null=True, validators=[emailValidator])  # Field name made lowercase.
    page_groups = models.TextField(db_column='page_groups', blank=True, null=True, validators=[pageValidator])  # Field name made lowercase.
    dbinsert_0_value = models.BooleanField(db_column='dbinsert_0_value', help_text=&quot;This will log values into the database even if MSGAGE, CURDEPTH, PUT, and GET are 0. NOTE: THIS SHOULD ONLY BE USED FOR EXTENSIVE TROUBLESHOOTING ONLY.&quot;)  # Checkbox
    qreset = models.BooleanField(db_column='qreset', help_text=&quot;Enables QRESET for the QMGR. Provides PUT/GET data.&quot;)  # Checkbox
    restapi_timeout = models.IntegerField(db_column='restapi_timeout', default=5, help_text=&quot;How long (in seconds) MONI waits for a response from the MQ REST API console.&quot;)  # Field name made lowercase.
    restapi_max_retry = models.IntegerField(db_column='restapi_max_retry', default=5, help_text=&quot;How many times MONI retries to connect to the REST API console if theres an issue connecting before initating a long cooldown.&quot;)  # Field name made lowercase.
    mq_rest_url = models.CharField(max_length=50, blank=True, null=True, help_text=&quot;URL to the MQ REST API console. Used to connect into the QMGR for data collection. (ex: http://url:port/)&quot;)
    opsmvs_rest_url = models.CharField(max_length=50, blank=True, null=True, help_text=&quot;URL to the OPSMVS REST API console. Used to send alerts for email/page.)
    dlqh = models.BooleanField(help_text=&quot;Enables DLQH for the QMGR.&quot;)
    dlqh_qdepth = models.SmallIntegerField(blank=True, null=True, help_text=&quot;QDEPTH at which will trigger the DLQH. Can be used with MSGAGE.&quot;)
    dlqh_msgage = models.SmallIntegerField(blank=True, null=True, help_text=&quot;MSGAGE at which will trigger the DLQH. Can be used with QDEPTH.&quot;)
    dlq_name = models.CharField(blank=True, null=True, help_text=&quot;DLQ for the QMGR (ex: QTN2.DEAD.QUEUE).&quot;)
    last_update = models.DateTimeField(db_column='last_update', blank=True, null=True)  # Field name made lowercase.
    date_created = models.DateTimeField(db_column='date_created', blank=True, null=True)  # Field name made lowercase.
    smds_monitor = models.BooleanField()
    monitor_queuestats = models.BooleanField()
    monitor_smds = models.BooleanField()
    monitor_rba = models.BooleanField()
    monitor_psid_bp = models.BooleanField()
    monitor_qhandles = models.BooleanField()
    monitor_chstatus_p2p = models.BooleanField()
    monitor_chstatus_svrconn = models.BooleanField()
    monitor_cfstatus = models.BooleanField()
    monitor_chstatus_cluschls = models.BooleanField()



    def __str__(self):
        return f&quot;QMGR: {self.qmgr} - RefreshINT: {self.rule_refresh_interval_in_sec} - QSG: {self.qsg} - MONI_CORE_ENABLED: {bool(self.moni_core_enabled)}&quot;

    class Meta:
        managed = False
        db_table = 'qmgr_def_rules'
        verbose_name_plural = &quot;QMGR_DEF_Rules&quot;




---------------------------------------


from django.contrib import admin

class QMGRDRAdmin(admin.ModelAdmin):
    # fields = (&quot;Title&quot;,)

    field_list = []
    for each in QmgrDefRules._meta.get_fields():
        field_list.append(each.name)
        
    # print(field_list)
    # print(QmgrDefRules._meta.get_field('qmgr'))

    list_display = field_list
    list_filter = ('date_created',)
    search_fields = ['qmgr','qsg']
    readonly_fields = ('date_created','last_update')
    list_display_links = ['qmgr','qsg']
    
    def get_queryset(self, request):
        return super().get_queryset(request).using('rules')
    
    def save_model(self, request, obj, form, change):
        obj.save(using='rules')
    
    def delete_model(self, request, obj):
        obj.delete(using='rules')
    
    def save_related(self, request, form, formsets, change):
        form.save_m2m()


</code></pre>
<p>I tried clearing out the migrations and starting from scratch as well, but that didn't seem to fix the problem unless I've done it incorrectly.</p>
",1,1,0,2025-12-18T17:55:34+00:00,0,78,False
79850663,22811437,,postgresql,How to import schema from a local database into a remote database in Supabase,"<p>I'm currently developing locally, with a remote database that is going to become the live, production database.</p>
<p>However, I have trouble applying my local changes as migrations to the remote database. <code>db diff</code> and <code>db push</code> always throw different errors. I'm not interested right now in resolving those errors. Instead, since my remote database is currently empty, I just want to copy my schema over from my local environment to the remote database.</p>
<p><strong>Is there a way to create a schema dump from my local database, and import it into the remote database?</strong> As there is currently no relevant data on the remote db, this works fine for me.</p>
",-2,0,2,2025-12-18T20:04:32+00:00,0,40,False
79851111,8209959,,postgresql,Can not Power on Aiven Database. Migrating a Backup to another provider is also not possible,"<p>I have a Postgres database in Aiven, with a free plan. It suddently turned off and when trying to get it back on, the status of the database is stuck in Rebuilding Phase. I can not contact support on Aiven side, since support is only for paid memberships.</p>
<p>Also, I can not backup an migrate the database, since that option is also only for paid subscriptions.</p>
<p>So I am stuck, with the application down. What to do?</p>
<p><strong>Update</strong>: For other people facing the same issue, this is what I did:</p>
<ol>
<li>start a trial in order to ask a question to support - no credit card needed -</li>
<li>ask for a backup or restore of some sorts.</li>
<li>Even though this would normally require payment, because it was the server's fault Aiven team was kind enough to do a restore for me.
They have automatic backups even for the free plans so stay calm, your 10 years worth of data are not lost.</li>
</ol>
<p>There are a lot of people in this situation so I hope this answer helps, so I will keep the thread alive even though I got downvoted for no reason, this is a specific case of helping each other trough our human experience, what Stackoverflow used to be and should be about.</p>
",-3,0,3,2025-12-19T12:45:17+00:00,0,53,False
79851291,25055931,,postgresql,SQL integration and multiply on huge dataset,"<p>I would like to find an efficient method to multiply 2 array element by element then integrate the result.</p>
<p>My table contain primary key, then 5 lists of real (16 bits) : time, channel A, B, C and D. Each list contains 2000 elements</p>
<p>My problem is that I tried with python or SQL based function like this :</p>
<pre><code>CREATE OR REPLACE FUNCTION trapezoid_mul_fast(
    a float4[],
    b float4[],
    x float8[]
)
RETURNS float8
LANGUAGE SQL
IMMUTABLE
AS $$
    SELECT SUM(
        (x[i] - x[i-1]) * ((a[i]*b[i]) + (a[i-1]*b[i-1])) * 0.5
    )
    FROM generate_subscripts(a, 1) AS i
    WHERE i &gt; 1;
$$;
</code></pre>
<p>then (for example)</p>
<pre><code> SELECT
    id,
    trapezoid_mul_fast(channel_a, channel_b, time_ns) AS int_ab
FROM my_table; 
</code></pre>
<p>But unfortunately it takes way too long to perform. Ideally, I would like to perform that on 300 millions lines, and just retrieving the result of each integration within like ~24h. Could someone tell me where I'm wrong or if it's even possible?</p>
",0,0,0,2025-12-19T16:46:39+00:00,2,114,True
79851509,17646242,,postgresql,Seeding database in GitLab CI,"<p>I'm currently working on the GitLab CI pipeline for my Node.js web app.</p>
<p>In my current GitLab CI setup I follow the same steps for a couple of CI jobs:</p>
<ol>
<li><p>Use the Node image</p>
</li>
<li><p>Spin up services: Postgres, Minio, web app</p>
</li>
<li><p>Run npm commands that create the data we need to have in the database</p>
</li>
</ol>
<p>Simplified, this is what the <code>gitlab-ci.yml</code> looks like:</p>
<pre class=""lang-yaml prettyprint-override""><code>validate-snippets:
  stage: test
  interruptible: true
  needs:
    - build docker image
    - npm_ci
  variables:
    FF_NETWORK_PER_BUILD: 'true'
    DATABASE_HOST: database
    DATABASE_PORT: 5432
    DATABASE_NAME: #
    DATABASE_USER: #
    DATABASE_PASSWORD: #
  services:
    - name: postgres:15@sha256:24d6c206bba8c0440bceb24a8d4bf642f60bf7aea94887051ea5761d29c22323
      alias: database
      variables:
        POSTGRES_DB: #
        POSTGRES_USER: #
        POSTGRES_PASSWORD: #

    - name: minio/minio:latest@sha256:14cea493d9a34af32f524e538b8346cf79f3321eff8e708c1e2960462bd8936e
      alias: minio
      entrypoint: ['sh', '-c', 'minio server /data']
      variables:
        MINIO_ROOT_USER: #
        MINIO_ROOT_PASSWORD: #

    - name: '$CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG'
      alias: appsemble
      variables:
        SECRET: secret
        AES_SECRET: secret
        DATABASE_HOST: #
        DATABASE_PORT: #
        DATABASE_NAME: #
        DATABASE_USER: #
        DATABASE_PASSWORD: #
  script:
    - npm run scripts create-user --
      --name $BOT_ACCOUNT_NAME
      --email $BOT_ACCOUNT_EMAIL
    - npm run scripts organization create testOrg
    - npm run scripts block publish 'blocks/*'
    - # // Run the command for this job
</code></pre>
<p>Is there a way to perform this seed job once to populate the database with data, create a copy of the database and then re-use that copy of the database in different jobs so they all start with their own copy of the database?</p>
<p>I unfortunately cannot make a simple <code>init.sql</code> file to seed the Postgres image, because the block publish command performs a lot of steps which I'm not sure can be replicated in the SQL file.</p>
",1,1,0,2025-12-19T23:53:02+00:00,1,52,False
79851628,3976630,,postgresql,Postgres replication corner case,"<p>I recently find that stack overflow <a href=""https://stackoverflow.com/questions/63700820/postgresql-synchronous-replication-consistency"">question</a> and what actually is interested for me is one of the answers which states:</p>
<blockquote>
<p>If the committing transaction is interrupted on the primary at the proper moment before <code>COMMIT</code> returns, <strong>the transaction will already be committed only on the primary</strong>. There is always a certain time window between the time the commit happens on the server and the time it is reported to the client, but that window increases considerably with streaming replication.</p>
</blockquote>
<p>1. What will happen then if the client see the data and the DB hard disk dies forever. The user can make some decisions using the information read (for example, it could be information that money is transferred from account A to account B)? How that scenario is handled in practice, because we have lost of data in that case? I think that this is super critical and important point and I guess there should be a way to handle this.</p>
<p>2. If the windows is increased with streaming replication, how it can be minimized? With logical replication?</p>
<p>3. How the statment above can be logically assumed from the official documentation. I believe it is true, but interested in how to find it.</p>
",0,0,0,2025-12-20T07:44:35+00:00,6,49,True
79851643,32081003,,postgresql,What are safe patterns for Service A to consume data from Service B without direct DB access?,"<p>I’m looking for practical ways to let <strong>Service A</strong> get the data it needs from <strong>Service B</strong> <strong>without</strong> querying Service B’s database directly. I’m not very experienced with distributed systems yet, so I want to gather <strong>implementation patterns</strong> and <strong>do’s/don’ts</strong> that are widely used in production.</p>
<p>Context:</p>
<ul>
<li><p>Today, Service A reads from Service B’s <strong>PostgreSQL</strong> using read-only credentials.</p>
</li>
<li><p>We want to <strong>avoid coupling</strong> to Service B’s internal schema and stop direct SQL access.</p>
</li>
<li><p><strong>Near real-time</strong> is preferred (seconds are okay); <strong>eventual consistency</strong> is acceptable.</p>
</li>
<li><p>Security/compliance: only <strong>contracted data</strong> should leave Service B (no internal table leaks).</p>
</li>
<li><p>We can add infra if needed (e.g., <strong>Kafka/Debezium</strong>), but solutions <strong>without Kafka</strong> are also welcome.</p>
</li>
</ul>
",0,0,0,2025-12-20T08:26:01+00:00,2,64,True
79851931,276363,,postgresql,Incremental/partial deserialization in Python for JSON?,"<p>I have a problem in which a program needs to:</p>
<ol>
<li>Read in a large (4K+) JSON document;</li>
<li>Modify one object deeply nested in it, often just one field of one object.</li>
<li>Write it back out.</li>
</ol>
<p>Even with something like `orjson`, that's a ton of overhead for a very small change. Is there something that could make this faster, that doesn't have to deserialize the entire JSON structure?</p>
<p>(The underlying storage engine is PostgreSQL, but I am up for considering other possible engines if it will radically improve performance.)</p>
<p>Doing this at the PostgreSQL level only partially solves the problem, as PostgreSQL doesn't currently have a way of doing in-place modifications to a portion of a JSONB field: it deserializes the whole thing, makes the change, and serializes it again.</p>
<p>The underlying data format isn't subject to change, and given that it has a wild variety of data in it, with deeply nested objects, it isn't practical to map it onto relational data instead.</p>
",-3,0,3,2025-12-20T18:59:24+00:00,0,93,False
79851940,16475190,,postgresql,Can&#39;t connect neondb postgresql database with my nestjs backend using Prisma(V 7.1.0),"<p>so let me clear some things first<br />
npx prisma migrate dev is working perfectly but when evey I am trying to hit the backend I am getting error like this</p>
<pre><code>
prisma:error undefined
[Nest] 17505  - 12/20/2025, 7:04:46 PM   ERROR [ExceptionsHandler] ErrorEvent {
  type: 'error',
  defaultPrevented: false,
  cancelable: false,
  timeStamp: 13278.977752
}
</code></pre>
<p>this is the prisma.service.ts file</p>
<pre class=""lang-js prettyprint-override""><code>
import { Injectable } from '@nestjs/common';
import { PrismaClient } from './generated/prisma/client.js';
import { PrismaPg } from '@prisma/adapter-pg';
import { PrismaNeon } from '@prisma/adapter-neon';
import { Pool, neonConfig } from '@neondatabase/serverless';
import ws from 'ws';
u/Injectable()
export class PrismaService extends PrismaClient {
  constructor() {
    //for neon postgres db
    // neonConfig.webSocketConstructor = ws;
    const adapter = new PrismaNeon({
      connectionString: process.env.DATABASE_URL as string,
    });
    //for local postgres db
    // const adapter = new PrismaPg({
    //   connectionString: process.env.DATABASE_URL as string,
    // });
    super({ adapter, log: ['query', 'error', 'warn'] });
  }
}
</code></pre>
<pre><code>and this is the prisma.config.ts

import &quot;dotenv/config&quot;;
import { defineConfig, env } from &quot;prisma/config&quot;;

export default defineConfig({
  schema: &quot;prisma/schema.prisma&quot;,
  migrations: {
    path: &quot;prisma/migrations&quot;,
  },
  datasource: {
    url: env(&quot;DATABASE_URL&quot;),
  },
});
</code></pre>
<pre><code>btw one more thing to mention , while development I was using a docker runned postgresql db and the application was working perfectly then .
help please
</code></pre>
",-2,0,2,2025-12-20T19:27:32+00:00,0,57,False
79851964,11894121,,postgresql,Pg_dumpall error &#39;FATAL: role &quot;postgres&quot; does not exist&#39;,"<p>I need some help here. We have an application (odoo) with a postgres database. This all runs in containers on an AWS EC2 instance. Last week we upgraded odoo, and with that we also upgraded postgres. We used to run postgres:13, and now we have postgres:15. Since the upgrade, our backup script does not work anymore.</p>
<p>The backup gets started from the instance like this:</p>
<pre><code>docker exec --user postgres risktool_db /var/lib/scripts/backup-db.sh
</code></pre>
<p>Then, inside the container, it basically starts this command:</p>
<pre><code>pg_dumpall --verbose 2&gt;${LOGFILE} | gzip -c | openssl smime -encrypt \
    -aes256 -binary -outform DEM \
    -out ${BACKUPFILE} &quot;${BACKUPKEY}&quot;
</code></pre>
<p>This always used to work. However, since upgrading the container for postgres we get the following error:</p>
<pre><code>pg_dumpall: error: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: FATAL:  role &quot;postgres&quot; does not exist
</code></pre>
<p>I already rebooted the instance, but that didn't work. I also checked for double postgres processed, but that seems to be all fine. Some googling then told me that I could check the role, by using <code>\du</code> and indeed the role does not exist. I also found the command to recreate the role, but I'm unsure about a lot of the options:</p>
<pre><code>CREATE ROLE postgres LOGIN PASSWORD 'your_password';
GRANT CONNECT ON DATABASE dbname TO postgres;
GRANT USAGE ON SCHEMA public TO postgres;
ALTER DATABASE dbname OWNER TO postgres;
</code></pre>
<p>We have about 150 databases, should I repeat the 2nd and 4th command for all these databases? Are these commands even correct?</p>
<p>Another manual mentioned that maybe doing this should be enough:</p>
<pre><code>CREATE ROLE postgres WITH SUPERUSER LOGIN PASSWORD 'your_password';
</code></pre>
<p>As clear by now, I'm not really a postgres admin, and have received this environment from an old friend who is no longer with us, so I can't really reach out. Any advice would be appreciated.</p>
",1,1,0,2025-12-20T20:25:20+00:00,1,43,True
79852004,6777695,,postgresql,Custom Security Provider works in fat jar but not in GraalVM native image while extracting PostgreSQL server certificates,"<p>I am trying to extract the public certificate of any PostgreSQL database. I got it working with plain java as a library and as a standalone fat jar, but not in a native-image build with GraalVM. I posted a bug report at GraalVM GitHub page <a href=""https://github.com/oracle/graal/issues/12703"" rel=""nofollow noreferrer"">here</a> but I have the feeling it is not a bug. I have the feeling I didn't configured graalvm correctly while using a custom security provider however, I am not sure.</p>
<p>Next to that it sometimes takes years before bug reports can be resolve at their page... Using custom security provider is not something exceptional as some use Bouncy Castle and it has proven to work with GraalVM native image so that is why I have the feeling that I might missed a configuration, however my knowledge on that area is quite low.</p>
<p>I spin up a PostgreSQL database with SSL enabled with this command:</p>
<pre class=""lang-bash prettyprint-override""><code>docker run --rm -e POSTGRES_PASSWORD=password -p 5432:5432 postgres:12 -c ssl=on -c ssl_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem -c ssl_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
</code></pre>
<p>I am using postgresql jdbc driver from here</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
    &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
    &lt;version&gt;42.7.8&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>I use my own library which provides the utility to capture certificates during the SSL Handshake. It can be found here: <a href=""https://github.com/Hakky54/ayza"" rel=""nofollow noreferrer"">GitHub - Ayza</a> And the Maven declaration:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;io.github.hakky54&lt;/groupId&gt;
    &lt;artifactId&gt;ayza&lt;/artifactId&gt;
    &lt;version&gt;10.0.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>And I am using the following plugins to build the native image with Maven:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
    &lt;version&gt;3.6.1&lt;/version&gt;
    &lt;configuration&gt;
        &lt;finalName&gt;crip&lt;/finalName&gt;
        &lt;transformers&gt;
            &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;
                &lt;mainClass&gt;${application-main-class}&lt;/mainClass&gt;
            &lt;/transformer&gt;
        &lt;/transformers&gt;
    &lt;/configuration&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;shade&lt;/goal&gt;
            &lt;/goals&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
&lt;plugin&gt;
    &lt;groupId&gt;org.graalvm.nativeimage&lt;/groupId&gt;
    &lt;artifactId&gt;native-image-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;21.2.0&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;goals&gt;
                &lt;goal&gt;native-image&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;phase&gt;package&lt;/phase&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
    &lt;configuration&gt;
        &lt;skip&gt;false&lt;/skip&gt;
        &lt;buildArgs&gt;
            --no-fallback
            -H:EnableURLProtocols=https
            -H:EnableURLProtocols=http
            -H:Name=crip
            -march=compatibility
            --future-defaults=all
            -H:AdditionalSecurityProviders=nl.altindag.ssl.provider.FenixProvider
            -H:AdditionalSecurityServiceTypes=nl.altindag.ssl.provider.FenixProvider
        &lt;/buildArgs&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>
<p>A small reproducable code snippet would be something like this:</p>
<pre class=""lang-java prettyprint-override""><code>import nl.altindag.ssl.SSLFactory;
import nl.altindag.ssl.util.ProviderUtils;
import nl.altindag.ssl.util.TrustManagerUtils;

import javax.net.ssl.X509ExtendedTrustManager;
import java.security.cert.X509Certificate;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;
import java.util.List;
import java.util.concurrent.CopyOnWriteArrayList;

public class App {

    public static void main(String[] args) {
        List&lt;X509Certificate&gt; capturedCertificates = new CopyOnWriteArrayList&lt;&gt;();
        X509ExtendedTrustManager trustManager = TrustManagerUtils.createCertificateCapturingTrustManager(capturedCertificates);
        SSLFactory sslFactory = SSLFactory.builder()
                .withTrustMaterial(trustManager)
                .build();

        ProviderUtils.configure(sslFactory);

        try (Connection conn = DriverManager.getConnection(&quot;jdbc:postgresql://localhost:5432/&quot;)) {
            // calling getConnection to trigger the SSL handshake
        } catch (SQLException ignored) {
            
        } finally {
            ProviderUtils.remove();
        }

        System.out.println(&quot;Amount of captured certificates: &quot; + capturedCertificates.size());
        System.out.println(&quot;Captured certificates: &quot;);
        capturedCertificates.forEach(System.out::println);
    }

}
</code></pre>
<p>The output of this main method is:</p>
<pre><code>Amount of captured certificates: 1
Captured certificates: 
[
[
  Version: V3
  Subject: CN=localhost
  Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11

  Key:  Sun RSA public key, 2048 bits
  params: null
  modulus: 31211996126076596147999338027719495623709022913370591887632988720610004032097760016008345790156702342195009365368156959125712771875783177237122222689823196867653884881232812806555219009894942490288929180418493391254446706821512257672335346037892662566871554703869495753177918085413564534189604133485556381243824728959475350934112799133217319962706911642363362829383506782715080237659432175302233892295525841514632241532146162042423493679178952709219542178492548540252419613055282285791992580920922147065723236149506037158805915815773798115281679259390591323530600590552570955839294244330974808478242461235603855182171
  public exponent: 65537
  Validity: [From: Tue Jan 14 03:32:28 CET 2025,
               To: Fri Jan 12 03:32:28 CET 2035]
  Issuer: CN=localhost
  SerialNumber: 26:97:cd:84:a8:93:e2:5d:d3:2c:a0:ea:40:8d:7c:93:bf:06:e4:1d

Certificate Extensions: 3
[1]: ObjectId: 2.5.29.19 Criticality=false
BasicConstraints:[
  CA:false
  PathLen: undefined
]

[2]: ObjectId: 2.5.29.17 Criticality=false
SubjectAlternativeName [
  DNSName: localhost
]

[3]: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: ED 8A 72 6D B7 87 AB 26   5E 6C 75 33 5B C9 BE E8  ..rm...&amp;^lu3[...
0010: 04 6E 1A 06                                        .n..
]
]

]
  Algorithm: [SHA256withRSA]
  Signature:
0000: 00 EE 2E CB 87 F7 20 FD   B6 36 AE E1 7B 3F AA 8F  ...... ..6...?..
0010: 44 38 42 94 BB 50 77 BE   69 21 CA 2B 4A 2F 90 1B  D8B..Pw.i!.+J/..
0020: 93 B0 3D B7 FA FB DB 56   40 BA F6 20 52 78 FC 0F  ..=....V@.. Rx..
0030: EA DE F1 66 13 6F 91 30   B9 48 6A B8 2A 32 32 FE  ...f.o.0.Hj.*22.
0040: 79 DF C8 DD B2 6D 83 C7   D7 56 04 5D 0F 4B 6B 98  y....m...V.].Kk.
0050: 73 AE C3 5C A5 3F 52 3C   A3 F1 6E CF 6D AF 28 E4  s..\.?R&lt;..n.m.(.
0060: 11 79 97 0D 69 02 D5 77   FF CA 9F B0 F7 ED D4 3F  .y..i..w.......?
0070: 17 ED 65 A4 9E CE 2D 42   C5 37 F5 52 98 D7 D9 C2  ..e...-B.7.R....
0080: 9B E5 91 54 A1 64 4C BA   17 BD 7C 14 B8 F2 51 51  ...T.dL.......QQ
0090: 0D 42 CA 2D 19 82 59 5A   AF BB 8E B4 AA 9C FB 37  .B.-..YZ.......7
00A0: 64 DC F4 78 EA 17 13 3D   07 88 45 2E FB 02 96 68  d..x...=..E....h
00B0: 9B F1 25 AF 6E 85 02 DB   77 5A CF 40 4E 70 5B 62  ..%.n...wZ.@Np[b
00C0: C1 83 15 3F 3E CE BC 32   BB 45 4F E3 AC 44 8E A5  ...?&gt;..2.EO..D..
00D0: 47 02 D6 D4 86 34 A4 19   04 3E B2 7B 8F 72 3F 62  G....4...&gt;...r?b
00E0: 19 02 AF F8 C6 9B 96 14   D1 36 AA D7 74 39 7F C3  .........6..t9..
00F0: AB 49 02 94 CE 96 7C B1   F2 D5 1F 5B A2 73 DE B9  .I.........[.s..

]
</code></pre>
<p>I will try to give some context. I am using postgres driver manager to interact with the actual database. When establishing the connection it will create an sslcontext instance under the covers by the postgres library itself, see <a href=""https://github.com/pgjdbc/pgjdbc/blob/bdd930bf458e3d490d93edc4dc24a67e3a2d5964/pgjdbc/src/main/java/org/postgresql/ssl/LibPQFactory.java#L121"" rel=""nofollow noreferrer"">here</a>. I am intercepting this by adding a custom security provider <a href=""https://github.com/Hakky54/certificate-ripper/pull/110/files#diff-8fe706de15f335f6dc0f7298b1d00697c51e237e9a4e919aacba489893bf469eR36"" rel=""nofollow noreferrer"">here</a>. This code snippet creates a custom security provider and inserts it in the first position. Under the cover the following code statements will be executed <a href=""https://github.com/Hakky54/ayza/blob/master/ayza/src/main/java/nl/altindag/ssl/util/ProviderUtils.java#L32-L40"" rel=""nofollow noreferrer"">first</a> and <a href=""https://github.com/Hakky54/ayza/blob/master/ayza/src/main/java/nl/altindag/ssl/provider/FenixProvider.java"" rel=""nofollow noreferrer"">second</a></p>
<p>With this logic I can easily replace an sslcontext of a different library with a custom one. In this case I provide my own one which is able to capture server certificates. This works when running in my IDE and with a fat-jar. However it does not work when running it with a native executable create with native image. Although I posted a small working app above, this issue is related to my changes to the project <a href=""https://github.com/Hakky54/certificate-ripper"" rel=""nofollow noreferrer"">Certificate Ripper</a>, the actual code changes can be found <a href=""https://github.com/Hakky54/certificate-ripper/pull/110/files"" rel=""nofollow noreferrer"">here</a></p>
<p>If you want to try it with the changes from the repository, you can run the following steps:</p>
<ol>
<li><code>git clone git@github.com:Hakky54/certificate-ripper.git</code></li>
<li><code>cd certificate-ripper</code></li>
<li><code>git switch feature/support-for-postgres-db</code></li>
<li><code>mvn clean install -Pfat-jar</code></li>
<li><code>docker run -d --rm -e POSTGRES_PASSWORD=password -p 5432:5432 postgres:12 -c ssl=on -c ssl_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem -c ssl_key_file=/etc/ssl/private/ssl-cert-snakeoil.key</code></li>
<li><code>java -jar target/crip.jar print -u=postgresql://localhost:5432/</code> --&gt; will print the certificate</li>
<li><code>mvn clean install -Pnative-image</code></li>
<li><code>./target/crip print -u=postgresql://localhost:5432/</code> --&gt; does not print the certificate</li>
</ol>
<p>So it is actually possible to capture the certificates of postgresql with these snippets. It works in plain java, but not anymore when compiled to a native image. I checked whether the custom Security Provider is present and it is. I am getting the following output:</p>
<pre><code>Fenix version 1.0, CertificateRipper version 1.0, SUN version 25, SunRsaSign version 25, SunEC version 25, SunJSSE version 25, SunJCE version 25, SunSASL version 25, JdkLDAP version 25, JdkSASL version 25, Apple version 25
</code></pre>
<p>As you can see it is also at the first position, so it should just work. I just have the feeling that at runtime the custom security provider is not used because it is not at the first place or not available maybe. As I am not sure whether I followed the guidelines correctly as explained here: <a href=""https://www.graalvm.org/latest/reference-manual/native-image/dynamic-features/JCASecurityServices/#provider-registration"" rel=""nofollow noreferrer"">https://www.graalvm.org/latest/reference-manual/native-image/dynamic-features/JCASecurityServices/#provider-registration</a></p>
<p><strong>Update 1 - alternative without jdbc driver</strong></p>
<p>As @Robert suggested in the comment section I also tried without the Postgresql jdbc driver and just plain code, however then I fail to connect to the server. Below is the code snippet which I used:</p>
<pre class=""lang-java prettyprint-override""><code>import nl.altindag.ssl.SSLFactory;
import nl.altindag.ssl.util.TrustManagerUtils;

import javax.net.ssl.SSLSocket;
import javax.net.ssl.X509ExtendedTrustManager;
import java.security.cert.X509Certificate;
import java.util.List;
import java.util.concurrent.CopyOnWriteArrayList;

public class App {

    public static void main(String[] args) {
        List&lt;X509Certificate&gt; capturedCertificates = new CopyOnWriteArrayList&lt;&gt;();
        X509ExtendedTrustManager trustManager = TrustManagerUtils.createCertificateCapturingTrustManager(capturedCertificates);
        SSLFactory sslFactory = SSLFactory.builder()
                .withTrustMaterial(trustManager)
                .build();

        try {
            SSLSocket socket = (SSLSocket) sslFactory.getSslSocketFactory().createSocket(&quot;localhost&quot;, 5432);
            socket.setSoTimeout(10000);
            socket.setUseClientMode(true);
            socket.startHandshake();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

}
</code></pre>
<p>And the following logs are being generated with the stacktrace:</p>
<pre><code>javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.056 CET|SSLCipher.java:423|jdk.tls.keyLimits:  entry = AES/GCM/NoPadding KeyUpdate 2^37. AES/GCM/NOPADDING:KEYUPDATE = 137438953472
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.066 CET|SSLCipher.java:423|jdk.tls.keyLimits:  entry =  ChaCha20-Poly1305 KeyUpdate 2^37. CHACHA20-POLY1305:KEYUPDATE = 137438953472
javax.net.ssl|WARNING|30|main|2025-12-22 24:46:43.159 CET|ServerNameExtension.java:265|Unable to indicate server name
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.159 CET|SSLExtensions.java:272|Ignore, context unavailable extension: server_name
javax.net.ssl|INFO|30|main|2025-12-22 24:46:43.162 CET|AlpnExtension.java:174|No available application protocols
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.162 CET|SSLExtensions.java:272|Ignore, context unavailable extension: application_layer_protocol_negotiation
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.162 CET|SessionTicketExtension.java:350|Stateless resumption supported
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.166 CET|SSLExtensions.java:272|Ignore, context unavailable extension: cookie
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.235 CET|SSLExtensions.java:272|Ignore, context unavailable extension: renegotiation_info
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.235 CET|PreSharedKeyExtension.java:659|No session to resume.
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.235 CET|SSLExtensions.java:272|Ignore, context unavailable extension: pre_shared_key
javax.net.ssl|DEBUG|30|main|2025-12-22 24:46:43.246 CET|ClientHello.java:638|Produced ClientHello handshake message (
&quot;ClientHello&quot;: {
  &quot;client version&quot;      : &quot;TLSv1.2&quot;,
  &quot;random&quot;              : &quot;2B9ADD1F533E9AFF111BBACDCF738A5FA7CA694973016903E7DE8BB38D2F2A6B&quot;,
  &quot;session id&quot;          : &quot;3CBEEDA9D55BFC0FA25D66DF92C37AF4C9E91B5F20607FFEFD7EB86115CDBE11&quot;,
  &quot;cipher suites&quot;       : &quot;[TLS_AES_256_GCM_SHA384(0x1302), TLS_AES_128_GCM_SHA256(0x1301), TLS_CHACHA20_POLY1305_SHA256(0x1303), TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384(0xC02C), TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256(0xC02B), TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256(0xCCA9), TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384(0xC030), TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256(0xCCA8), TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256(0xC02F), TLS_DHE_RSA_WITH_AES_256_GCM_SHA384(0x009F), TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256(0xCCAA), TLS_DHE_DSS_WITH_AES_256_GCM_SHA384(0x00A3), TLS_DHE_RSA_WITH_AES_128_GCM_SHA256(0x009E), TLS_DHE_DSS_WITH_AES_128_GCM_SHA256(0x00A2), TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384(0xC024), TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384(0xC028), TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256(0xC023), TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256(0xC027), TLS_DHE_RSA_WITH_AES_256_CBC_SHA256(0x006B), TLS_DHE_DSS_WITH_AES_256_CBC_SHA256(0x006A), TLS_DHE_RSA_WITH_AES_128_CBC_SHA256(0x0067), TLS_DHE_DSS_WITH_AES_128_CBC_SHA256(0x0040), TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA(0xC00A), TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA(0xC014), TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA(0xC009), TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA(0xC013), TLS_DHE_RSA_WITH_AES_256_CBC_SHA(0x0039), TLS_DHE_DSS_WITH_AES_256_CBC_SHA(0x0038), TLS_DHE_RSA_WITH_AES_128_CBC_SHA(0x0033), TLS_DHE_DSS_WITH_AES_128_CBC_SHA(0x0032), TLS_EMPTY_RENEGOTIATION_INFO_SCSV(0x00FF)]&quot;,
  &quot;compression methods&quot; : &quot;00&quot;,
  &quot;extensions&quot;          : [
    &quot;status_request (5)&quot;: {
      &quot;certificate status type&quot;: ocsp
      &quot;OCSP status request&quot;: {
        &quot;responder_id&quot;: &lt;empty&gt;
        &quot;request extensions&quot;: {
          &lt;empty&gt;
        }
      }
    },
    &quot;supported_groups (10)&quot;: {
      &quot;named groups&quot;: [x25519, secp256r1, secp384r1, secp521r1, x448, ffdhe2048, ffdhe3072, ffdhe4096, ffdhe6144, ffdhe8192]
    },
    &quot;ec_point_formats (11)&quot;: {
      &quot;formats&quot;: [uncompressed]
    },
    &quot;status_request_v2 (17)&quot;: {
      &quot;cert status request&quot;: {
        &quot;certificate status type&quot;: ocsp_multi
        &quot;OCSP status request&quot;: {
          &quot;responder_id&quot;: &lt;empty&gt;
          &quot;request extensions&quot;: {
            &lt;empty&gt;
          }
        }
      }
    },
    &quot;extended_master_secret (23)&quot;: {
      &lt;empty&gt;
    },
    &quot;session_ticket (35)&quot;: {
      &lt;empty&gt;
    },
    &quot;signature_algorithms (13)&quot;: {
      &quot;signature schemes&quot;: [ecdsa_secp256r1_sha256, ecdsa_secp384r1_sha384, ecdsa_secp521r1_sha512, ed25519, ed448, rsa_pss_rsae_sha256, rsa_pss_rsae_sha384, rsa_pss_rsae_sha512, rsa_pss_pss_sha256, rsa_pss_pss_sha384, rsa_pss_pss_sha512, rsa_pkcs1_sha256, rsa_pkcs1_sha384, rsa_pkcs1_sha512, dsa_sha256, ecdsa_sha224, rsa_sha224, dsa_sha224, ecdsa_sha1, rsa_pkcs1_sha1, dsa_sha1]
    },
    &quot;supported_versions (43)&quot;: {
      &quot;versions&quot;: [TLSv1.3, TLSv1.2]
    },
    &quot;psk_key_exchange_modes (45)&quot;: {
      &quot;ke_modes&quot;: [psk_dhe_ke]
    },
    &quot;signature_algorithms_cert (50)&quot;: {
      &quot;signature schemes&quot;: [ecdsa_secp256r1_sha256, ecdsa_secp384r1_sha384, ecdsa_secp521r1_sha512, ed25519, ed448, rsa_pss_rsae_sha256, rsa_pss_rsae_sha384, rsa_pss_rsae_sha512, rsa_pss_pss_sha256, rsa_pss_pss_sha384, rsa_pss_pss_sha512, rsa_pkcs1_sha256, rsa_pkcs1_sha384, rsa_pkcs1_sha512, dsa_sha256, ecdsa_sha224, rsa_sha224, dsa_sha224, ecdsa_sha1, rsa_pkcs1_sha1, dsa_sha1]
    },
    &quot;key_share (51)&quot;: {
      &quot;client_shares&quot;: [  
        {
          &quot;named group&quot;: x25519
          &quot;key_exchange&quot;: {
            0000: 9B 70 95 2B 4B 25 76 42   52 BD 2E 09 C8 DA A8 08  .p.+K%vBR.......
            0010: A2 39 04 4C 40 F5 EC 86   33 49 65 71 BD 0F 9F 6E  .9.L@...3Ieq...n
          }
        },
        {
          &quot;named group&quot;: secp256r1
          &quot;key_exchange&quot;: {
            0000: 04 57 5B 93 97 52 BF 01   91 B1 4B E1 F0 E9 BB B2  .W[..R....K.....
            0010: AB 6F 44 0E 2F BA 39 03   E1 3F 1B 06 E9 10 94 D3  .oD./.9..?......
            0020: AB 02 2F 05 3E A2 A5 FC   4B 81 10 E5 21 76 17 7F  ../.&gt;...K...!v..
            0030: D3 7D 00 2F C4 8C DA 2D   10 C5 4B 83 D2 79 D9 6A  .../...-..K..y.j
            0040: BB 
          }
        },
      ]
    }
  ]
}
)
javax.net.ssl|ERROR|30|main|2025-12-22 24:46:43.251 CET|TransportContext.java:368|Fatal (HANDSHAKE_FAILURE): Couldn't kickstart handshaking (
&quot;throwable&quot; : {
  javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
    at java.base/sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1716)
    at java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514)
    at java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1421)
    at java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:455)
    at java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:426)
    at nl.altindag.certificate.ripper@2.6.2-SNAPSHOT/nl.altindag.crip.App2.main(App2.java:25)
  Caused by: java.io.EOFException: SSL peer shut down incorrectly
    at java.base/sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:494)
    at java.base/sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:483)
    at java.base/sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160)
    at java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:111)
    at java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506)
    ... 4 more}

)
</code></pre>
",3,3,0,2025-12-20T22:02:35+00:00,1,135,True
79852398,735307,"Yerevan, Armenia",postgresql,How do I version translations while keeping relationships?,"<p>Some part of a post has translations:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE &quot;posts&quot; (
    &quot;id&quot; int4 NOT NULL,
    &quot;title&quot; varchar,
    PRIMARY KEY (&quot;id&quot;)
);

CREATE TABLE &quot;post_translations&quot; (
    &quot;post_id&quot; int4 NOT NULL,
    &quot;language_code&quot; char(2) NOT NULL,
    &quot;content&quot; text DEFAULT NULL,
    CONSTRAINT &quot;post_translations_post_id_fkey&quot; FOREIGN KEY (&quot;post_id&quot;) REFERENCES &quot;posts&quot;(&quot;id&quot;),
    PRIMARY KEY (&quot;post_id&quot;,&quot;language_code&quot;)
);
</code></pre>
<p>Posts with translations (<a href=""https://sqlfiddle.com/postgresql/online-compiler?id=4474db98-5528-4c19-b08e-9257f4ca8855"" rel=""nofollow noreferrer"">SQL Fiddle</a>):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM &quot;posts&quot;
INNER JOIN &quot;post_translations&quot; ON &quot;post_translations&quot;.&quot;post_id&quot; = &quot;posts&quot;.&quot;id&quot;
</code></pre>
<p>I need to also keep a draft version of a post with draft translations. The user can delete the post or a specific translation. They will either commit the draft version and make it live (committing the post with its translations) or discard the draft. Each post can have at most one draft.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE &quot;posts&quot; (
    &quot;id&quot; int4 NOT NULL,
    &quot;title&quot; varchar,
    &quot;draft_id&quot; int4 DEFAULT NULL,
    &quot;deleted_at&quot; TIMESTAMP WITHOUT TIME ZONE DEFAULT NULL,
    CONSTRAINT &quot;draft_id_unique&quot; UNIQUE (&quot;draft_id&quot;),
    CONSTRAINT &quot;post_draft_id&quot; FOREIGN KEY (&quot;draft_id&quot;) REFERENCES &quot;posts&quot;(&quot;id&quot;),
    PRIMARY KEY (&quot;id&quot;)
);
</code></pre>
<p><code>draft_id</code> references the live version while indicating this row is a draft. When editing a post I insert a draft version if it's not already draft; otherwise I update the draft. On post removal I create a new draft, indicating it is deleted. Different parts of the application are either working with only the live version of the posts or the most recent version (either live or draft).</p>
<p>To query the most recent version of posts (<a href=""https://sqlfiddle.com/postgresql/online-compiler?id=86a60365-7190-42ae-bdce-526029a98c28"" rel=""nofollow noreferrer"">SQL Fiddle</a>):</p>
<pre class=""lang-sql prettyprint-override""><code>WITH
  &quot;drafts&quot; (&quot;id&quot;, &quot;title&quot;, &quot;draft_id&quot;) AS (
    SELECT
      *
    FROM
      &quot;posts&quot;
    WHERE
      &quot;draft_id&quot; IS NOT NULL
  )
SELECT
  *
FROM
  &quot;drafts&quot;
UNION ALL
SELECT *
FROM &quot;posts&quot;
WHERE &quot;draft_id&quot; IS NULL AND &quot;id&quot; NOT IN (SELECT &quot;draft_id&quot; FROM &quot;drafts&quot;)
</code></pre>
<p>How do I version <code>post_translations</code> while keeping the relationships and ability to query posts with all translations (either recent or live)?</p>
",1,1,0,2025-12-21T18:39:15+00:00,1,65,False
79852627,32088018,,postgresql,How to solve column reference &quot;description&quot; is ambiguous error,"<p>This query is called <code>get_all_with_columns()</code>. I am trying to create a procdure which returns 2 columns in PostgreSQL, but I am facing an error.</p>
<p>Here is the procedure with the query:</p>
<pre><code>CREATE OR REPLACE FUNCTION get_all_with_columns()
RETURNS TABLE (category_name character varying(30),
                description character varying(255))
AS $$
BEGIN
    RETURN QUERY
    SELECT category_name, description FROM categories;
END;
$$ LANGUAGE plpgsql;
</code></pre>
<p>When I call the procedure:</p>
<pre><code>SELECT get_all_with_columns();
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>ERROR: column reference &quot;description&quot; is ambiguous It could refer to either a PL/pgSQL variable or a table column.<br />
SQL state: 42702 Detail: It could refer to either a PL/pgSQL variable or a table column.<br />
Context: PL/pgSQL function get_all_with_columns() line 3 at RETURN QUERY</p>
</blockquote>
<p>Can anyone help me solve this?</p>
",-3,0,3,2025-12-22T05:26:36+00:00,1,39,True
79853492,23650057,,postgresql,Logical replication not resuming after upgrading subscriber from PostgreSQL 13 to 18,"<p>My publisher is running PostgreSQL 17 and the subscriber is on PostgreSQL 13.</p>
<p>What I am trying to do</p>
<p>My goal is to upgrade PostgreSQL from version 13 to 18 on the subscriber side.</p>
<p>Because our infrastructure is managed by Puppet, the PostgreSQL version is enforced cluster-wide, so I cannot directly provision a new node on PostgreSQL 18.</p>
<p>To keep a rollback option, I am doing the following:</p>
<p>Create a replica (slave) on PostgreSQL 13 for the existing subscriber.</p>
<p>Use this replica as a safety fallback.</p>
<p>Promote that replica to become the new master.</p>
<p>Disable the subscription.</p>
<p>Perform the PostgreSQL version upgrade from 13 to 18.</p>
<p>Re-enable the subscription to resume logical replication.</p>
<p>The problem</p>
<p>After re-enabling the subscription, replication does not resume.</p>
<p>My questions</p>
<p>Why does replication stop working after this process?</p>
<p>What could be the underlying reason for this behavior?</p>
<p>Is there a better or recommended approach to achieve a PostgreSQL 13 → 18 upgrade while keeping logical replication and rollback safety?</p>
<p>Any guidance or best practices would be appreciated.</p>
",-1,0,1,2025-12-23T09:08:44+00:00,0,25,False
79853607,16716563,,postgresql,postgres: High CPU usage with JSONB after adding composite functional indexes,"<h2>Context</h2>
<p>A bit of context. I have a service responsible for handling search queries from our users. The goal of the project is to build a unified storage system that can hold various entities from our logistics domain. At the moment, it only stores logistics orders.</p>
<p>The idea is that if, for instance, I add a new delivery method using drones, I wouldn’t need to add new columns like <strong><code>drone_model</code></strong> in the database schema — instead, that data would simply be stored as part of the unified order record in a JSONB field. This way, users could search across entities by these custom attributes.</p>
<p>To implement this idea, I decided to store entity data in JSONB.</p>
<p>As a result, I have a table with the following fields:</p>
<p><strong><code>id</code></strong>, <strong><code>entity_type</code></strong>, <strong><code>entity_id</code></strong>, <strong><code>created_at</code></strong> and <strong><code>entity_data (JSONB)</code></strong>.<br />
The main task is to retrieve <strong><code>entity_id</code></strong> values based on jsonpath queries.</p>
<h3>Implementation issues</h3>
<p>Initially, the table had only one GIN index:<br />
<strong><code>gin(entity_data jsonb_path_ops)</code></strong>.</p>
<p>During a dry-run under load, I noticed that some queries took over 20 seconds to complete — especially those involving date-based sorting, e.g.<br />
<strong><code>.date &gt;= &quot;...&quot; &amp;&amp; .date &lt;= &quot;....&quot;</code></strong>.</p>
<p>To improve performance, I added indexes on specific JSON fields:</p>
<pre><code>CREATE INDEX entity__btree__date__idx
ON entity USING btree ((entity_data-&gt;&gt;'date'))
WHERE entity_type = 'order' AND entity_data-&gt;&gt;'date' IS NOT NULL;
</code></pre>
<p>and like this:</p>
<pre><code>CREATE INDEX entity__btree__first_dispatch_loading_arrival_at__idx
ON entity USING btree (
(entity_data-&gt;'attributes'-&gt;'order_dispatch_route'-&gt;'dispatches'-&gt;0-&gt;'loading_point'-&gt;&gt;'arrival_at')
)
WHERE entity_type = 'order'
AND entity_data-&gt;'attributes'-&gt;'order_dispatch_route'-&gt;'dispatches' IS NOT NULL
AND jsonb_typeof(entity_data-&gt;'attributes'-&gt;'order_dispatch_route'-&gt;'dispatches') = 'array'
AND jsonb_array_length(entity_data-&gt;'attributes'-&gt;'order_dispatch_route'-&gt;'dispatches') &gt; 0
AND entity_data-&gt;'attributes'-&gt;'order_dispatch_route'-&gt;'dispatches'-&gt;0-&gt;'loading_point'-&gt;&gt;'arrival_at' IS NOT NULL;
</code></pre>
<p>To actually use these indexes, I extract the part of the incoming jsonpath condition that can be optimized (i.e. has an index) and append it to the final query via an AND clause.</p>
<p>After that, I ran another dry-run. The 20+ second queries were still there. Looking at the most CPU-hungry queries, I found that most were of this form:</p>
<pre><code>$ ? (!((@.status == &quot;CANCELED&quot; || @.status == &quot;COMPLETE&quot;)) &amp;&amp; (@.tariff_type == &quot;Shuttle&quot;) &amp;&amp; @.vehicle_plate_number like_regex &quot;.*E292.*&quot; &amp;&amp; (@.organization_id == 2226) &amp;&amp; @.namespace == &quot;some-namespace&quot;)
</code></pre>
<p>Execution time: 35 seconds</p>
<p>After that I tried adding a composite index to optimize such queries:</p>
<pre><code>CREATE INDEX entity__btree__organization_id__namespace__tariff_type__idx
ON entity
USING btree (
((entity_data-&gt;&gt;'organization_id')::bigint),
(entity_data-&gt;&gt;'namespace'),
(entity_data-&gt;&gt;'tariff_type')
)
WHERE entity_type = 'order'
AND entity_data-&gt;&gt;'organization_id' IS NOT NULL
AND entity_data-&gt;&gt;'namespace' IS NOT NULL
AND entity_data-&gt;&gt;'tariff_type' IS NOT NULL;
</code></pre>
<p>I hoped this would help with the heaviest queries.
<strong>However, after running another dry-run, CPU usage spiked to 1000%+, and my service throughput (on read) dropped from 50 RPS down to 20 RPS.</strong></p>
<p>I had to urgently shut down the service because the database was hitting CPU saturation.</p>
<p>Even when repeated the test with zero write load, the issue persisted.</p>
<p>So now the main question — <strong>why did adding this particular index cause such severe CPU overhead?</strong> Interestingly, the queries I targeted with this index did get faster — they now complete in 1–3 seconds instead of 20+.</p>
<p>Overall, I’d love to hear your opinion on my plan for implementing unified search via JSONB. I understand there’s no single “magic index” to solve everything, but I want to get a clearer sense of which direction I should move in.</p>
<p><strong>And some more info:</strong><br />
Total rows: 3,287,327</p>
<p>Memory usage:<br />
<code>SELECT pg_size_pretty(pg_total_relation_size('entity'));</code> 9 GB</p>
<p>Just table size:<br />
<code>SELECT pg_size_pretty(pg_relation_size('entity')); </code>4 GB</p>
<p>And ~18% of JSONB records exceed 2KB (of total row count)</p>
<pre><code>explain (analyze, buffers)
SELECT entity_id
FROM entity
WHERE entity_type = 'order'
  AND ((entity_data-&gt;&gt;'organization_id')::bigint) = 2226
  AND (entity_data-&gt;&gt;'namespace') = 'some-orders'
  AND (entity_data-&gt;&gt;'tariff_type') = 'Shuttle'
  AND entity_data @? '$ ? (!((@.status == &quot;CANCELED&quot; || @.status == &quot;COMPLETE&quot;)))'
ORDER BY LENGTH(entity_id) DESC, entity_id DESC
LIMIT 10 OFFSET 0;
</code></pre>
<pre><code>Limit  (cost=2.47..2.47 rows=1 width=12) (actual time=30039.434..30039.446 rows=10 loops=1)
  -&gt;  Sort  (cost=2.47..2.47 rows=1 width=12) (actual time=30039.432..30039.443 rows=10 loops=1)
&quot;        Sort Key: (length(entity_id)) DESC, entity_id DESC&quot;
        Sort Method: top-N heapsort  Memory: 25kB
        -&gt;  Index Scan using entity__btree__organization_id__namespace__tariff_type__idx on entity  (cost=0.43..2.46 rows=1 width=12) (actual time=1.090..29975.095 rows=51424 loops=1)
              Index Cond: ((((entity_data -&gt;&gt; 'organization_id'::text))::bigint = 2226) AND ((entity_data -&gt;&gt; 'namespace'::text) = 'some-orders'::text) AND ((entity_data -&gt;&gt; 'tariff_type'::text) = 'Shuttle'::text))
&quot;              Filter: (entity_data @? '$?(!(@.&quot;&quot;status&quot;&quot; == &quot;&quot;CANCELED&quot;&quot; || @.&quot;&quot;status&quot;&quot; == &quot;&quot;COMPLETE&quot;&quot;))'::jsonpath)&quot;
              Rows Removed by Filter: 19459
Planning Time: 2.627 ms
Execution Time: 30039.531 ms
</code></pre>
",-1,0,1,2025-12-23T11:25:56+00:00,0,44,False
79853766,8772319,,postgresql,How do you translate (+) joins when they appear across multiple tables?,"<p>I have the following query in a legacy Oracle database. (I've simplified it a bit; the real query has a lot of columns that are common across all joins which I've collapsed to one <code>COL_1</code>). I'm trying to translate it into a &quot;standard&quot; query in a new PostgreSQL database.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    *
FROM
    A,
    B,
    C,
    D
WHERE
    A.COL_1 = B.COL_1
    AND C.COL_1 = B.COL_1
    AND C.COL_2 = B.COL_2
    AND A.COL_1 = D.COL_1(+)  
    AND C.COL_1 = D.COL_1(+)  
    AND C.COL_2 = D.COL_2(+)  
    AND B.COL_1 = D.COL_1(+)  
    AND B.COL_2 = D.COL_2(+)
</code></pre>
<p>This is what I have:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    *
FROM
    A
    JOIN B ON
        A.COL_1 = B.COL_1
    JOIN C ON
        C.COL_1 = B.COL_1
        AND C.COL_2 = B.COL_2
    LEFT JOIN D ON
        A.COL_1 = D.COL_1
        AND C.COL_1 = D.COL_1
        AND C.COL_2 = D.COL_2
        AND B.COL_1 = D.COL_1
        AND B.COL_2 = D.COL_2
</code></pre>
<p>What I'm not sure about is whether I'm handling the <code>LEFT JOIN</code> right (the two queries are, as far as I can tell, returning the same data, but that could be dumb luck). What is the correct interpretation of the <code>(+)</code> syntax when it is matched across multiple tables?</p>
",2,2,0,2025-12-23T15:06:47+00:00,3,116,True
79853858,32090968,,postgresql,PostgreSQL Bitmap Index Extension Crash with Large Range Scans - TIDBitmap Corruption,"<h2><strong>PostgreSQL version and environment</strong></h2>
<ul>
<li><p>PostgreSQL version: 17.4</p>
</li>
<li><p>OS/Architecture: Ubuntu 22.04 x86_64</p>
</li>
<li><p>Compilation method: Source code</p>
</li>
<li><p>Extended Address: <a href=""https://github.com/xifeiya/bitmap.git"" rel=""nofollow noreferrer"">https://github.com/xifeiya/bitmap.git</a></p>
</li>
</ul>
<h2>Problem Description</h2>
<p>When I use my bitmap query for range queries using getbitmap, once the query range is too large and a large number of tids are returned, a series of tbm problems will be triggered. Sometimes an error occurs at the beginning of the tbm_begin_iterate function, and sometimes an error occurs in the tbm_free function. I have verified the correctness of the tid inserted into the tbm</p>
<h2>Reproduction steps</h2>
<p>You can clone my extension code to the contrib folder in psql for compilation and installation</p>
<p>Please execute test.sql to obtain the basic people table and index</p>
<p>This error might be the strangest one. My select* can output normally, but select count(*) reports an error. Logically speaking, the tids added for both should be the same.</p>
<pre class=""lang-sql prettyprint-override""><code>psql (17.4)
Type &quot;help&quot; for help.

test=# select * from people where age between 25 and 27;
NOTICE:  ScanKey[0]: att=1 flags=128 strategy=4 func=150 arg=25
NOTICE:  ScanKey[1]: att=1 flags=128 strategy=2 func=149 arg=27
NOTICE:  ItemPos block=1 offset=12
NOTICE:  ItemPos block=1 offset=2
NOTICE:  ItemPos block=1 offset=3
 id  |    name    | age | birth_date | gender | salary 
-----+------------+-----+------------+--------+--------
   1 | Person_1   |  26 | 1990-01-08 | 女     | 4001.0
   2 | Person_2   |  27 | 1990-01-15 | 男     | 4002.0
  11 | Person_11  |  25 | 1990-03-19 | 女     | 4011.0
  12 | Person_12  |  26 | 1990-03-26 | 男     | 4012.0
  13 | Person_13  |  27 | 1990-04-02 | 女     | 4013.0
  22 | Person_22  |  25 | 1990-06-04 | 男     | 4022.0
  23 | Person_23  |  26 | 1990-06-11 | 女     | 4023.0
  24 | Person_24  |  27 | 1990-06-18 | 男     | 4024.0
  33 | Person_33  |  25 | 1990-08-20 | 女     | 4033.0
  34 | Person_34  |  26 | 1990-08-27 | 男     | 4034.0
  35 | Person_35  |  27 | 1990-09-03 | 女     | 4035.0
......
 967 | Person_967 |  35 | 2008-07-14 | 女     | 4967.0
 968 | Person_968 |  25 | 2008-07-21 | 男     | 4968.0
(240 rows)
test=# select count(*) from people where age between 25 and 27;
NOTICE:  ScanKey[0]: att=1 flags=128 strategy=4 func=150 arg=25
NOTICE:  ScanKey[1]: att=1 flags=128 strategy=2 func=149 arg=27
NOTICE:  ItemPos block=1 offset=12
NOTICE:  ItemPos block=1 offset=2
NOTICE:  ItemPos block=1 offset=3
server closed the connection unexpectedly
        This probably means the server terminated abnormally
        before or while processing the request.
The connection to the server was lost. Attempting reset: Failed.
The connection to the server was lost. Attempting reset: Failed.
</code></pre>
<h2>Error stack</h2>
<pre><code>pagetable_destroy(pagetable_hash * tb) (\home\wangyu\postgres-dev\src\include\lib\simplehash.h:474)
tbm_free(TIDBitmap * tbm) (\home\wangyu\postgres-dev\src\backend\nodes\tidbitmap.c:325)
ExecEndBitmapHeapScan(BitmapHeapScanState * node) (\home\wangyu\postgres-dev\src\backend\executor\nodeBitmapHeapscan.c:648)
ExecEndPlan(EState * estate, PlanState * planstate) (\home\wangyu\postgres-dev\src\backend\executor\execMain.c:1485)
standard_ExecutorEnd(QueryDesc * queryDesc) (\home\wangyu\postgres-dev\src\backend\executor\execMain.c:495)
PortalCleanup(Portal portal) (\home\wangyu\postgres-dev\src\backend\commands\portalcmds.c:299)
PortalDrop(Portal portal, _Bool isTopCommit) (\home\wangyu\postgres-dev\src\backend\utils\mmgr\portalmem.c:502)
exec_simple_query(const char * query_string) (\home\wangyu\postgres-dev\src\backend\tcop\postgres.c:1288)
PostgresMain(const char * dbname, const char * username) (\home\wangyu\postgres-dev\src\backend\tcop\postgres.c:4767)
BackendMain(char * startup_data, size_t startup_data_len) (\home\wangyu\postgres-dev\src\backend\tcop\backend_startup.c:105)
postmaster_child_launch(BackendType child_type, BackendType child_type@entry, char * startup_data, char * startup_data@entry, size_t startup_data_len, size_t startup_data_len@entry, ClientSocket * client_sock, ClientSocket * client_sock@entry) (\home\wangyu\postgres-dev\src\backend\postmaster\launch_backend.c:277)
BackendStartup(ClientSocket * client_sock) (\home\wangyu\postgres-dev\src\backend\postmaster\postmaster.c:3594)
ServerLoop() (\home\wangyu\postgres-dev\src\backend\postmaster\postmaster.c:1676)
PostmasterMain(int argc, int argc@entry, char ** argv, char ** argv@entry) (\home\wangyu\postgres-dev\src\backend\postmaster\postmaster.c:1374)
main(int argc, char ** argv) (\home\wangyu\postgres-dev\src\backend\main\main.c:199)
</code></pre>
",1,1,0,2025-12-23T17:16:24+00:00,1,58,True
79854146,5371505,,postgresql,Error 20 at 0 depth lookup: unable to get local issuer certificate for self signed PostgreSQL when run from outside docker,"<p>I am getting an error when verifying certificates generated using openssl outside docker but it works perfectly when verified from within docker</p>
<pre class=""lang-none prettyprint-override""><code>openssl verify -CAfile ./certs/docker/development/postgres/root.crt ./certs/docker/development/postgres/client_express_server.crt
</code></pre>
<p>The command above gives me an error when run from my host machine</p>
<pre class=""lang-none prettyprint-override""><code>CN=postgres_server.development.ch_api
error 20 at 0 depth lookup: unable to get local issuer certificate
error ./certs/docker/development/postgres/client_express_server.crt: verification failed
</code></pre>
<p>These certificates are generated inside a docker container called postgres_certs.development.ch_api using the method <a href=""https://www.postgresql.org/docs/18/ssl-tcp.html#SSL-CERTIFICATE-CREATION"" rel=""nofollow noreferrer"">suggested by postgresql SSL docs</a></p>
<p><strong><code>gen-test-certs.sh</code></strong></p>
<pre class=""lang-bash prettyprint-override""><code>#!/usr/bin/env bash

set -e

# Directory where certificates will be stored
OUTPUT_DIR=&quot;tests/tls&quot;
mkdir -p &quot;$OUTPUT_DIR&quot;
cd &quot;$OUTPUT_DIR&quot;

openssl dhparam -out postgres.dh 2048

# 1. Create Root CA
openssl req \
  -new \
  -nodes \
  -text \
  -out root.csr \
  -keyout root.key \
  -subj &quot;/CN=root.development.ch_api&quot;

chmod 0600 root.key

openssl x509 \
  -req \
  -in root.csr \
  -text \
  -days 3650 \
  -extensions v3_ca \
  -signkey root.key \
  -out root.crt

# 2. Create Server Certificate
# CN must match the hostname the clients use to connect
openssl req \
  -new \
  -nodes \
  -text \
  -out server.csr \
  -keyout server.key \
  -subj &quot;/CN=postgres_server.development.ch_api&quot;
chmod 0600 server.key

openssl x509 \
  -req \
  -in server.csr \
  -text \
  -days 365 \
  -CA root.crt \
  -CAkey root.key \
  -CAcreateserial \
  -out server.crt

# 3. Create Client Certificate for Express Server
# For verify-full, the CN should match the database user the Express app uses
openssl req \
  -days 365 \
  -new \
  -nodes \
  -subj &quot;/CN=ch_user&quot; \
  -text \
  -keyout client_express_server.key \
  -out client_express_server.csr
chmod 0600 client_express_server.key

openssl x509 \
  -days 365 \
  -req \
  -CAcreateserial \
  -in client_express_server.csr \
  -text \
  -CA root.crt \
  -CAkey root.key \
  -out client_express_server.crt

# 4. Create Client Certificate for local machine psql
# For verify-full, the CN should match your local database username
openssl req \
  -days 365 \
  -new \
  -nodes \
  -subj &quot;/CN=ch_user&quot; \
  -text \
  -keyout client_psql.key \
  -out client_psql.csr
chmod 0600 client_psql.key

openssl x509 \
  -days 365 \
  -req \
  -CAcreateserial \
  -in client_psql.csr \
  -text \
  -CA root.crt \
  -CAkey root.key \
  -out client_psql.crt

# WORKS PERFECTLY HERE!!!
openssl verify -CAfile root.crt client_psql.crt
openssl verify -CAfile root.crt client_express_server.crt
openssl verify -CAfile root.crt server.crt

chown -R postgres:postgres ./*.key
chown -R node:node ./client_express_server.key

# Clean up CSRs and Serial files
rm ./*.csr ./*.srl

</code></pre>
<p>The above script is run from inside a Docker container whose Dockerfile looks like this</p>
<pre><code>FROM debian:12.12-slim
RUN apt update &amp;&amp; \
    apt upgrade --yes &amp;&amp; \
    apt install --yes openssl &amp;&amp; \
    apt autoremove --yes &amp;&amp; \
    apt autoclean --yes &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
WORKDIR /home
RUN set -eux; \
    groupadd -r -g 999 postgres; \
    useradd -r -g postgres -u 999 postgres;
RUN set -eux; \
    groupadd -g 1000 node; \
    useradd -g node -u 1000 node
COPY ./docker/development/postgres_certs/gen-test-certs.sh ./
RUN chmod u+x ./gen-test-certs.sh
RUN mkdir -p /home/tests/tls
CMD [&quot;./gen-test-certs.sh&quot;]
</code></pre>
<p>Once the certificates are generated, the container above is shut down.</p>
<p>The volume containing these certs are mounted from /home/tests/tls above into the postgres container called &quot;postgres_server.development.ch_api&quot; and node.js express container called &quot;express_server.development.ch_api&quot;</p>
<p>I have SSL mode for postgres set to verify-full and node.js express works perfectly with it (I tested).</p>
<p>Once the certificate generating container finishes, I simply issue a docker cp and copy the files to &quot;${PWD}/certs/docker/development/postgres&quot;
and run the following commands</p>
<pre class=""lang-none prettyprint-override""><code>openssl verify -CAfile ./certs/docker/development/postgres/root.crt ./certs/docker/development/postgres/client_psql.crt          

CN=postgres_server.development.ch_api
error 20 at 0 depth lookup: unable to get local issuer certificate
error ./certs/docker/development/postgres/client_psql.crt: verification failed

openssl verify -CAfile ./certs/docker/development/postgres/root.crt ./certs/docker/development/postgres/client_express_server.crt
CN=postgres_server.development.ch_api
error 20 at 0 depth lookup: unable to get local issuer certificate
error ./certs/docker/development/postgres/client_express_server.crt: verification failed
</code></pre>
<p>This command works perfectly</p>
<pre class=""lang-none prettyprint-override""><code>docker exec -it postgres_server.development.ch_api psql &quot;port=47293 host=localhost user=ch_user dbname=ch_api sslcert=/etc/ssl/certs/client_psql.crt sslkey=/etc/ssl/certs/client_psql.key sslrootcert=/etc/ssl/certs/root.crt sslmode=require password=password&quot;
psql (18.1 (Debian 18.1-1.pgdg12+2))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off, ALPN: postgresql)
Type &quot;help&quot; for help.

ch_api=# select 1
ch_api-# ;
 ?column? 
----------
        1
(1 row)

ch_api=# \q
</code></pre>
<p>I believe this has something to do with the CN value. Does anyone know what is wrong with the CN value, needs to work both from within docker and outside docker (local machine)?</p>
<h3>UPDATE 1</h3>
<p>Verification works perfectly when openssl is installed inside both the postgres and express docker container and then run using the following commands</p>
<pre class=""lang-none prettyprint-override""><code>docker exec -it express_server.development.ch_api openssl verify -CAfile /home/node/ch_api/certs/docker/development/postgres/root.crt /home/node/ch_api/certs/docker/development/postgres/client_express_server.crt

/home/node/ch_api/certs/docker/development/postgres/client_express_server.crt: OK

docker exec -it express_server.development.ch_api openssl verify -CAfile /home/node/ch_api/certs/docker/development/postgres/root.crt /home/node/ch_api/certs/docker/development/postgres/client_psql.crt

/home/node/ch_api/certs/docker/development/postgres/client_psql.crt: OK

docker exec -it postgres_server.development.ch_api openssl verify -CAfile /etc/ssl/certs/root.crt /etc/ssl/certs/client_express_server.crt

/etc/ssl/certs/client_express_server.crt: OK

docker exec -it postgres_server.development.ch_api openssl verify -CAfile /etc/ssl/certs/root.crt /etc/ssl/certs/client_psql.crt

/etc/ssl/certs/client_psql.crt: OK
</code></pre>
<p>Run the same commands from localhost and it immediately goes bust</p>
<pre class=""lang-none prettyprint-override""><code>openssl verify -CAfile ./certs/docker/development/postgres/root.crt ./certs/docker/development/postgres/client_express_server.crt

CN=postgres_server.development.ch_api
error 20 at 0 depth lookup: unable to get local issuer certificate
error ./certs/docker/development/postgres/client_express_server.crt: verification failed

openssl verify -CAfile ./certs/docker/development/postgres/root.crt ./certs/docker/development/postgres/client_psql.crt

CN=postgres_server.development.ch_api
error 20 at 0 depth lookup: unable to get local issuer certificate
error ./certs/docker/development/postgres/client_psql.crt: verification failed
</code></pre>
<p>The files copied over to the localhost are not the same as the ones inside the postgres and express containers, looking into it</p>
",-1,0,1,2025-12-24T05:45:08+00:00,1,37,False
79854159,12108591,"Bengaluru, Karnataka, India",postgresql,Prisma is giving error while trying to migrate it or push to NeonDB,"<blockquote>
<p>The datasource property <code>url</code> is no longer supported in schema files. Move connection URLs for Migrate to <code>prisma.config.ts</code> and pass either <code>adapter</code> for a direct database connection or <code>accelerateUrl</code> for Accelerate to the <code>PrismaClient</code> constructor. See <a href=""https://pris.ly/d/config-datasource"" rel=""nofollow noreferrer"">https://pris.ly/d/config-datasource</a> and <a href=""https://pris.ly/d/prisma7-client-configPrisma"" rel=""nofollow noreferrer"">https://pris.ly/d/prisma7-client-configPrisma</a></p>
</blockquote>
<p>My prisma.schema file:</p>
<pre><code>// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

// Looking for ways to speed up your queries, or scale easily with your serverless or edge functions?
// Try Prisma Accelerate: https://pris.ly/cli/accelerate-init

generator client {
  provider = &quot;prisma-client&quot;
  output   = &quot;../app/generated/prisma&quot;
}

datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}

model User {
  id    Int     @id @default(autoincrement())
  email String  @unique
  name  String?
  posts Post[]
}

model Post {
  id        Int     @id @default(autoincrement())
  title     String
  content   String?
  published Boolean @default(false)
  authorId  Int
  author    User    @relation(fields: [authorId], references: [id])
}
</code></pre>
<p>And prisma.config.ts file</p>
<pre><code>
// This file was generated by Prisma and assumes you have installed the following:
// npm install --save-dev prisma dotenv
import &quot;dotenv/config&quot;;
import { defineConfig, env } from &quot;prisma/config&quot;;

export default defineConfig({
  schema: &quot;prisma/schema.prisma&quot;,
  migrations: {
    path: &quot;prisma/migrations&quot;,
  },
  engine: &quot;classic&quot;,
  datasource: {
    url: env(&quot;DATABASE_URL&quot;),
  },
});
</code></pre>
",0,0,0,2025-12-24T06:13:29+00:00,1,58,False
79854214,2345325,"California, USA",postgresql,Error when running sudo -i -u postgres psql (connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: No such file or directory),"<p>I just want to run Postgres locally for development purposes.</p>
<p>Running <code>sudo -i -u postgres psql</code> returns the error message:</p>
<pre><code>psql: error: connection to server on socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot; failed: No such file or directory
    Is the server running locally and accepting connections on that socket?
</code></pre>
<p>This is on a fresh installation of Postgres on Pop_OS! 24.04 LTS (which is based on Ubuntu).</p>
<pre><code>sudo apt update
sudo apt-get install postgresql postgresql-contrib libpq-dev
</code></pre>
",0,0,0,2025-12-24T08:24:52+00:00,1,31,False
79854231,10660712,,postgresql,Postgres query performance issue,"<p>In postgres there’s a query as a chain of <code>WITH</code> expressions with a final query as a simple columns arrangement from the step subquery. And there’s a performance issue related with a column with just make simple arithmetic computation of</p>
<pre class=""lang-sql prettyprint-override""><code>with ....., final_composition as (...)
select
  ...,
  current_postion,
  window_start,
  window_end,
  (current_postion - window_start) / (window_end - window_start) as progress
from final_composition
</code></pre>
<p>the total number of rows fetched from <code>final_composition</code> is just around 20k. With omitting just a single progress column the query is executed (according to psql statistics) around <code>700ms</code> still adding it back to a query makes query to run around <code>1800ms</code> (more then twice longer). This looks very weird as all the columns are regularly fetched aside works fine and just adding simple math increases time drastically.
The very same problem reproduced just adding even only a subtraction part as <code>(current_postion - window_start)</code></p>
<p>Just to compare a query</p>
<pre class=""lang-sql prettyprint-override""><code>select sum((random() - random())/(random() - random())) from generate_series(0, 20000);
</code></pre>
<p>tooks around <code>8ms</code> on my environment
So why actually such a simple maths adding around one second against initial <code>700ms</code> without it.
How this really works and how to fix this after all?</p>
<p>Thank you</p>
",-3,0,3,2025-12-24T08:51:13+00:00,0,67,False
79854938,17317410,,postgresql,Postgres FORCE RLS: `new row violates row-level security policy` even with `WITH CHECK (true)` on UPDATE?,"<p>I am running into a Row-Level Security (RLS) issue in my PostgreSQL database. I am implementing a &quot;soft delete&quot; feature where I update a <code>deleted</code> column to <code>true</code>.</p>
<p>I have isolated the issue to a purely SQL reproduction in pgAdmin, eliminating the application layer, but the error persists.</p>
<h2>The Setup</h2>
<ol>
<li><strong>Role:</strong> I have a dedicated database role <code>rls</code> that owns the tables.</li>
<li><strong>Strictness:</strong> The role has <code>NOBYPASSRLS</code>.</li>
<li><strong>FORCE RLS:</strong> Tables have <code>FORCE ROW LEVEL SECURITY</code> enabled (so the owner is subject to RLS).</li>
<li><strong>Goal:</strong> Allow the owner to &quot;soft delete&quot; (UPDATE) any row, while hiding deleted rows from SELECTs.</li>
</ol>
<h2>The Problem</h2>
<p>When I try to soft-delete a row:</p>
<pre class=""lang-sql prettyprint-override""><code>UPDATE posts SET deleted = true WHERE id = 12410;
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>ERROR:  new row violates row-level security policy for table &quot;posts&quot;
SQL state: 42501</p>
</blockquote>
<h2>My Configuration (Minimal Reproduction)</h2>
<p>I have stripped the table down to the basics. No triggers are active.</p>
<ol>
<li><p><strong>Enable RLS:</strong></p>
<pre class=""lang-sql prettyprint-override""><code>ALTER TABLE &quot;posts&quot; ENABLE ROW LEVEL SECURITY;
ALTER TABLE &quot;posts&quot; FORCE ROW LEVEL SECURITY;
</code></pre>
</li>
<li><p><strong>Policies:</strong>
I split the policies to be as permissive as possible for the UPDATE operation.</p>
<pre class=""lang-sql prettyprint-override""><code>-- READ: Hide deleted rows
CREATE POLICY posts_read ON &quot;posts&quot; FOR SELECT
  USING (deleted IS NULL OR deleted = false);

-- INSERT: Allow anything
CREATE POLICY posts_insert ON &quot;posts&quot; FOR INSERT
  WITH CHECK (true);

-- UPDATE: THE PROBLEM AREA
-- I want to allow updating ANY row (USING true)
-- and writing ANY state (WITH CHECK true)
CREATE POLICY posts_update ON &quot;posts&quot; FOR UPDATE
  USING (true)
  WITH CHECK (true);
</code></pre>
</li>
<li><p><strong>Verification:</strong>
Checking <code>pg_policies</code> confirms the policy is loaded correctly:</p>
<ul>
<li><code>tablename</code>: posts</li>
<li><code>policyname</code>: posts_update</li>
<li><code>permissive</code>: PERMISSIVE</li>
<li><code>cmd</code>: UPDATE</li>
<li><code>qual</code> (USING): <code>true</code></li>
<li><code>with_check</code>: <code>true</code></li>
</ul>
</li>
</ol>
<h2>What I've Tried</h2>
<ol>
<li><strong>Unified Policy:</strong> Originally I had <code>USING (deleted=false) WITH CHECK (true)</code>, which failed (understandably, as <code>USING</code> limits the scope).</li>
<li><strong>Split Policies:</strong> Moved to the explicit <code>FOR UPDATE</code> policy above with <code>USING (true)</code>. Still fails.</li>
<li><strong>Checking Triggers:</strong> Ran <code>SELECT * FROM pg_trigger</code> for the table. <strong>Result:</strong> No active triggers.</li>
<li><strong>Checking Foreign Keys:</strong> The table does have Foreign Keys to other tables (which also have RLS), but I am purely updating a local boolean column, so I wouldn't expect referential integrity checks to fire unless I was changing the FK column.</li>
<li><strong>Manual Test:</strong> Running the query as the <code>rls</code> owner user in pgAdmin directly.</li>
</ol>
<h2>My Question</h2>
<p>If I have an RLS policy for UPDATE defined as <code>USING (true) WITH CHECK (true)</code>, is there <em>any</em> scenario where a <code>new row violates row-level security policy</code> error is possible?</p>
<p>Does <code>FORCE RLS</code> interact with Foreign Key validation in a way that masks a referential integrity error as an RLS violation?</p>
<p>Any insights on how to debug this &quot;impossible&quot; policy failure would be appreciated.</p>
",3,3,0,2025-12-25T16:27:32+00:00,1,51,True
79854966,1873075,,postgresql,Best database book for 10-thousand-foot flyer?,"<p>I'm grokking data analytics as part of the Google DA course, and I realize I'm going to need a bit more about databases than the course teaches.</p>
<p>Any ideas for a great introductory database book?  I'm looking for ways for setting up online DB instances [cheaply!] without bogging my laptop down with myriad RDBMS programs.  I'd also like to practice my SQL kungfu without all the usual storage baggage.  Thanks!</p>
",0,0,0,2025-12-25T17:20:54+00:00,3,67,True
79854986,10640995,,postgresql,This release adds the capability to access both the previous (`OLD`) and current (`NEW`) values in the [`RETURNING` clause],"<p>Now that Postgres 18 has this new feature</p>
<blockquote>
<p>This release adds the capability to access both the previous (<code>OLD</code>) and current (<code>NEW</code>) values in the <a href=""https://www.postgresql.org/docs/18/dml-returning.html"" rel=""nofollow noreferrer""><code>RETURNING</code> clause</a> for <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> and <code>MERGE</code> commands. PostgreSQL 18 also adds UUIDv7 generation through the <a href=""https://www.postgresql.org/docs/18/functions-uuid.html#FUNC_UUID_GEN_TABLE"" rel=""nofollow noreferrer""><code>uuidv7()</code></a> function, letting you generate random UUIDs that are timestamp-ordered to support better caching strategies. PostgreSQL 18 includes <a href=""https://www.postgresql.org/docs/18/functions-uuid.html#FUNC_UUID_GEN_TABLE"" rel=""nofollow noreferrer""><code>uuidv4()</code></a> as an alias for <code>gen_random_uuid()</code>.</p>
</blockquote>
<p>How do we do this?</p>
<p>The NEW and OLD words are still highlighted in Red when I try this in IntelliJ. It also doesn't run properly.</p>
<pre><code>INSERT INTO organisations.organisations(
    organisation_name,
    country,
    created_by,
    modified_by
)
VALUES (
   p_organisation.organisation_name,
   p_organisation.country,
   auth.get_current_app_user_id(),
   auth.get_current_app_user_id()
)
ON CONFLICT (organisation_name)
    WHERE is_deleted IS NOT TRUE
DO NOTHING
RETURNING 
    COALESCE(NEW.*, OLD.*) INTO v_organisation;

-- if not found, raise an exception
IF v_organisation.organisation_id IS NULL THEN
    RAISE EXCEPTION 'could not create organisation: ID is NULL.';
END IF;
</code></pre>
<p>The below does work</p>
<pre><code>INSERT INTO organisations.organisations(
    organisation_name,
    country,
    created_by,
    modified_by
)
VALUES (
   p_organisation.organisation_name,
   p_organisation.country,
   auth.get_current_app_user_id(),
   auth.get_current_app_user_id()
)
ON CONFLICT (organisation_name)
    WHERE is_deleted IS NOT TRUE
DO NOTHING
RETURNING * INTO v_organisation;

-- if nothing returned, it already existed - fetch it
IF v_organisation.organisation_id IS NULL THEN
    SELECT * INTO v_organisation
    FROM organisations.organisations
    WHERE organisation_name = p_organisation.organisation_name
      AND is_deleted IS NOT TRUE;
END IF;

-- if not found, raise an exception
IF v_organisation.organisation_id IS NULL THEN
    RAISE EXCEPTION 'could not create organisation: ID is NULL.';
END IF;
</code></pre>
",0,1,1,2025-12-25T18:34:36+00:00,1,57,True
79855026,30636598,,postgresql,How to recursively inject WHERE clauses into deep subqueries and nested expressions?,"<p>I am building a SQL interceptor in Java to implement multi-tenant isolation. My goal is to parse incoming SQL statements using JSqlParser and inject a mandatory predicate <code>tenant_id = '123'</code> into every table access. Assume every table has a <code>tenant_id</code> column.</p>
<p>I have a working prototype that handles simple <code>SELECT</code>, <code>UPDATE</code>, and <code>DELETE</code> statements. However, my current approach involves manually checking specific expression types such as <code>InExpression</code>, <code>ExistsExpression</code>, <code>SubSelect</code>, and others.</p>
<p><strong>Current Approach (Manual)</strong></p>
<p>I am currently manually unwrapping expressions. This works for top-level queries but gets messy quickly:</p>
<pre class=""lang-java prettyprint-override""><code>public void modifyPlainSelect(PlainSelect plainSelect, String simId) {
    // 1. Add to main table
    if (plainSelect.getFromItem() instanceof Table) {
        addWhereCondition(plainSelect, simId);
    }
    
    // 2. Manually check WHERE clause for subqueries
    Expression where = plainSelect.getWhere();
    if (where instanceof InExpression inExpr) {
        // Manually handle the right side
        if (inExpr.getRightItemsList() instanceof SubSelect) {
             modifySelect(((SubSelect) inExpr.getRightItemsList()).getSelectBody());
        }
    } 
    // I have to add manual checks for Exists, Between, BinaryExpression, etc...
}
</code></pre>
<p><strong>The Problem</strong></p>
<p>This fails to cover complex scenarios automatically. For example:</p>
<p>Input:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM orders o 
WHERE o.customer_id IN (
    SELECT c.id FROM customers c 
    WHERE c.status = (SELECT s.code FROM status s WHERE s.active = 1)
)
</code></pre>
<p>Desired Output:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM orders o 
WHERE o.simulation_id = '123' -- Injected
AND o.customer_id IN (
    SELECT c.id FROM customers c 
    WHERE c.simulation_id = '123' -- Injected (Recursion Level 1)
    AND c.status = (
        SELECT s.code FROM status s 
        WHERE s.simulation_id = '123' -- Injected (Recursion Level 2)
        AND s.active = 1
    )
)
</code></pre>
<p><strong>My Question</strong></p>
<p>Is there a standard pattern in JSqlParser, such as <code>StatementVisitor</code>, <code>TablesNamesFinder</code>, or <code>ExpressionDeParser</code>, that allows me to do the following:</p>
<ol>
<li><p>Visit <em>every</em> node in the AST automatically, with recursion handled by the library.</p>
</li>
<li><p>Identify whenever a <code>Table</code> is accessed, whether in <code>FROM</code>, <code>JOIN</code>, <code>UPDATE</code>, or <code>DELETE</code>.</p>
</li>
<li><p>Inject the <code>WHERE</code> clause into the parent <code>SELECT</code> or <code>UPDATE</code> object.</p>
</li>
</ol>
<p>I want to avoid writing manual traversal logic for every possible expression type to ensure there are no blind spots.</p>
",2,2,0,2025-12-25T20:36:11+00:00,1,121,False
79855066,10894456,"NY, USA",postgresql,Does PostgreSQL read committed isolation level guarantee 2 transactions: subtract 3 and subtract 5 together always result 10 - 8 = 2?,"<p>Does PostgreSQL read committed isolation level guarantee 2 transactions: subtract 3 and subtract 5 together always result 10 - 8 = 2 ?</p>
<p>i.e. PostgreSQL</p>
<pre><code>Origin_value = 10
1 Tr = 1) read Origin_value  2) subtract 3  3) commit
2 Tr = 1) read Origin_value  2) subtract 5  3) commit
</code></pre>
<p>is the result</p>
<pre><code>Origin_value = 2
</code></pre>
<p>guaranteed?</p>
",0,0,0,2025-12-25T23:53:31+00:00,1,54,True
79855117,5371505,,postgresql,How do I specify that CN=&lt;container_name&gt; and localhost at the same time for openssl self signed certificates between postgres and express?,"<ul>
<li>Postgres is running inside a docker container named postgres_server.development.ch_api</li>
<li>Express is running inside another docker container named express_server.development.ch_api</li>
<li>I am trying to setup self signed SSL certificates for PostgeSQL using openssl</li>
<li>This is taken from the documentation as <a href=""https://www.postgresql.org/docs/18/ssl-tcp.html#SSL-CERTIFICATE-CREATION"" rel=""nofollow noreferrer"">per PostgreSQL here</a></li>
<li>If CN is localhost, the docker containers of express and postgres are not able to connect to each other</li>
<li>If CN is set to the container name, I am not able to connect psql from my local machine to the postgres server because same thing CN mismatch</li>
<li>How do I make it work at both places?</li>
</ul>
<pre><code>#!/usr/bin/env bash

set -e

if [ &quot;$#&quot; -ne 1 ]; then
    echo &quot;Usage: $0 &lt;postgres-container-name&gt;&quot;
    exit 1
fi

# Directory where certificates will be stored
CN=&quot;${1}&quot;
OUTPUT_DIR=&quot;tests/tls&quot;
mkdir -p &quot;${OUTPUT_DIR}&quot;
cd &quot;${OUTPUT_DIR}&quot; || exit 1

openssl dhparam -out postgres.dh 2048

# 1. Create Root CA
openssl req \
  -new \
  -nodes \
  -text \
  -out root.csr \
  -keyout root.key \
  -subj &quot;/CN=root.development.ch_api&quot;

chmod 0600 root.key

openssl x509 \
  -req \
  -in root.csr \
  -text \
  -days 3650 \
  -extensions v3_ca \
  -signkey root.key \
  -out root.crt

# 2. Create Server Certificate
# CN must match the hostname the clients use to connect
openssl req \
  -new \
  -nodes \
  -text \
  -out server.csr \
  -keyout server.key \
  -subj &quot;/CN=${CN}&quot;
chmod 0600 server.key

openssl x509 \
  -req \
  -in server.csr \
  -text \
  -days 365 \
  -CA root.crt \
  -CAkey root.key \
  -CAcreateserial \
  -out server.crt

# 3. Create Client Certificate for Express Server
# For verify-full, the CN should match the database user the Express app uses
openssl req \
  -days 365 \
  -new \
  -nodes \
  -subj &quot;/CN=ch_user&quot; \
  -text \
  -keyout client_express_server.key \
  -out client_express_server.csr
chmod 0600 client_express_server.key

openssl x509 \
  -days 365 \
  -req \
  -CAcreateserial \
  -in client_express_server.csr \
  -text \
  -CA root.crt \
  -CAkey root.key \
  -out client_express_server.crt

# 4. Create Client Certificate for local machine psql
# For verify-full, the CN should match your local database username
openssl req \
  -days 365 \
  -new \
  -nodes \
  -subj &quot;/CN=ch_user&quot; \
  -text \
  -keyout client_psql.key \
  -out client_psql.csr
chmod 0600 client_psql.key

openssl x509 \
  -days 365 \
  -req \
  -CAcreateserial \
  -in client_psql.csr \
  -text \
  -CA root.crt \
  -CAkey root.key \
  -out client_psql.crt

openssl verify -CAfile root.crt client_psql.crt
openssl verify -CAfile root.crt client_express_server.crt
openssl verify -CAfile root.crt server.crt

chown -R postgres:postgres ./*.key
chown -R node:node ./client_express_server.key

# Clean up CSRs and Serial files
rm ./*.csr ./*.srl

</code></pre>
<ul>
<li>How do I specify that CN should be both postgres_server.development.ch_api and localhost at the same time?</li>
</ul>
",0,0,0,2025-12-26T04:54:36+00:00,0,22,False
79855374,27358891,,postgresql,Fatal error postgres password authentication failed for user &quot;postgres&quot;,"<p>I'm unsure why this error is happening, with this project, I first initially have a yaml file which contains the database credentials:</p>
<pre class=""lang-yaml prettyprint-override""><code>host: localhost  
port: 5432  
username: postgres  
password: postgres  
database: AirlineReports  
driver: postgresql+psycopg2
</code></pre>
<p>The I created another file with a method to read these credentials, this worked since version was printing out</p>
<pre><code>def create_engine_from_creds():
    &quot;&quot;&quot;
    Creates a SQLAlchemy engine using database credentials stored in a YAML file.
    Args:
        None
    Returns:
        engine: SQLAlchemy engine object connected to the specified database.
    &quot;&quot;&quot;

    database_creds = yaml.safe_load(open(&quot;database_creds.yaml&quot;)) # Load database credentials from YAML file

    engine = create_engine(f'{database_creds[&quot;driver&quot;]}://{database_creds[&quot;username&quot;]}:\
    {database_creds[&quot;password&quot;]}@{database_creds[&quot;host&quot;]}:\
    {database_creds[&quot;port&quot;]}/{database_creds[&quot;database&quot;]}')  #creating the database engine from the credentials

    try:
        with engine.connect() as connection: 
            version = connection.execute(text(&quot;SELECT version();&quot;)).scalar() 
            print(version) # confirming connection by printing database version
    except ConnectionError as e:
        print(f&quot;An Connection error has occured to the database : {e}&quot;) # if connection fails, print error message
    
    return engine
</code></pre>
<p>Then in another file I created classes for the tables I'm going to use currently have created one called airlines</p>
<pre><code>Base = declarative_base()   

class Airline(Base):
    &quot;&quot;&quot;
    Represents the Airline table in the database.
    &quot;&quot;&quot;
    __tablename__ = &quot;Airline&quot; 

    IATA = Column(String(2), primary_key=True) # e.g., &quot;AA&quot; 
    Airline = Column(String(100), nullable=False) # Airline name 
    Country = Column(String(100)) # Country of origin 
    Region = Column(String(100)) # Region (e.g., Europe, Asia)
</code></pre>
<p>Finally I wanted to read one table into the database and see if what I'm doing is going well however that is where the fatal error password authentication is not working</p>
<pre><code>from database_connection_utils import create_engine_from_creds
from create_classes_for_tables import Airline
import pandas as pd

def read_airline_data_into_table(csv_file_path):
    &quot;&quot;&quot;
    Reads airline data from a CSV file and inserts it into the Airline table in the database.
    Args:
        csv_file_path (str): Path to the CSV file containing airline data.
    Returns:
        None        
    &quot;&quot;&quot;
    
    engine = create_engine_from_creds() # Create database engine
   
    # Read CSV data into a DataFrame
    airline_df = pd.read_csv(csv_file_path)

    # Insert data into the Airline table
    airline_df.to_sql(Airline.__tablename__, engine, if_exists='append', index=False)
    print(f&quot;Inserted {len(airline_df)} records into the Airline table.&quot;)

if __name__ == &quot;__main__&quot;:
    read_airline_data_into_table(&quot;airline.csv&quot;)
</code></pre>
<p>I am using PGADMIN4  the version I'm using is PostgreSQL 17.5 on x86_64-windows, compiled by msvc-19.44.35209, 64-bit and I am connected using the same password as my yaml file.</p>
<p>If anyone can help on where I am going wrong?</p>
",2,3,1,2025-12-26T15:05:44+00:00,1,51,False
79683153,2613166,"Brisbane, Australia",mysql,SQL query optimization in first nominal databases that require row filters that select using subqueries,"<p><strong>The setup</strong></p>
<p>I am working in  a MySQL database that still runs 5.6 (so I can't use MySQL 8 language constructs). The final SQL will embed in a Jasper Report (but that is a later problem)</p>
<p>The tables appear like this</p>
<p>Table Acts  - there is also an act_detail tables that has (act_id, name, value)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fieldname</th>
<th>type</th>
<th>extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>act_id</td>
<td>bigint(20)</td>
<td>AIPK</td>
</tr>
<tr>
<td>created</td>
<td>datetime</td>
<td></td>
</tr>
<tr>
<td>arch_short_name</td>
<td>varchar(100)</td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>varchar(255)</td>
<td></td>
</tr>
<tr>
<td>description</td>
<td>varchar(255)</td>
<td></td>
</tr>
<tr>
<td>active</td>
<td>bit(1)</td>
<td></td>
</tr>
<tr>
<td>activity_start_time</td>
<td>datetime</td>
<td></td>
</tr>
<tr>
<td>activity_end_time</td>
<td>datetime</td>
<td></td>
</tr>
<tr>
<td>status</td>
<td>varchar(255)</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Table Entities - there is also an entity_detail table that has (entity_id , name, value)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fieldname</th>
<th>type</th>
<th>extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>entity_id</td>
<td>bigint(20)</td>
<td>AIPK</td>
</tr>
<tr>
<td>created</td>
<td>datetime</td>
<td></td>
</tr>
<tr>
<td>created_id</td>
<td>bigint(20)</td>
<td></td>
</tr>
<tr>
<td>arch_short_name</td>
<td>varchar(100)</td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>varchar(100)</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Table: participations (this reflects an entity participating in an act)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fieldname</th>
<th>type</th>
<th>extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>participation_id</td>
<td>bigint(20)</td>
<td>AIPK</td>
</tr>
<tr>
<td>arch_short_name</td>
<td>varchar(100)</td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>varchar(255)</td>
<td></td>
</tr>
<tr>
<td>entity_id</td>
<td>bigint(20)</td>
<td></td>
</tr>
<tr>
<td>act_id</td>
<td>bigint(20)</td>
<td></td>
</tr>
<tr>
<td>activity_start_time</td>
<td>datetime</td>
<td></td>
</tr>
<tr>
<td>activity_end_time</td>
<td>datetime</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Table: act_relationships (this reflects a relationship between 2 acts)</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fieldname</th>
<th>type</th>
<th>extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>act_relationship_id</td>
<td>bigint(20)</td>
<td>AIPK</td>
</tr>
<tr>
<td>arch_short_name</td>
<td>varchar(100)</td>
<td></td>
</tr>
<tr>
<td>name</td>
<td>varchar(255)</td>
<td></td>
</tr>
<tr>
<td>description</td>
<td>varchar(255)</td>
<td></td>
</tr>
<tr>
<td>source_id</td>
<td>bigint(20)</td>
<td></td>
</tr>
<tr>
<td>target_id</td>
<td>bigint(20)</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>The data is basically what I would describe as &quot;practical&quot; 1st nominal form.<br />
The data objects are</p>
<ul>
<li>product stored in entity</li>
<li>&quot;stock location&quot; stored in entity</li>
<li>delivery stored as act</li>
<li>&quot;delivery items&quot; stored as act</li>
</ul>
<p>A stock location stores products<br />
A delivery contains many delivery items and is delivered to 1 stock location<br />
A delivery item contains 1 product</p>
<p>I am trying to product a data set of product that shows the most recent delivery item for each product that has been delivered to a stockLocation.</p>
<p>There are many deliveries over time to each location, each delivery can contain few or many items.</p>
<p><strong>The Problem</strong></p>
<p>This is my current SQL</p>
<pre class=""lang-sql prettyprint-override""><code>
    select
    e.entity_id as pid,
    e.name as product,
    pt.name as productType,
    l.name as uom,
    sl.name as stockLocation,
    round(if(d2.value,d2.value,&quot;0&quot;),2) as currentStock,
    round(max(d3.value),2) as idealStock,
    round(max(ppud.value),2) as unitcost,
    round(max(ppfd.value),2) as fixedcost,
    if(round(max(ppud.value),2),round(max(ppud.value),2),round(max(ppfd.value),2)) as cost
    from
    entities e inner join products p on e.entity_id = p.product_id
    left join entity_links r on e.entity_id = r.source_id and r.arch_short_name = &quot;entityLink.productType&quot;
    left join entities pt on r.target_id = pt.entity_id
    left outer join entity_details d on d.entity_id = e.entity_id and d.name = &quot;sellingUnits&quot;
    left outer join lookups l on l.code = d.value and l.arch_short_name = &quot;lookup.uom&quot;
    left join entity_links s on s.source_id = e.entity_id and s.arch_short_name = &quot;entityLink.productStockLocation&quot;
    left join entity_link_details d2 on s.id = d2.id and d2.name = &quot;quantity&quot;
    left outer join entity_link_details d3 on d3.id = s.id  and d3.name = &quot;idealQty&quot;
    left join entities sl on sl.entity_id = s.target_id
    left outer join product_prices ppu on ppu.product_id = p.product_id and ppu.arch_short_name = &quot;productPrice.unitPrice&quot; and ppu.end_time is null
    left outer join product_price_details ppud on ppud.product_price_id = ppu.product_price_id and ppud.name = &quot;cost&quot;
    left outer join product_prices ppf on ppf.product_id = p.product_id and ppf.arch_short_name = &quot;productPrice.fixedPrice&quot;  and ppf.end_time is null
    left outer join product_price_details ppfd on ppfd.product_price_id = ppf.product_price_id and ppfd.name = &quot;cost&quot;
    where e.name like &quot;%&quot; and
    e.active = 1 and
    pt.entity_id not in (114,121,132,528343,833974) and
    pt.name like &quot;%&quot; and
    sl.name like &quot;%&quot; and
    (
        (e.arch_short_name = &quot;product.medication&quot;) or
        (e.arch_short_name = &quot;product.merchandise&quot;)
    ) and
    round(if(d2.value,d2.value,&quot;0&quot;),2) &gt; 0.00
    group by sl.entity_id, e.entity_id
    order by sl.name, e.name

</code></pre>
<p>It does work but it doesn't include the delivery stuff.</p>
<p>On production server it returns about 10000 lines and is pretty quick.<br />
On my local data (smaller dataset) it returns 600 rows.</p>
<p><strong>The Problem</strong></p>
<p>The problem is when I add all the joins to add the deliveries my query times out. I am also having issues selecting the most recent delivery line item for each item and location.<br />
So I worked to produce SQL that returns the delivery items</p>
<p>I came up with</p>
<pre class=""lang-sql prettyprint-override""><code>
    SELECT 
        MAX(delivery_item.activity_start_time) as deliveryDate,
        stockLocation.entity_id as location,
        prod.entity_id as product
    FROM acts delivery_item
    LEFT JOIN participations prod_deliv_part on prod_deliv_part.act_id = delivery_item.act_id 
            AND prod_deliv_part.arch_short_name = &quot;participation.stock&quot;
    LEFT JOIN entities prod on prod.entity_id = prod_deliv_part.entity_id
    LEFT JOIN act_relationships deliv_item_r ON deliv_item_r.target_id = delivery_item.act_id AND deliv_item_r.arch_short_name = 'actRelationship.supplierDeliveryItem'
    LEFT JOIN acts delivery ON delivery.act_id = deliv_item_r.source_Id
    LEFT JOIN participations delivery_location_p ON delivery_location_p.act_id = delivery.act_id AND delivery_location_p.arch_short_name = 'participation.stockLocation'
    LEFT JOIN entities stockLocation on stockLocation.entity_id = delivery_location_p.entity_id
    WHERE delivery_item.arch_short_name = &quot;act.supplierDeliveryItem&quot; and delivery.status = &quot;POSTED&quot;
    GROUP BY location,product

</code></pre>
<p>This works, producing a table like</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>locationId</th>
<th>productID</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-06-11 00:00:00</td>
<td>68</td>
<td>22196</td>
</tr>
<tr>
<td>2025-05-23 00:00:00</td>
<td>68</td>
<td>22112</td>
</tr>
</tbody>
</table></div>
<p>.... and so on</p>
<p>locally about 642 rows</p>
<ul>
<li>on production, it times out... Production is a 600 GB database the acts table has 85 million entries.   It's big.</li>
</ul>
<p>So that's the first issue.  Once I solve that I have to link the data from this select back to my original select, and I have been trying to nest the above select and alias it but I just can't work it out.</p>
<p>This was sort of my idea.</p>
<pre class=""lang-sql prettyprint-override""><code>    LEFT JOIN (SELECT 
        MAX(delivery_item.activity_start_time) as deliveryDate,
        stockLocation.entity_id as location,
        prod.entity_id as product
    FROM acts delivery_item
    LEFT JOIN participations prod_deliv_part on prod_deliv_part.act_id =  delivery_item.act_id 
            AND prod_deliv_part.arch_short_name = &quot;participation.stock&quot;
    LEFT JOIN entities prod on prod.entity_id = prod_deliv_part.entity_id
    LEFT JOIN act_relationships deliv_item_r ON deliv_item_r.target_id = delivery_item.act_id AND deliv_item_r.arch_short_name = 'actRelationship.supplierDeliveryItem'
    LEFT JOIN acts delivery ON delivery.act_id = deliv_item_r.source_Id
    LEFT JOIN participations delivery_location_p ON delivery_location_p.act_id = delivery.act_id AND delivery_location_p.arch_short_name = 'participation.stockLocation'
    LEFT JOIN entities stockLocation on stockLocation.entity_id = delivery_location_p.entity_id
    WHERE delivery_item.arch_short_name = &quot;act.supplierDeliveryItem&quot;
    GROUP BY location,product) deliveries on deliveries.deliveryDate = delivery_item.activity_start_time and deliveries.location = stockLocation.entity_Id and deliveries.product = prod.entity_id
</code></pre>
",1,1,0,2025-06-28T16:09:06+00:00,2,81,True
79683255,14760126,,mysql,How to handle constraint errors like unique or not null from DB in Django,"<p>I have a write operation which adds a record in rider model. The rider_code is unique in this. Now 1 way is to first check if any record with that rider_code exists then return error else create rider. But it requires 2 db calls when the db can return constraint error on create which would be a single call. But how to catch these constraint errors correctly in Django.</p>
<p>This is 1 of the solutions I got but it does not seem right:</p>
<pre><code>     def create_rider(request):
        serializer = RiderSerializer(data=request.data, context={&quot;request&quot;: request})
        if serializer.is_valid():
            try:
                rider = serializer.save()
                return True, rider
            except IntegrityError as e:
                error_msg = str(e)
    
                if &quot;ds_riders.ds_riders_rider_code_d4b087a9_uniq&quot; in error_msg:
                    return False, {&quot;details&quot;: &quot;Rider Code already exists.&quot;}
                if &quot;application_users.username&quot; in error_msg:
                    return False, {&quot;details&quot;: &quot;Username already exists.&quot;}
    
                return False, {&quot;details&quot;: error_msg}
    
        return False, serializer.errors`
</code></pre>
",-1,0,1,2025-06-28T18:48:39+00:00,0,25,False
79683375,17177984,,mysql,"SQL Error: 1406, SQLState: 22001 on Java Spring Boot: Data truncation trying to insert 300 character string in a varchar(15000) or longtext in MYSQL 8","<p>Following several other threads I can confirm:</p>
<p>I am using Mysql: 8</p>
<p>Springboot: 2.6.6 with driver:</p>
<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.33&lt;/version&gt;
</code></pre>
<p>which according to documentation use utf8mb4 as default.</p>
<p>The field in question is &quot;comment&quot; defined varchar(15000) with utf8mb4_0900_ai_ci</p>
<p>I have changed the field to varchar(2000) to try to avoid exceeding row limits.</p>
<p>I have also changed the field to longtext and got the same issue.</p>
<p>The total characters being inserted do not exceed 65000. ( I'm just trying to insert 300 characters)</p>
<p>But the backend can only insert until 255 characters, if above I got:</p>
<pre><code>2025-06-28 18:00:51.208  WARN 74004 --- [io-8087-exec-10] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 1406, SQLState: 22001
2025-06-28 18:00:51.209 ERROR 74004 --- [io-8087-exec-10] o.h.engine.jdbc.spi.SqlExceptionHelper   : Data truncation: Data too long for column 'comment' at row 1
2025-06-28 18:00:51.211  INFO 74004 --- [io-8087-exec-10] o.h.e.j.b.internal.AbstractBatchImpl     : HHH000010: On release of batch it still contained JDBC statements
</code></pre>
<p>Full trace:</p>
<pre><code>org.springframework.orm.jpa.JpaSystemException: Unable to perform beforeTransactionCompletion callback: org.hibernate.exception.DataException: could not execute statement; nested exception is org.hibernate.HibernateException: Unable to perform beforeTransactionCompletion callback: org.hibernate.exception.DataException: could not execute statement
    at org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:331)
    at org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:233)
    at org.springframework.orm.jpa.JpaTransactionManager.doCommit(JpaTransactionManager.java:566)
    at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:743)
    at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:711)
    at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:654)
    at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:407)
    at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:174)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
    at jdk.proxy2/jdk.proxy2.$Proxy332.saveAll(Unknown Source)
    at itk.dbradar.comments.CommentService.create(CommentService.java:37)
    at itk.dbradar.comments.CommentController.commentcreate(CommentController.java:59)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1067)
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:963)
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
    at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:681)
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:764)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:227)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.keycloak.adapters.springsecurity.filter.KeycloakAuthenticatedActionsFilter.doFilter(KeycloakAuthenticatedActionsFilter.java:57)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.keycloak.adapters.springsecurity.filter.KeycloakSecurityContextRequestFilter.doFilter(KeycloakSecurityContextRequestFilter.java:61)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.keycloak.adapters.springsecurity.filter.KeycloakPreAuthActionsFilter.doFilter(KeycloakPreAuthActionsFilter.java:96)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.keycloak.adapters.springsecurity.filter.KeycloakAuthenticationProcessingFilter.successfulAuthentication(KeycloakAuthenticationProcessingFilter.java:214)
    at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:233)
    at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:213)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:111)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:327)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:115)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:81)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:122)
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:116)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:126)
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:81)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:109)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.keycloak.adapters.springsecurity.filter.KeycloakAuthenticatedActionsFilter.doFilter(KeycloakAuthenticatedActionsFilter.java:74)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.keycloak.adapters.springsecurity.filter.KeycloakSecurityContextRequestFilter.doFilter(KeycloakSecurityContextRequestFilter.java:92)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:149)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:103)
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:89)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.keycloak.adapters.springsecurity.filter.KeycloakAuthenticationProcessingFilter.successfulAuthentication(KeycloakAuthenticationProcessingFilter.java:214)
    at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:233)
    at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:213)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.keycloak.adapters.springsecurity.filter.KeycloakPreAuthActionsFilter.doFilter(KeycloakPreAuthActionsFilter.java:96)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:91)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.header.HeaderWriterFilter.doHeadersAfter(HeaderWriterFilter.java:90)
    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:75)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:110)
    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:80)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:55)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:211)
    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:183)
    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:197)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97)
    at org.keycloak.adapters.tomcat.AbstractAuthenticatedActionsValve.invoke(AbstractAuthenticatedActionsValve.java:67)
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:541)
    at org.keycloak.adapters.tomcat.AbstractKeycloakAuthenticatorValve.invoke(AbstractKeycloakAuthenticatorValve.java:181)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:135)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:360)
    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:399)
    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65)
    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:889)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1743)
    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
    at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
    at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
    at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.hibernate.HibernateException: Unable to perform beforeTransactionCompletion callback: org.hibernate.exception.DataException: could not execute statement
    at org.hibernate.engine.spi.ActionQueue$BeforeTransactionCompletionProcessQueue.beforeTransactionCompletion(ActionQueue.java:960)
    at org.hibernate.engine.spi.ActionQueue.beforeTransactionCompletion(ActionQueue.java:525)
    at org.hibernate.internal.SessionImpl.beforeTransactionCompletion(SessionImpl.java:2426)
    at org.hibernate.engine.jdbc.internal.JdbcCoordinatorImpl.beforeTransactionCompletion(JdbcCoordinatorImpl.java:449)
    at org.hibernate.resource.transaction.backend.jdbc.internal.JdbcResourceLocalTransactionCoordinatorImpl.beforeCompletionCallback(JdbcResourceLocalTransactionCoordinatorImpl.java:183)
    at org.hibernate.resource.transaction.backend.jdbc.internal.JdbcResourceLocalTransactionCoordinatorImpl.access$300(JdbcResourceLocalTransactionCoordinatorImpl.java:40)
    at org.hibernate.resource.transaction.backend.jdbc.internal.JdbcResourceLocalTransactionCoordinatorImpl$TransactionDriverControlImpl.commit(JdbcResourceLocalTransactionCoordinatorImpl.java:281)
    at org.hibernate.engine.transaction.internal.TransactionImpl.commit(TransactionImpl.java:101)
    at org.springframework.orm.jpa.JpaTransactionManager.doCommit(JpaTransactionManager.java:562)
    ... 133 more
Caused by: javax.persistence.PersistenceException: org.hibernate.exception.DataException: could not execute statement
    at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:154)
    at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:181)
    at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:188)
    at org.hibernate.internal.SessionImpl.doFlush(SessionImpl.java:1411)
    at org.hibernate.internal.SessionImpl.flush(SessionImpl.java:1394)
    at org.hibernate.envers.internal.synchronization.AuditProcess.doBeforeTransactionCompletion(AuditProcess.java:178)
    at org.hibernate.envers.internal.synchronization.AuditProcessManager$1.doBeforeTransactionCompletion(AuditProcessManager.java:47)
    at org.hibernate.engine.spi.ActionQueue$BeforeTransactionCompletionProcessQueue.beforeTransactionCompletion(ActionQueue.java:954)
    ... 141 more
Caused by: org.hibernate.exception.DataException: could not execute statement
    at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:52)
    at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:37)
    at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:113)
    at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:99)
    at org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.executeUpdate(ResultSetReturnImpl.java:200)
    at org.hibernate.engine.jdbc.batch.internal.NonBatchingBatch.addToBatch(NonBatchingBatch.java:46)
    at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:3375)
    at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:3908)
    at org.hibernate.action.internal.EntityInsertAction.execute(EntityInsertAction.java:107)
    at org.hibernate.engine.spi.ActionQueue.executeActions(ActionQueue.java:604)
    at org.hibernate.engine.spi.ActionQueue.lambda$executeActions$1(ActionQueue.java:478)
    at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
    at org.hibernate.engine.spi.ActionQueue.executeActions(ActionQueue.java:475)
    at org.hibernate.event.internal.AbstractFlushingEventListener.performExecutions(AbstractFlushingEventListener.java:344)
    at org.hibernate.event.internal.DefaultFlushEventListener.onFlush(DefaultFlushEventListener.java:40)
    at org.hibernate.event.service.internal.EventListenerGroupImpl.fireEventOnEachListener(EventListenerGroupImpl.java:107)
    at org.hibernate.internal.SessionImpl.doFlush(SessionImpl.java:1407)
    ... 145 more
Caused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'comment' at row 1
    at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:104)
    at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)
    at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1061)
    at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1009)
    at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1320)
    at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:994)
    at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)
    at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)
</code></pre>
<p>Table is created like:</p>
<pre><code>CREATE TABLE `comment` (
  `commentid` bigint NOT NULL AUTO_INCREMENT,
  `issueid` int DEFAULT NULL,
  `ctype` smallint DEFAULT '0',
  `commentexercise` varchar(15000) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
  `comment` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `internal` tinyint(1) DEFAULT '1',
  `creator` int DEFAULT NULL,
  `createtime` datetime DEFAULT NULL,
  `modifier` int DEFAULT NULL,
  `lastupdated` datetime DEFAULT NULL,
  `approvedby` int DEFAULT NULL,
  `value1` double DEFAULT NULL,
  `approved` bit(1) DEFAULT b'0',
  PRIMARY KEY (`commentid`) USING BTREE,
  KEY `commentid` (`commentid`) USING BTREE,
  KEY `fk_comment_creator` (`creator`) USING BTREE,
  KEY `fk_comment_modifier` (`modifier`) USING BTREE,
  KEY `fk_comment_issueid` (`issueid`) USING BTREE,
  KEY `fk_commnets_ctype` (`ctype`),
  KEY `fk_comments_approveby` (`approvedby`),
  CONSTRAINT `fk_comment_creator` FOREIGN KEY (`creator`) REFERENCES `main`.`users1` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `fk_comment_issuedid` FOREIGN KEY (`issueid`) REFERENCES `issue` (`issueid`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `fk_comment_modifier` FOREIGN KEY (`modifier`) REFERENCES `main`.`users1` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `fk_comments_approveby` FOREIGN KEY (`approvedby`) REFERENCES `main`.`users1` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `fk_commnets_ctype` FOREIGN KEY (`ctype`) REFERENCES `ctype` (`idtype`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=291 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
</code></pre>
<p>This is the pojo class:</p>
<pre><code>@Entity
@Getter
@Setter
@AllArgsConstructor
@EntityListeners(AuditingEntityListener.class)
@Audited
@Table(name = &quot;comment&quot;, schema = &quot;radar&quot;, catalog = &quot;&quot;)
public class Comment {
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    @Id
    private Long commentid;
    private Integer issueid;
    private Short ctype;
    private String comment;
    @JsonFormat(shape = JsonFormat.Shape.NUMBER)
    private Boolean internal;
    private Integer approvedby;
    private Boolean approved;
    private Double value1;
    //@JsonIgnore
    @CreatedBy
    @Column(name = &quot;creator&quot;, nullable = false, updatable=false)
    private Integer creator;
    @CreatedDate
    @Column(name = &quot;createtime&quot;, nullable = false, updatable=false)
    private LocalDateTime createtime;
    @LastModifiedBy
    @Column(name = &quot;modifier&quot;)
    private Integer modifier;
    @LastModifiedDate
    @Column(name=&quot;lastupdated&quot;)
    private LocalDateTime lastupdated;
    //private   Integer amount;
    //private   Integer telecom;
    @Transient
    Page&lt;Attachmentsc&gt; attachments;
    public Comment() {
    }
}
</code></pre>
<p>The problem seems not to be related to MySQL 8, because if I use Navicat I can insert an string much larger than 255 characters.  It seems to rely on Hibernate or Spring Boot.</p>
",0,0,0,2025-06-28T22:41:16+00:00,1,124,False
79683726,5661749,,mysql,does an update need to rewrite the entire row in mysql?,"<p>I have this table:</p>
<p><code>id</code>, <code>heartbeat_at</code>, <code>data_json</code>, ...</p>
<p>The column <code>heartbeat_at</code> is frequently updated with this query:</p>
<pre><code>UPDATE table SET heartbeat_at = NOW() WHERE id IN (...)
</code></pre>
<p>The <code>data_json</code> column is much less frequently updated, but it contains a lot of data (5-50kb per row)</p>
<p>I am wondering if this combination of small, frequently updated column with large, rarely updated column is responsible for high CPU usage of writes.</p>
<p>Does MySQL need to rewrite the entire row, even if I only update <code>heartbeat_at</code>?
If so, would it be better if <code>data_json</code> lives in a separate table?</p>
",0,0,0,2025-06-29T12:20:19+00:00,2,99,True
79684155,7590783,Australia,mysql,Update JSON column with JSON_SET,"<p>Trying to update a JSON column. QUERY1 works fine, but QUERY2 which has a variable seems to update the amount field to 'null'. Tried using CAST as well but still the column updates to 'null'. I am sure it is something to do with the varibale usage.</p>
<p>QUERY1</p>
<pre><code>SET @uuid = '0ab09975-9bdd-4af8-8e76-57a65bf5c53f'; 
update `json` 
set `attributes` = JSON_SET(`attributes`,&quot;$.analyzeResult.documents[0].fields[0].InvoiceTotal.valueCurrency.amount&quot;, 1504.55) where `uuid` = uuid;
</code></pre>
<p>QUERY2</p>
<pre><code>SET @uuid = '0ab09975-9bdd-4af8-8e76-57a65bf5c53f'; 
SET @in_amount = 1504.55;
update `json` 
set `attributes` = JSON_SET(`attributes`,&quot;$.analyzeResult.documents[0].fields[0].InvoiceTotal.valueCurrency.amount&quot;, in_amount) where `uuid` = uuid;
</code></pre>
",2,2,0,2025-06-30T01:06:28+00:00,1,72,True
79687520,3763555,,mysql,Rows from a table that are not in in the same table,"<p>this shows me all customers (DISTINCT) with unfulfilled orders:</p>
<pre><code>SELECT DISTINCT
customer
FROM
orders
WHERE shipped='yes'
ORDER BY customer ASC
</code></pre>
<p>Works fine, now i want another query which only shows customers that</p>
<p>a) have the condition like above but also
b) exclude all customers with already fufilled orders</p>
<p>I tried to extend it with this one but it doesn't show any results:</p>
<pre><code>[...] AND NOT EXISTS (SELECT 1 FROM orders WHERE customer=customer AND shipped='no')
</code></pre>
<p>What did i wrong? Thank you!</p>
",0,0,0,2025-07-02T13:36:56+00:00,2,54,False
79687927,13814896,,mysql,Cannot order by date field that is formatted to string in select distinct,"<p>I am trying to query distinct rows while converting a date field to string format:</p>
<pre><code>SELECT DISTINCT M.col1, M.col2, DATE_FORMAT(M.date_col, '%m/%d/%y') as date_col
FROM t1 M
ORDER BY col1, date_col;
</code></pre>
<p>This can results in ordering issues since its now ordering date_col as a string instead of a date field. I figured I could fix this by converting date_col back to date in the ORDER clause:</p>
<pre><code>...
ORDER BY col1, STR_TO_DATE(date_col, '%m/%d/%y');
</code></pre>
<p>This results in error 3065 - Expression in order clause not in select distinct list.
I also tried using a unique alias:</p>
<pre><code>SELECT DISTINCT M.col1, M.col2, DATE_FORMAT(M.date_col, '%m/%d/%y') as formatted_date_col
FROM t1 M
ORDER BY col1, STR_TO_DATE(formatted_date_col, '%m/%d/%y');
</code></pre>
<p>which results in the same error.</p>
<p>Why does MySQL say both date_col and formatted_date_col are not part of the select? Of course removing distinct makes the above work correctly but I need to keep distinct.</p>
",0,0,0,2025-07-02T18:39:49+00:00,2,74,True
79688269,29465719,,mysql,Multiple Select from same table,"<p>Tablename: <code>tbl_test</code></p>
<pre class=""lang-none prettyprint-override""><code>Code     |amount |Date   |Mos   |Remarks 
R1        300     1/7/25  Jul    Collect
R2        150     1/7/25  Jul    collect
R3        150     1/7/25  Jul    release    
R2        200     2/7/25  Jul    release
R3        200     3/7/25  Jul    release
R3        200     3/7/25  Jul    Cashout
R1        200     4/7/25  Jul    CashOut
</code></pre>
<p>I want to sum the amount and group by code</p>
<pre><code>select
    code
    , sum(amount) as ttlcollect from tbl_test where remarks = 'collect'
    , (select sum(amount)) from tbl_test where remarks = 'release') as ttlrelease
    , (select sum(amount)) from tbl_test where remarks = 'cashout') as ttlcashout
group by code
</code></pre>
<p>How can I achieve the output below?</p>
<pre class=""lang-none prettyprint-override""><code>CODE    |ttlCollect     |ttlrelease     |ttlcashout
R1      |300            |0              |200
R2      |150            |200            |0
R3      |0              |450            |200 
</code></pre>
",-1,0,1,2025-07-03T03:29:22+00:00,2,91,True
79688553,4273463,,mysql,Why the index is not used here?,"<p>MySQL version is 8.0.37. I have a table with this index.</p>
<pre><code>mysql&gt; show index from catalogue_product where Key_name='cl_pub_need_tags'\G
*************************** 1. row ***************************
        Table: catalogue_product
   Non_unique: 1
     Key_name: cl_pub_need_tags
 Seq_in_index: 1
  Column_name: product_class_id
    Collation: A
  Cardinality: 2
     Sub_part: NULL
       Packed: NULL
         Null: YES
   Index_type: BTREE
      Comment: 
Index_comment: 
      Visible: YES
   Expression: NULL
*************************** 2. row ***************************
        Table: catalogue_product
   Non_unique: 1
     Key_name: cl_pub_need_tags
 Seq_in_index: 2
  Column_name: is_public
    Collation: A
  Cardinality: 3
     Sub_part: NULL
       Packed: NULL
         Null: 
   Index_type: BTREE
      Comment: 
Index_comment: 
      Visible: YES
   Expression: NULL
*************************** 3. row ***************************
        Table: catalogue_product
   Non_unique: 1
     Key_name: cl_pub_need_tags
 Seq_in_index: 3
  Column_name: NULL
    Collation: A
  Cardinality: 5
     Sub_part: NULL
       Packed: NULL
         Null: YES
   Index_type: BTREE
      Comment: 
Index_comment: 
      Visible: YES
   Expression: cast(json_unquote(json_extract(`data`,_utf8mb4\'$.&quot;need_tags&quot;\')) as unsigned)
</code></pre>
<p>With this query the index is used:</p>
<pre><code>mysql&gt; explain SELECT distinct catalogue_product.id, is_public
  FROM catalogue_product
 INNER JOIN catalogue_productclass
    ON (product_class_id = catalogue_productclass.id)
 WHERE (catalogue_productclass.name = 'event' AND catalogue_product.is_public = 1 AND JSON_EXTRACT(data, '$.\&quot;need_tags\&quot;') = (1))
 LIMIT 1\G
    -&gt;     -&gt;     -&gt;     -&gt;     -&gt; *************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_productclass
   partitions: NULL
         type: ALL
possible_keys: PRIMARY
          key: NULL
      key_len: NULL
          ref: NULL
         rows: 3
     filtered: 33.33
        Extra: Using where; Using temporary
*************************** 2. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_is_public_1cf798c5,cl_pub_need_tags,start_index
          key: cl_pub_need_tags
      key_len: 15
          ref: metagrocery.catalogue_productclass.id,const,const
         rows: 16357
     filtered: 100.00
        Extra: NULL
</code></pre>
<p>But as soon as I try to order, the longer index stops being used, but instead it uses the short one for just one column:</p>
<pre><code>mysql&gt; explain SELECT distinct catalogue_product.id, is_public
  FROM catalogue_product
 INNER JOIN catalogue_productclass
    ON (product_class_id = catalogue_productclass.id)
 WHERE (catalogue_productclass.name = 'event' AND catalogue_product.is_public = 1 AND JSON_EXTRACT(data, '$.\&quot;need_tags\&quot;') = (1))
 order by is_public LIMIT 1\G
    -&gt;     -&gt;     -&gt;     -&gt;     -&gt; *************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_is_public_1cf798c5,cl_pub_need_tags,start_index
          key: catalogue_product_is_public_1cf798c5
      key_len: 1
          ref: const
         rows: 32715
     filtered: 10.00
        Extra: Using where; Using temporary
*************************** 2. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_productclass
   partitions: NULL
         type: eq_ref
possible_keys: PRIMARY
          key: PRIMARY
      key_len: 4
          ref: metagrocery.catalogue_product.product_class_id
         rows: 1
     filtered: 33.33
        Extra: Using where; Distinct
</code></pre>
<p>I understand the indexed column may be used for sorting, just need to put the order by columns in the same order as in the key?</p>
<p>Update:</p>
<pre><code>mysql&gt; explain SELECT * FROM `catalogue_product`  INNER JOIN `catalogue_productclass`     ON (`catalogue_product`.`product_class_id` = `catalogue_productclass`.`id`)  WHERE (`catalogue_productclass`.`name` = 'event' AND `catalogue_product`.`is_public` = 1 AND JSON_EXTRACT(`catalogue_product`.`data`, '$.\&quot;need_tags\&quot;') = (1)) order by product_class_id, is_public\G
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_is_public_1cf798c5,cl_pub_need_tags,start_index
          key: catalogue_product_is_public_1cf798c5
      key_len: 1
          ref: const
         rows: 32239
     filtered: 10.00
        Extra: Using where; Using temporary; Using filesort
*************************** 2. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_productclass
   partitions: NULL
         type: ALL
possible_keys: PRIMARY
          key: NULL
      key_len: NULL
          ref: NULL
         rows: 3
     filtered: 33.33
        Extra: Using where; Using join buffer (hash join)
</code></pre>
",1,1,0,2025-07-03T08:47:20+00:00,4,82,True
79689011,30097622,,mysql,How to query parts of a PHP variable separately?,"<p>I have an application with a search bar. With the code I have right now, if someone enters the value <code>book 1: conquest</code> into the search bar, the SQL query looks for a column value of ALL the words in the search bar.</p>
<p>But, if someone spells a word wrong in the search bar, or enters <code>Conquest</code> instead of <code>conquest</code>, they will get the dreaded <code>no results found</code> result. But, every other search bar I've came across doesn't suffer from this problem. Is it possible to change this?</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php
  define(&quot;DB_HOST&quot;, &quot;localhost&quot;);
  define(&quot;DB_NAME&quot;, &quot;officiallasereyeswebsite&quot;);
  define(&quot;DB_CHARSET&quot;, &quot;utf8&quot;);
  define(&quot;DB_USER&quot;, &quot;root&quot;);
  define(&quot;DB_PASSWORD&quot;, &quot;&quot;);
  $search = $_GET['search'];
  $pdo = new PDO(
    &quot;mysql:host=&quot;.DB_HOST.&quot;;charset=&quot;.DB_CHARSET.&quot;;dbname=&quot;.DB_NAME,
    DB_USER, DB_PASSWORD, [
      PDO::ATTR_ERRMODE =&gt; PDO::ERRMODE_EXCEPTION,
      PDO::ATTR_DEFAULT_FETCH_MODE =&gt; PDO::FETCH_ASSOC
    ]);

  $stmt = $pdo-&gt;prepare(&quot;SELECT * FROM `items` WHERE `item` LIKE ? OR `code` LIKE ?&quot;);
  $stmt-&gt;execute([&quot;%&quot;.$search.&quot;%&quot;, &quot;%&quot;.$search.&quot;%&quot;]);
  $results = $stmt-&gt;fetchAll();
  if (isset($_GET[&quot;ajax&quot;])) { echo json_encode($results); }
?&gt;
</code></pre>
<p>EDIT: This question is focusing on how to do this specifically with PHP. The other question is about how to do this in mysql. I am looking for a way with PHP.</p>
",-1,3,4,2025-07-03T14:28:39+00:00,1,206,True
79690194,7025628,,mysql,Always getting false while reading a fields from Mysql,"<p>I have this code:</p>
<pre><code>
@Service
@RequiredArgsConstructor
public class DeviceService {

  private static final LoggerWrapper logger = LoggerWrapper.getLogger(DeviceService.class);
  private final UserHistoryMobileDeviceRepository userHistoryMobileDeviceRepository;

  public List&lt;DeviceData&gt; getDeviceData(long userId) {

    logger.info(String.format(&quot;fetching device data for userId: %d&quot;, userId));
    return userHistoryMobileDeviceRepository.findByUserId(userId).stream()
        .map(currentRecord -&gt; {

          logger.info(String.format(&quot;Device from DB - deviceId: %s, isPrimary: %s&quot;,
              currentRecord.getDeviceUid(),
              currentRecord.getDeviceIsPrimary()));

          return DeviceData.builder()
              .deviceId(currentRecord.getDeviceUid())
              .isPrimary(Objects.equals(currentRecord.getDeviceIsPrimary(), IS_PRIMARY_Y))
              .build();
        })
        .toList();
  }
}
</code></pre>
<p>Entity class:</p>
<pre><code>import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.GeneratedValue;
import jakarta.persistence.GenerationType;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Builder.Default;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;

@Builder
@Entity
@Table(name = &quot;tbl_user_history_mobile_devices&quot;)
@Getter
@Setter
@AllArgsConstructor
@NoArgsConstructor
public class UserHistoryMobileDevice {

  @Id
  @GeneratedValue(strategy = GenerationType.IDENTITY)
  private Long id;

  @Column(name = &quot;fk_user_id&quot;)
  private long userId;

  @Column(name = &quot;device_uid&quot;)
  private String deviceUid;

  @Column(name = &quot;is_primary&quot;)
  @Default
  private String deviceIsPrimary = &quot;N&quot;;

}

</code></pre>
<p>The issue is this log:
<code>Device from DB - deviceId: %s, isPrimary: %s</code>
is always printing:
<code>&quot;message&quot;=&gt;&quot;Device from DB - deviceId: 02dxxxxx59axx, isPrimary: N&quot;}</code></p>
<p>even when the database has value <code>Y</code></p>
<p>Furthermore, the thing which is more astonishing is that it is working on non-prod environment while facing this issue only on prod env.</p>
<p>What I have done till now to resolve this:</p>
<ol>
<li>Checked mysql version on prod and non-prod</li>
<li>private String deviceIsPrimary was Boolean, thus changed it to String. And in code, changed in build from <code>.isPrimary(currentRecord.getDeviceIsPrimary()</code> to : <code>.isPrimary(Objects.equals(currentRecord.getDeviceIsPrimary(), IS_PRIMARY_Y))</code></li>
<li>Checked prod and non-prod env DDLs for this table. Here is the DDL for isPrimary on non-prod:
<code>is_primary</code> enum('Y','N') COLLATE utf8mb3_unicode_ci NOT NULL DEFAULT 'N',`</li>
</ol>
<p>and isPrimary on Prod:</p>
<pre><code># Field Type    Null    Key Default Extra

is_primary  enum('Y','N')   NO  MUL N   
</code></pre>
<p>Any lead are highly appreciable, I have already spent a lot of time on it and no success till now.</p>
",1,1,0,2025-07-04T12:37:23+00:00,1,109,False
79690741,12642829,,mysql,SQL capturing group: prioritizing possible?,"<p>Is it possible to order or prioritize something in a capturing group? Like this:</p>
<p><code>SELECT * IN table WHERE name = &quot;(f|o|d)(b|a|z).*&quot; ORDER BY ?;</code></p>
<p>so the result is ordered by the order of the characters inside the capturing groups.
Like:</p>
<pre><code>fbx
fawq
fz0
dz4242
obcrf
oaty
oz4r
</code></pre>
<p>I don't know in beforehand what characters (or strings) might be in the groups or how many groups there are, these are generated on the fly. So the leftmost element in each group has the highest priority. And then the next to the right and so on...</p>
",2,2,0,2025-07-05T01:01:09+00:00,3,148,True
79690790,15232943,,mysql,Why does SELECT ... FOR UPDATE with a subquery create a snapshot in MySQL but INSERT ... SELECT does not?,"<p>I'm trying to understand when a consistent snapshot (read view) is created in MySQL, especially under the REPEATABLE READ isolation level, which is the default for InnoDB.</p>
<p><strong>Scenario</strong><br />
I have two tables:</p>
<pre><code>CREATE TABLE table1 (  
  name VARCHAR(100) NOT NULL,  
  INDEX idx_table1_name (name)  
);  

CREATE TABLE table2 (  
  name VARCHAR(100) NOT NULL,  
  INDEX idx_table2_name (name)  
);  
</code></pre>
<p><strong>Case 1: SELECT ... FOR UPDATE with a subquery</strong></p>
<p><strong>Transaction 1:</strong></p>
<pre><code>START TRANSACTION;  
SELECT * FROM table1 
WHERE name = (
  SELECT name FROM table2 WHERE name = 'a'
) FOR UPDATE;
</code></pre>
<p><strong>Transaction 2:</strong></p>
<pre><code>START TRANSACTION;  
INSERT INTO table2 VALUES ('b');  
COMMIT;  
</code></pre>
<p><strong>Back in Transaction 1:</strong></p>
<pre><code>SELECT * FROM table2 WHERE name = 'b';   
-- returns EMPTY SET  
</code></pre>
<p>This shows that a snapshot was created at the time of the SELECT ... FOR UPDATE with subquery, since it cannot see the committed value 'b' from Transaction 2.</p>
<p><strong>Case 2: INSERT INTO ... SELECT with subquery</strong><br />
<strong>Transaction 1:</strong></p>
<pre><code>START TRANSACTION;   
INSERT INTO table1    
SELECT 'c' FROM table2 WHERE name = 'a';  
</code></pre>
<p><strong>Transaction 2:</strong></p>
<pre><code>START TRANSACTION;  
INSERT INTO table2 VALUES ('b');  
COMMIT;  
</code></pre>
<p><strong>Back in Transaction 1:</strong></p>
<pre><code>SELECT * FROM table2 WHERE name = 'b';  
-- returns 'b'  
</code></pre>
<p>This means no snapshot was created by the INSERT ... SELECT, and it can see the committed data from Transaction 2.
According to this page, a regular SELECT ... FOR UPDATE is treated as a DML operation rather than a regular SELECT.</p>
<ul>
<li><a href=""https://dev.mysql.com/doc/refman/8.4/en/innodb-consistent-read.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/refman/8.4/en/innodb-consistent-read.html</a></li>
</ul>
<p>Questions</p>
<ol>
<li>Why does SELECT ... FOR UPDATE with a subquery create a snapshot?</li>
<li>Why does INSERT INTO ... SELECT not create a snapshot in the same REPEATABLE READ isolation level?</li>
<li>Is this behavior documented anywhere in the MySQL official documentation?</li>
<li>Does the snapshot timing depend on the presence of subqueries in DML?</li>
</ol>
",4,4,0,2025-07-05T04:21:41+00:00,1,73,True
79691120,21316640,,mysql,How to write a SQL query to produce a result which has more than 2 levels of depth,"<p>tell me, is it possible to get all the data from 4 tables with one SQL query?</p>
<p><strong>Here is the DB diagram with connections:</strong>
<a href=""https://i.sstatic.net/wWP8rlY8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wWP8rlY8.png"" alt=""enter image description here"" /></a></p>
<p>Thus, a product can have 1 category, many configurations in which many options.</p>
<p><strong>I would like to get something like this result after the request:</strong></p>
<pre><code>{
  &quot;product_1&quot;: {
    &quot;name&quot;: &quot;product1&quot;,
    &quot;category&quot;: &quot;cat1&quot;,
    &quot;equipments&quot;: {
      &quot;0&quot;: {
        &quot;name&quot;: &quot;equip1&quot;,
        &quot;options&quot;: {
          &quot;options1&quot;: &quot;options 1&quot;,
          &quot;options2&quot;: &quot;options 2&quot;,
          &quot;options3&quot;: &quot;options 3&quot;
        }
      },
      &quot;1&quot;: {
        &quot;name&quot;: &quot;equip2&quot;,
        &quot;options&quot;: {
          &quot;options1&quot;: &quot;options 3&quot;,
          &quot;options2&quot;: &quot;options 2&quot;
        }
      }
    }
  },
  &quot;product_2&quot;: {
    &quot;name&quot;: &quot;product2&quot;,
    &quot;category&quot;: &quot;cat2&quot;,
    &quot;equipments&quot;: {
      &quot;0&quot;: {
        &quot;name&quot;: &quot;equip1&quot;,
        &quot;options&quot;: {
          &quot;options1&quot;: &quot;options 1&quot;,
          &quot;options2&quot;: &quot;options 3&quot;
        }
      },
      &quot;1&quot;: {
        &quot;name&quot;: &quot;equip2&quot;,
        &quot;options&quot;: {
          &quot;options1&quot;: &quot;options 3&quot;
        }
      }
    }
  }
}
</code></pre>
<p>Is it possible to get this with one query? I'm already tired of these Join and JSON_ARRAYAGG functions, it doesn't work, I tried to combine them, nothing... Maybe I'm wasting my time and should I use several queries? Or is it possible to get all the data for all the connections in one query? I'm writing in PHP using PDO. Please help me with my problem.</p>
<p><strong>UPD:</strong></p>
<p>Here is an example of what I tried, this is one of the options. Here I was able to get a value only from two tables &quot;Equipments&quot; and &quot;Options&quot;, then I tried to combine another JOIN and it doesn't work at all.</p>
<pre><code>$stmt = $link-&gt;prepare(&quot;select equipment.*, JSON_ARRAYAGG(JSON_OBJECT(
            'name', options.name,
            'descript', options.descript
            )) AS Options FROM equipment
            LEFT JOIN options ON equipment.id=options.equipment_id GROUP BY equipment.id
          &quot;);
</code></pre>
",2,3,1,2025-07-05T14:59:08+00:00,3,201,True
79691469,30959664,,mysql,How do I get my SELECT COUNT() statement to count and return the result rows that have no values?,"<p>I have a <code>SELECT</code> statement utilizing <code>JOIN</code> to combine two tables in MySQL. The query I'm attempting to complete is to combine the two tables, <code>INSTRUCTOR</code> and <code>SECTION</code>, and count the number of sections that each instructor has. My current query returns all instructors with the number of queries they have, if they at least one section. There are instructors in the <code>INSTRUCTOR</code> table that do not have any sections in the <code>SECTION</code> table and I need those to be displayed in the table as well. Below is the <code>SELECT</code> statement that I currently have:</p>
<pre><code>SELECT CONCAT(INSTRUCTOR.INSTRUCTOR_ID, ' ', INSTRUCTOR.FIRST_NAME, ' ', INSTRUCTOR.LAST_NAME) AS Instructor,
CONCAT(SUBSTRING(INSTRUCTOR.INSTRUCTOR_SSN, 1, 3), '-', SUBSTRING(INSTRUCTOR.INSTRUCTOR_SSN, 4, 2), '-', SUBSTRING(INSTRUCTOR.INSTRUCTOR_SSN, 6, 4)) AS 'Instructor SSN',
COUNT(SECTION.INSTRUCTOR_ID) AS '# of Sections'
FROM INSTRUCTOR
JOIN SECTION
ON INSTRUCTOR.INSTRUCTOR_ID = SECTION.INSTRUCTOR_ID
GROUP BY SECTION.INSTRUCTOR_ID, '# of Sections';
</code></pre>
<p>I have tried swapping the order of the tables in the statement as well as the different <code>JOIN</code> types.</p>
",0,0,0,2025-07-06T03:17:34+00:00,1,83,True
79694120,1351605,"Berkshire County, Massachusetts",mysql,TRIM in SELECT statement that works in Stand Alone Query Fails in Stored Procedure (MySQL),"<p>New to stored procedure development. I'm developing a stored procedure where I create and populate a table with the results of a query, where I get an identifier number (DCNumber), FirstName, LastName, MiddleName, NameSuffix (and other data) from a Department of Corrections database.</p>
<p>The chunk of stored procedure at issue is this:</p>
<pre><code>CREATE TABLE `full_record_valid_probation` 
        (id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,
        DCNumber VARCHAR(6), 
        LastName VARCHAR(50),
        FirstName VARCHAR(50),
        MiddleName VARCHAR(50), 
        NameSuffix VARCHAR(10), 
        Race VARCHAR(2), 
        Sex VARCHAR(2), 
        BirthDate VARCHAR(10), 
        SupervisionTerminationDate VARCHAR(10), 
        ReceiptDate VARCHAR(10), 
        supvtype_description VARCHAR(255), 
        facility_description VARCHAR(100), 
        supvstatus_description VARCHAR(255), 
        race_descr VARCHAR(50))
    SELECT t1.DCNumber, `t1`.`LastName`, `t1`.`FirstName`, `t1`.`MiddleName`, `t1`.`NameSuffix`, Race, Sex, BirthDate, SupervisionTerminationDate, ReceiptDate, supvtype_description, facility_description, supvstatus_description, race_descr 
    FROM `fl_doc`.`offender_root` t1
    INNER JOIN `fl_doc`.`dcnumbers_by_county_convicted` t2 ON t1.DCNumber = t2.DCNumber
    WHERE t1.SupervisionTerminationDate &lt;&gt; ''
    AND t1.FirstName &lt;&gt; '' AND t1.LastName &lt;&gt; ''
    AND t1.supvtype_description LIKE 'PROBATION%';
</code></pre>
<p>This works as is, except that I noticed some last names (LastName) don't sort correctly because they have a leading space in the name. The space was introduced at the source, but now I need to clean the values of leading and trailing spaces for my own use.</p>
<p>Ex:
<a href=""https://i.sstatic.net/rUyIdakZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUyIdakZ.png"" alt=""enter image description here"" /></a></p>
<p>The above table snippet is from the table after the stored procedure runs. Everything is fine, but you can see the last name &quot;JOHNSON&quot; comes first in a sort by LastName. That's because there is a leading space in &quot; JOHNSON&quot;.</p>
<p>Then I tried to use TRIM in the SELECT statement, like so:</p>
<pre><code>SELECT t1.DCNumber, trim(`t1`.`LastName`), trim(`t1`.`FirstName`), `t1`.`MiddleName`, `t1`.`NameSuffix`, Race, Sex, BirthDate, SupervisionTerminationDate, ReceiptDate, supvtype_description, facility_description, supvstatus_description, race_descr 
    FROM `fl_doc`.`offender_root` t1
    INNER JOIN `fl_doc`.`dcnumbers_by_county_convicted` t2 ON t1.DCNumber = t2.DCNumber
    WHERE t1.SupervisionTerminationDate &lt;&gt; ''
    AND t1.FirstName &lt;&gt; '' AND t1.LastName &lt;&gt; ''
    AND t1.supvtype_description LIKE 'PROBATION%';
</code></pre>
<p>This works in a stand alone query, but it doesn't work inside a stored procedure. When used in a stored procedure, it does this:</p>
<p><a href=""https://i.sstatic.net/O9X83aD1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O9X83aD1.png"" alt=""enter image description here"" /></a></p>
<p>As you can see in the field name header, instead of performing the TRIM operation on trim(LastName) and trim(FirstName), it reads these literally and makes field names called 'trim(FirstName)' and 'trim(LastName)'.</p>
<p>Is there a different approach to space removal when constructing a stored procedure?</p>
<p>Thanks ahead.</p>
",1,1,0,2025-07-08T10:56:05+00:00,2,73,True
79695636,5169301,,mysql,"&quot;Order By&quot; - Return One Match, Plus the Records Directly Previous and Following the Match","<p>I am using MySQL with PHP. I have a query which will always return exactly one match. I now want to also find the one row which directly precedes the matching row, and the row which is immediately after the match, when sorted by a certain field.</p>
<p>I know that I can run the query to return the whole table sorted by my sort field, then iterate over this return set, and find the three rows I want. Of course, this is pretty inefficient when the target row would be near the end of the result set. What is the best practice for this case?</p>
<p>The field I am sorting on is a varchar, so the sort is alphabetical. My only thought is to change the query to use &quot;LIKE&quot;, and match the first letter of the value of this field on my target row, and also the letters before and after this letter (all letters of the alphabet are represented in the data, so this will be sufficient). This would limit the result set, but still seems kind of hacky to me.</p>
<p>I know I need to handle the cases where my target row is either the first or last record in the sort, but I'm pretty sure I can figure that out on my own.</p>
<p>Clarifications:
I only need one field returned.
The answer provided by buzz8year worked for me.</p>
",0,1,1,2025-07-09T12:41:55+00:00,4,87,False
79695927,2331686,,mysql,mysql subquery unknown column in where clause,"<p>I have a query like below</p>
<pre><code>SELECT DISTINCT c.PkID, 
                c.CategoryName, 
                d.Name as Dep,
                (
                    SELECT SUM(total) 
                        FROM (
                               ( SELECT COUNT(e2.PkID) as total 
                                 FROM hc_events e2
                                 LEFT JOIN hc_eventcategories ec2 ON (e2.PkID = ec2.EventID)
                                 LEFT JOIN hc_eventcities ce2 ON (e2.PkID = ce2.EventID)
                                 LEFT JOIN hc_localize_level_0 v2 ON (ce2.CityID = v2.PkID) 
                                 WHERE ec2.CategoryID = c.PkID 
                                 AND d.PkID = v2.DepartementID 
                                 AND e2.StartDate &gt;= CURRENT_DATE() 
                                 AND e2.SeriesID IS NULL GROUP BY e.PkID
                                 )
                              UNION ALL
                               ( SELECT COUNT(DISTINCT e2.SeriesID) as total 
                                 FROM hc_events e2
                                 LEFT JOIN hc_eventcategories ec2 ON (e2.PkID = ec2.EventID)
                                 LEFT JOIN hc_eventcities ce2 ON (e2.PkID = ce2.EventID)
                                 LEFT JOIN hc_localize_level_0 v2 ON (ce2.CityID = v2.PkID) 
                                 WHERE ec2.CategoryID = c.PkID 
                                 AND d.PkID = v2.DepartementID 
                                 AND e2.StartDate &gt;= CURRENT_DATE() 
                                 AND e2.SeriesID IS NOT NULL GROUP BY e.SeriesID
                                )
                            ) as i
                ) as NbreEvenements, 
                v.Name as City
FROM hc_eventcategories ec
LEFT JOIN hc_categories c ON (ec.CategoryID = c.PkID)
LEFT JOIN hc_events e ON (e.PkID = ec.EventID)
LEFT JOIN hc_eventcities ce ON (e.PkID = ce.EventID)
LEFT JOIN hc_localize_level_0 v ON (ce.CityID = v.PkID) 
LEFT JOIN hc_localize_level_1 d ON (v.DepartementID = d.PkID) 
WHERE d.PkID = '75' 
AND e.StartDate &gt;= CURRENT_DATE() 
AND e.IsActive &gt;= 1 
AND e.IsApproved != 2 
AND v.Pays = 'FR'
</code></pre>
<p>It returns mysql error unknown column c.PkID</p>
<p>On my subquery i need to sum 2 querys, when i had just one it worked</p>
<p>Since i add the second with SUM(total) [...]</p>
<p>Nothing works</p>
",0,0,0,2025-07-09T15:49:47+00:00,0,26,False
79696006,31003935,,mysql,How to speed up MySQL query containing LEFT JOIN and GROUP BY with 4M+ rows?,"<p>I have the tables &quot;projects&quot; (362K rows) and &quot;projects_employees&quot; (4.27M rows), one-to-many. I'm trying to get aggregated data for every employee and it takes 6-7 seconds. Is there an opportunity to make it faster or it is the best one?</p>
<p>Example of tables with mock fields:</p>
<pre><code>CREATE TABLE `projects` (
    `id` int NOT NULL AUTO_INCREMENT,
    `client_id` int DEFAULT NULL,
    `manager_id` int DEFAULT NULL,
    `team_size` int DEFAULT NULL,
    `status_code` int DEFAULT NULL,
    `priority_level` int DEFAULT NULL,
    `risk_level` int DEFAULT NULL,
    `estimated_hours` int DEFAULT NULL,
    `actual_hours` int DEFAULT NULL,
    `remaining_hours` int DEFAULT NULL,
    `budget_cents` int DEFAULT NULL,
    `cost_cents` int DEFAULT NULL,
    `progress_percent` int DEFAULT NULL,
    `tasks_total` int DEFAULT NULL,
    `tasks_completed` int DEFAULT NULL,
    `bugs_found` int DEFAULT NULL,
    `bugs_fixed` int DEFAULT NULL,
    `meetings_held` int DEFAULT NULL,
    `files_uploaded` int DEFAULT NULL,
    `comments_posted` int DEFAULT NULL,
    `reviews_requested` int DEFAULT NULL,
    `approvals_received` int DEFAULT NULL,
    `escalations` int DEFAULT NULL,
    `feedback_score` int DEFAULT NULL,
    `archived` tinyint DEFAULT NULL,
    PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=363243 DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci

CREATE TABLE `projects_employees` (
    `id` int NOT NULL AUTO_INCREMENT,
    `project_id` int NOT NULL,
    `employee_id` int NOT NULL,
    `department_id` int DEFAULT NULL,
    `role_code` int DEFAULT NULL,
    `hours_allocated` int DEFAULT NULL,
    `hours_logged` int DEFAULT NULL,
    `is_active` tinyint DEFAULT '1',
    `joined_at` date DEFAULT NULL,
    `left_at` date DEFAULT NULL,
    PRIMARY KEY (`id`),
    UNIQUE KEY `idx_employee_project` (`employee_id`,`project_id`),
    KEY `project_id` (`project_id`),
    CONSTRAINT `projects_employees_ibfk_1` FOREIGN KEY (`project_id`) REFERENCES `projects` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=4325311 DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci
</code></pre>
<p>The query:</p>
<pre><code>-- It takes 6-7 seconds
    EXPLAIN SELECT
        projects_employees.employee_id,
        SUM(projects_employees.hours_allocated) as hours_allocated, 
        SUM(projects_employees.hours_logged) as hours_logged,
        SUM(projects.estimated_hours) as estimated_hours,
        SUM(projects.actual_hours) as actual_hours,
        SUM(projects.budget_cents) as budget_cents,
        SUM(projects.cost_cents) as cost_cents,
        SUM(projects.tasks_total) as tasks_total,
        SUM(projects.bugs_fixed) as bugs_fixed
    FROM projects_employees
    LEFT JOIN projects ON projects.id=projects_employees.project_id
    GROUP BY projects_employees.employee_id;
</code></pre>
<p><em>N.B.: the <code>LEFT</code> is not functionally desired, except without it it was even slower.</em></p>
<pre><code>    +----+-------------+--------------------+------------+--------+----------------------+----------------------+---------+-------------------------------------+---------+----------+-------------+
    | id | select_type | table              | partitions | type   | possible_keys        | key                  | key_len | ref                                 | rows    | filtered | Extra       |
    +----+-------------+--------------------+------------+--------+----------------------+----------------------+---------+-------------------------------------+---------+----------+-------------+
    |  1 | SIMPLE      | projects_employees | NULL       | index  | idx_employee_project | idx_employee_project | 8       | NULL                                | 4259544 |   100.00 | Using index |
    |  1 | SIMPLE      | projects           | NULL       | eq_ref | PRIMARY              | PRIMARY              | 4       | rates.projects_employees.project_id |       1 |   100.00 | NULL        |
    +----+-------------+--------------------+------------+--------+----------------------+----------------------+---------+-------------------------------------+---------+----------+-------------+
</code></pre>
<p>Noticed that:</p>
<pre><code>-- It takes 4.8-5.2 seconds
    EXPLAIN SELECT SUM(hours_allocated) a, SUM(hours_logged)
    FROM projects_employees
    GROUP BY projects_employees.employee_id;

-- It takes 4.11 seconds
    EXPLAIN SELECT projects_employees.id, projects.estimated_hours
    FROM projects_employees
    LEFT JOIN projects ON projects.id=projects_employees.project_id;
</code></pre>
<p>Some points:</p>
<ol>
<li>Resources: 6 CPU, 16GB RAM, 12GB innodb_buffer_pool_size</li>
<li>MySQL version 8.0.42, ENGINE InnoDB</li>
<li>Since we need data for a single year, both tables are already separated by years: 'projects_2024', 'projects_employees_2024', etc, in production</li>
<li>On the basic query, there are no filters by 'projects', so all rows should be retrieved. But there will be an opportunity to customize a query with filters by 'projects'</li>
</ol>
<p>UPD1.</p>
<pre><code>-- Via JOIN (8.7-9.3, the order in Explanation is different)
EXPLAIN SELECT
    projects_employees.employee_id,
    SUM(projects.estimated_hours) as estimated_hours,
    SUM(projects.actual_hours) as actual_hours,
    SUM(projects.budget_cents) as budget_cents,
    SUM(projects.cost_cents) as cost_cents,
    SUM(projects.tasks_total) as tasks_total,
    SUM(projects.bugs_fixed) as bugs_fixed
FROM projects_employees
JOIN projects ON projects.id=projects_employees.project_id
GROUP BY projects_employees.employee_id;

+----+-------------+--------------------+------------+------+---------------------------------+------------+---------+-------------------+--------+----------+-----------------+
| id | select_type | table              | partitions | type | possible_keys                   | key        | key_len | ref               | rows   | filtered | Extra           |
+----+-------------+--------------------+------------+------+---------------------------------+------------+---------+-------------------+--------+----------+-----------------+
|  1 | SIMPLE      | projects           | NULL       | ALL  | PRIMARY                         | NULL       | NULL    | NULL              | 360819 |   100.00 | Using temporary |
|  1 | SIMPLE      | projects_employees | NULL       | ref  | idx_employee_project,project_id | project_id | 4       | rates.projects.id |     11 |   100.00 | NULL            |
+----+-------------+--------------------+------------+------+---------------------------------+------------+---------+-------------------+--------+----------+-----------------+

-- Via STRAIGHT_JOIN (6-7seconds, the order in Explanation the same)
EXPLAIN SELECT
    projects_employees.employee_id,
    SUM(projects.estimated_hours) as estimated_hours,
    SUM(projects.actual_hours) as actual_hours,
    SUM(projects.budget_cents) as budget_cents,
    SUM(projects.cost_cents) as cost_cents,
    SUM(projects.tasks_total) as tasks_total,
    SUM(projects.bugs_fixed) as bugs_fixed
FROM projects_employees
STRAIGHT_JOIN projects ON projects.id=projects_employees.project_id
GROUP BY projects_employees.employee_id;

+----+-------------+--------------------+------------+--------+---------------------------------+----------------------+---------+-------------------------------------+---------+----------+-------------+
| id | select_type | table              | partitions | type   | possible_keys                   | key                  | key_len | ref                                 | rows    | filtered | Extra       |
+----+-------------+--------------------+------------+--------+---------------------------------+----------------------+---------+-------------------------------------+---------+----------+-------------+
|  1 | SIMPLE      | projects_employees | NULL       | index  | idx_employee_project,project_id | idx_employee_project | 8       | NULL                                | 4259544 |   100.00 | Using index |
|  1 | SIMPLE      | projects           | NULL       | eq_ref | PRIMARY                         | PRIMARY              | 4       | rates.projects_employees.project_id |       1 |   100.00 | NULL        |
+----+-------------+--------------------+------------+--------+---------------------------------+----------------------+---------+-------------------------------------+---------+----------+-------------+
</code></pre>
",5,5,0,2025-07-09T17:16:09+00:00,4,233,True
79696772,4314825,"Gdynia, Poland",mysql,AppsScript + JDBC + MySQL - retrieve DECIMAL value and add it to sheet as number,"<p>I have MySQL db which contains a stored procedure named <code>spYearlyTotals</code>.
The procedure accepts a year as a param and returns 13 columns. The first one is some string label, the latter ones are <code>DECIMAL(15,2)</code> which represent currency totals for the given label for each month.</p>
<p>I have an AppScript function which basically runs this procedure and displays the values in a google sheet:</p>
<pre><code>function displayYearlyTotals(ss) {
  let totalsTab = ss.getSheetByName(&quot;totals tab&quot;);
  let year = totalsTab.getRange(&quot;A3&quot;).getValue();

  let conn = Jdbc.getConnection(_dbUrl, _dbUsername, _dbPassword);  
  try {
    let stmt = conn.prepareStatement('CALL spYearlyTotals(?)');
    stmt.setInt(1, year);
    let results = stmt.executeQuery();

    let row = Array();
    let data = Array();
    
    while (results.next()) {
      row = [];
      row.push(results.getString(1)); // label
      for (let i = 1; i &lt;= 12; i++) {
        let amnt = results.getFloat(1 + i); // monthly values

        row.push(results.wasNull() ? null : amnt);
      }

      data.push(row);
    }

    totalsTab.getRange(3, 1, data.length, 13).setValues(data);
  }
  finally {
    conn.close();
  }

}
</code></pre>
<p>Using <code>getFloat</code> leads to small inaccuracies - decimal values are treated as floats which sometimes makes my values to differ by 0.01. This is not acceptable for me.
From the other hand - values are pushed to the range as numbers and google sheet formatting works fine for my output.</p>
<p>I have tried replacing <code>getFloat(i+1)</code> by <code>getString(i+1)</code> and <code>getBigDecimal(i+1).toFixed(2)</code>. Values are retrieved accurately but they are treated as strings - thus my formatting in the sheet is lost.</p>
<p>What is the proper way to handle <code>DECIMAL</code> columns so that they are correctly passed to the sheet as numbers?</p>
",0,0,0,2025-07-10T08:51:33+00:00,1,57,True
79696924,21687867,,mysql,Can we use the HAVING clause instead of WHERE to filter rows by a column like id in MySQL?,"<p>In MySQL, to fetch a record <code>where id = 1</code>, the common approach is:</p>
<pre><code>SELECT * FROM emp1 WHERE id = 1;
</code></pre>
<p>I’m curious whether it’s possible to use the <code>HAVING</code> clause for the same purpose, like:</p>
<pre><code>SELECT * FROM emp1 HAVING id = 1;
</code></pre>
<p>From what I understand, <code>HAVING</code> is usually used with <code>GROUP BY</code> to filter aggregated results. But in this case, I’m not using any aggregation or <code>GROUP BY</code>.</p>
",-2,0,2,2025-07-10T10:45:47+00:00,3,98,True
79697414,3010796,,mysql,"Catastrophic query planning, non-use of index in MySQL 8+","<p>I have a legacy Drupal 7 site that is used strictly internally. It's replacement is under active development, but in the meantime it is using Aurora2, MySQL 5.7 compatible and is in very expensive extended support.</p>
<p>I have tried multiple times to migrate the data to Aurora 3, MySQL 8.4, MySQL 9.3 and MariaDB 11.4 but in each case the performance degradation is catastrophic. I understand I'm not the only one displeased with the performance, but some queries take about 40,000 times longer. Not only is its optimization bad but it clearly is not using an available index.</p>
<p>Consider the following query to reveal the last 100 records edited:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
  n.nid,
  n.type,
  n.title,
  u.name,
  u.uid,
  n.changed AS date
FROM node n
JOIN node_revision nr ON n.vid = nr.vid
JOIN users u ON nr.uid = u.uid
ORDER BY n.changed DESC
LIMIT 100;
</code></pre>
<p>MySQL 5.7
100 rows in set (0.002 sec)</p>
<p>MySQL 9.3.0
100 rows in set (1 min 37.906 sec)</p>
<p>11.4.5-MariaDB
100 rows in set (1 min 23.105 sec)</p>
<p>8.0.mysql_aurora.3.09.0
100 rows in set (1 min 52.591 sec)</p>
<p>Here is the explain on the query, broken down by server type:</p>
<p>MySQL 5.7</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>partitions</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>filtered</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>n</td>
<td>NULL</td>
<td>index</td>
<td>vid</td>
<td>node_changed</td>
<td>4</td>
<td>NULL</td>
<td>100</td>
<td>100.00</td>
<td>Using where</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>nr</td>
<td>NULL</td>
<td>eq_ref</td>
<td>PRIMARY,uid</td>
<td>PRIMARY</td>
<td>4</td>
<td>db_id.n.vid</td>
<td>1</td>
<td>100.00</td>
<td>NULL</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>u</td>
<td>NULL</td>
<td>eq_ref</td>
<td>PRIMARY</td>
<td>PRIMARY</td>
<td>4</td>
<td>db_id.nr.uid</td>
<td>1</td>
<td>100.00</td>
<td>Using where</td>
</tr>
</tbody>
</table></div>
<p>MySQL 9.3.0
11.4.5-MariaDB
8.0.mysql_aurora.3.09.0</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>u</td>
<td>index</td>
<td>PRIMARY</td>
<td>name</td>
<td>182</td>
<td>NULL</td>
<td>74</td>
<td>Using index; Using temporary; Using filesort</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>nr</td>
<td>ref</td>
<td>PRIMARY,uid</td>
<td>uid</td>
<td>4</td>
<td>db_id.u.uid</td>
<td>5636</td>
<td>Using where; Using index</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>n</td>
<td>eq_ref</td>
<td>vid</td>
<td>vid</td>
<td>5</td>
<td>db_id.nr.vid</td>
<td>1</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>If I force it to use the node_changed index performance is still worse, but acceptable.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
  n.nid,
  n.type,
  n.title,
  u.name,
  u.uid,
  n.changed AS date
FROM node n
  FORCE INDEX (node_changed)
JOIN node_revision nr ON n.vid = nr.vid
JOIN users u ON nr.uid = u.uid
ORDER BY n.changed DESC
LIMIT 100;
</code></pre>
<p>MySQL 5.7
100 rows in set (0.002 sec)</p>
<p>MySQL 9.3.0
100 rows in set (0.022 sec)</p>
<p>11.4.5-MariaDB
100 rows in set (0.019 sec)</p>
<p>8.0.mysql_aurora.3.09.0
100 rows in set (0.010 sec)</p>
<p>MySQL 5.7
11.4.5-MariaDB</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>n</td>
<td>index</td>
<td>NULL</td>
<td>node_changed</td>
<td>4</td>
<td>NULL</td>
<td>100</td>
<td>Using where</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>nr</td>
<td>eq_ref</td>
<td>PRIMARY,uid</td>
<td>PRIMARY</td>
<td>4</td>
<td>db_id.n.vid</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>u</td>
<td>eq_ref</td>
<td>PRIMARY</td>
<td>PRIMARY</td>
<td>4</td>
<td>db_id.nr.uid</td>
<td>1</td>
<td>Using where</td>
</tr>
</tbody>
</table></div>
<p>MySQL 9.3.0
8.0.mysql_aurora.3.09.0</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>partitions</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>filtered</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>n</td>
<td>NULL</td>
<td>index</td>
<td>NULL</td>
<td>node_changed</td>
<td>4</td>
<td>NULL</td>
<td>100</td>
<td>100.00</td>
<td>Using where; Backward index scan</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>nr</td>
<td>NULL</td>
<td>eq_ref</td>
<td>PRIMARY,uid</td>
<td>PRIMARY</td>
<td>4</td>
<td>db_id.n.vid</td>
<td>1</td>
<td>100.00</td>
<td>NULL</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>u</td>
<td>NULL</td>
<td>eq_ref</td>
<td>PRIMARY</td>
<td>PRIMARY</td>
<td>4</td>
<td>db_id.nr.uid</td>
<td>1</td>
<td>100.00</td>
<td>Using where</td>
</tr>
</tbody>
</table></div>
<p>Some context:</p>
<ul>
<li>node is big: 5,343,344 rows</li>
<li>node_revision is bigger: 25,558,491 rows</li>
<li>users is not big: 74 rows</li>
<li>I migrated Aurora2 to Aurora3 using a blue-green deployment in AWS, migrations to MySQL and MariaDB were performed using MySQL dump restoration</li>
</ul>
<p>Based on similar questions/research I have attempted:</p>
<ul>
<li>optimizer_switch = prefer_ordering_index=on,derived_merge=off</li>
<li>Setting innodb_stats_persistent_sample_pages=1000;</li>
<li>converting legacy utf8mb3 to utf8mb4 on the tables in question</li>
<li>Running ANALYZE on the tables in question</li>
<li>creating, dropping and recreating an index on node.changed</li>
</ul>
<p>What I can't really do:</p>
<ul>
<li>Resolve this by rewriting queries - this is a CMS and though this query is a particular bad actor performance degradation is like this across the board</li>
</ul>
<p>If I can't resolve this it will just live on extended support for the forseeable future while the replacement is built on more modern tech.</p>
<p>It really seems there should be some configuration that I'm missing at the DB level. Any help finding that would be appreciated.</p>
<p>Thanks for you consideration</p>
<pre><code>CREATE TABLE `node` (
  `nid` int unsigned NOT NULL AUTO_INCREMENT COMMENT 'The primary identifier for a node.',
  `vid` int unsigned DEFAULT NULL COMMENT 'The current node_revision.vid version identifier.',
  `type` varchar(32) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `language` varchar(12) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `uid` int NOT NULL DEFAULT '0' COMMENT 'The users.uid that owns this node; initially, this is the user that created it.',
  `status` int NOT NULL DEFAULT '1' COMMENT 'Boolean indicating whether the node is published (visible to non-administrators).',
  `created` int NOT NULL DEFAULT '0' COMMENT 'The Unix timestamp when the node was created.',
  `changed` int NOT NULL DEFAULT '0' COMMENT 'The Unix timestamp when the node was most recently saved.',
  `comment` int NOT NULL DEFAULT '0' COMMENT 'Whether comments are allowed on this node: 0 = no, 1 = closed (read only), 2 = open (read/write).',
  `promote` int NOT NULL DEFAULT '0' COMMENT 'Boolean indicating whether the node should be displayed on the front page.',
  `sticky` int NOT NULL DEFAULT '0' COMMENT 'Boolean indicating whether the node should be displayed at the top of lists in which it appears.',
  `tnid` int unsigned NOT NULL DEFAULT '0' COMMENT 'The translation set id for this node, which equals the node id of the source post in each set.',
  `translate` int NOT NULL DEFAULT '0' COMMENT 'A boolean indicating whether this translation page needs to be updated.',
  PRIMARY KEY (`nid`),
  UNIQUE KEY `vid` (`vid`),
  KEY `node_changed` (`changed`),
  KEY `node_created` (`created`),
  KEY `node_frontpage` (`promote`,`status`,`sticky`,`created`),
  KEY `node_status_type` (`status`,`type`,`nid`),
  KEY `node_title_type` (`title`,`type`(4)),
  KEY `node_type` (`type`(4)),
  KEY `uid` (`uid`),
  KEY `tnid` (`tnid`),
  KEY `translate` (`translate`),
  KEY `language` (`language`),
  KEY `idx_node_created` (`created`),
  KEY `idx_node_created_desc` (`created` DESC)
) ENGINE=InnoDB AUTO_INCREMENT=5746465 DEFAULT CHARSET=utf8mb3 COMMENT='The base table for nodes.'

CREATE TABLE `node_revision` (
  `nid` int unsigned NOT NULL DEFAULT '0' COMMENT 'The node this version belongs to.',
  `vid` int unsigned NOT NULL AUTO_INCREMENT COMMENT 'The primary identifier for this version.',
  `uid` int NOT NULL DEFAULT '0' COMMENT 'The users.uid that created this version.',
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `log` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `timestamp` int NOT NULL DEFAULT '0' COMMENT 'A Unix timestamp indicating when this version was created.',
  `status` int NOT NULL DEFAULT '1' COMMENT 'Boolean indicating whether the node (at the time of this revision) is published (visible to non-administrators).',
  `comment` int NOT NULL DEFAULT '0' COMMENT 'Whether comments are allowed on this node (at the time of this revision): 0 = no, 1 = closed (read only), 2 = open (read/write).',
  `promote` int NOT NULL DEFAULT '0' COMMENT 'Boolean indicating whether the node (at the time of this revision) should be displayed on the front page.',
  `sticky` int NOT NULL DEFAULT '0' COMMENT 'Boolean indicating whether the node (at the time of this revision) should be displayed at the top of lists in which it appears.',
  PRIMARY KEY (`vid`),
  KEY `nid` (`nid`),
  KEY `uid` (`uid`)
) ENGINE=InnoDB AUTO_INCREMENT=43684354 DEFAULT CHARSET=utf8mb3 COMMENT='Stores information about each saved version of a node.'
</code></pre>
<p>Explain analyze:</p>
<blockquote>
<p>-&gt; Limit: 100 row(s)  (actual time=94688..94689 rows=100 loops=1)
-&gt; Sort: n.<code>changed</code> DESC, limit input to 100 row(s) per chunk (actual time=94688..94689 rows=100 loops=1)<br />
-&gt; Stream results  (cost=206946 rows=456804) (actual time=7476..93266 rows=5.34e+6 loops=1)<br />
-&gt; Nested loop inner join  (cost=206946 rows=456804) (actual time=7476..90610 rows=5.34e+6 loops=1)<br />
-&gt; Nested loop inner join  (cost=45982 rows=456804) (actual time=2.16..17600 rows=25.6e+6 loops=1)<br />
-&gt; Covering index scan on u using name  (cost=7.65 rows=74) (actual time=0.0427..0.159 rows=74 loops=1)<br />
-&gt; Filter: (nr.uid = u.uid)  (cost=12.3 rows=6173) (actual time=0.105..212 rows=345385 loops=74)<br />
-&gt; Covering index lookup on nr using uid (uid=u.uid)  (cost=12.3 rows=6173) (actual time=0.104..177 rows=345385 loops=74)<br />
-&gt; Single-row index lookup on n using vid (vid=nr.vid)  (cost=0.252 rows=1) (actual time=0.00272..0.00273 rows=0.209 loops=25.6e+6) 1 row in set (1 min 34.705 sec)</p>
</blockquote>
",4,4,0,2025-07-10T17:27:12+00:00,2,172,False
79697654,16817823,,mysql,Make repeated expressions in a select query more efficient,"<p>I am trying to do a time series analysis project to learn more and test my own skills. For this time series analysis, I want to show moving averages and percent differences over the periods, daily, monthly, and annually.</p>
<p>I am doing this by making a temporary table containing a LAG function to get values to do the arithmetic for the moving averages and percent differences. I believe the issue of efficiency is from referencing the table too many times in the select query, and that's causing it to take forever to compile. Along with a large case function that is repeated three times. I am not the most familiar with SQL, but is there any way I can use recursion or make a function to call upon for a repeated expression?  Any help to make this more efficient is greatly appreciated!
I am also using MySQL for this project.</p>
<p>Code attached below.</p>
<pre><code>WITH debt_lag AS(
         SELECT *, LAG(`Debt Held by the Public`, 7) OVER (ORDER BY `Record Date`)   AS week_b4_dhpb,
                 LAG(`Debt Held by the Public`, 7) OVER (ORDER BY `Record Date`)   AS month_b4_dhpb,
                 LAG(`Debt Held by the Public`, 365) OVER (ORDER BY `Record Date`) AS year_b4_dhpb,
                 LAG(`Intragovernmental Holdings`, 7) OVER (ORDER BY `Record Date`)   AS week_b4_ig,
                 LAG(`Intragovernmental Holdings`, 7) OVER (ORDER BY `Record Date`)   AS month_b4_ig,
                 LAG(`Intragovernmental Holdings`, 365) OVER (ORDER BY `Record Date`) AS year_b4_ig,
                 LAG(`Total Public Debt Outstanding`, 7) OVER (ORDER BY `Record Date`)   AS week_b4_tdpo,
                 LAG(`Total Public Debt Outstanding`, 7) OVER (ORDER BY `Record Date`)   AS month_b4_tdpo,
                 LAG(`Total Public Debt Outstanding`, 365) OVER (ORDER BY `Record Date`) AS year_b4_tdpo

          FROM DebtPenny_19930401_20250623
)

SELECT `Record Date`, (((`Debt Held by the Public` - week_b4_dhpb) / week_b4_dhpb )* 100) AS DHPB_percent_change_weekly,
      ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),3) AS DHPB_WEEKLY_MOVING_AVERAGE,
       (((`Debt Held by the Public` - month_b4_dhpb) / month_b4_dhpb )* 100) AS DHPB_percent_change_monthly, 
CASE
    WHEN SUBSTRING(`Record Date`,6,2) = '01' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '02' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 27 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '03' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '04' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '05' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '06' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '07' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '08' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '09' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '10' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '11' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '12' THEN ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    
END AS DHPB_MONTHLY_MOVING_AVERAGE,
       (((`Debt Held by the Public` - year_b4_dhpb) / year_b4_dhpb )* 100) AS DHPB_percent_change_annual,
       ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 364 PRECEDING AND CURRENT ROW),3) AS DHPB_ANNUAL_MOVING_AVERAGE,
       

       (((`Intragovernmental Holdings` - week_b4_ig) / week_b4_ig )* 100) AS IG_percent_change_weekly,
      ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),3) AS IG_WEEKLY_MOVING_AVERAGE,
       (((`Intragovernmental Holdings` - month_b4_ig) / month_b4_ig)* 100) AS IG_percent_change_monthly, 
CASE
    
    WHEN SUBSTRING(`Record Date`,6,2) = '01' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '02' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 27 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '03' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '04' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '05' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '06' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '07' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '08' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '09' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '10' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '11' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '12' THEN ROUND(AVG(`Intragovernmental Holdings`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    
END AS IG_MONTHLY_MOVING_AVERAGE,

       (((`Debt Held by the Public` - year_b4_ig) / year_b4_ig )* 100) AS IG_percent_change_annual,
       ROUND(AVG(`Debt Held by the Public`) OVER (ORDER BY `Record Date` ROWS BETWEEN 364 PRECEDING AND CURRENT ROW),3) AS IG_ANNUAL_MOVING_AVERAGE,


       (((`Total Public Debt Outstanding` - week_b4_tdpo) / week_b4_tdpo )* 100) AS TDPO_percent_change_weekly,
      ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),3) AS TDPO_WEEKLY_MOVING_AVERAGE,
       (((`Total Public Debt Outstanding` - month_b4_ig) / month_b4_ig)* 100) AS TDPO_percent_change_monthly, 
CASE
    
    WHEN SUBSTRING(`Record Date`,6,2) = '01' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '02' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 27 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '03' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '04' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '05' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '06' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '07' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '08' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '09' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '10' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '11' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),3)
    WHEN SUBSTRING(`Record Date`,6,2) = '12' THEN ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 30 PRECEDING AND CURRENT ROW),3)
    
END AS TDPO_MONTHLY_MOVING_AVERAGE,

       (((`Total Public Debt Outstanding` - year_b4_tdpo) / year_b4_tdpo )* 100) AS TDPO_percent_change_annual,
       ROUND(AVG(`Total Public Debt Outstanding`) OVER (ORDER BY `Record Date` ROWS BETWEEN 364 PRECEDING AND CURRENT ROW),3) AS TDPO_ANNUAL_MOVING_AVERAGE
     
FROM debt_lag;
</code></pre>
",1,1,0,2025-07-10T22:32:09+00:00,2,107,True
79698324,16537052,,mysql,How can I insert data into virtual json columns in mysql?,"<p>How can I insert data into virtual json columns in mysql?
I am creating a table that has a jsonData field and a virtual id column.</p>
<pre><code>CREATE TABLE table_json(
    JsonData JSON NOT NULL,
    id INT GENERATED ALWAYS AS (JsonData-&gt;&gt;'$.id') VIRTUAL NOT NULL,
    UNIQUE KEY id(id)
)ENGINE=InnoDB;
</code></pre>
<p>After creation, there is not a single &quot;Json&quot; document in the table.</p>
<p>I am creating an insert query :</p>
<pre><code>INSERT INTO `table_json`(`JsonData`, `id`)
VALUES (JSON_OBJECT('id', 87, 'name', 'carrot'))
</code></pre>
<p>But I get an error:</p>
<blockquote>
<p>#1136 - The number of columns does not match the number of values in record 1</p>
</blockquote>
<p>Если я создаю запрос insert:</p>
<pre><code>INSERT INTO `table_json`(`JsonData`, `id`) 
VALUES (JSON_OBJECT('id', 87, 'name', 'carrot'), 01)
</code></pre>
<p>But I get an error:</p>
<blockquote>
<p>#3105 - The value specified for generated column 'id' in table 'table_json' is not allowed.</p>
</blockquote>
<p>How do I insert my json document into this table if there are virtual generated columns?</p>
",-2,0,2,2025-07-11T12:19:24+00:00,1,47,False
79698797,25474027,,mysql,Dynamically loading row header,"<p>Here is the task description for MySQL:</p>
<p>Table called clients with columns id, mac_address</p>
<p>Table called streams with columns client_id, title, quality (Enum : 720p, 1080p, 2160p, 4320p), traffic</p>
<p>The user has provided an image containing a request to create a query for an online streaming service. The query should return a list of total stream traffic in HD and UHD quality for each client.</p>
<p>Here's a breakdown of the requirements:</p>
<p><strong>Output Columns:</strong></p>
<pre><code>quality: Quality group name (&quot;HD&quot; or &quot;UHD&quot;).

%client_1%...%client_N%: Traffic data for a specific client.

Column name format: MAC address of the client.

Columns sorted in ascending order by MAC address.

Record format: ## (##) where:

First ##: Total traffic amount for a specific quality group, rounded to two decimal places, with unit (MiB or GiB).

Second ## (in parentheses): Total number of streams for a specific quality group and client.
</code></pre>
<p><strong>Definitions:</strong></p>
<p>UHD quality group: 2160p and above.</p>
<p>1 MiB = 1024^2 bytes.</p>
<p>1 GiB = 1024^3 bytes.</p>
<p><strong>Expected Output</strong></p>
<pre><code> quality    | 9F-A1-2C-BD-55-77 | AB-4E-79-10-AC-22 | CF-91-08-DE-43-A2
---------------------------------------------------------------------
HD         | 2.11 GiB (2)      | 3.76 GiB (5)      | 1.89 GiB (3)
UHD        | 6.23 GiB (4)      | 975.45 MB (2)     | 4.12 GiB (3)
</code></pre>
<p>I am not clear how to select the mac_address as part of row header, till now I worked only on how to select the columns as row values.</p>
<p>What is the correct way to solve this problem?</p>
",-1,0,1,2025-07-11T20:23:40+00:00,1,67,False
79699068,2978063,,mysql,How to know the character set use in mysql column row event,"<p>I have written a python program to parse mysql binlog.  In the Query Event, there is a Q_CHARSET_CODE which indicate what character set is used for the logged query.  This indicated that the logged query is based on the client connection character set.</p>
<p>But in Row Base Events, the Insert Row, Update Row, and Delete Row, I cannot find out what character set is used in the log for each corresponding column?  Was is based on the client connection character set or it is based on the column character set in the Information Schema.</p>
",0,0,0,2025-07-12T06:44:39+00:00,1,75,True
79699937,28516124,,mysql,How to Track Full Update History of TPT Entities in EF Core with MySQL,"<p>I'm designing a .NET Core application using EF Core with a MySQL database, following a Table-per-Type (TPT) inheritance strategy.</p>
<p>I have a base Report class with several derived types such as QualityReport, MaintenanceReport, SafetyReport, etc.
As expected with TPT, each derived type maps to its own table.</p>
<p>I want to implement a custom audit system that logs every update to any Report or its derived types. showing the old and new values when a record is updated, or deleted.</p>
<p>I know that SQL Server supports system-versioned (temporal) tables for this kind of use case, but since I'm using MySQL, that option isn't available.</p>
<p>What path should I follow to implement a reliable, property-level audit system that captures all changes across TPT entities in EF Core with MySQL?</p>
<pre><code>// Base report class with polymorphism
[Table(&quot;Reports&quot;)]
[JsonPolymorphic(TypeDiscriminatorPropertyName = &quot;Type&quot;)]
[JsonDerivedType(typeof(QualityReport), &quot;qualityReport&quot;)]
[JsonDerivedType(typeof(MaintenanceReport), &quot;maintenanceReport&quot;)]
[JsonDerivedType(typeof(SafetyReport), &quot;safetyReport&quot;)]
public abstract class Report
{
    [Key]
    public int Id { get; set; }

    [Required]
    public DateTime CreatedDate { get; set; }

    [Required]
    public DateTime UpdatedDate { get; set; }

    [Required, MaxLength(200)]
    public string Title { get; set; } = null!;

    [Required, MaxLength(100)]
    public string CreatedBy { get; set; } = null!;

    [MaxLength(100)]
    public string? UpdatedBy { get; set; }
}
</code></pre>
<pre><code>public class QualityReport : Report
{
    public string QualityMetric { get; set; }
    public string InspectorName { get; set; }
    public DateTime InspectionDate { get; set; }
    public string Summary { get; set; }
    public int SeverityLevel { get; set; }
}
</code></pre>
",1,3,2,2025-07-13T11:53:46+00:00,1,105,True
79701000,17346443,,mysql,MySQL: Soft Delete + Unique Constraint,"<p>I understand that soft deletion is useful for preserving data history and avoiding the need to manually handle child records.
However, soft deletion can cause issues with unique constraints.
For example, if the <code>email</code> column in the users table is unique, I can’t reuse <code>email</code> if it already exists in a soft-deleted row.</p>
<pre><code>CREATE TABLE users (
  id INTEGER NOT NULL AUTO_INCREMENT,
  email VARCHAR(50) NOT NULL,
  deleted_at TIMESTAMP(0) NULL,

  UNIQUE INDEX `users_id_unique` (`id`),
  UNIQUE INDEX `users_email_unique` (`email`),
  PRIMARY KEY (`id`)
);
</code></pre>
<p>What are the best practices or efficient solutions for handling unique constraints alongside soft deletion in MySQL?
Also, does Prisma provide any built-in support or recommended workaround for this scenario?</p>
",2,2,0,2025-07-14T14:01:01+00:00,2,226,True
79701532,945871,,mysql,mysql proper API call,"<p>The page at <a href=""https://dev.mysql.com/doc/c-api/9.2/en/mysql-use-result.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/c-api/9.2/en/mysql-use-result.html</a> states:</p>
<blockquote>
<p>When using <code>mysql_use_result()</code>, you must execute <code>mysql_fetch_row()</code> until a <code>NULL</code> value is returned, otherwise, the unfetched rows are returned as part of the result set for your next query. The C API gives the error <code>Commands out of sync; you can't run this command now</code> if you forget to do this!</p>
</blockquote>
<p>However I plan to use another call to MYSQL on another thread using different MYSQL result set.</p>
<p>Will this work properly? Documentation does not say anything in this regards...</p>
",4,4,0,2025-07-15T03:53:06+00:00,1,84,True
79702241,9321849,,mysql,Vert.x mysql timeouts,"<p>I have these 2 verticles in my java application in vert.x that are supposed to be running in multiple batches.</p>
<pre><code>public class TestVerticle4   extends AbstractVerticle{
    private static final Logger logger = LoggerFactory.getLogger(TestVerticle4.class);
    
    @Override
    public void start(Promise&lt;Void&gt; startPromise) {
        try {
            MySQLConnectOptions connectOptions = new MySQLConnectOptions()
                    .setPort(Integer.parseInt(config().getString(&quot;sqlPort&quot;)))
                    .setHost(config().getString(&quot;host&quot;))
                    .setDatabase(config().getString(&quot;database&quot;))
                    .setUser(config().getString(&quot;user&quot;))
                    .setPassword(config().getString(&quot;password&quot;))
                    .setConnectTimeout(30000) // timeout if more than 30 second query
                    .setIdleTimeout(300)
                    .setConnectTimeout(5000)
                    .setReconnectInterval(2000)
                    ;
            
            PoolOptions poolOptions = new PoolOptions().setMaxSize(60);
            MySQLPool client = MySQLPool.pool(vertx,connectOptions,poolOptions);
            
            CircuitBreakerOptions cbOptions = new CircuitBreakerOptions()
                    .setMaxRetries(3) // Retry 3 times
                    .setResetTimeout(5000) // Reset timeout (ms) after success
                    .setTimeout(1000) // Operation timeout (ms)
                    .setFallbackOnFailure(true); // Enable fallback on failure
            CircuitBreaker breaker = CircuitBreaker.create(&quot;TestVerticle7&quot;, vertx, cbOptions);


            
            
            logger.info(&quot;Step3:------------ Recieving at Verticle4 &quot;);
            logger.info(&quot;Recieving at task.runVerticle4....&quot;);
            EventBus bus =vertx.eventBus();
            bus.&lt;JsonObject&gt;consumer(&quot;task.runVerticle4&quot;,msg-&gt;{
                JsonObject bodyCon = msg.body();
                //String symbol = bodyCon.getString(&quot;symbol&quot;);
                Long hashcode=bodyCon.getLong(&quot;hashcode&quot;);
                Long minId= bodyCon.getLong(&quot;minId&quot;);
                Long maxId= bodyCon.getLong(&quot;maxId&quot;);
                Long prodId=bodyCon.getLong(&quot;prodId&quot;);
                breaker.execute(promise -&gt; {
                client.getConnection(
                        ar-&gt;{
                            if(ar.succeeded()) {
                                SqlConnection conn =ar.result();
                                conn.preparedQuery(insertQueryEvent())
                                .execute(Tuple.of(hashcode,prodId,minId,maxId),res-&gt;{
                                    if(res.succeeded()) {
                                        logger.info(&quot;Successfully Inserted Batch for custIds {} to {} for product {} for hashcode {}&quot;,minId,maxId,prodId,hashcode);
                                    }else {
                                        logger.error(&quot;Failed to execute &quot;+res.cause().getMessage());
                                    }
                                    conn.close();
                                });
                                
                            }else {
                                logger.error(&quot;Failed to connect to db&quot;+ ar.cause().getMessage());
                            }
                        }
                ); //
                }).onComplete(ar -&gt; {
                    if (ar.succeeded()) {
                        // Handle the result after successful execution or fallback
                        System.out.println(&quot;Query executed successfully (or fallback used)&quot;);
                    } else {
                        System.err.println(&quot;Query failed after retries: &quot; + ar.cause().getMessage());
                    }
                });
                
            });         
            
            startPromise.complete();
        }catch(Exception e) {
            e.printStackTrace();
            startPromise.fail(e);
        }
    }
    
    public static String insertQueryEvent() {
        return &quot;  INSERT IGNORE INTO ResearchCall_HashQueue(Customer_Id,Product_Id,HashCode)  SELECT Customer_Id,Reco_Product_1, ?  FROM  RankedPickList_Stories WHERE Reco_Product_1= ?  AND Customer_Id BETWEEN  ? AND ?  AND Story_Id=3;&quot;;
    }
}

</code></pre>
<p>and</p>
<pre><code>public class TestVerticle7  extends AbstractVerticle{
private static final Logger logger = LoggerFactory.getLogger(TestVerticle7.class);
    
    @Override
    public void start(Promise&lt;Void&gt; startPromise) {
        try {
            MySQLConnectOptions connectOptions = new MySQLConnectOptions()
                    .setPort(Integer.parseInt(config().getString(&quot;sqlPort&quot;)))
                    .setHost(config().getString(&quot;host&quot;))
                    .setDatabase(config().getString(&quot;database&quot;))
                    .setUser(config().getString(&quot;user&quot;))
                    .setPassword(config().getString(&quot;password&quot;))
                    .setConnectTimeout(30000)
                    .setIdleTimeout(300)
                    .setConnectTimeout(5000)
                    .setReconnectInterval(2000)
                    ;
            
            
            PoolOptions poolOptions = new PoolOptions().setMaxSize(60);
            MySQLPool client = MySQLPool.pool(vertx,connectOptions,poolOptions);
            
            CircuitBreakerOptions cbOptions = new CircuitBreakerOptions()
                    .setMaxRetries(3) // Retry 3 times
                    .setResetTimeout(5000) // Reset timeout (ms) after success
                    .setTimeout(1000) // Operation timeout (ms)
                    .setFallbackOnFailure(true); // Enable fallback on failure
            CircuitBreaker breaker = CircuitBreaker.create(&quot;TestVerticle7&quot;, vertx, cbOptions);

            
            
            logger.info(&quot;Recieving to task.runVerticle7....&quot;);
            EventBus bus =vertx.eventBus();
            bus.&lt;JsonObject&gt;consumer(&quot;task.runVerticle7&quot;,msg-&gt;{
                JsonObject msg_body = msg.body();
                Long minId= msg_body.getLong(&quot;minId&quot;);
                Long maxId= msg_body.getLong(&quot;maxId&quot;);
                Long prodId= msg_body.getLong(&quot;prodId&quot;);
                Long hashcode= msg_body.getLong(&quot;hashcode&quot;);
                breaker.execute(promise -&gt; {
                client.getConnection(
                        ar-&gt;{
                            if(ar.succeeded()) {
                                SqlConnection conn =ar.result();
                                
                                conn.preparedQuery(updateProductArray())
                                .execute(Tuple.of(minId,maxId,prodId,hashcode),res-&gt;{
                                    if(res.succeeded()) {
                                        logger.info(&quot;Successfull Updated product array to Customer_ResearchCall_Master for Cust_Ids between {} and {}&quot;,minId,maxId);
                                    }else {
                                        logger.error(&quot;Failed to  update product array to Customer_ResearchCall_Master for Cust_Ids between {} and {}&quot;,minId,maxId+res.cause().getMessage());
                                    }
                                    conn.close();
                                });
                                
                                // Run ResearchCall_ProductResearchCallMapping
                                
                                conn.preparedQuery(updateProductJson())
                                .execute(Tuple.of(minId,maxId),res-&gt;{
                                    if(res.succeeded()) {
                                        logger.info(&quot;Successfull Updated product_Json  to Customer_ResearchCall_Master&quot;);
                                    }else {
                                        logger.error(&quot;Failed to  update product_json to Customer_ResearchCall_Master&quot;+res.cause().getMessage());
                                    }
                                    conn.close();
                                });
                                
                            }else {
                                logger.error(&quot;Failed to connect to db&quot;+ ar.cause().getMessage());
                            }
                        }
                        );  //
                }).onComplete(ar -&gt; {
                    if (ar.succeeded()) {
                        // Handle the result after successful execution or fallback
                        System.out.println(&quot;Query executed successfully (or fallback used)&quot;);
                    } else {
                        System.err.println(&quot;Query failed after retries: &quot; + ar.cause().getMessage());
                    }
                });
            });
            
            startPromise.complete();
        }catch(Exception e) {
            startPromise.fail(e);
        }
    }
    
    
    public static String updateProductArray() {
        return &quot;CALL  ResearchCall_UpdateProductArray(?,?,?,?);&quot;;
    }
    
    public static String updateProductJson() {
        return &quot;CALL  ResearchCall_UpdateProductJson(?,?);&quot;;
    }
}

</code></pre>
<p>Both verticles have MySQL queries/procedures that are supposed to execute.</p>
<p>I have put up a circuit breaker is supposed to do multiple tries of this SQL query even when it fails.</p>
<p>The issue with this is that the logs I am getting following errors in the logs like below</p>
<pre><code>2025-07-15 19:18:27 [vert.x-eventloop-thread-3] ERROR jarBot.TestVerticle4 - Failed to connect to dbTimeout
2025-07-15 19:18:27 [vert.x-eventloop-thread-3] ERROR jarBot.TestVerticle4 - Failed to connect to dbTimeout
2025-07-15 19:18:27 [vert.x-eventloop-thread-3] ERROR jarBot.TestVerticle4 - Failed to connect to dbTimeout
2025-07-15 19:18:27 [vert.x-eventloop-thread-3] ERROR jarBot.TestVerticle4 - Failed to connect to dbTimeout
2025-07-15 19:18:46 [vert.x-eventloop-thread-6] ERROR jarBot.TestVerticle7 - Failed to  update product array to Customer_ResearchCall_Master for Cust_Ids between 1 and 100000{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
2025-07-15 19:18:47 [vert.x-eventloop-thread-3] ERROR jarBot.TestVerticle4 - Failed to execute{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
2025-07-15 19:18:47 [vert.x-eventloop-thread-3] ERROR jarBot.TestVerticle4 - Failed to execute{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
2025-07-15 19:19:37 [vert.x-eventloop-thread-6] ERROR jarBot.TestVerticle7 - Failed to  update product_json to Customer_ResearchCall_Master{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
2025-07-15 19:19:48 [vert.x-eventloop-thread-6] ERROR jarBot.TestVerticle7 - Failed to  update product array to Customer_ResearchCall_Master for Cust_Ids between 400001 and 500000{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
2025-07-15 19:19:57 [vert.x-eventloop-thread-6] ERROR jarBot.TestVerticle7 - Failed to  update product_json to Customer_ResearchCall_Master{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
2025-07-15 19:19:57 [vert.x-eventloop-thread-6] ERROR jarBot.TestVerticle7 - Failed to  update product_json to Customer_ResearchCall_Master{errorMessage=Lock wait timeout exceeded; try restarting transaction, errorCode=1205, sqlState=HY000}
</code></pre>
<p>My MySQL DB Details look like following :</p>
<p><strong>MAX_CONNECTIONS=1000</strong></p>
<p><strong>innodb_buffer_pool_size=107374182400</strong></p>
<p><strong>max_connections=1000</strong></p>
<p><strong>max_allowed_packet=1073741824</strong></p>
<p><strong>wait_timeout=28800</strong></p>
<p><strong>interactive_timeout=28800</strong></p>
<p><strong>net_read_timeout=30</strong></p>
<p><strong>net_write_timeout=60</strong></p>
<p>MySQL tables are indexed and have keys and partitioned for 100k which is also the batch size of insertion and updates , but this should not be a problem since I am trying multiple retries</p>
<p>What am I doing wrong to cause so many timeouts despite retries.</p>
<p>Any help is appreciated</p>
",0,0,0,2025-07-15T14:35:42+00:00,0,76,False
79705653,1513485,,mysql,MySQL REGEXP for multiple (special) characters,"<p>Basically I have the following statement (that works) ...</p>
<pre><code>SELECT * FROM `my_table` WHERE `my_field` REGEXP '[\[|,]24[\]|,]';
</code></pre>
<p>My string data is ...</p>
<pre><code>[24,1,2] =&gt; FOUND
[1,24,2] =&gt; FOUND
[1,2,24] =&gt; FOUND
[1,2,3] =&gt; NOT FOUND
...
</code></pre>
<p>The &quot;24&quot; will of course change into other numbers. The following statement didn't work.</p>
<pre><code>SELECT * FROM `my_table` WHERE `my_field` REGEXP '[\[|,]24[,|\]]';
</code></pre>
<ol>
<li>It returns an empty result set. Why doesn't it work?</li>
<li>I tested my code with <a href=""https://regex101.com/"" rel=""nofollow noreferrer"">https://regex101.com/</a>. There is used ...</li>
</ol>
<pre><code>(\[|,)24(\]|,)
(\[|,)24(,|\])
</code></pre>
<p>and all versions worked. MySQL cannot use &quot;(&quot; and &quot;)&quot; ?</p>
",-2,1,3,2025-07-18T03:47:55+00:00,2,123,False
79709473,20929534,,mysql,MySQL Query Returning Incorrect Value,"<p>I am using two tables:  Income and Expenses, extracting subtotals from each.</p>
<ul>
<li>When I sum the rent.income column in a mysql query I get 10150, which is correct (7 incomes of 1450 = 10150).</li>
<li>When I sum the cost.expenses column in the same mysql query (UNION), I get 4576.51, which is also correct (195 expenses totaling 4576.51).</li>
<li>But when I try to determine the difference between these two totals (for a grand total), I get 1947214.43, which is incorrect.</li>
</ul>
<pre class=""lang-sql prettyprint-override""><code>     SELECT
        (SELECT sum(rent) FROM income),
        (SELECT sum(cost) FROM expenses),
        sum(rent)-sum(cost) 
     FROM income, expenses
</code></pre>
<p>How can I get a <code>sum(rent)-sum(cost)</code> of 5573.49?</p>
",-4,1,5,2025-07-21T18:08:32+00:00,3,103,True
79710284,4299809,France,mysql,MySQL Select from selected schemas names,"<p>I have several schemas in my MySQL db with the same structure, each concerned schema has a similar name (let's say concerned_schemas_XX) and each one contains a table named &quot;object&quot;.</p>
<p>I want to retreive a list of the first line of each concerned_shema_XX.object where the field iobjid is -1</p>
<p>My first attempt is to get the list of all concerned schemas by querying the &quot;SCHEMATA&quot;, but when I try to select from the selected list of table names, I just get the table list itself, and not the data in each concerned table.</p>
<pre><code>USE INFORMATION_SCHEMA;
SELECT T.* FROM (
    SELECT CONCAT( `SCHEMA_NAME`, '.object' ) FROM `SCHEMATA` WHERE `SCHEMA_NAME` LIKE &quot;concerned_schemas_%&quot;
) as T
WHERE T.iobjid = -1
</code></pre>
<p>So the error is :
Error Code: 1054. Unknown column 'T.iobjid' in 'where clause'</p>
<p>An idea of how this could be possible ?</p>
",0,0,0,2025-07-22T10:23:14+00:00,0,31,False
79711547,29282840,,mysql,Check a MySQL 8 database table to see if a user has logged,"<p>I need to check a MySQL 8 database table to see if a user has logged in for at least two days in January of the current year.</p>
<p>If there are at least two days recorded, I update a second MySQL table in the same database with the value OK, otherwise with the value KO.</p>
<p>This is my sproc.</p>
<p>I have this error because in fact the dates on which the user <em>foo</em> logged in are 2, the days:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>tDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-05</td>
</tr>
<tr>
<td>2025-01-02</td>
</tr>
</tbody>
</table></div>
<blockquote>
<p>Procedure execution failed</p>
<p>1172 - Result consisted of more than one row</p>
<p>Time: 0,518s</p>
</blockquote>
<pre><code>CREATE DEFINER=`root`@`%` PROCEDURE `dCreate_20250723`( )
BEGIN
    
DECLARE retval INT;
  
FLUSH HOSTS;
    
SET @s = CONCAT('SELECT DISTINCT COUNT(B.tDate) INTO @n 
             FROM `dotable_1` B 
             WHERE 
             YEAR ( B.tDate ) = 2025 
             AND MONTH ( B.tDate ) = 1 
             AND B.tUsers = ''foo''
             GROUP BY DATE(B.tDate);');

FLUSH HOSTS; 

PREPARE `stmt` FROM @`s`;
SET @`s` := NULL;
EXECUTE `stmt`;
DEALLOCATE PREPARE `stmt`;
     
SET retval := NULL;
 
SET retval = @n;     
 
IF retval &gt;= 2 THEN
        
UPDATE `dotable_2` SET tCheck = &quot;OK&quot; WHERE tUsers = 'foo';
     
ELSE

UPDATE `dotable_2` SET tCheck = &quot;KO&quot; WHERE tUsers = 'foo';  
    
END IF;
     
SET retval := NULL; 

END
</code></pre>
",3,3,0,2025-07-23T08:34:29+00:00,2,74,True
79711592,15445261,,mysql,KWIC in Text database,"<p>In a text database every word of the text is in a single row an has it's unique Id, which determines the sequence. The DB is static, the text will not be changed, no insertions or deletions will be made. So it looks like this:</p>
<p>Tablename = &quot;text&quot;</p>
<pre><code>Id | Word
---------
1   | The
2   | texts
3   | articles
4   | and
5   | conversations
6   | are
7   | brief
8   | and
9   | appropriate
10  | for
11  | all
... | ...
</code></pre>
<p>Now, the user should be able to do a KWIC (keyword in context) search, the result should be, e.g. when the user searches for &quot;conversations&quot; and context is 2 (that is 2 words to the left and 2 word to the right) the result should be &quot;articles and conversations are brief&quot;.</p>
<p>When the user searches for a word, which occurs multiple times, they should get multiple rows as a result, e.g. if they search for &quot;and&quot; in the given example and the context is 2 again, they should get both &quot;texts articles and conversations are&quot; and &quot;are brief and appropriate for&quot;.</p>
<p>So far I have tried this:</p>
<pre><code>SELECT * FROM text 
   WHERE Id 
       BETWEEN ((SELECT Id FROM text WHERE Word='conversations') - 2) 
           AND ((SELECT Id FROM text WHERE Word='conversations') + 2)
</code></pre>
<p>This works fine, but only if the searched word occurs only once in the text. (Which is  the case with &quot;conversations&quot;.) If the search term occurs multiple times in the text (e.g. if &quot;conversations&quot; in the query is substituted with &quot;and&quot;), it crashes, probably because the result of the inner SELECTs in this case is not a single Id, but a list of IDs. I would appreciate if somebody could give me advice on how to deal with multiple occurrences of the search term. Thank you!</p>
<p>---- Edit -----</p>
<p>Thank you, Dale, for the hint with LEAD and LAG! I've tried this together with the CONCAT function and the query now looks like this:</p>
<pre><code>SELECT Word, temp FROM
     (SELECT Word, CONCAT ( 
          LAG(Word, 1) OVER (ORDER BY Id), 
          ' &lt;B&gt;', Word, '&lt;/B&gt; ', 
          LEAD(Word, 1) OVER (ORDER BY Id)) 
          AS temp FROM text) t 
          WHERE Word='and';
</code></pre>
<p>It now gives the results &quot;articels <strong>and</strong> conversations&quot; and &quot;brief <strong>and</strong> appropriate&quot; as expected. Furthermore, the searched word (&quot;and&quot;) can be bolded, which is a great byproduct.
Now, I am stuck with the problem, how to expand the range from one single word left and right of the searched word to a another number, eg. 10 or max. 20. This number should be determined by the user in the final application. Is there a way, how this could be accomplished?</p>
",4,4,0,2025-07-23T09:03:59+00:00,2,199,True
79712124,1107821,"Dublin, Ireland",mysql,How do I query the relationship of a pivot table using Laravel Eloquent or DB query builder?,"<p>I'm working on an application that accepts various payment methods via different payment providers.</p>
<p>One of the payment methods is Mobile Money. Mobile Money is accepted in a number of different countries and I have a settings table that holds the different mobile networks and currencies for each country.</p>
<p>All countries only accept one currency but can have different mobile networks.</p>
<p>For example, Ireland could have two different mobile networks and one currency.</p>
<p>My database structure is as follows:</p>
<h4><code>payment_methods</code></h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Mobile Money</td>
</tr>
</tbody>
</table></div>
<h4><code>countries</code></h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>iso_code</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Ireland</td>
<td>IE</td>
</tr>
<tr>
<td>2</td>
<td>UK</td>
<td>UK</td>
</tr>
</tbody>
</table></div>
<h4><code>country_payment_method</code></h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>country_id</th>
<th>payment_method_id</th>
<th>is_active</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>true</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>1</td>
<td>true</td>
</tr>
</tbody>
</table></div>
<h4><code>mobile_networks</code></h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Eir</td>
</tr>
<tr>
<td>2</td>
<td>Three</td>
</tr>
<tr>
<td>3</td>
<td>02</td>
</tr>
</tbody>
</table></div>
<h4><code>currencies</code></h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>code</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Euro</td>
<td>EUR</td>
</tr>
<tr>
<td>2</td>
<td>Pound</td>
<td>GBP</td>
</tr>
</tbody>
</table></div>
<h4><code>payment_method_configurations</code></h4>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>country_payment_method_id</th>
<th>config_key</th>
<th>config_value</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>mobile_network_id</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>mobile_network_id</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>currency_id</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
<td>mobile_network_id</td>
<td>2</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>mobile_network_id</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
<td>currency_id</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>My question is, how do I access the configurations for a country/payment_method using either Eloquent relationship or DB query if I know the country?</p>
",2,2,0,2025-07-23T14:49:44+00:00,2,103,True
79712911,5911998,"Goa, India",mysql,Error 1045 (28000): Access denied for user &#39;&#39;@&#39;localhost&#39; (using password: NO) exit status 1,"<p>I am trying to implement the tutorial from <a href=""https://go.dev/doc/tutorial/database-access"" rel=""nofollow noreferrer"">https://go.dev/doc/tutorial/database-access</a> on windows 11 machine</p>
<p>I have the set variables using the following code in my workspace terminal</p>
<pre><code>set DBUSER=root
set DBPASS=root
</code></pre>
<p>I access these credetials</p>
<pre><code>cnf.User = os.Getenv(&quot;DBUSER&quot;)
cnf.Passwd = os.Getenv(&quot;DBPASS&quot;)
</code></pre>
<p>I tried uing <code>setx</code> also but same error</p>
",0,1,1,2025-07-24T07:09:07+00:00,1,65,True
79713642,130028,Portugal,mysql,C3P0 Test Connection On Checkout NOT working,"<p>Any ideas why I'm getting &quot;invalid&quot; connections from the pool?</p>
<p>Using:</p>
<pre><code>mysql-connector-java-5.1.49.jar
c3p0-0.11.2.jar
Java 8
</code></pre>
<p>C3P0 config:</p>
<pre><code>pool.setJdbcUrl(url);
pool.setUser(user);
pool.setPassword(password);
pool.setTestConnectionOnCheckout(true);
</code></pre>
<p>Still, after a period of inactivity, the MySQL connection returned from the pool is invalid!</p>
<p>I get a NEW connection in every database method, to ensure that the pool does the underlying job.</p>
<p>This is the error LOG from the app:</p>
<pre><code>Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 44.587.068 milliseconds ago.  The last packet sent successfully to the server was 44.587.069 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.
    jul 23 14:54:19 dev-server services[811]:         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    jul 23 14:54:19 dev-server services[811]:         at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    jul 23 14:54:19 dev-server services[811]:         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    jul 23 14:54:19 dev-server services[811]:         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.Util.handleNewInstance(Util.java:403)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3706)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2506)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2675)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2465)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4776)
    jul 23 14:54:19 dev-server services[811]:         ... 6 more
    jul 23 14:54:19 dev-server services[811]: Caused by: java.net.SocketException: Túnel quebrado (Write failed)
    jul 23 14:54:19 dev-server services[811]:         at java.net.SocketOutputStream.socketWrite0(Native Method)
    jul 23 14:54:19 dev-server services[811]:         at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
    jul 23 14:54:19 dev-server services[811]:         at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
    jul 23 14:54:19 dev-server services[811]:         at sun.security.ssl.SSLSocketOutputRecord.deliver(SSLSocketOutputRecord.java:341)
    jul 23 14:54:19 dev-server services[811]:         at sun.security.ssl.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:1106)
    jul 23 14:54:19 dev-server services[811]:         at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
    jul 23 14:54:19 dev-server services[811]:         at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
    jul 23 14:54:19 dev-server services[811]:         at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3688)
</code></pre>
",0,0,0,2025-07-24T15:52:24+00:00,0,35,False
79713969,469367,,mysql,Mysql Stored Procedure Parameter getting rounded,"<p>The following stored procedure is converting the <code>thisAmount</code> parameter, which for example I provide 2387.97, rounds to 2388, but it is declared in the column, and within the parameter list to be a decimal, and finally I tried to cast as a decimal, and still no luck.  I can't figure this out.  I have searched and searched and tried many different options, but to no avail. The procedure works, except for the rounding issue. This isn't the first time this has happened, so I figured I would ask someone with more knowledge than me. Any help would be greatly appreciated.</p>
<pre><code>    CREATE DEFINER=`myUserName`@`%` PROCEDURE `billing_addOCIPTrueUp`(IN `thisQuoteID` BIGINT,IN `thisAmount` DECIMAL, IN `thisTable` BLOB, IN `thisDetailTable` BLOB)
BEGIN
    
    SET @thisAmount = CAST(thisAmount AS DECIMAL(10,2));
    SET @newJobName = (SELECT JobName FROM Quotes WHERE QuoteID = thisQuoteID);
    
    SET @q = CONCAT('INSERT INTO ', thisTable, ' (QuoteID, ClientID, PrimaryClientContactID, PrimaryJobContactID, ParentQuoteID, QuoteDate, BidDueDate, DateSent, QuoteTotal, JobName, `Status`, ApprovalStatus, PmtStatus, WrapType, QuoteType, Retention, ClosingDay)

    SELECT ', 
        thisTable, '.QuoteID + 1 AS QuoteID,', 
        thisTable, '.ClientID,',  
        thisTable, '.PrimaryClientContactID,', 
        thisTable, '.PrimaryJobContactID,',  
        thisTable, '.ParentQuoteID, 
        DATE(NOW()) AS QuoteDate,',  
        thisTable, '.BidDueDate,',  
        thisTable, '.DateSent,',
        @thisAmount, ' AS QuoteTotal, &quot;',
        @newJobName, ' y||y Insurance Deduct&quot; AS JobName, 
        3 AS `Status`, 
        2 AS ApprovalStatus,',  
        thisTable, '.PmtStatus,',  
        thisTable, '.WrapType, 
        14 AS QuoteType,',  
        thisTable, '.Retention,',  
        thisTable, '.ClosingDay
    FROM
        ', thisTable, '
    WHERE
        ', thisTable, '.QuoteID LIKE &quot;', thisQuoteID,'00%&quot;
    ORDER BY
        ', thisTable, '.QuoteID DESC
    LIMIT 1;
    
    INSERT INTO ', thisDetailTable, '(QuoteID, Heading, Description, Qty, Rate, Total, Misc, Variable, Color, Production, PlanPageNumber, DetailPageNumber, DetailNumber, Draw, DrawApproved, Measured, Mat, Cut, Fab, Coat, Delivered, Installed, ItemType, ItemTypeStyle, InQueue)VALUES(
            (
                SELECT IFNULL((SELECT '
                , thisTable, '.QuoteID + 1 AS QuoteID
                FROM ',
                    thisTable, 
                ' WHERE
                    ', thisTable,'.QuoteID LIKE &quot;', thisQuoteID, '10%&quot;
                ORDER BY
                    ', thisTable,'.QuoteID DESC
                LIMIT 1), ', thisQuoteID, '1001)
            ),
            &quot;OCIP Insurance Deduction&quot;,
            &quot;&quot;,
            1,',
            thisAmount, ',',
            thisAmount, ',
            0.00,
            0.00,
            &quot;n/a&quot;,
            0,
            &quot;&quot;,
            &quot;&quot;,
            &quot;&quot;,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            100,
            NULL,
            0       
            )');
            
            SELECT CAST(@q AS CHAR(10000) CHARACTER SET utf8) AS text;
            
            #PREPARE stmt FROM @q;
            #EXECUTE stmt;
            #DEALLOCATE PREPARE stmt;

END

CREATE DEFINER=`tigtech_admin`@`%` PROCEDURE `billing_addOCIPTrueUp`(IN `thisQuoteID` BIGINT,IN `thisAmount` DECIMAL, IN `thisTable` BLOB, IN `thisDetailTable` BLOB)
BEGIN
    
    SET @thisAmount = CAST(thisAmount AS DECIMAL(10,2));
    SET @newJobName = (SELECT JobName FROM Quotes WHERE QuoteID = thisQuoteID);
    
    SET @q = CONCAT('INSERT INTO ', thisTable, ' (QuoteID, ClientID, PrimaryClientContactID, PrimaryJobContactID, ParentQuoteID, QuoteDate, BidDueDate, DateSent, QuoteTotal, JobName, `Status`, ApprovalStatus, PmtStatus, WrapType, QuoteType, Retention, ClosingDay)

    SELECT ', 
        thisTable, '.QuoteID + 1 AS QuoteID,', 
        thisTable, '.ClientID,',  
        thisTable, '.PrimaryClientContactID,', 
        thisTable, '.PrimaryJobContactID,',  
        thisTable, '.ParentQuoteID, 
        DATE(NOW()) AS QuoteDate,',  
        thisTable, '.BidDueDate,',  
        thisTable, '.DateSent,',
        @thisAmount, ' AS QuoteTotal, &quot;',
        @newJobName, ' y||y Insurance Deduct&quot; AS JobName, 
        3 AS `Status`, 
        2 AS ApprovalStatus,',  
        thisTable, '.PmtStatus,',  
        thisTable, '.WrapType, 
        14 AS QuoteType,',  
        thisTable, '.Retention,',  
        thisTable, '.ClosingDay
    FROM
        ', thisTable, '
    WHERE
        ', thisTable, '.QuoteID LIKE &quot;', thisQuoteID,'00%&quot;
    ORDER BY
        ', thisTable, '.QuoteID DESC
    LIMIT 1;
    
    INSERT INTO ', thisDetailTable, '(QuoteID, Heading, Description, Qty, Rate, Total, Misc, Variable, Color, Production, PlanPageNumber, DetailPageNumber, DetailNumber, Draw, DrawApproved, Measured, Mat, Cut, Fab, Coat, Delivered, Installed, ItemType, ItemTypeStyle, InQueue)VALUES(
            (
                SELECT IFNULL((SELECT '
                , thisTable, '.QuoteID + 1 AS QuoteID
                FROM ',
                    thisTable, 
                ' WHERE
                    ', thisTable,'.QuoteID LIKE &quot;', thisQuoteID, '10%&quot;
                ORDER BY
                    ', thisTable,'.QuoteID DESC
                LIMIT 1), ', thisQuoteID, '1001)
            ),
            &quot;OCIP Insurance Deduction&quot;,
            &quot;&quot;,
            1,',
            thisAmount, ',',
            thisAmount, ',
            0.00,
            0.00,
            &quot;n/a&quot;,
            0,
            &quot;&quot;,
            &quot;&quot;,
            &quot;&quot;,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            100,
            NULL,
            0       
            )');
            
            #PREPARE stmt FROM @q;
            #EXECUTE stmt;
            #DEALLOCATE PREPARE stmt;

END
</code></pre>
",1,2,1,2025-07-24T21:12:40+00:00,1,51,True
79715432,7129252,,mysql,MySQL Specify Multiple Search Conditions for One Column,"<p>table1</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Printer</th>
<th>Tray</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Printer_1</td>
<td>Tray1</td>
<td>0</td>
</tr>
<tr>
<td>Printer_1</td>
<td>Tray2</td>
<td>0</td>
</tr>
<tr>
<td>Printer_1</td>
<td>Tray3</td>
<td>0</td>
</tr>
<tr>
<td>Printer_2</td>
<td>Tray1</td>
<td>0</td>
</tr>
<tr>
<td>Printer_2</td>
<td>Tray2</td>
<td>50</td>
</tr>
<tr>
<td>Printer_2</td>
<td>Tray3</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>I want to get a result where Printer's Tray 1, 2, 3 are 0 percentage. Here is my code.</p>
<pre><code>SELECT Printer 
FROM table1 
WHERE Percentage = '0' 
and Tray in 'Tray1','Tray2','Tray3'`;
</code></pre>
<p>Which will return Printer_1 and Printer_2, please help...</p>
",0,0,0,2025-07-26T05:18:24+00:00,1,66,False
79715963,31151566,,mysql,“CryptographicException: Chain instance is empty.” error when connecting a Unity server in a Docker container to a MySQL server on a Linux VM,"<p>I have a Linux Virtual Machine that I rent. On that Linux VM I have installed a MySQL server with two users, besides the root one, &quot;unity@127.0.0.1&quot; and &quot;unity2@localhost&quot;.  I also have Docker installed on that Linux VM.</p>
<p>I am going to use Docker containers as servers for my multiplayer game. I have already successfully  deployed a Unity server in a Docker container and sent data to a client on my PC.</p>
<p>Now my last step is to connect that Unity server in a Docker container directly to the MySQL server that runs on the same Linux VM. To do so I have installed MySQL through NuGet for Unity.
<a href=""https://i.sstatic.net/oMwqcBA4.png"" rel=""nofollow noreferrer"">Screenshot of MySQL through NuGet for Unity</a></p>
<p>Here is my code that I use to connect to the MySQL server. I use Debug.Log() to see where exactly my code errors, which is number 3, meaning &quot;dbcon.Open();&quot; fails.</p>
<pre><code>using System;
using System.Data;
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using MySql.Data.MySqlClient;
using Unity.Netcode;

public class MySQLScript : MonoBehaviour
{
    
    void Start()
    {
          MySQLFunction();
    }

    [Rpc(SendTo.Server)]
    public void MySQLFunction()
    {
        Debug.Log(&quot;MySQL Function START&quot;);
        string connectionString =
           &quot;Server=127.0.0.1;&quot; +
           &quot;Database=unity_data;&quot; +
           &quot;UserID=unity;&quot; +
           &quot;Password=&lt;not gonna type my pass in public&gt;;&quot; +
           &quot;Pooling=false&quot;;
           Debug.Log(&quot;1&quot;);

        IDbConnection dbcon;
        Debug.Log(&quot;2&quot;);
        dbcon = new MySqlConnection(connectionString);
        Debug.Log(&quot;3&quot;);
        dbcon.Open();
        Debug.Log(&quot;4&quot;);
        IDbCommand dbcmd = dbcon.CreateCommand();
        Debug.Log(&quot;5&quot;);

        string sql =
            &quot;SELECT userid, username &quot; +
            &quot;FROM unity_table&quot;;
        Debug.Log(&quot;6&quot;);
        dbcmd.CommandText = sql;
        Debug.Log(&quot;7&quot;);
        IDataReader reader = dbcmd.ExecuteReader();
        Debug.Log(&quot;8&quot;);

        while (reader.Read())
        {
            Debug.Log(&quot;READ Start&quot;);
            string UserId = (string)reader[&quot;userid&quot;];
            string UserName = (string)reader[&quot;username&quot;];
            Debug.Log(&quot;userid: &quot; + UserId + &quot;username: &quot; + UserName);
            Debug.Log(&quot;READ End&quot;);
        }

        Debug.Log(&quot;9&quot;);
        reader.Close();
        Debug.Log(&quot;10&quot;);
        reader = null;
        Debug.Log(&quot;11&quot;);
        dbcmd.Dispose();
        Debug.Log(&quot;12&quot;);
        dbcmd = null;
        Debug.Log(&quot;13&quot;);
        dbcon.Close();
        Debug.Log(&quot;14&quot;);
        dbcon = null;
        Debug.Log(&quot;MySQL Function END&quot;);
    }
}
</code></pre>
<p>Code is taken from the following site: <a href=""https://www.mono-project.com/docs/database-access/providers/mysql/"" rel=""nofollow noreferrer"">https://www.mono-project.com/docs/database-access/providers/mysql/</a>
I tried code from the official website: <a href=""https://www.nuget.org/packages/mysql.data/"" rel=""nofollow noreferrer"">https://www.nuget.org/packages/mysql.data/</a>
But I have the same error.</p>
<p>I also tried to change the user from &quot;unity&quot; to &quot;unity2&quot; and server address that I use from &quot;127.0.0.1&quot; to &quot;localhost&quot; but both of them give the same error.</p>
<p>I use this command to start my Docker container with the Unity server build inside:</p>
<pre><code>docker run --network host -p 7777:7777/udp private_repository_name:tag_name
</code></pre>
<p>I use &quot;--network host&quot; because as I understand this makes the container run as a localhost, which is what I think need. The port is 7777 as this is the udp port that the Unity Transport has in the Unity server build.</p>
<p>Here are the error logs from the Linux VM after I start the docker container with my Unity server that tries to connect to the MySQL server:</p>
<pre><code>CryptographicException: Chain instance is empty.
  at Mono.Unity.UnityTlsContext.ProcessHandshake () [0x00047] in &lt;6297cf863591482c869f99d22a768126&gt;:0
  at Mono.Net.Security.MobileAuthenticatedStream.ProcessHandshake (Mono.Net.Security.AsyncOperationStatus status, System.Boolean renegotiate) [0x000da] in &lt;6297cf863591482c869f99d22a768126&gt;:0
  at (wrapper remoting-invoke-with-check) Mono.Net.Security.MobileAuthenticatedStream.ProcessHandshake(Mono.Net.Security.AsyncOperationStatus,bool)
  at Mono.Net.Security.AsyncHandshakeRequest.Run (Mono.Net.Security.AsyncOperationStatus status) [0x00006] in &lt;6297cf863591482c869f99d22a768126&gt;:0
  at Mono.Net.Security.AsyncProtocolRequest.ProcessOperation (System.Threading.CancellationToken cancellationToken) [0x000fc] in &lt;6297cf863591482c869f99d22a768126&gt;:0
Rethrow as AuthenticationException: Authentication failed, see inner exception.
  at Mono.Net.Security.MobileAuthenticatedStream.ProcessAuthentication (System.Boolean runSynchronously, Mono.Net.Security.MonoSslAuthenticationOptions options, System.Threading.CancellationToken cancellationToken) [0x00262] in &lt;6297cf863591482c869f99d22a768126&gt;:0
  at MySql.Data.Common.Ssl.StartSSLAsync (System.IO.Stream baseStream, System.Text.Encoding encoding, System.String connectionString, System.Threading.CancellationToken cancellationToken, System.Boolean execAsync) [0x003c2] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySql.Data.MySqlClient.NativeDriver.OpenAsync (System.Boolean execAsync, System.Threading.CancellationToken cancellationToken) [0x006c7] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySql.Data.MySqlClient.Driver.OpenAsync (System.Boolean execAsync, System.Threading.CancellationToken cancellationToken) [0x00092] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySql.Data.MySqlClient.Driver.CreateAsync (MySql.Data.MySqlClient.MySqlConnectionStringBuilder settings, System.Boolean execAsync, System.Threading.CancellationToken cancellationToken) [0x00152] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySql.Data.MySqlClient.Driver.CreateAsync (MySql.Data.MySqlClient.MySqlConnectionStringBuilder settings, System.Boolean execAsync, System.Threading.CancellationToken cancellationToken) [0x0020b] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySql.Data.MySqlClient.MySqlConnection.OpenAsync (System.Boolean execAsync, System.Threading.CancellationToken cancellationToken) [0x00592] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySql.Data.MySqlClient.MySqlConnection.Open () [0x00012] in &lt;e5b4169d41fc4f35b3d1470d22ebf230&gt;:0
  at MySQLScript.MySQLFunction () [0x0003a] in C:----\Assets\MySQL\MySQLScript.cs:46
  at MySQLScript.Start () [0x00001] in C:----\Assets\MySQL\MySQLScript.cs:19
</code></pre>
<p>I have already double checked if my mysql 3306 port is included in the ufw firewall and tried updating my certificates on the Linux VM by doing this:</p>
<pre><code>update-ca-certificates
apt-get install -y ca-certificates
certmgr -list -c Trust
cp -R /usr/share/.mono/certs/Trust/* ~/.config/.mono/certs/Trust/
</code></pre>
<p>I am not that good with Sys Admin stuff and the Unity Engine, so I don't know where to go from here. Would appreciate any help or general directions where I should look to figure this out.</p>
<p>Edit:</p>
<p>My Dockerfile:</p>
<pre><code>FROM ubuntu:latest
WORKDIR /app
COPY . .
EXPOSE 7777
CMD [&quot;./Build_exp1.x86_64&quot;]
</code></pre>
",0,0,0,2025-07-26T19:58:07+00:00,1,62,True
79717417,22432971,,mysql,Outputting a MySQL query result without column name or an alias,"<p>I have recently given a hackerrank test for IBM for a Developer position.
There was a MySQL problem that i had to solve.
The question was simple however the problem was with the output.</p>
<p>The answer I wrote was</p>
<pre><code>Select c.mac 
from clients c
join traffic t
    on c.id = t.client_id 
order by mac desc
</code></pre>
<p>this produced output like following</p>
<pre>
mac  ---> column name
mac_1 ---> rows
mac_2
mac_3
</pre>
<p>which was wrong.
the expected output was something like this</p>
<pre>
mac_1
mac_2
mac_3
</pre>
<p>As you can see the expected output did not include the column name.
How can I modify the query to remove the column name from the output?</p>
<p>I tried setting the c.mac as '' but the result still contained an empty line at the top of the output.</p>
<pre><code>Select c.mac as ''
from clients c
join traffic t
    on c.id = t.client_id 
order by mac desc
</code></pre>
<p>How could I fix this?</p>
",0,1,1,2025-07-28T13:43:45+00:00,3,117,True
79717501,3735561,"Meran, Bozen, Italien",mysql,SELECT the (composite) primary key from a table,"<p>Given a table structure like</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>foreign_id</th>
<th>type</th>
<th>payload</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>I</td>
</tr>
<tr>
<td>1</td>
<td>B</td>
<td>II</td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>III</td>
</tr>
</tbody>
</table></div>
<p>with a named constraint (could be primary key, could be any other unique key) like <code>tbl_pk(foreign_id,type)</code> I would like to query something like:</p>
<pre><code>SELECT
    t.*,
    KEY('tbl_pk') as primary_key
FROM
    table t
</code></pre>
<p>and get:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>foreign_id</th>
<th>type</th>
<th>payload</th>
<th>primary_key</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>I</td>
<td>1-A</td>
</tr>
<tr>
<td>1</td>
<td>B</td>
<td>II</td>
<td>1-B</td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>III</td>
<td>2-A</td>
</tr>
</tbody>
</table></div>
<p><strong>UPDATE</strong> to clarify some of the comments:</p>
<ul>
<li>We still have mysql5 machines in production as well as mariadb11. I would need a solution for both, but to minimize confusion, I removed the mariadb tag.</li>
<li>I wanted the column to be readily available so that I could use it on <code>UPDATE</code>s. So a stored column would be fine, a stored procedure not, a view maybe (I'm not fully in the know about updateable views)</li>
<li>the idea was to use the a hash of the key for the frontend editing, so that code could be shared across multiple tables with potentially different key lenghts and number of key columns.</li>
</ul>
",0,1,1,2025-07-28T14:49:03+00:00,3,113,True
79719547,1648712,,mysql,Can&#39;t convert geometry from back to WKT after exporting from MySQL,"<p>I have a MySQL table with a <code>multipolygon</code> column, which has OSM geometry for each country's territorial waters. This is exported (by AWS) to a parquet file every night, which I then download - but I can't load it back into MySQL locally through a Python script.</p>
<p>Here is my code:</p>
<pre><code>from shapely import wkb
import pandas as pd

df = pd.read_parquet(&quot;...&quot;)

name = df.iloc[26][&quot;name&quot;]
coords = df.iloc[26][&quot;coords&quot;]
geometry = df.iloc[26][&quot;geometry&quot;]

print(&quot;name&quot;, name)
print(&quot;coords&quot;, coords)
print(wkb.loads(coords).wkt)
print(&quot;geometry&quot;, geometry)
print(wkb.loads(geometry).wkt)
</code></pre>
<p>outputs</p>
<pre><code>name Niue
coords b'\x00\x00\x00\x00\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
POINT (7.291122019556398e-304 0)
geometry b'\x00\x00\x00\x00\x01\x06\x00\x00\x00\x01\x00\x00\x00\x01\x03\x00\x00\x00\x01\x00\x00\x00F\x00\x00\x00.Bg\x88\x19De\xc0\x1c\x17\xc4\xf6I\xf82\xc0Y\xffB\xeaQDe\xc0h\x14\xdcY\x05\xfb2\xc0\xa8\xda\x13\xc9\xbcDe\xc0\xaf\xbd\x05\xc8\x86\x023\xc0\t\xc9]\xdf\xe2De\xc0n\x81\xa9\x0b\x1d\x063\xc0N\x19\xee\x12\x0bEe\xc0Kl\x88;(\x0c3\xc0\xc0c\xd3\xa5\x1aEe\xc0\xab\xaa&lt;&amp;\xf7\x143\xc0P\xc5\x8d[\x0cEe\xc0:\x9a~\xe41\x1a3\xc0\xc9H\xac{\xdaDe\xc0\x1d\xe76\xe1^!3\xc0\xaf\x00KS\xa9De\xc0.\xfc\x85\xc3w%3\xc0T\xbdF\xdc-De\xc0\xa2\x9b\xfd\x81r-3\xc0\x92xy:\xd7Ce\xc0p\xe9=:\x1a43\xc0/\x17\xf1\x9dXCe\xc0\xd84\xa5\xab*&lt;3\xc0+\xce\x07\xba\xacBe\xc0\x12\x93&amp;\x94tC3\xc0\xb6i\xc7h\xb8Ae\xc0\xb2\xaa\x14\xf1\x8cJ3\xc0\xe1\x85cL0Ae\xc0\xfc\xefw\xde|M3\xc0\x19R\xea7x@e\xc0Iw!g\xabP3\xc0\x08\xb7k\xf8\x8c?e\xc0-\xf9\x8e\xd0\x85S3\xc0\xb4\xab\x90\xf2\x93&gt;e\xc0\x91\xcee\x0fYW3\xc0\xee\xe2\xa2\xffJ&gt;e\xc0yk\xaa8\x1fX3\xc0/|\x8e\xd9\xe3&lt;e\xc0\n\xe1,\x80\x84Z3\xc0^emS&lt;&lt;e\xc0,\xf2\xeb\x87\xd8Z3\xc0\x91J\x0c]\xe4;e\xc0\xac\xf7ez\xd3Z3\xc0\x07\xaf\x02!\xbe:e\xc0\x06]\x0c/\xd2Y3\xc0\xdaH#\xb0\x03:e\xc0]\xd0\x8c\x8f`X3\xc0Oyt#\xac8e\xc0\x0b\xad\x98[kU3\xc0\x80\xb7@\x82&quot;8e\xc0\x14\xb5\x8fp\xb5S3\xc0c0\xda&gt;\xff6e\xc0\xe9pci\x96N3\xc0\xc6\xcf5\x82\x036e\xc0\xff@\xb9m\xdfG3\xc0\xec\xba\\i\xbe4e\xc0\x95OE\xe0\xfe&lt;3\xc0:\xbbk\xbf\xeb3e\xc0\xceZe\x01n53\xc0\x14\x03\xc9\xe0M3e\xc0\xd5\xca)\xa6w.3\xc0\x00\x98!D\xe82e\xc0\x884\x85)%)3\xc0\xae$\x1e\xf5|2e\xc0@1\xb2d\x8e!3\xc0\x955\xa0\x94?2e\xc0{\xe2\x94\x14\xb3\x1b3\xc0\x82\x88y\xfb\x182e\xc0\x00!\xed\xda9\x143\xc0\xacY\xc25\x122e\xc0w\x0b\xc9n\x0b\x0f3\xc0\xcag\x1ee)2e\xc0\xac\xbc.\r\r\x073\xc0\xe4\xe8\xe0OC2e\xc0\xa7\xdb#\xe58\x033\xc0KC\xe8\xfb\x842e\xc0K+t\x14\xd6\xfc2\xc0*\x0b\x15\xd2\xd02e\xc0\xfe\xa1\xf4\xe0I\xf82\xc0h\xa9ad\xee2e\xc0\x14\xf3\x07^\x88\xf42\xc0Z\\\x88z&amp;3e\xc0ze(8\x06\xee2\xc0\xce&amp;v#b3e\xc0\xda\xc8uS\xca\xe72\xc0\xd2\xf2M\xe4\x933e\xc0\x1a%\x9e\xa2\xd9\xe32\xc0\x94h\xc9\xe3\xe93e\xc0\xa8mho\x95\xde2\xc0\xf74q\xbcL4e\xc0\x94m\xe0\x0e\xd4\xd92\xc0\x1e2\x8a\x8a\x1d5e\xc0\x9e}\xe5Az\xd22\xc0Cz\xe5\xd5\x146e\xc0\xbf\x9fu\xe8O\xcc2\xc0vy)+|6e\xc0\xb8\x97\x8f\xff]\xca2\xc0\x00\x82\x83\x07\x177e\xc0\xbf\xc28n\xe7\xc72\xc0\x93\xa5{B[7e\xc0\xc62\xfd\x12\xf1\xc62\xc0\xda7\xf7W\x8f8e\xc0:\xa0[\x9e\xbd\xc32\xc0w\xa5\xc0]\x919e\xc0\xe7\'\x8b\xb1\x02\xc22\xc0**\xd1Hz:e\xc0\x0e\x05&quot;z\x08\xc12\xc0\x98m\xa7\xad\x91;e\xc0nza|\xe2\xc02\xc0\xd4Z=\xcc\x03&lt;e\xc0\x85\x11\xb1w5\xc12\xc0\xea\xbd\xe7v\xe5&lt;e\xc0\x8c\xca&lt;\xa8z\xc22\xc0R97\xb7S&gt;e\xc0\x80\xd1\x8a\x14o\xc52\xc0\x88i\xdf\xdc\xdf&gt;e\xc0\xfc\x12;d\x14\xc72\xc0\x7f\xf6#Ed?e\xc0\x0b\x9a\x96X\x19\xc92\xc0D*1tQ@e\xc0\x03\x0b`\xca\xc0\xcd2\xc0\xa0;\x1c\xb8\xde@e\xc0\xe1\x8e\x81\x86H\xd12\xc0\xe2\xf8\xeb_cAe\xc03\t\xcdd\xee\xd42\xc0\xac\x91\xb8&quot;\x0cBe\xc0\x9c\x84w^\xcf\xda2\xc0\xc7\xf6Z\xd0{Be\xc0\xfai\x92\x0e\xc5\xdf2\xc0\x00\x14m\xd8\x00Ce\xc0x\xa5\x1b\x17\xc4\xe42\xc0\x04kA\x94TCe\xc0\x03\xc0g\xda\xb4\xe82\xc0\x05\xe7B\x9b\x92Ce\xc0\x86!r\xfaz\xec2\xc0A\xc1{\xfd\xffCe\xc0\x11&quot;t+\xdf\xf42\xc0.Bg\x88\x19De\xc0\x1c\x17\xc4\xf6I\xf82\xc0'
POINT (8.814425696238783e-280 8.658207398329322e-304)
</code></pre>
<p>even though the query on the original DB:</p>
<pre><code>select st_aswkt(w.coords), st_aswkt(w.geometry) from world_location as w where w.name = 'Niue'
</code></pre>
<p>outputs</p>
<pre><code>POINT(0 0)
MULTIPOLYGON(((-170.1281168 -18.9698786,-170.1349994 -18.9805504,-170.1480451 -19.0098691,-170.1526944 -19.0238807,-170.1576018 -19.0474889,-170.1595029 -19.0818962,-170.1577585 -19.1023238,-170.1516703 -19.130354,-170.1456696 -19.1463587,-170.1305982 -19.1775285,-170.120023 -19.2035252,-170.1045675 -19.2350261,-170.0835848 -19.2634976,-170.0537609 -19.2912131,-170.0371458 -19.3026866,-170.0146751 -19.3151154,-169.9859583 -19.3262606,-169.95556 -19.3412027,-169.9466551 -19.3442264,-169.9028137 -19.3535843,-169.882364 -19.3548665,-169.8716264 -19.3547894,-169.8357091 -19.3508634,-169.8129502 -19.3452234,-169.771013 -19.3336694,-169.7542125 -19.3269873,-169.7186579 -19.3069826,-169.6879283 -19.280753,-169.6482436 -19.2382641,-169.6225278 -19.2087098,-169.6032566 -19.1815132,-169.5908528 -19.1607233,-169.5777536 -19.131079,-169.5702613 -19.1082013,-169.5655496 -19.0790078,-169.5647229 -19.0587682,-169.5675531 -19.0275429,-169.5707168 -19.0125869,-169.5787334 -18.9876416,-169.5879908 -18.9698773,-169.5916006 -18.9552058,-169.5984471 -18.9297824,-169.6057298 -18.905431,-169.6118032 -18.8900396,-169.622301 -18.8694677,-169.6343672 -18.850892,-169.6598561 -18.822178,-169.6900434 -18.7980943,-169.7026573 -18.7904968,-169.7215612 -18.7808751,-169.7298901 -18.777116,-169.767498 -18.7646121,-169.7989949 -18.7578536,-169.8274273 -18.7540356,-169.861533 -18.7534559,-169.8754636 -18.7547221,-169.9030108 -18.7596841,-169.9477192 -18.7712262,-169.964827 -18.7776549,-169.98099 -18.785543,-170.0099431 -18.803723,-170.0271874 -18.8175129,-170.0433807 -18.8317626,-170.0639814 -18.8547267,-170.077614 -18.8741006,-170.0938532 -18.8936171,-170.1040746 -18.9090096,-170.1116463 -18.9237515,-170.1249988 -18.9565303,-170.1281168 -18.9698786)))
</code></pre>
<p>(I'm aware that Niue isn't actually at (0,0))</p>
<p>Note that the export to Parquet is handled by AWS RDS exports, so I do not control it.</p>
",0,0,0,2025-07-30T06:46:33+00:00,0,148,False
79719632,275088,,mysql,Does $dbh-&gt;prepare() provide any performance benefits for MySQL access?,"<p>The DBI docs on <a href=""https://metacpan.org/pod/DBI#prepare"" rel=""nofollow noreferrer""><code>prepare()</code></a> state that it's behaviour differ for various drivers:</p>
<blockquote>
<p>Drivers for engines without the concept of preparing a statement will
typically just store the statement in the returned handle and process
it when <code>$sth-&gt;execute</code> is called.</p>
</blockquote>
<p>I have general query log enabled in MySQL and I don't see any SQL statements other than <code>SELECT</code>s or <code>INSERT</code>s that I pass to <code>$sth-&gt;execute()</code>. Does it mean that calling <code>prepare()</code> is a no-op for the DBI's MySQL driver and I can just use <code>$dbh</code> shortcuts (like <a href=""https://metacpan.org/pod/DBI#do"" rel=""nofollow noreferrer""><code>$dbh-&gt;do()</code></a> or <a href=""https://metacpan.org/pod/DBI#selectall_arrayref"" rel=""nofollow noreferrer""><code>$dbh-&gt;selectall_arrayref()</code></a>)? Or is there any other simple way to measure if adding the prepare step provides any benefits?</p>
<p>P.S. I mostly care about performance difference as the simpler <code>$dbh</code> shortcuts provide the same functionality for parameterization etc. Also, I'm asking <strong>specifically about MySQL</strong>, not an abstract DBI driver.</p>
",0,3,3,2025-07-30T07:58:23+00:00,2,186,True
79721545,4273463,,mysql,MySQL index to use for range tests,"<p>I have this index:</p>
<pre><code>create index main_cp_index on catalogue_product(
  product_class_id, is_public,
  (cast(coalesce(data-&gt;&gt;'$.&quot;need_tags&quot;', 0) as unsigned)) ASC);
</code></pre>
<p>When i'm trying to check for need_tags being 0 or 1, it's used:</p>
<pre><code>mysql&gt; explain SELECT count(*) FROM `catalogue_product` WHERE (product_class_id = 3 AND is_public = True AND cast(COALESCE(data-&gt;&gt;'$.&quot;need_tags&quot;', 0) as unsigned) = 1)\G
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_product_class_id_0c6c5b54_fk_catalogue,catalogue_product_is_public_1cf798c5,main_cp_index
          key: main_cp_index
      key_len: 14
          ref: const,const,const
         rows: 2740
     filtered: 100.00
        Extra: NULL
1 row in set, 1 warning (0.00 sec)
</code></pre>
<p>But if trying to use IN operator or comparison, the index is not used:</p>
<pre><code>mysql&gt; explain SELECT count(*) FROM `catalogue_product` WHERE (product_class_id = 3 AND is_public = True AND cast(COALESCE(data-&gt;&gt;'$.&quot;need_tags&quot;', 0) as unsigned) in (0, 1))\G
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_product_class_id_0c6c5b54_fk_catalogue,catalogue_product_is_public_1cf798c5,main_cp_index
          key: catalogue_product_product_class_id_0c6c5b54_fk_catalogue
      key_len: 5
          ref: const
         rows: 3471
     filtered: 20.00
        Extra: Using where
1 row in set, 1 warning (0.00 sec)

mysql&gt; explain SELECT count(*) FROM `catalogue_product` WHERE (product_class_id = 3 AND is_public = True AND cast(COALESCE(data-&gt;&gt;'$.&quot;need_tags&quot;', 0) as unsigned) &lt; 2)\G
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_product_class_id_0c6c5b54_fk_catalogue,catalogue_product_is_public_1cf798c5,main_cp_index
          key: catalogue_product_product_class_id_0c6c5b54_fk_catalogue
      key_len: 5
          ref: const
         rows: 3471
     filtered: 33.33
        Extra: Using where
1 row in set, 1 warning (0.00 sec)
</code></pre>
<p>Is it possible to somehow use the index here? It's a part of a bigger query, so I think I need to have some clause for need_tags even though I don't care about the value thereof, for the index prefix to be in the correct order.</p>
<p>PS. Table is this:</p>
<pre><code>mysql&gt; show create table catalogue_product\G
*************************** 1. row ***************************
       Table: catalogue_product
Create Table: CREATE TABLE `catalogue_product` (
  `id` int NOT NULL AUTO_INCREMENT,
  `structure` varchar(10) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci NOT NULL,
  `upc` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci DEFAULT NULL,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci NOT NULL,
  `slug` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci NOT NULL,
  `description` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci NOT NULL,
  `rating` double DEFAULT NULL,
  `date_created` datetime(6) NOT NULL,
  `date_updated` datetime(6) NOT NULL,
  `is_discountable` tinyint(1) NOT NULL,
  `parent_id` int DEFAULT NULL,
  `product_class_id` int DEFAULT NULL,
  `is_public` tinyint(1) NOT NULL,
  `meta_description` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci,
  `meta_title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci DEFAULT NULL,
  `data` json NOT NULL DEFAULT (_utf8mb4'{}'),
  `title_en` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci DEFAULT NULL,
  `title_sl` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci DEFAULT NULL,
  `nutrition` json NOT NULL DEFAULT (_utf8mb4'{}'),
  `brand_id` int DEFAULT NULL,
  `contents` json NOT NULL DEFAULT (_utf8mb4'{}'),
  `allergens` json NOT NULL DEFAULT (_utf8mb4'[]'),
  `description_en` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci,
  `description_sl` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci,
  `country` varchar(2) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_as_ci NOT NULL,
  `priority` smallint NOT NULL,
  `code` varchar(255) COLLATE utf8mb4_0900_as_ci DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `upc` (`upc`),
  UNIQUE KEY `code` (`code`),
  KEY `catalogue_product_parent_id_9bfd2382_fk_catalogue_product_id` (`parent_id`),
  KEY `catalogue_product_product_class_id_0c6c5b54_fk_catalogue` (`product_class_id`),
  KEY `catalogue_product_slug_c8e2e2b9` (`slug`),
  KEY `catalogue_product_date_updated_d3a1785d` (`date_updated`),
  KEY `catalogue_product_date_created_d66f485a` (`date_created`),
  KEY `catalogue_product_is_public_1cf798c5` (`is_public`),
  KEY `catalogue_product_brand_id_74599134_fk_products_brand_id` (`brand_id`),
  KEY `catalogue_product_priority_983a8f56` (`priority`),
  KEY `need_tags_index` ((cast(json_extract(`data`,_utf8mb4'$.&quot;need_tags&quot;') as char(5) charset utf8mb4))),
  KEY `start_index` ((cast(json_extract(`data`,_utf8mb4'$.&quot;start&quot;') as char(10) charset utf8mb4))),
  KEY `touristic_index` ((cast(json_extract(`data`,_utf8mb4'$.&quot;touristic&quot;') as char(2) charset utf8mb4))),
  KEY `main_cp_index` (`product_class_id`,`is_public`,(cast(coalesce(json_unquote(json_extract(`data`,_utf8mb4'$.&quot;need_tags&quot;')),0) as unsigned))),
  CONSTRAINT `catalogue_product_brand_id_74599134_fk_products_brand_id` FOREIGN KEY (`brand_id`) REFERENCES `products_brand` (`id`),
  CONSTRAINT `catalogue_product_parent_id_9bfd2382_fk_catalogue_product_id` FOREIGN KEY (`parent_id`) REFERENCES `catalogue_product` (`id`),
  CONSTRAINT `catalogue_product_product_class_id_0c6c5b54_fk_catalogue` FOREIGN KEY (`product_class_id`) REFERENCES `catalogue_productclass` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=18229 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_as_ci
</code></pre>
<p>PPS. Between the same:</p>
<pre><code>mysql&gt; explain SELECT count(*) FROM `catalogue_product` WHERE (product_class_id = 3 AND is_public = True AND cast(COALESCE(data-&gt;&gt;'$.&quot;need_tags&quot;', 0) as unsigned) between 0 and 1)\G
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: catalogue_product
   partitions: NULL
         type: ref
possible_keys: catalogue_product_product_class_id_0c6c5b54_fk_catalogue,catalogue_product_is_public_1cf798c5,main_cp_index
          key: catalogue_product_product_class_id_0c6c5b54_fk_catalogue
      key_len: 5
          ref: const
         rows: 3471
     filtered: 11.11
        Extra: Using where
</code></pre>
",1,2,1,2025-07-31T15:50:10+00:00,3,104,True
79722140,1460828,"Cornwall, United Kingdom",mysql,How to copy a resultset into an existing column,"<p>I have a working query</p>
<pre><code>select count(rider) as ridecount
from rides
inner join Participants on Participants.rideID = rides.rideID
group by rider
</code></pre>
<p>I have also created a new column 'ridecount' with a default value of zero.
How can I copy this resultset into my 'ridecount' column? I tried</p>
<pre><code>select count(rider)
into ridecount
from rides
</code></pre>
<p>but this gives</p>
<blockquote>
<p>undeclared variable: ridecount</p>
</blockquote>
<p>presumably because <code>SELECT INTO</code> is used for a new table rather than a new column.</p>
<p>I have now also looked at  <a href=""https://stackoverflow.com/questions/1262786/mysql-update-query-based-on-select-query"">suggested answers</a> but these appear to be dealing with two tables; I only have a single table to update and/or I was confused by the accepted answer.</p>
<p>Note: this will be a once-only operation to deal with historical data; the 'ridecount' column will be updated as time progresses.</p>
",2,2,0,2025-08-01T06:27:57+00:00,1,138,True
79728678,2307934,"Lyon, France",mysql,How to make a left join in an update with sqlalchemy and mysql?,"<p>I am trying to craft a query with SQLAlchemy 2.5. My main goal is to do an update while :</p>
<ul>
<li>doing a join on a &quot;parent&quot; table</li>
<li>and doing a left join between the &quot;parent&quot; table and a third table</li>
</ul>
<p>I can't seem to make this work : I am always having either errors or Cartesian products.</p>
<p>I have tried a few options. Currently I am trying with a subquery but getting a Cartesian product :</p>
<pre class=""lang-py prettyprint-override""><code>from sqlalchemy import (
    DateTime,
    ForeignKey,
    Integer,
    create_engine,
    func,
    select,
    update,
)
from sqlalchemy.orm import DeclarativeBase, mapped_column, sessionmaker


###########
# MODELS  #
###########
class Base(DeclarativeBase):
    pass


class Parent(Base):
    __tablename__ = &quot;parent&quot;

    id = mapped_column(Integer, primary_key=True, nullable=False)
    updated_at = mapped_column(DateTime, nullable=True)


class Child(Base):
    __tablename__ = &quot;child&quot;

    id = mapped_column(Integer, primary_key=True, nullable=False)
    parent_id = mapped_column(Integer, ForeignKey(&quot;parent.id&quot;))
    last_status_change = mapped_column(DateTime, nullable=True)


class OtherChild(Base):
    __tablename__ = &quot;other_child&quot;

    id = mapped_column(Integer, primary_key=True, nullable=False)
    parent_id = mapped_column(Integer, ForeignKey(&quot;parent.id&quot;))


###########
# DB INIT #
###########

engine = create_engine(&quot;mysql://root:@127.0.0.1/dev?charset=utf8mb4&quot;)
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

###########
# EXAMPLE #
###########

subq = (
    select(Parent.id, Parent.updated_at)
    .outerjoin(OtherChild)
    .where(OtherChild.id.is_(None))
).subquery()

stmt = (
    update(Child)
    .where(Child.parent_id.in_(select(subq.c.id)))
    .values(
        last_status_change=func.CONVERT_TZ(subq.c.updated_at, &quot;Europe/Paris&quot;, &quot;UTC&quot;)
    )
)

compiled = stmt.compile(dialect=engine.dialect, compile_kwargs={&quot;literal_binds&quot;: True})
# print(compiled)
with Session() as session:
    session.execute(stmt)

</code></pre>
<p>In an nutshell, the SQL that I am trying to generate is like (I am aware of the possibility to use <code>text()</code> and raw SQL, but I'd like to make this work with the ORM) :</p>
<pre class=""lang-sql prettyprint-override""><code>UPDATE
    child
JOIN parent ON
    parent.id = child.parent_id
LEFT JOIN other_child ON
    other_child.parent_id = parent.id
    SET
    last_status_change = CONVERT_TZ(parent.updated_at, 'Europe/Paris', 'UTC')
WHERE
    other_child.id IS NULL
</code></pre>
<p>What am I missing ?</p>
",3,4,1,2025-08-07T13:47:41+00:00,1,111,True
79735259,4202099,,mysql,Optimize big MySQL table for faster MAX query latency,"<p>I have the following table in my MySQL DB:</p>
<pre><code>CREATE TABLE messages (
  id         VARCHAR(36)   NOT NULL PRIMARY KEY,
  chat_id    VARCHAR(36)   NOT NULL,
  author_id  VARCHAR(36)   NOT NULL,
  content    VARCHAR(500)  character set utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  visible    TINYINT NOT NULL DEFAULT 1,
  request_id VARCHAR(128),
  created_at BIGINT signed NOT NULL,
  updated_at BIGINT signed NOT NULL,

  UNIQUE INDEX messages_chat_id_created_at(chat_id, created_at DESC)
);
</code></pre>
<p>It has a size of ~400GB and ~700 mil rows.</p>
<p>The only query I run on this table is that one:</p>
<pre><code>            SELECT
               *
            FROM
               messages
            WHERE
               chat_id = :chatId
               AND created_at &lt;= :createdAt
               AND visible = 1
            ORDER BY created_at DESC
            LIMIT 20
</code></pre>
<p>The table is continuously growing, and although in 90% of cases only the most recent data is fetched, I have to keep old messages in the DB both due to our retention policy and to support cases where users come back to their old conversations.</p>
<p>The problem I have is that although p99 oscillates around 60ms, I get a pretty consistent MAX latency of as much as 750ms.</p>
<pre><code>Index range scan on m using messages_chat_id_created_at, with index condition: ((m.chat_id = ?) and (m.created_at))

Rows returned: 10

Latency: 370.9 ms
</code></pre>
<p>Is there a quick win I can apply to speed things up a little?</p>
",2,2,0,2025-08-14T10:15:57+00:00,2,126,True
79735286,16599493,,mysql,How can I count the occurrences of a value in between two specific values?,"<p>I have a table in SQL that shows different rule firings with timestamp ordered from oldest to newest - like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Timestamp</th>
<th>Customer ID</th>
<th>Rule Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-21 11:21:20</td>
<td>customer 1</td>
<td>quick_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:22:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:23:20</td>
<td>customer 1</td>
<td>slow_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:24:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:25:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:25:40</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:26:20</td>
<td>customer 2</td>
<td>quick_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:27:20</td>
<td>customer 3</td>
<td>quick_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:28:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:29:20</td>
<td>customer 1</td>
<td>slow_hit_rule</td>
</tr>
<tr>
<td>2025-07-21 11:30:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:30:20</td>
<td>customer 1</td>
<td>decline_rule</td>
</tr>
<tr>
<td>2025-07-21 11:31:20</td>
<td>customer 3</td>
<td>decline_rule</td>
</tr>
</tbody>
</table></div>
<p>I'd like to group by date and <code>customer id</code> to create the following table that counts the number of times 'decline_rule' occurs after a rule containing '_hit' occurs but before the next '_hit' rule occurs for that customer. This is to know how many 'decline_rule' is caused from the most recent hit rule.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Hit Rule Name</th>
<th style=""text-align: right;"">Hit Rule Count</th>
<th style=""text-align: right;"">Count of Transactions on Decline Rule</th>
<th>Count of Customers on Decline Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td>1 (customer 1)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>slow_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">4</td>
<td>1 (customer 1 has 4 decline_rule after this slow_hit_rule but before the next slow_hit_rule)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
<td>0 (customer 2 didn't have any decline_rule after the hit so it's zero)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td>1 (customer 3)</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>slow_hit_rule</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td>1 (customer 1)</td>
</tr>
</tbody>
</table></div>
<p>Then the final table is a group by the date and rulename:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Hit Rule Name</th>
<th>Hit Rule Count</th>
<th>Count of Transactions on Decline Rule</th>
<th>Count of Customers on Decline Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-07-21</td>
<td>quick_hit_rule</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>2025-07-21</td>
<td>slow_hit_rule</td>
<td>2</td>
<td>6</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>I've tried with <code>LAG</code>/<code>LEAD</code> functions, but I need help on how to approach this logic.</p>
<p>For the LEAD attempt:</p>
<pre><code>with hitrule as (
  select timestamp, rulename, customerid 
  from table_a 
  where rulename != &quot;decline_rule&quot;
) 
select distinct a.timestamp, 
  t.rulename, 
  a.customerid, 
  lead(t.timestamp) over (partition by t.customerid order by t.timestamp) as next_timestamp, 
  row_number() over (partition by t.customerid order by t.timestamp) as rn 
from hitrule a 
left join table_a on date(a.timestamp) = date(b.timestamp) 
  and a.customerid = b.customerid 
where t.rulename LIKE '%_hit%'
</code></pre>
<p>This was to get the next hit rule timestamp</p>
",1,1,0,2025-08-14T10:43:32+00:00,2,154,True
79735438,9616557,"S&#227;o Paulo, SP, Brasil",mysql,How to get view definition without database schema in the view definition?,"<p>I use this query to get view ddl</p>
<pre><code>SELECT CONCAT(
        'CREATE VIEW `', TABLE_NAME, '` AS ', VIEW_DEFINITION, ';'
    ) AS create_view_statement
FROM INFORMATION_SCHEMA.VIEWS
WHERE TABLE_SCHEMA = 'db_here';
</code></pre>
<p>but for some views it return something like:</p>
<pre><code>select dbname.tb_name.column_name from dbname.tb_name;
</code></pre>
<p>I need to return only</p>
<pre><code>select column_name from tb_name;
</code></pre>
<p>How to do it?</p>
",0,0,0,2025-08-14T13:02:41+00:00,3,92,True
79737255,3771449,,mysql,Insert value into mysql if this value isn&#39;t the latest value,"<p>As an example, imagine I want to log the outside temperature in a table. It is read every minute but changes only 20 times a day or so, hence I don't want useless entries every minute. A value shall only be written when it is != the last saved value.</p>
<p>Currently I am using two separate queries. One SELECT to get the last value, then one INSERT if necessary.<br />
Now I am curious if this can be done in a single query somehow like<br />
<code>@lastval = (SELECT ... ORDER BY datetime DESC LIMIT 1); IF (@lastval != newVal) THEN INSERT ... </code></p>
<p>But it seems that IF is not possible for this. Indeed there are lots of threads about &quot;how to avoid duplicates&quot;, sometimes advised to solve with UNIQUE indexes and whatever, but all this doesn't apply to what I need.<br />
I also saw some monstrous queries to solve problems that perhaps are like mine, but before doing that I'd prefer to stay with my solution.</p>
<p>Does an elegant modification exist?</p>
",1,1,0,2025-08-16T14:35:11+00:00,3,104,True
79738459,1136807,,mysql,Create multi table index/composite key,"<p>I have a query which contains 3 tables. If the fetched data length is small (about 50,000) then it works very fast (nearly under a second), but when the data starts exceeding it becomes slower (9 seconds for &lt;150,000).</p>
<p>I have Indexing enabled for all the columns (except text datatype) for all the tables along with composite keys (while researching for the cure), but could not overcome this issue. Even though, I have pagination enabled and fetching 50 records per page.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE `users` (
  `id` BIGINT UNSIGNED AUTO_INCREMENT,
  `user_name` VARCHAR(20) DEFAULT NULL,
  ...
  PRIMARY KEY (`id`),
  KEY `user_name`(`user_name`),
  ...
);

CREATE TABLE `transactions` (
  `user_id` BIGINT UNSIGNED,
  `UniquePaymentRequestId` VARCHAR(100),
  `TransactionNumber` VARCHAR(100),
  `CustomerNumber` VARCHAR(100),
  `OrderId` VARCHAR(40), 
  `Amount` DECIMAL(12,6),
  `UserCharge` DECIMAL(12,6),
  `DepartmentCharge` DECIMAL(12,6),
  `ServiceCode` INT UNSINGED,
  `payment_date` date GENERATED ALWAYS AS (cast(`creation_time` as date)) VIRTUAL,
  `creation_time` DATETIME,
  ...

  KEY `UniquePaymentRequestId`(`UniquePaymentRequestId`),
  KEY `TransactionNumber`(`TransactionNumber`),
  KEY `CustomerNumber`(`CustomerNumber`),
  KEY `OrderId`(`OrderId`),
  KEY `Amount`(`Amount`),
  KEY `UserCharge`(`UserCharge`),
  KEY `ServiceCode`(`ServiceCode`),
  KEY `DepartmentCharge`(`DepartmentCharge`),
  KEY `creation_time`(`creation_time`),
  KEY `payment_date` (`payment_date`),
  KEY `transactions` (
    `UniquePaymentRequestId`, `TransactionNumber`, `CustomerNumber`, `OrderId`, 
    `Amount`, `UserCharge`, `DepartmentCharge`, `creation_time`
  )
);

CREATE TABLE `service_departments` (
  `id` BIGINT UNSIGNED AUTO_INCREMENT,
  `title` VARCHAR(255),
  `type` VARCHAR(3) DEFAULT 'non',
  ...
  PRIMARY KEY (`id`),
  UNIQUE `title`(`title`),
  KEY `type`(`type`),
  ...
);

CREATE TABLE `services` (
  `service_department_id` BIGINT UNSIGNED,
  `title` VARCHAR(255),
  ...
  KEY `service_department_id`(`service_department_id`),
  UNIQUE `title`(`title`),
  ...
);
</code></pre>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
  U.user_name, S.department, S.service_code, S.service_name, 
  T.UniquePaymentRequestId, T.TransactionNumber, T.CustomerNumber, T.OrderId, 
  T.Amount, T.UserCharge, T.DepartmentCharge, T.creation_time 
FROM transactions AS T 
INNER JOIN ( 
  SELECT user_id, user_name FROM users 
  WHERE user_type = 5 
) AS U ON U.user_id = T.user_id 
LEFT JOIN ( 
  SELECT SD.title AS department, S.service_code, S.title AS service_name FROM services AS S 
  INNER JOIN service_departments AS SD ON SD.type = &quot;non&quot; AND SD.id = S.service_department_id 
) AS S ON S.service_code = T.ServiceCode 
WHERE T.payment_date &gt;= '2025-07-19' 
ORDER BY T.creation_time DESC 
</code></pre>
<p>EXPLAIN:</p>
<pre><code>    id  select_type  table   type    possible_keys                       key                    key_len  ref                            rows    Extra                        
------  -----------  ------  ------  ----------------------------------  ---------------------  -------  -----------------------------  ------  -----------------------------
     1  SIMPLE       T       ALL     user_index,payment_date             (NULL)                 (NULL)   (NULL)                         206192  Using where; Using filesort  
     1  SIMPLE       users   eq_ref  PRIMARY,user_type                   PRIMARY                4        gramaone_production.T.user_id  1       Using where                  
     1  SIMPLE       SD      ref     PRIMARY,type                        type                   63       const                          8       Using where                  
     1  SIMPLE       S       ref     service_code,service_department_id  service_department_id  5        gramaone_production.SD.id      3       Using where                  
</code></pre>
<p>How do I resolve this issue?</p>
",2,2,0,2025-08-18T08:54:17+00:00,3,229,True
79738480,3868743,"Lyon, France",mysql,ST_Transform issue with MySQL,"<p>The spatial function <code>ST_Transform</code> returns weird results in MySQL. I tested the following query and compared with PostGIS and obtained very different results:</p>
<p>The query (exactly the same used in MySQL and PostGIS): <code>SELECT ST_AsText(ST_Transform(ST_GeomFromText('POINT(5 45)', 4326), 2154));</code></p>
<ul>
<li><p>Using PostGIS (version: 3.5.2 with proj version: 9.4.0), I get the following result:
<code>POINT(857581.8993196681 6435414.747835401)</code></p>
</li>
<li><p>Using MySQL (version: 8.0.41) I get the following result:
<code>POINT(6297547.01826163 3142340.2635587133)</code></p>
</li>
</ul>
<p>I checked with other tools and the result from PostGIS is correct. So why is the result from MySQL so different? I guess it's not a bug or it would have been reported for a while, but then what am I doing wrong with this query?</p>
<p>Note that I tried for multiple lat/lon values and the results are always different, even for <code>POINT(0 0)</code>, though the difference is small in this case.</p>
",2,2,0,2025-08-18T09:14:06+00:00,1,68,True
79739381,31173361,,mysql,Update MySQL database column to round all values to 2 decimal places,"<p>I have a MySQL database of some 5000 products imported from Excel a while ago. Unfortunately, Excel truncates values with trailing zeroes so they were imported into the Price column in a mix of values with 0, 1, 2, or (occasionally) more decimal places. Some products have no price, and need to stay that way (£CALL for price).</p>
<p>I want to clean up the Price column and force all values to be stored in the database to 2-DP. The column itself is VARCHAR and, no, I can't ALTER column to DECIMAL(10,2) ;)</p>
<p>Easy, I thought. My test:</p>
<pre><code>SELECT ID, Price, ROUND(Price, 2) as Fixed
FROM table
WHERE Price REGEXP '[0-9]+.?[0-9]*'
</code></pre>
<p>Output:</p>
<pre><code>ID      Price   Fixed
1001332 60.31   60.31
1001334 66      66.00
1001336 32.2879 32.29
1001338 66      66.00
1001340 42.2    42.20
1001342 42.2    42.20
1001344 42.2    42.20
1001346 42.2    42.20
1001348 29.54   29.54
1001350 31.07   31.07
...
</code></pre>
<p>So I figured I'd let it rip:</p>
<pre><code>UPDATE table
SET Price = ROUND(Price, 2)
WHERE Price REGEXP '[0-9]+.?[0-9]*'
</code></pre>
<p>Output:
0 Errors. 0 rows affected.</p>
<p>I've tried a few other variants:</p>
<pre><code>UPDATE table
SET Price = ROUND(Price, 2)
WHERE Price &lt;&gt; &quot;&quot;
</code></pre>
<p>Output:
0 Errors. 0 rows affected.</p>
<pre><code>UPDATE table
SET Price = ROUND(Price, 2)
WHERE 1
</code></pre>
<p>Output:
Truncated incorrect DOUBLE value: ''
(because of those empty values)</p>
<p>Do I need a subquery?</p>
<p>e.g.</p>
<pre><code>UPDATE table AS t1
SET Price = (
    SELECT ROUND(Price, 2)
    FROM table AS t2
    WHERE t1.ID = t2.ID
)
WHERE Price REGEXP '[0-9]+.?[0-9]*'
</code></pre>
<p>(I haven't dared run that in case it doesn't match the rows correctly and trashes the database!)</p>
<p>Other questions on SO imply that my first UPDATE <em>should</em> work, so I suspect maybe the problem lies in the fact the column type is VARCHAR, but I'm not sure if I need to CAST it or perform some trickery to get it to work, and my SQL-fu is exhausted at this point...</p>
",5,5,0,2025-08-19T01:29:27+00:00,2,137,True
79741094,4108345,India,mysql,MySQL Full-Text Search with ngram parser not prioritizing exact word matches,"<p>I have enabled a full-text index on my MySQL table with the ngram parser as shown below:</p>
<pre><code>SHOW VARIABLES LIKE 'ngram_token_size'; -- Default set to 2

ALTER TABLE MyTable ADD FULLTEXT INDEX FT_MyTable (value) WITH PARSER ngram;
OPTIMIZE TABLE MyTable;
</code></pre>
<p>I inserted the following records into MyTable:</p>
<pre><code>INSERT INTO MyTable VALUES (1, 'file');
INSERT INTO MyTable VALUES (1, 'transformation');
INSERT INTO MyTable VALUES (1, 'prefixvedant');
INSERT INTO MyTable VALUES (1, 'trans');
INSERT INTO MyTable VALUES (1, 'ISADemo-LegacyPointer');
INSERT INTO MyTable VALUES (1, '#Sync');
INSERT INTO MyTable VALUES (1, 'prefixvedant_status');
INSERT INTO MyTable VALUES (1, 'midtale');
INSERT INTO MyTable VALUES (1, '#tale');
INSERT INTO MyTable VALUES (1, 'my name is vedant');
INSERT INTO MyTable VALUES (1, '#vedant');
INSERT INTO MyTable VALUES (1, 'coldplay_strawberrysnip.mp3');
INSERT INTO MyTable VALUES (1, 'tale');
INSERT INTO MyTable VALUES (1, 'sale');
INSERT INTO MyTable VALUES (1, '#storiesMeanstalesYes...');
INSERT INTO MyTable VALUES (1, '#stories_tales_Yes...');
INSERT INTO MyTable VALUES (1, 'stories_tales_Yes...');
INSERT INTO MyTable VALUES (1, 'coldplay-yellow');
INSERT INTO MyTable VALUES (1, 'prefixvedantsuffix');
INSERT INTO MyTable VALUES (1, 'prefixvedasuffix');
INSERT INTO MyTable VALUES (1, 'prefixvedsuffix');
INSERT INTO MyTable VALUES (1, 'prefix vedant suffix');
INSERT INTO MyTable VALUES (1, '#hellotale');
INSERT INTO MyTable VALUES (1, 'prefix_vedant_suffix');
INSERT INTO MyTable VALUES (1, 'prefixvedantstatues');
INSERT INTO MyTable VALUES (1, '#statue');
INSERT INTO MyTable VALUES (1, 'prefixvedantstatus');
INSERT INTO MyTable VALUES (1, 'prefixmysqlstatus');
INSERT INTO MyTable VALUES (1, 'prefixmysqlstatues');
INSERT INTO MyTable VALUES (1, 'prefixvedant status');
</code></pre>
<p>I want records with exact word matches (e.g., tale, midtale) to appear at the top of the results when searching for terms like straw* and tale*. However, my current query does not prioritize exact matches, and records like file, ISADemo-LegacyPointer, and sale are appearing higher in the results. Here’s the query I’m using:</p>
<pre><code>SELECT id, value, MATCH(value) AGAINST('&gt;&quot;tale&quot; &gt;&quot;straw&quot; &lt;tale* &lt;straw*' IN BOOLEAN MODE) AS relevance
FROM MyTable
WHERE MATCH(value) AGAINST('&gt;&quot;tale&quot; &gt;&quot;straw&quot; &lt;tale* &lt;straw*' IN BOOLEAN MODE)
ORDER BY relevance DESC;
</code></pre>
<p>I tried increasing ngram_token_size to 3 or 4, but it didn’t resolve the issue. I considered using multiple MATCH...AGAINST clauses with different operators and assigning weights to prioritize exact matches, but this would impact performance since my real table contains millions of records.</p>
<p>What is the best way to configure MySQL full-text search with the ngram parser (or another approach) to prioritize exact word matches while maintaining good performance for large datasets?</p>
",0,0,0,2025-08-20T13:01:18+00:00,0,83,False
79741570,3377484,,mysql,Performance issues when combining queries with multiple selects,"<p>I have 2 queries with the same 3 related tables and each one works fine separately. But I would ideally like to combine them into one query. So this one:</p>
<pre><code>select me.id, me.title, me.start_date_time, 
count(mti.checkin) from mco_event me 
left outer join mco_ticket_instance as mti on mti.event_id = me.id and mti.checkin is not null
where me.start_date_time &gt; '2024-01-01'
group by me.id order by me.start_date_time asc;
</code></pre>
<p>gets me the correct count of checkins.</p>
<pre><code>select me.id, me.title, me.start_date_time, 
count(mol.status) from mco_event as me
join mco_ticket_instance mti1 on mti1.event_id=me.id
join mco_order_line mol on mol.id = mti1.order_line_id and mol.status='order_paid'
where me.start_date_time &gt; '2024-01-01'
group by me.id order by me.start_date_time asc;
</code></pre>
<p>Second one gives me the valid (order_paid) count. But if I combine these:</p>
<pre><code>select me.id, me.title, me.start_date_time, 
(select sum(mol.quantity) from mco_order_line mol
inner join mco_ticket_type mtt on mol.ticket_type_id = mtt.id and mtt.event_id = me.id
where mol.status='order_paid') as quantity,
(select count(mti.checkin) from mco_ticket_instance mti
where mti.event_id = me.id and mti.checkin is not null) as checkins
from mco_event me
where   me.start_date_time &gt; '2024-01-01'
group by me.id order by me.start_date_time asc;
</code></pre>
<p>This gives correct values, but it spins for 15 seconds instead of a few microseconds like the individual, separate queries do.
Backstory: mco_event is the &quot;main&quot; table, mco_ticket_instance has an event_id which has a mco_event.id and an order_line_id which is mco_order_line.id.</p>
<p>So right now my app code is running the 2 separate queries and then stitching the 2 arrays together which just seems inefficient (and also does take some time).</p>
",-1,0,1,2025-08-20T20:43:14+00:00,0,43,False
79741580,18312482,,mysql,MySQL 8 slow queries on indexed table with WHERE event = ? AND merchant = ? AND created_at &gt;=?,"<p>I’m troubleshooting slow queries on MySQL 8 and need advice.</p>
<p>I have a table salla_events (~1M rows). The queries look like this:</p>
<pre><code>SELECT * 
FROM salla_events 
WHERE event = 'order.created' 
  AND merchant = 2021223349 
  AND created_at &gt;= '2025-08-15 22:09:20';
</code></pre>
<p>According to the slow query log, each query takes between 7–11 seconds and scans ~1.1M rows.</p>
<p>I’ve already created indexes:</p>
<pre><code>CREATE INDEX idx_event_merchant_created 
  ON salla_events (event, merchant, created_at);
</code></pre>
<p>But the slow logs still show Rows_examined: 1120350+.</p>
<p>Here is part of mysql slow log:</p>
<blockquote>
<p>User@Host: lamour_bot[lamour_bot] @ localhost [127.0.0.1] Id: 424571 # Query_time: 9.989759 Lock_time: 0.000005 Rows_sent: 5 Rows_examined: 1120379 SET timestamp=1755371566; select * from salla_events where event = 'order.created' and merchant = 1056778926 and created_at &gt;= '2025-08-15 22:12:46';</p>
</blockquote>
<p>Why is MySQL ignoring or underusing this index?</p>
<p>I’m using MySQL 8.0 with InnoDB and the queries are run frequently from a Laravel app.</p>
<p>Any advice on why this indexed query is still slow would be appreciated.</p>
",0,0,0,2025-08-20T20:57:07+00:00,1,98,False
79741939,31316051,,mysql,SELECT query over 4 tables including MAX function in JOIN,"<p>I have 4 tables in a <a href=""/questions/tagged/mysql"" class=""s-tag post-tag"" title=""show questions tagged &#39;mysql&#39;"" aria-label=""show questions tagged &#39;mysql&#39;"" rel=""tag"" aria-labelledby=""tag-mysql-tooltip-container"" data-tag-menu-origin=""Unknown"">mysql</a> database:</p>
<p><strong>Results</strong></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Result_ID</th>
<th>Season_ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>37</td>
<td>5</td>
</tr>
<tr>
<td>38</td>
<td>5</td>
</tr>
</tbody>
</table></div>
<p><strong>Players</strong></p>
<pre><code>Player_ID | Player | Player_Type    | Player_Order
1           Fred     Defender-Left   1
2           Bert     Defender-Left   1
3           Joe      Defender-Right  2
4           Harry    Midfielder      3
5           Simon    Midfielder      3
6           Tate     Striker         4
7           Graeme   Striker         4
8           Jeff     Keeper          5
…           …        …               …
</code></pre>
<p><strong>PointTypes</strong></p>
<pre><code>PointType_ID | L1_Value   | L2_Value    | Point_Value | L1_Order
1              Appearance   Keeper        1             1
2              Appearance   Outfield      1             1
3              Goal         Keeper        4             1
4              Goal         Outfield      4             1
5              Assist       Keeper        3             1
6              Assist       Outfield      3             2
…              …            …             …             …
</code></pre>
<p><strong>Points</strong></p>
<pre><code>Point_ID | Player_ID | Result_ID | PointType_ID
1          1           1           1
2          2           1           2
3          3           1           2
…          …           …           …
1          1           37          1
2          2           37          2
3          3           37          2
2          4           37          2
3          6           37          2
4          4           37          6
6          6           37          4
1          1           38          1
2          2           38          2
3          3           38          2
2          4           38          2
3          5           38          2
4          2           38          4
6          5           38          6
</code></pre>
<p>I'm trying to run a query that produces the following result set:</p>
<pre><code>Player_Order | Player_Type   | Player_ID | Player_Name | Sum(Point_Value) | Season_ID
1              Defender-Left   1           Fred          2                  5
1              Defender-Left   2           Bert          6                  5
2              Defender-Right  3           Joe           2                  5
3              Midfielder      4           Harry         5                  5
3              Midfielder      5           Simon         4                  5
4              Striker         6           Tate          5                  5
…              …               …           …             …                  …
</code></pre>
<p>The following query provides the all data I want (excluding the Season_ID field).</p>
<pre><code>SELECT Players.Player_Order, Players.Player_Type, Players.Player_ID, Players.Player, SUM(PointTypes.Point_Value) AS Points
FROM Points, Players, PointTypes AS PT
WHERE Players.Player_ID = Points.Player_ID AND Points.PointType_ID = PointType.PointType_ID
GROUP BY Player_ID
ORDER BY Player_Order, Points DESC, Player;
</code></pre>
<p>How do I update the query (or even use a different type of query) to filter the data to show the same fields but adding a filter to show only the results where <code>Results.Season_ID = Max(Results.Season_ID)</code>.</p>
",2,2,0,2025-08-21T06:54:24+00:00,1,104,True
79743442,1030576,"Colorado Springs, CO",mysql,Finding records with no joined records or joined records that have been marked as deleted,"<p>Two tables, <code>s</code> and <code>c</code>, are in a many-to-many relationship with <code>cs</code> as the middle table. Each table has a primary key called <code>id</code>, and the <code>cs</code> and <code>c</code> tables have a column for deleted (0 means not deleted, 1 means deleted). Note: I cannot delete records from the tables. My only option is to set <code>del</code> to <code>0</code> or <code>1</code>.</p>
<pre><code>s    cs      c
--   -----   -------
id   c_id    id
     s_id    del
     del
</code></pre>
<p>The goal is to find all <code>s</code> records that do not have any <strong>active</strong> c records, which means:</p>
<ul>
<li>There may be no <code>cs</code> records, or there may be only deleted <code>cs</code> records</li>
<li>There may be active <code>cs</code> records that do not join to any <code>c</code> records or join to only deleted <code>c</code> records.</li>
</ul>
<p>Here is an example:</p>
<pre><code>s.id
----
1
2
3

cs.del cs.s_id cs.c_id
------ ------- -------
0      1       @      
0      1       $      
1      1       #      
0      3       @
0      3       #

c.id c.del
---- -----
@    1    
$    0    
#    1    
</code></pre>
<p>Results:</p>
<ul>
<li>s.id = 1 should <strong>not be found</strong> because there is an active c record through cs.id = 2</li>
<li>s.id = 2 should <strong>be found</strong> because it is not joined any cs records</li>
<li>s.id = 3 should <strong>be found</strong> because it is joined to only deleted c records</li>
</ul>
<p>Here is my attempt</p>
<pre><code>SELECT
  s.id

FROM
  s
  LEFT JOIN cs on s.id = cs.s_id
  LEFT JOIN c  on c.id = cs.c_id

WHERE
      (cs.del = 0 OR cs.del IS NULL)
  AND (c.del  = 0 OR c.del  IS NULL)

GROUP BY
  s.id

HAVING
  COUNT(s.id) &gt; 0
</code></pre>
<p>Unfortunately, it does not work (no records are found). I have tried many times to no avail, so I am not sure what is wrong. Any help would be greatly appreciated.</p>
",1,1,0,2025-08-22T13:21:30+00:00,3,78,True
79748997,13774079,,mysql,Is there a way to change column collation (from latin to utf8) without erasing part of data?,"<p>In my MySQL database, I have a column of type mediumtext and of collation latin1_swedish_ci. It stores texts in french with paragraphs and all. But I've been asked to add emojis support, so I tried to switch collation to utf8mb4_general_ci... Which erased a good chunk of data (anytime an accentueted character was encountered at least).
Is there a way to support french characters, line breaks and emojis ? And is there a way to go from my current column's situation (I reverted collation and data to where it was before trying to change) to that multi-supports solution without losing data ?</p>
<p>I'm using phpMyAdmin, in case this information is relevant.</p>
",-1,0,1,2025-08-28T11:19:17+00:00,0,45,False
79749469,9616557,"S&#227;o Paulo, SP, Brasil",mysql,Why some query like select * from table where timestamp_column = &#39;0&#39; does not work in mysql 8.4 anymore?,"<p>I've upgraded from MySQL 5.7 to 8.4.
But some queries with conditions like:</p>
<pre><code>SELECT * FROM table WHERE timestamp_column = '0';
ERROR 1525 (HY000): Incorrect TIMESTAMP value: '0'
</code></pre>
<p>do not work in MySQL 8.4.</p>
<p>The same query above works in mysql 5.7</p>
<p>What has changed? I am looking for documentation explaining what has changed.</p>
<p>sql_mode in both mysql 5.7 and 8.4 is empty ''</p>
<pre><code>mysql&gt; show global variables like '%sql_mode';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| sql_mode      |       |
+---------------+-------+
1 row in set (0.00 sec)
</code></pre>
",0,0,0,2025-08-28T18:34:08+00:00,1,68,True
79749983,9736351,,mysql,Mysql performance issues,"<p>we have some tough performance issues that seem to us that they shouldn't exist, but we don't see any way of solving them.</p>
<p>We have a Gcloud Mysql database, with 4Cpu, 16go Ram. The full instance contains around 50Go (on ssd) but our main database only 15Go.</p>
<p>We get extreme slowdowns on most pages of our app in the middle of the day, when it is the most used. With some pages taking 40s (or more) to load.</p>
<p>We've extracted the queries and by themselves they take 'only' around 500ms, but as soon as I throw concurrent queries (using Jmeter), they baloon. 5 concurrent queries get us to 3s avg, 10 concurrent to 7s, 20 to 30s...</p>
<p>We've tried all kind of optimizations, indexes (on every column separately, composite of each column of the query). In the end I managed to split the time by two and then three, but it's just pushing the problem further.</p>
<p>We didn't augment the resources of the database further, because we've played the queries on a less powerful database and we only saw a linear downgrade, and we kinda expect that if we double the resources we'd just get twice the gain. Which still doesn't feel like it's enough (on top of being more expensive).</p>
<p>Here is an exemple of one of the problematic queries (a count, used for pagination). This is a simplified version, optimized as much as we could, and still it takes around 150ms which is not enough to prevent it from slowing down when doing concurrent queries.</p>
<p>Each filter in the where clause are user inputs and can't be stored. As I said, we have indexes on issue_date, customer_id, status, workflow and customer_id+status+workflow+issue_date. issue_date is a date, customer_id bigint, status and workflow INTEGER.</p>
<pre><code>select count(i1_0.id) 
from invoice i1_0 
where i1_0.issue_date BETWEEN '2025-01-01' and '2025-07-30' 
and (i1_0.customer_id = 20 
and (i1_0.status, i1_0.workflow) in ((1,1), (2,1), (3,1), (4,1), (5,1), (6,1), (7,1))
);
</code></pre>
<p>That specific query returns 70k results for around 4M without any filter.</p>
<p>We've tried experimenting with read locks but didn't get any result.</p>
<p>We don't have any DB admin here so it's possible that we're missing something obvious, regarding the query, the resources, the locks...</p>
<p>Does someone has any idea ?</p>
<p><strong>Update</strong>
Here is the &quot;explain&quot; for that query</p>
<pre><code>|id |select_type|table|partitions|type |possible_keys                                                                                            |key       |key_len|ref|rows   |filtered|Extra                   |
|---|-----------|-----|----------|-----|---------------------------------------------------------------------------------------------------------|----------|-------|---|-------|--------|------------------------|
|1  |SIMPLE     |i1_0 |          |range|invoice_issue_date_IDX,invoice_customer_id_IDX,invoice_workflow_num_IDX,invoice_status_num_IDX,composite_idx|composite_idx|23     |   |145 523|100     |Using where; Using index|
</code></pre>
<p><strong>Update 2</strong>
I've upgraded the database to 8cpu, 32Go and tried again. The query alone doesn't go faster (no surprise) but concurrent queries are obviously better.
I had to push to 100 concurrent queries to get 1.5s avg, 200 for a 3s average. Obviously it's better but it doesn't feel like a real solution ?</p>
<p><strong>Update 3</strong>
The full table. I've removed the indexes because as I stated, I've tried all different kinds and while there were some improvements, nothing solved the core problem. (Which, following @Shadow advice, is probably somewhere else)</p>
<pre><code>CREATE TABLE `invoice` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `invoice_type_code` varchar(10) NOT NULL COMMENT ,
  `issue_date` date DEFAULT NULL,
  `category_code` varchar(3) DEFAULT NULL COMMENT ,
  `document_currency_code` varchar(3) DEFAULT NULL COMMENT ,
  `tax_exclusive_amount` decimal(19,6) DEFAULT NULL COMMENT ,
  `tax_inclusive_amount` decimal(19,6) DEFAULT NULL COMMENT ,
  `payable_amount` decimal(19,6) DEFAULT NULL COMMENT ,
  `payment_means_code` varchar(11) DEFAULT NULL COMMENT ,
  `datecreated` timestamp NULL DEFAULT CURRENT_TIMESTAMP,
  `dateupdated` timestamp NULL DEFAULT NULL,
  `tax_amount` decimal(19,6) DEFAULT NULL COMMENT ,
  `percent` decimal(19,1) DEFAULT NULL,
  `supplier_id` bigint DEFAULT NULL,
  `customer_id` bigint DEFAULT NULL,
  `downloaded` bit(1) DEFAULT NULL COMMENT ,
  `status_date` datetime DEFAULT NULL COMMENT ,
  `submission_type` varchar(100) DEFAULT NULL COMMENT ,
  `tax_type` varchar(100) DEFAULT NULL COMMENT ,
  `market_number` varchar(100) DEFAULT NULL COMMENT ,
  `obligation_number` varchar(50) DEFAULT NULL,
  `comment_status` varchar(2000) DEFAULT NULL,
  `invoice_number` varchar(20) NOT NULL COMMENT,
  `flux_id` bigint DEFAULT NULL,
  `supplier_service` varchar(100) DEFAULT NULL,
  `customer_service` varchar(100) DEFAULT NULL,
  `metadata` longtext NOT NULL COMMENT,
  `due_date` date DEFAULT NULL COMMENT,
  `iban` varchar(34) DEFAULT NULL COMMENT,
  `department_id` bigint DEFAULT NULL,
  `origin_number` varchar(20) DEFAULT NULL,
  `supplier_auxiliary_code` varchar(100) DEFAULT NULL COMMENT ,
  `supplier_type` varchar(100) DEFAULT NULL COMMENT ,
  `supplier_type_code` varchar(100) DEFAULT NULL COMMENT ,
  `accounting_date` date DEFAULT NULL COMMENT ,
  `custom_info` varchar(500) DEFAULT NULL,
  `payment_date` date DEFAULT NULL COMMENT ,
  `invoice_format` enum() NOT NULL,
  `portal` enum() DEFAULT NULL,
  `is_subscriber_deposit` bit(1) DEFAULT b,
  `owner_type` enum() DEFAULT NULL,
  `from_emission` bit(1) DEFAULT b,
  `extracted_from_document` varchar(255) DEFAULT NULL,
  `invoice_number_back_03_25` varchar(20) DEFAULT NULL,
  `document_type` bigint NOT NULL,
  `purchase_accounting_downloaded` bit(1) DEFAULT b,
  `status` int DEFAULT NULL,
  `workflow` int DEFAULT NULL,
  PRIMARY KEY (`id`),
  CONSTRAINT `fk_invoice_to_customer` FOREIGN KEY (`customer_id`) REFERENCES `structure` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `fk_invoice_to_flux` FOREIGN KEY (`flux_id`) REFERENCES `invoice_flux` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `fk_invoice_to_supplier` FOREIGN KEY (`supplier_id`) REFERENCES `structure` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE=InnoDB AUTO_INCREMENT=5104843 DEFAULT CHARSET=utf8mb3 COMMENT=
</code></pre>
<p><strong>Update 4: Answers to Rick James</strong></p>
<p>Result of the <code>SHOW TABLE STATUS LIKE 'invoice';</code></p>
<pre><code>|Name   |Engine|Version|Row_format|Rows     |Avg_row_length|Data_length  |Max_data_length|Index_length |Data_free|Auto_increment|Create_time        |Update_time        |Check_time|Collation         |Checksum|Create_options|Comment                                             |
|-------|------|-------|----------|---------|--------------|-------------|---------------|-------------|---------|--------------|-------------------|-------------------|----------|------------------|--------|--------------|----------------------------------------------------|
|invoice|InnoDB|10     |Dynamic   |4 295 224    |319           |1 370 488 832    |0              |3 908 190 208 |4 194 304|5 109 810        |2025-09-02 02:42:28|2025-09-02 02:54:21|          |utf8mb3_general_ci|        |              |
</code></pre>
<p>The column metadata contains strings of length between 40 and 170 bytes, also very often null.</p>
<p>On the environment where I'm realizing my tests:</p>
<ul>
<li><em>innodb_buffer_pool_size</em>: 4 563 402 752</li>
<li><em>RAM</em>: 8GB (but I raised it to 32GB during my test, before lowering it back)</li>
<li>Volume on the ssd 37GB (shared with other databases)</li>
<li>Volume in the DB 16GB</li>
<li>Volume on the table 4GB</li>
</ul>
<p>On the actual production environment:</p>
<ul>
<li><em>innodb_buffer_pool_size</em>: 11 811 160 064</li>
<li><em>RAM</em>: 16GB</li>
<li>Total Volume on the ssd 50GB (shared with other databases)</li>
<li>Volume in the DB 16GB</li>
<li>Volume on the table 4GB</li>
</ul>
<p>As for the last test, the count query prefers that index (by using explain):</p>
<p><em>customer_id, workflow_num, status_num, issue_date</em></p>
",2,2,0,2025-08-29T08:08:46+00:00,2,118,True
79750849,1200713,,mysql,Can&#39;t specify target table for update in FROM clause,"<pre><code>UPDATE
  `details_audit` a
SET
  a.count_changes_below_100 = (
    SELECT
      COUNT(*)
    FROM
      `details_audit`
    WHERE
      sort_id &lt; 100
  );
</code></pre>
<p>This worked on a previous version of MySQL but not with version 8.0.43:</p>
<blockquote>
<p>Error Code: 1093<br />
You can't specify target table 'a' for update in FROM clause</p>
</blockquote>
<p>How can I solve this?</p>
",2,2,0,2025-08-29T23:44:21+00:00,2,141,True
79752002,1371968,New Zealand,mysql,Acumatica custom table and error creating snapshot,"<p>For MYOB Acumatica, I have a custom table that I create for MySQL:</p>
<pre><code>CREATE TABLE IF NOT EXISTS PMProjectServiceEquipment 
(
    ID INT AUTO_INCREMENT PRIMARY KEY,
    ProjectID INT NULL,
    ServiceEquipmentID INT NULL,

    -- Acumatica system columns
    CompanyID INT NOT NULL DEFAULT 1,
    tstamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
 
    NoteID CHAR(36) NULL,
  
    CreatedByID CHAR(36) NULL,
    CreatedByScreenID CHAR(8) NULL,
    CreatedDateTime DATETIME NULL,

    LastModifiedByID CHAR(36) NULL,
    LastModifiedByScreenID CHAR(8) NULL,
    LastModifiedDateTime DATETIME NULL
);
</code></pre>
<p>I have the corresponding DAC like so</p>
<pre><code>using System;
using PX.Data;
using PX.Data.BQL;
using PX.Objects.PM;
using PX.Objects.FS;

namespace MyCustomisation
{
    [Serializable]
    public class PMProjectServiceEquipment : PXBqlTable, IBqlTable
    {
        #region ID
        [PXDBIdentity(IsKey = true)]  // Auto-increment primary key
        [PXUIField(DisplayName = &quot;Record ID&quot;)]
        public virtual int? ID { get; set; }

        public abstract class id : BqlInt.Field&lt;id&gt; { }
        #endregion

        #region ProjectID
        [PXDBInt]
        [PXUIField(DisplayName = &quot;Project&quot;, Visibility = PXUIVisibility.SelectorVisible)]
        [PXSelector(typeof(Search&lt;PMProject.contractID&gt;),
                    SubstituteKey = typeof(PMProject.contractCD),
                    DescriptionField = typeof(PMProject.description))]
        public virtual int? ProjectID { get; set; }
        public abstract class projectID : BqlInt.Field&lt;projectID&gt; { }
        #endregion

        #region ServiceEquipmentID
        [PXDBInt]
        [PXUIField(DisplayName = &quot;Service Equipment&quot;, Visibility = PXUIVisibility.SelectorVisible)]
        [PXSelector(typeof(Search&lt;FSEquipment.SMequipmentID&gt;),
                    typeof(FSEquipment.refNbr),
                    typeof(FSEquipment.descr),
                    typeof(FSEquipment.equipmentTypeID),
                    typeof(FSEquipment.serialNumber),
                    typeof(FSEquipment.ownerID),
                    SubstituteKey = typeof(FSEquipment.refNbr),
                    DescriptionField = typeof(FSEquipment.descr))]
        public virtual int? ServiceEquipmentID { get; set; }
        public abstract class serviceEquipmentID : BqlInt.Field&lt;serviceEquipmentID&gt; { }
        #endregion
  
        #region NoteID
        [PXNote] // maps to CHAR(36) in MySQL here
        public virtual Guid? NoteID { get; set; }
        public abstract class noteID : BqlGuid.Field&lt;noteID&gt; { }
        #endregion

        #region Audit columns (MUST use these attributes)
        [PXDBCreatedByID]       
        public virtual Guid? CreatedByID { get; set; }

        public abstract class createdByID : BqlGuid.Field&lt;createdByID&gt; { }

        [PXDBCreatedDateTime]   
        public virtual DateTime? CreatedDateTime { get; set; }

        public abstract class createdDateTime : BqlDateTime.Field&lt;createdDateTime&gt; { }

        [PXDBLastModifiedByID]  
        public virtual Guid? LastModifiedByID { get; set; }

        public abstract class lastModifiedByID : BqlGuid.Field&lt;lastModifiedByID&gt; { }

        [PXDBLastModifiedDateTime] 
        public virtual DateTime? LastModifiedDateTime { get; set; }

        public abstract class lastModifiedDateTime : BqlDateTime.Field&lt;lastModifiedDateTime&gt; { }

        [PXDBCreatedByScreenID]
        public virtual string CreatedByScreenID { get; set; }

        public abstract class createdByScreenID : PX.Data.BQL.BqlString.Field&lt;createdByScreenID&gt; { }
    
        [PXDBLastModifiedByScreenID]
        public virtual string LastModifiedByScreenID { get; set; }

        public abstract class lastModifiedByScreenID : PX.Data.BQL.BqlString.Field&lt;lastModifiedByScreenID&gt; { }

      
        [PXDBTimestamp]         
        public virtual byte[] Tstamp { get; set; } // concurrency token

        public abstract class tstamp : BqlByteArray.Field&lt;tstamp&gt; { }
        #endregion
    }  
}
</code></pre>
<p>I do have screen setup and working as expected.</p>
<p>But whenever create a snapshot, I get this error:</p>
<p><a href=""https://i.sstatic.net/6usNuPBM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6usNuPBM.png"" alt=""enter image description here"" /></a></p>
<p>Is there something not quite right in the way I've setup the table or DAC? Could it be because MySQL puts table name into lower case?</p>
<p>Any help would be appreciated.</p>
",-1,0,1,2025-08-31T20:32:56+00:00,1,87,True
79753404,11515453,,mysql,SQL get all items that belong to the same user group of the requesting user,"<p>I wish figure out how to get all appointments that belong to the same user group of the requesting user in just one SQL query.
My only data is the user_id of the requesting user.</p>
<p>Minimal reproducable example:</p>
<pre><code>appointments
----------------
id    user_id
1    2
2    3
3    1
4    2
5    2
</code></pre>
<pre><code>users
----------------
id    group_id
1    5
2    8
3    8
</code></pre>
<p>Task:
I'm user 3 and want to get all appointments that are from users in my group.
user 3 is in group 8, so is user 2.
appointments that belong to user 2 and 3 are: 1,2,4,5</p>
<p>What I have done so far, is using two SQL statements:</p>
<pre><code>SELECT group_id
FROM users
WHERE id = 3
</code></pre>
<p>then knowing my group_id:</p>
<pre><code>SELECT a.*
FROM appointments a
JOIN users u ON a.user_id = u.id
WHERE u.group_id = my_group_id
</code></pre>
",4,4,0,2025-09-02T10:58:17+00:00,5,200,True
79754325,8064427,,mysql,Cumulative Stock Query without the participation of an Initial Stock Table,"<p>I'm designing a stock management program, here I have an <strong>Incoming Table</strong> like below:</p>
<ul>
<li><strong>id</strong> (<em>integer</em>)</li>
<li><strong>date</strong> (<em>date</em>)</li>
<li><strong>type</strong> (<em>varchar</em>)</li>
<li><strong>warehouse_id</strong> (<em>tinyint</em>)</li>
<li><strong>goods_id</strong> (<em>integer</em>)</li>
<li><strong>quantity</strong> (<em>decimal</em>)</li>
<li><strong>price</strong> (<em>decimal</em>)</li>
<li><strong>amount</strong> (<em>decimal</em>)</li>
</ul>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>id</strong></th>
<th style=""text-align: center;""><strong>date</strong></th>
<th style=""text-align: center;""><strong>type</strong></th>
<th style=""text-align: center;""><strong>warehouse_id</strong></th>
<th style=""text-align: center;""><strong>goods_id</strong></th>
<th style=""text-align: center;""><strong>quantity</strong></th>
<th style=""text-align: center;""><strong>price</strong></th>
<th style=""text-align: center;""><strong>amount</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">¥1.50</td>
<td style=""text-align: center;"">¥300</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">¥1.50</td>
<td style=""text-align: center;"">¥450</td>
</tr>
</tbody>
</table></div>
<p>And an <strong>Outcoming Table</strong> like below:</p>
<ul>
<li><strong>id</strong> (<em>integer</em>)</li>
<li><strong>incoming_id</strong> (<em>integer</em>)</li>
<li><strong>date</strong> (<em>date</em>)</li>
<li><strong>type</strong> (<em>varchar</em>)</li>
<li><strong>quantity</strong> (<em>decimal</em>)</li>
</ul>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>id</strong></th>
<th style=""text-align: center;""><strong>incoming_id</strong></th>
<th style=""text-align: center;""><strong>date</strong></th>
<th style=""text-align: center;""><strong>type</strong></th>
<th style=""text-align: center;""><strong>quantity</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-2</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">50</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-3</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-4</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">30</td>
</tr>
<tr>
<td style=""text-align: center;"">4</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-6</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">70</td>
</tr>
<tr>
<td style=""text-align: center;"">5</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-7</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">90</td>
</tr>
</tbody>
</table></div>
<p>And I can easily design a query to combine the Incoming &amp; Outcoming Tables as below:</p>
<pre><code>WITH cte AS (
    SELECT id, incoming_id, date, type, quantity, SUM(quantity) OVER w AS cum_qty
    FROM outcoming
    WINDOW w AS (PARTITION BY incoming_id ORDER BY date, id)
)
SELECT A.id, A.date AS in_date, A.type AS in_type, A.warehouse_id AS wh_id, A.goods_id AS g_id, A.quantity AS in_qty, B.date AS out_date, B.type AS out_type, B.quantity AS out_qty, B.cum_qty, A.quantity-IFNULL(B.cum_qty,0) AS left_qty
FROM incoming AS A
LEFT JOIN cte AS B ON A.id=B.incoming_id
ORDER BY A.date, A.id, B.date, B.id
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>id</strong></th>
<th style=""text-align: center;""><strong>in_date</strong></th>
<th style=""text-align: center;""><strong>in_type</strong></th>
<th style=""text-align: center;""><strong>wh_id</strong></th>
<th style=""text-align: center;""><strong>g_id</strong></th>
<th style=""text-align: center;""><strong>in_qty</strong></th>
<th style=""text-align: center;""><strong>out_date</strong></th>
<th style=""text-align: center;""><strong>out_type</strong></th>
<th style=""text-align: center;""><strong>out_qty</strong></th>
<th style=""text-align: center;""><strong>cum_qty</strong></th>
<th style=""text-align: center;""><strong>left_qty</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">25-8-2</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">150</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">25-8-3</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">60</td>
<td style=""text-align: center;"">110</td>
<td style=""text-align: center;"">90</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">25-8-4</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: center;"">140</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">25-8-6</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">230</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purachse</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">25-8-7</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;"">90</td>
<td style=""text-align: center;"">160</td>
<td style=""text-align: center;"">140</td>
</tr>
</tbody>
</table></div>
<p>My question is how can I achieve a <strong>Cumulative Stock Query</strong> across the Incoming parents to have the following effect, and without the participation of an Initial Stock Table?</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><strong>wh_id</strong></th>
<th style=""text-align: center;""><strong>g_id</strong></th>
<th style=""text-align: center;""><strong>date</strong></th>
<th style=""text-align: center;""><strong>type</strong></th>
<th style=""text-align: center;""><strong>in_qty</strong></th>
<th style=""text-align: center;""><strong>out_qty</strong></th>
<th style=""text-align: center;""><strong>cum_in_qty</strong></th>
<th style=""text-align: center;""><strong>cum_out_qty</strong></th>
<th style=""text-align: center;""><strong>left_qty</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-1</td>
<td style=""text-align: center;"">purchase</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">200</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-2</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">50</td>
<td style=""text-align: center;"">150</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-3</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">60</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">110</td>
<td style=""text-align: center;"">90</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-4</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">30</td>
<td style=""text-align: center;"">200</td>
<td style=""text-align: center;"">140</td>
<td style=""text-align: center;"">60</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-5</td>
<td style=""text-align: center;"">purchase</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">500</td>
<td style=""text-align: center;"">140</td>
<td style=""text-align: center;"">360</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-6</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">70</td>
<td style=""text-align: center;"">500</td>
<td style=""text-align: center;"">210</td>
<td style=""text-align: center;"">290</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">25-8-7</td>
<td style=""text-align: center;"">consume</td>
<td style=""text-align: center;""></td>
<td style=""text-align: center;"">90</td>
<td style=""text-align: center;"">500</td>
<td style=""text-align: center;"">300</td>
<td style=""text-align: center;"">200</td>
</tr>
</tbody>
</table></div>
<p>I don't want an <strong>Initial Stock Table</strong>, because that'll demand a Settlement operation to generate Initial Stock records every month, and the <strong>Incoming Table</strong> is acting as a de facto <strong>Initial Stock Table</strong>.</p>
<p>Thanks a lot in advance.</p>
",3,3,0,2025-09-03T08:26:39+00:00,2,83,True
79756652,31426125,,mysql,The word &quot;again&quot; is not getting indexed when using MySQL InnoDB FullText index with ngram parser with token size 2,"<p>We noticed that full-text index searches were not returning results for the <code>&quot;again&quot;</code> search param.
We excluded the possibility that the word <code>&quot;again&quot;</code> is part of the MySQL FullText stopword list. However, for some reason, when we use the ngram parser with the default setting <code>ngram_token_size = 2</code>, the parser does not index the substrings <code>&quot;ag&quot;</code>, <code>&quot;ga&quot;</code>, <code>&quot;ai&quot;</code>, <code>&quot;in&quot;</code>. As a result, searches for the word <code>&quot;again&quot;</code> return no matches, even though the word exists in the data.</p>
<p>We tested the same case with <code>ngram_token_size = 3</code>, and the problem with <code>&quot;again&quot;</code> was not reproducible, the row containing the word is matched and returned.</p>
<p>Still, we haven’t been able to find the exact reason why the substrings <code>&quot;ag&quot;</code>, <code>&quot;ga&quot;</code>, <code>&quot;ai&quot;</code>, <code>&quot;in&quot;</code> are not being indexed when <code>ngram_token_size = 2</code>. Also, this issue seems to happen only for the word <code>&quot;again&quot;</code>, while other words work as expected.</p>
<p>So my question is why exactly is <code>&quot;again&quot;</code> not being indexed with <code>ngram_token_size = 2</code>?</p>
<p><strong>Environment:</strong></p>
<p>MySQL 8 (InnoDB)</p>
<p><strong>Minimal reproducible example:</strong></p>
<pre><code>-- Create a test table
CREATE TABLE test_ft (
    id INT AUTO_INCREMENT PRIMARY KEY,
    content TEXT,
    FULLTEXT KEY ft_content (content) WITH PARSER ngram
) ENGINE=InnoDB;

-- Insert sample data
INSERT INTO test_ft (content) VALUES
('again'),
('hello again'),
('try again later'),
('simple test');

-- Try searching for the word &quot;again&quot;
SELECT * 
FROM test_ft
WHERE MATCH(content) AGAINST('again' IN BOOLEAN MODE);
</code></pre>
<p>This will return no results, but if you match against <code>'later'</code> for example, it will match with the <code>'try again later'</code> row.</p>
",3,3,0,2025-09-05T10:02:19+00:00,0,85,False
79757584,3771449,,mysql,Make MySQL do very lazy writes,"<p>I am running MySQL on a Pi with the database stored on an SSD. It stores sensor data that come every 2-3 minutes. This means heavy write access to the SSD and I would like to use all options to keep this as low as possible.</p>
<p>Can I configure MySQL so that data is summed up in memory for a long while and only flushed to the SSD after X minutes?<br />
I can live with potential loss of data cached this way in case of crash or power failure, as it is rare.</p>
<p>I know about memory tables that I could use as an intermediate storage. That would mean to keep structures twice, queries on two tables, be prone to errors and have things more complex to develop, modify and generally understand.<br />
Hence it would be really great to know about a configuration option that does all this in the background.</p>
<p>Of course it would be even better if this lazy flushing could be configured per table individually, happened automatically when shutting down and also could be triggered manually when necessary.</p>
",1,1,0,2025-09-06T13:33:53+00:00,2,76,True
79758087,10635844,,mysql,Get distinct items from one column with specific criteria,"<p>Please picture the following table:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">book_id</th>
<th>title</th>
<th>lang</th>
<th>price</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">1</td>
<td>Le tour du monde en 80 jours</td>
<td>fr</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td>Le petit prince</td>
<td>fr</td>
<td>25</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td>Asterix le Gaulois</td>
<td>fr</td>
<td>30</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td>Othello</td>
<td>en</td>
<td>15</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td>Of Mice and Men</td>
<td>en</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">6</td>
<td>The reluctant fundamentalist</td>
<td>en</td>
<td>18</td>
</tr>
<tr>
<td style=""text-align: right;"">7</td>
<td>Damals war es Friedrich</td>
<td>de</td>
<td>28</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td>Der kleine Raabe Socke</td>
<td>de</td>
<td>28</td>
</tr>
<tr>
<td style=""text-align: right;"">9</td>
<td>Ritter Rost</td>
<td>de</td>
<td>25</td>
</tr>
</tbody>
</table></div>
<p>From this table, I'd like to get the most expensive book for each language. Further languages may be added. If the top price for one book is the same, then the book with the higher book_id should be returned. Never more than one line per language.</p>
<p>The table I'm hoping to achieve looks something like this (I don't care about the order):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">book_id</th>
<th>title</th>
<th>lang</th>
<th>price</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">3</td>
<td>Asterix le Gaulois</td>
<td>fr</td>
<td>30</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td>Of Mice and Men</td>
<td>en</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">8</td>
<td>Der kleine Raabe Socke</td>
<td>de</td>
<td>28</td>
</tr>
</tbody>
</table></div>
<p>I've already tried the following query, adapted from what I found <a href=""https://stackoverflow.com/questions/4662464/how-to-select-only-the-first-rows-for-each-unique-value-of-a-column"">here</a>:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT book_id, title, lang, MAX(price) FROM books GROUP BY lang;
</code></pre>
<p>That gave me a table where the book_id and the title did not match the price. It looked like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">book_id</th>
<th>title</th>
<th>lang</th>
<th>MAX(price)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">7</td>
<td>Damals war es Friedrich</td>
<td>de</td>
<td>28</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td>Othello</td>
<td>en</td>
<td>20</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td>Le tour du monde en 80 jours</td>
<td>fr</td>
<td>30</td>
</tr>
</tbody>
</table></div>
<p>I suppose I could find a way to request each language with a separate SQL-Prompt, but then I'd have to add a new prompt to the list whenever there is a book in a new language added.</p>
<p>Is there a better solution to this?</p>
<p>Thanks in advance!</p>
<p>Here is the query needed to enter the data from my sample table into an SQL database of your own:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE IF NOT EXISTS `books` (
  `book_id` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(50) NOT NULL DEFAULT '',
  `lang` varchar(2) NOT NULL DEFAULT '',
  `price` int(11) NOT NULL DEFAULT 0,
  PRIMARY KEY (`book_id`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;

INSERT INTO `books` (`book_id`, `title`, `lang`, `price`) VALUES
    (1, 'Le tour du monde en 80 jours', 'fr', 20),
    (2, 'Le petit prince', 'fr', 25),
    (3, 'Asterix le Gaulois', 'fr', 30),
    (4, 'Othello', 'en', 15),
    (5, 'Of Mice and Men', 'en', 20),
    (6, 'The reluctant fundamentalist', 'en', 18),
    (7, 'Damals war es Friedrich', 'de', 28),
    (8, 'Der kleine Raabe Socke', 'de', 28),
    (9, 'Ritter Rost', 'de', 25);
</code></pre>
",0,2,2,2025-09-07T11:31:55+00:00,1,144,True
79762883,785523,"Bengaluru, India",mysql,How to log an environment variable with each logs from Liquibase docker container?,"<p>I am running liquibase via docker like below</p>
<pre><code>docker run --rm --network=“$DOCKER_NETWORK”  \
-v “$(pwd)/changeLogs:/liquibase/changelogs”  \
-e INSTALL_MYSQL=true  \
-e GIT_SHA=&quot;$GIT_SHA&quot; \
liquibase/liquibase:4.33  \
–log-level=“$LOG_LEVEL”  \
–url=“jdbc:mysql://mysql_db:3306/mydatabase” \ 
–username=user \ 
–password=password \ 
–changeLogFile=“$CHANGELOG_FILE” \ 
update
</code></pre>
<p>Now logs are being shown like below</p>
<pre><code>[2025-09-12 11:25:18] INFO [liquibase.ui] Liquibase Open Source 4.33.0 by Liquibase
[2025-09-12 11:25:18] INFO [liquibase.integration] Starting command execution.
[2025-09-12 11:25:18] INFO [liquibase.changelog] Reading from mydatabase.DATABASECHANGELOG
Database is up to date, no changesets to execute
[2025-09-12 11:25:18] INFO [liquibase.ui] Database is up to date, no changesets to execute
[2025-09-12 11:25:18] INFO [liquibase.changelog] Reading from mydatabase.DATABASECHANGELOG
</code></pre>
<p>How can I log the ENV_VAR with each logging message that Liquibase java code is generating?</p>
",0,0,0,2025-09-12T11:40:01+00:00,1,49,True
79763171,438458,"Edmonton, AB",mysql,How to force the collation on a JSON_VALUE return type in MySQL,"<p>I needed to do a case-insensitive comparison with a JSON_VALUE() result type.</p>
<p>By default, the return type of JSON_VALUE is case-sensitive.</p>
<p>As per the documentation at <a href=""https://dev.mysql.com/doc/refman/8.4/en/json-search-functions.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/refman/8.4/en/json-search-functions.html</a></p>
<blockquote>
<p>If not specified by a RETURNING clause, the JSON_VALUE() function's
return type is VARCHAR(512). When no character set is specified for
the return type, JSON_VALUE() uses utf8mb4 with the binary collation,
which is case-sensitive; <strong>if utf8mb4 is specified as the character set</strong>
<strong>for the result, the server uses the default collation for this</strong>
<strong>character set, which is not case-sensitive.</strong></p>
</blockquote>
<p>So I attempted to retrieve and compare the value as follows, but the comparison still <strong>failed</strong>:</p>
<pre><code>SET @userId := JSON_VALUE(
    '{&quot;UserId&quot;:&quot;Fred&quot;}', 
    '$.UserId' RETURNING CHAR(100) CHARACTER SET utf8mb4
);

SELECT @userId = 'fred';
-- Returns 0
</code></pre>
<p>I checked the server/database/connection collation using <strong><code>show variables like &quot;%collat%&quot;;</code></strong>, and they were all set correctly at &quot;utf8mb4_0900_ai_ci&quot;.</p>
<p>The server version was MySQL 8.4.4.</p>
<p>I could not find any documentation that showed where to place the <code>COLLATE</code> correctly. The AI answers and other things I read all told me to place it right after the CHARACTER SET as part of the RETURNING clause.  That consistently failed to compile.</p>
",0,0,0,2025-09-12T16:29:16+00:00,1,48,True
79763470,23120423,,mysql,Is a clustered primary key in InnoDB considered a covering index when selecting only PK columns?,"<p>I have a table like this (InnoDB):</p>
<pre><code>CREATE TABLE group_member (
  group_id   INT NOT NULL,
  member_id  INT NOT NULL,
  content    TEXT,
  created_at DATETIME NOT NULL,
  updated_at DATETIME NOT NULL,
  PRIMARY KEY (group_id, member_id)
) ENGINE=InnoDB;
</code></pre>
<p>The table has about 3 million rows and its total size is about 5 GB.</p>
<p>When I run this query:</p>
<pre><code>EXPLAIN SELECT group_id, member_id FROM group_member;
</code></pre>
<p>the execution plan shows Extra: Using index, which normally means a covering index.
But this is using only the PRIMARY KEY (the clustered index).</p>
<p><strong>Confusion</strong></p>
<ol>
<li>In InnoDB, the clustered primary key already stores the entire row in the leaf pages.</li>
<li>If I only select group_id and member_id, then technically everything is available in the PK B+Tree.</li>
<li>So EXPLAIN says &quot;Using index&quot;, which suggests it is a covering index.</li>
</ol>
<p>But in practice, scanning 3M rows still takes a very long time (minutes), because the PK leaf pages are large (due to the content column). <strong>So the usual performance benefit of a covering index (avoiding a back-to-table lookup)</strong> doesn’t really exist here <strong>(and I’m not even sure it’s correct to call this “covering” in the first place, since it’s just the PK)</strong>.</p>
<p><strong>Question</strong></p>
<ol>
<li><p>Is a clustered primary key (InnoDB) considered a covering index when selecting only PK columns?</p>
</li>
<li><p>Or is it misleading to think about “covering” in this case, since there is no back-to-PK lookup anyway?</p>
</li>
<li><p>How should I interpret Extra: Using index when the index is the primary key itself?</p>
</li>
</ol>
",3,3,0,2025-09-13T01:59:06+00:00,1,80,True
79764101,30931524,,mysql,How to bulk load CSV file in mySQL workbench without import wizard?,"<p>I am trying to write a stored procedure to import CSV data from local system to mySQL tables. I do not want to use import wizard as I want SP to create, import and display time taken to import.</p>
<p>However, when I execute this statement:</p>
<p><code>LOAD DATA INFILE 'path/to/your/file.csv' INTO TABLE your_table_name FIELDS TERMINATED BY ',' ENCLOSED BY '&quot;' LINES TERMINATED BY '\n' IGNORE 1 LINES; -- Use this if your file has a header row</code></p>
<p>I receive <code>Error Code: 1290. The MySQL server is running with the --secure-file-priv option so it cannot execute this statement</code>.</p>
<p>Things I have tried:</p>
<ul>
<li>Transfer files to path in secure_file_priv <code>C:\ProgramData\MySQL\MySQL Server 8.0\Uploads\</code></li>
<li>Updated 'secure_file_priv'=&quot;&quot; and 'local_infile'=1 in my.ini file at various locations, getting default file location from cmd, but the path is not updated when I check their updated values.
I restart mySQL server each time after making changes but no luck.</li>
<li>Unchecked Safe updates in mySQL preferences</li>
</ul>
<p>Please let me know any workaround.</p>
",1,2,1,2025-09-14T05:36:24+00:00,1,131,False
79764934,1136807,,mysql,MySQL: Writing better compound indexes,"<p>I recently came to know about the performance boosting capabilities of <code>compound indexes</code> or <code>composite keys</code> in <code>mysql</code> through this question of mine <a href=""https://stackoverflow.com/questions/79738459/create-multi-table-index-composite-key"">Create multi table index/composite key</a>.</p>
<p>Since then, I have been trying to improve myself and I came to know that in order to fully utilize the power of compound indexes(that, lets say, has 2 columns - mobile &amp; email), there must be 2 indexes with exchanged places such as:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE users (
  ...
  mobile BIGINT UNSIGNED,
  email VARCHAR(100),
  ...

  KEY `mobile_email` (`mobile`, `email`),
  KEY `email_mobile` (`email`, `mobile`),

  ...
);
</code></pre>
<p>Because the user can enter anything in the search input.</p>
<p>So, my question is that if the table has more than 2 columns, lets say 10, that are searchable via an <code>input-box</code>, how should the indexes be created as according to the previous question creating multiple indexes - each for an individual column - is not or may not be a good idea.</p>
<p>Also, adding multiple compound indexes with exchanged places doesn't feel right. For example:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE users (
  ...
  mobile BIGINT UNSIGNED,
  mobile_alt BIGINT UNSIGNED,
  email VARCHAR(100),
  ...

  KEY `mobile_mobile_alt_email` (`mobile`, `mobile_alt`, `email`),
  KEY `email_mobile_mobile_alt` (`email`, `mobile`, `mobile_alt`),
  KEY `mobile_alt_mobile_email` (`mobile_alt`, `mobile`, `email`),
  // and so on

  ...
);
</code></pre>
",2,2,0,2025-09-15T08:58:14+00:00,2,351,True
79765875,1633272,,mysql,CASE-WHEN impacted by where clause,"<p>I have table saa_prices (id, symbol, price, date). And I create view to query quarterly prices.</p>
<pre><code>CREATE OR REPLACE VIEW saa_quarterly_prices_final1 AS
WITH last_dates AS (
    SELECT 
        symbol,
        MAX(date) AS last_date,
        YEAR(MAX(date)) AS report_year,
        QUARTER(MAX(date)) AS report_quarter
    FROM 
        saa.saa_prices
    GROUP BY 
        symbol, YEAR(date), QUARTER(date)
)
SELECT 
    p.symbol,
    ld.last_date AS date,
    p.price,
    ld.report_year AS year,
    ld.report_quarter AS quarter,
    CASE ld.report_quarter
        WHEN 1 THEN CAST(CONCAT(ld.report_year, '-03-31') AS DATE)
        WHEN 2 THEN CAST(CONCAT(ld.report_year, '-06-30') AS DATE)
        WHEN 3 THEN CAST(CONCAT(ld.report_year, '-09-30') AS DATE)
        WHEN 4 THEN CAST(CONCAT(ld.report_year, '-12-31') AS DATE)
    END AS report_date
FROM 
    last_dates ld
JOIN 
    saa.saa_prices p ON ld.symbol = p.symbol AND ld.last_date = p.date;
</code></pre>
<p>Query：</p>
<pre><code>SELECT p.* FROM saa.saa_quarterly_prices_final1 p
WHERE p.symbol IN ('600519') AND p.report_date &gt; '2025-01-01';
</code></pre>
<p>Query result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>symbol</th>
<th>date</th>
<th>price</th>
<th>year</th>
<th>quarter</th>
<th>report_date</th>
</tr>
</thead>
<tbody>
<tr>
<td>600519</td>
<td>2025-03-31</td>
<td>1561</td>
<td>2025</td>
<td>1</td>
<td>2020-03-31</td>
</tr>
<tr>
<td>600519</td>
<td>2025-06-30</td>
<td>1409.52</td>
<td>2025</td>
<td>2</td>
<td>2020-03-31</td>
</tr>
<tr>
<td>600519</td>
<td>2025-07-31</td>
<td>1421.67</td>
<td>2025</td>
<td>3</td>
<td>2020-03-31</td>
</tr>
</tbody>
</table></div>
<p>Why <code>year</code>, <code>quarter</code> is calculated correctly, but report_date is always '2020-03-31' and conflict with <code>where</code> statement?</p>
<p>If I query using this sql it returns correctly:</p>
<pre><code>SELECT p.*,
CASE p.quarter
        WHEN 1 THEN CAST(CONCAT(p.year, '-03-31') AS DATE)
        WHEN 2 THEN CAST(CONCAT(p.year, '-06-30') AS DATE)
        WHEN 3 THEN CAST(CONCAT(p.year, '-09-30') AS DATE)
        WHEN 4 THEN CAST(CONCAT(p.year, '-12-31') AS DATE)
    END AS report_date1
FROM saa.saa_quarterly_prices_final1 p
WHERE p.symbol IN ('600519') AND p.report_date &gt; '2025-01-01';
</code></pre>
<p>Result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>symbol</th>
<th>date</th>
<th>price</th>
<th>year</th>
<th>quarter</th>
<th>report_date</th>
<th>report_date1</th>
</tr>
</thead>
<tbody>
<tr>
<td>600519</td>
<td>2025-03-31</td>
<td>1561</td>
<td>2025</td>
<td>1</td>
<td>2020-03-31</td>
<td>2025-03-31</td>
</tr>
<tr>
<td>600519</td>
<td>2025-06-30</td>
<td>1409.52</td>
<td>2025</td>
<td>2</td>
<td>2020-03-31</td>
<td>2025-06-30</td>
</tr>
<tr>
<td>600519</td>
<td>2025-07-31</td>
<td>1421.67</td>
<td>2025</td>
<td>3</td>
<td>2020-03-31</td>
<td>2025-09-30</td>
</tr>
</tbody>
</table></div>
<p>MySQL version and saa_prices table:</p>
<pre><code>SELECT VERSION();
# VERSION()
'8.0.25'

show create table saa.saa_prices;
# Table, Create Table
'saa_prices', 'CREATE TABLE `saa_prices` (\n  `id` int NOT NULL AUTO_INCREMENT,\n  `symbol` varchar(8) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,\n  `price` double NOT NULL,\n  `date` date NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `symbol_date_UNIQUE` (`symbol`,`date`)\n) ENGINE=InnoDB AUTO_INCREMENT=5414885 DEFAULT CHARSET=utf8mb3'
</code></pre>
",0,0,0,2025-09-16T07:08:30+00:00,1,148,True
79766461,30215582,,mysql,Using cursor object when using SQL Alcehmy engine,"<p>I have a MySQL connection in Python where I connect to a MySQL server and use workbench to transform the data. I now want to put the SQL code itself in Python, and want to use the cursor object to run my SQL code. How do I establish a cursor object when I'm using an engine? Here is the code I'm working with:</p>
<pre><code>import mysql.connector as mysql
import pandas as pd
from sqlalchemy import create_engine



connection_string = &quot;mysql+mysqlconnector://root:xxx@localhost:xxx/xxx&quot;
engine = create_engine(connection_string)  
</code></pre>
<p>And what I want to do is create a cursor object so I can run all of my SQL code like so:</p>
<pre><code>cursor = connection.cursor()
sql = &quot;XYZ&quot;
cursor.execute(sql)
</code></pre>
<p>I've been trying to figure out the proper syntax to go from the engine I've created in the first block of code to running the SQL through a cursor in the second. Any help would be greatly appreciated</p>
",-1,0,1,2025-09-16T16:14:58+00:00,0,37,False
79768612,3011244,,mysql,MySQL 8 runs very slow when including a NOT IN in the where clause,"<p>I have a website that displays on the main page the 12 latest uploads from the site's users. In that query, users can filter out uploads from certain other users (hence the NOT IN clause). Here is an example query:</p>
<pre><code>select * from user_uploads
join users on user_uploads.user_id = users.id 
join user_settings on users.id = user_settings.user_id 
where post_on &lt;= '2025-09-13T12:02:56' and queued != 1 and users.user_status_id = 2
and user_uploads.user_id NOT IN ( 21881 ) order by post_on desc limit 12;
</code></pre>
<p>When I run the same query without the <code>and user_uploads.user_id NOT IN ( 21881 )</code>, the query takes 0.01s to run. Add in that clause, and it takes 4.9s to run. I have indexes on all of the user_id fields (primary and foreign keys). What can I do to speed up this query? Is there a different way of running this query without using the NOT IN clause?</p>
<p><strong>EDIT</strong>: I ran an EXPLAIN on the above query, and it's apparently not using the index on one of the tables. I'm really confused about this one.</p>
<pre><code>+----+-------------+---------------+------------+--------+--------------------------------------+-------------+---------+--------------------------------+-------+----------+----------------------------------------------+
| id | select_type | table         | partitions | type   | possible_keys                        | key         | key_len | ref                            | rows  | filtered | Extra                                        |
+----+-------------+---------------+------------+--------+--------------------------------------+-------------+---------+--------------------------------+-------+----------+----------------------------------------------+
|  1 | SIMPLE      | user_settings | NULL       | ALL    | idx_user_id,idx_user                 | NULL        | NULL    | NULL                           | 10853 |    56.77 | Using where; Using temporary; Using filesort |
|  1 | SIMPLE      | users         | NULL       | eq_ref | PRIMARY,user_status_id,idx_status    | PRIMARY     | 8       | db_name.user_settings.user_id  |     1 |    23.39 | Using where                                  |
|  1 | SIMPLE      | user_uploads  | NULL       | ref    | idx_user_id,idx_posted_on,idx_search | idx_user_id | 8       | db_name.user_settings.user_id  |    23 |    45.00 | Using where                                  |
+----+-------------+---------------+------------+--------+--------------------------------------+-------------+---------+--------------------------------+-------+----------+----------------------------------------------+
</code></pre>
<p><strong>EDIT 2</strong>:
Here are the create tables for the three tables involved.</p>
<pre><code>mysql&gt; show create table users\G
*************************** 1. row ***************************
       Table: users
Create Table: CREATE TABLE `users` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `username` varchar(45) CHARACTER SET latin1 COLLATE latin1_swedish_ci NOT NULL,
  `email_address` varchar(255) CHARACTER SET latin1 COLLATE latin1_swedish_ci NOT NULL DEFAULT 'no set',
  `password` varchar(150) CHARACTER SET latin1 COLLATE latin1_swedish_ci DEFAULT NULL,
  `referred_by` varchar(45) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `first_name` varchar(45) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
  `last_name` varchar(45) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
  `user_status_id` int unsigned NOT NULL DEFAULT '1',
  `biography` text CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `gender_id` int unsigned NOT NULL DEFAULT '1',
  `pronoun_id` int unsigned NOT NULL DEFAULT '3',
  `custom_subject` varchar(20) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `custom_object` varchar(20) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `custom_possessive` varchar(20) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `custom_self` varchar(20) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `birthday` date DEFAULT NULL,
  `birthday_visibility` enum('Private','Hide Year','Public') CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci NOT NULL DEFAULT 'Private',
  `state` varchar(255) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `country_id` int unsigned NOT NULL DEFAULT '1',
  `avatar_type` enum('None','System','Image','Gravatar') CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci NOT NULL DEFAULT 'None',
  `avatar_shape` enum('square','circle','hexagon') CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci NOT NULL DEFAULT 'square',
  `avatar_id` bigint unsigned DEFAULT NULL,
  `timezone` varchar(255) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci NOT NULL DEFAULT 'UTC',
  `gallery_views` int unsigned NOT NULL DEFAULT '0',
  `featured_tokens` int unsigned NOT NULL DEFAULT '0',
  `last_featured_on` date DEFAULT NULL,
  `confirmed` tinyint unsigned NOT NULL DEFAULT '0',
  `confirm_code` varchar(40) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `lastlogin` datetime DEFAULT NULL,
  `lastlogin_ip` varchar(255) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `last_seen` datetime DEFAULT NULL,
  `last_seen_ip` varchar(255) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `pw_changed` datetime DEFAULT NULL,
  `pw_reset_code` varchar(255) CHARACTER SET utf8mb3 COLLATE utf8mb3_unicode_ci DEFAULT NULL,
  `user_type_expires_on` date DEFAULT NULL,
  `reinstate_on` date DEFAULT NULL,
  `delete_on` date DEFAULT NULL,
  `created_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updated_at` timestamp NULL DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `username` (`username`),
  UNIQUE KEY `email_address` (`email_address`),
  KEY `user_status_id` (`user_status_id`),
  KEY `gender_id` (`gender_id`),
  KEY `country_id` (`country_id`),
  KEY `confirm_code` (`confirm_code`),
  KEY `referred_by` (`referred_by`),
  KEY `featured_date` (`last_featured_on`),
  KEY `idx_status` (`user_status_id`)
) ENGINE=InnoDB AUTO_INCREMENT=21885 DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci COMMENT='User accounts'
</code></pre>
<pre><code>mysql&gt; show create table user_uploads\G
*************************** 1. row ***************************
       Table: user_uploads
Create Table: CREATE TABLE `user_uploads` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `user_id` bigint unsigned NOT NULL,
  `upload_type_id` int unsigned NOT NULL,
  `filename` text CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `hashed_filename` varchar(255) CHARACTER SET latin1 COLLATE latin1_general_cs NOT NULL,
  `parsed_document` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `file_sha1` char(40) CHARACTER SET latin1 COLLATE latin1_general_cs DEFAULT NULL,
  `filesize` int unsigned NOT NULL,
  `cover_filename` varchar(255) DEFAULT NULL,
  `upload_category_id` int unsigned NOT NULL,
  `upload_rating_id` int unsigned NOT NULL,
  `qualifier` varchar(6) DEFAULT NULL,
  `upload_class_id` int unsigned NOT NULL,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,
  `description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `views` int unsigned NOT NULL DEFAULT '0',
  `uploaded_on` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `post_on` datetime DEFAULT NULL,
  `allow_comments` tinyint unsigned NOT NULL DEFAULT '1',
  `show_comments` enum('Show','Screen','Hide') NOT NULL DEFAULT 'Show',
  `comment_type_desired` enum('Any Kind','Casual Only','Light Critiques','Heavy Critiques') NOT NULL DEFAULT 'Any Kind',
  `allow_favoriting` tinyint unsigned NOT NULL DEFAULT '1',
  `allow_sharing` tinyint unsigned NOT NULL DEFAULT '1',
  `hide` tinyint(1) NOT NULL DEFAULT '0',
  `queued` tinyint unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  UNIQUE KEY `idx_hashed_id` (`hashed_filename`),
  KEY `idx_user_id` (`user_id`),
  KEY `idx_upload_type` (`upload_type_id`),
  KEY `idx_category` (`upload_category_id`),
  KEY `idx_rating` (`upload_rating_id`),
  KEY `idx_post_on` (`post_on`),
  KEY `idx_search` (`post_on`,`hide`,`queued`,`user_id`)
) ENGINE=InnoDB AUTO_INCREMENT=354730 DEFAULT CHARSET=latin1
</code></pre>
<pre><code>mysql&gt; show create table user_settings\G
*************************** 1. row ***************************
       Table: user_settings
Create Table: CREATE TABLE `user_settings` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `user_id` bigint unsigned NOT NULL,
  `visibility` enum('Public','Registered','Registered-Age') NOT NULL DEFAULT 'Public',
  `is_18_plus` tinyint(1) NOT NULL DEFAULT '0',
  `show_online_status` tinyint unsigned NOT NULL DEFAULT '1',
  `allow_museum_adds` tinyint unsigned NOT NULL DEFAULT '1',
  `allow_friend_requests` tinyint unsigned NOT NULL DEFAULT '1',
  `allow_user_contact` tinyint unsigned NOT NULL DEFAULT '1',
  `allow_add_to_favorites` tinyint unsigned NOT NULL DEFAULT '1',
  `show_social_links` tinyint unsigned NOT NULL DEFAULT '1',
  `hidden_from_user_directory` tinyint unsigned NOT NULL DEFAULT '0',
  `filter_categories` varchar(255) DEFAULT NULL,
  `filter_ratings` varchar(255) DEFAULT NULL,
  `filter_tags` text,
  `show_stats` tinyint unsigned NOT NULL DEFAULT '1',
  `show_m_thumbnails` tinyint unsigned NOT NULL DEFAULT '0',
  `show_adult_content` tinyint unsigned NOT NULL DEFAULT '0',
  `show_signatures` tinyint unsigned NOT NULL DEFAULT '1',
  `email_notifications` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_pm` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_comment` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_friend_request` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_mention` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_favorite` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_museum_add` tinyint unsigned NOT NULL DEFAULT '1',
  `updated_on` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `notify_on_friend_uploads` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_followed_uploads` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_friend_journals` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_followed_journals` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_journal_comment` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_tutorial_comment` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_new_follower` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_group_submission` tinyint unsigned NOT NULL DEFAULT '1',
  `notify_on_group_member_join` tinyint unsigned NOT NULL DEFAULT '1',
  PRIMARY KEY (`id`),
  KEY `idx_user_id` (`user_id`),
  KEY `idx_visibility` (`visibility`),
  KEY `idx_is_18_plus` (`is_18_plus`)
) ENGINE=InnoDB AUTO_INCREMENT=42399 DEFAULT CHARSET=latin1
</code></pre>
<p>Here is the full query as created by the ORM (hence using the simplified query above):</p>
<pre><code>SELECT me.id, me.user_id, me.upload_type_id, me.filename, me.hashed_filename, me.parsed_document, me.file_sha1, me.filesize, me.cover_filename, me.upload_category_id, me.upload_rating_id, me.qualifier, me.upload_class_id, me.title, me.description, me.views, me.uploaded_on, me.post_on, me.allow_comments, me.show_comments, me.comment_type_desired, me.allow_favoriting, me.allow_sharing, me.hide, me.queued, user.id, user.username, user.first_name, user.last_name, user.password, user.birthday, user.birthday_visibility, user.email_address, user.referred_by, user.user_status_id, user.biography, user.gender_id, user.pronoun_id, user.custom_subject, user.custom_object, user.custom_possessive, user.custom_self, user.state, user.country_id, user.avatar_type, user.avatar_shape, user.avatar_id, user.timezone, user.gallery_views, user.featured_tokens, user.last_featured_on, user.confirmed, user.confirm_code, user.lastlogin, user.lastlogin_ip, user.last_seen, user.last_seen_ip, user.pw_changed, user.pw_reset_code, user.user_type_expires_on, user.reinstate_on, user.delete_on, user.created_at, user.updated_at, settings.id, settings.user_id, settings.visibility, settings.is_18_plus, settings.show_online_status, settings.allow_museum_adds, settings.allow_friend_requests, settings.allow_user_contact, settings.allow_add_to_favorites, settings.show_social_links, settings.hidden_from_user_directory, settings.filter_categories, settings.filter_ratings, settings.filter_tags, settings.show_stats, settings.show_m_thumbnails, settings.show_adult_content, settings.show_signatures, settings.email_notifications, settings.notify_on_pm, settings.notify_on_comment, settings.notify_on_friend_request, settings.notify_on_mention, settings.notify_on_favorite, settings.notify_on_museum_add, settings.notify_on_friend_uploads, settings.notify_on_followed_uploads, settings.notify_on_friend_journals, settings.notify_on_followed_journals, settings.notify_on_journal_comment, settings.notify_on_tutorial_comment, settings.notify_on_new_follower, settings.notify_on_group_submission, settings.notify_on_group_member_join, settings.updated_on 
FROM user_uploads me  
JOIN users user ON user.id = me.user_id  
JOIN user_settings settings ON settings.user_id = user.id 
WHERE ( ( post_on &lt;= '2025-09-13T12:02:56' AND queued != '1' AND me.user_id NOT IN ( '21881' ) AND user.user_status_id = '2' ) ) 
ORDER BY post_on DESC 
LIMIT 12;
</code></pre>
<p>Note, the execution times for both the full query and the simplified query at the top are nearly identical under the same conditions (with and without the NOT IN clause).</p>
<p>Thanks for the help.</p>
",1,1,0,2025-09-18T15:02:16+00:00,1,104,False
79771697,1223228,London,mysql,MYSQL Trigger after insert,"<p>I am writing a trigger to run whenever a new row is inserted into <code>table_a</code>.  The trigger adds a new row into <code>table_b</code>, see insert statement.</p>
<pre><code>CREATE TRIGGER `trigger` AFTER INSERT ON `table_a`
 FOR EACH ROW BEGIN     
    INSERT INTO table_b (added, email) VALUES (NOW(), NEW.email);
END
</code></pre>
<p>The above works fine but I want to make a slight change to the trigger.<br />
I would like to prevent the insert statment from running if:</p>
<pre><code>table_b.email = NEW.email AND submitted = 0
</code></pre>
<p>Basically I want to check first to see if the email exists in <code>table_b</code> AND submitted has the value 0.  I only want to insert if this condition is false.  I am struggling to see how this is done within a trigger.</p>
",1,1,0,2025-09-22T13:52:21+00:00,1,85,True
79773259,4730308,,mysql,SQL Query to get the Min &amp; Max Date,"<p>I have 3 tables:</p>
<ol>
<li><p>Schedule (scheduleId, scheduleDate, requestBranch, assignBranch, customerName, customerAddress, invoiceNo, refkey)</p>
</li>
<li><p>Branches (branchId, branchName)</p>
</li>
<li><p>ActivityLog (activityLogId, invoiceNo, refkey, activityDate, status)</p>
<p>*<em>requestBranch</em> &amp; <em>assignBranch</em> are <strong>branchId of branchName</strong></p>
</li>
</ol>
<p>Result I want to achieve:</p>
<pre><code>REQUESTBRANCH | ASSIGNBRANCH |     NAME     |   ADDRESS  |    MINDATE    |    MAXDATE 
Bulacan       |    Manila    |  CUSTOMER A  |  Bulacan   |  2025-09-12   |  2025-09-15
Cavite        |    Manila    |  CUSTOMER B  |  Cavite    |  2025-09-18   |  2025-09-22
Cebu          |    Manila    |  CUSTOMER C  |  Cebu      |  2025-09-20   |  2025-09-24
</code></pre>
<p>And here is my Query:</p>
<pre><code>SELECT scheduleDate, requestBranch, assignBranch, 
    invoiceNo, customerName, customerAddress,
    (SELECT MIN(activityDate) FROM schedule INNER JOIN activitylog USING (refkey)) minDate,
    (SELECT MAX(activityDate) FROM schedule INNER JOIN activitylog USING (refkey)) maxDate,
FROM schedule a INNER JOIN activitylog USING (refkey)
LEFT JOIN branches b ON a.requestBranch=b.branchId
LEFT JOIN branches c ON a.assignBranch=c.branchId
WHERE a.assignBranch='012';
</code></pre>
<p>Result of above query:</p>
<pre><code>REQUESTBRANCH | ASSIGNBRANCH |    NAME      |   ADDRESS  |    MINDATE    |    MAXDATE 
Bulacan       |    Manila    |  CUSTOMER A  |  Bulacan   |  2025-09-12   |  2025-09-15
Cavite        |    Manila    |  CUSTOMER B  |  Cavite    |  2025-09-12   |  2025-09-15
Cebu          |    Manila    |  CUSTOMER C  |  Cebu      |  2025-09-12   |  2025-09-15
</code></pre>
<p>Problem is:
The minDate &amp; maxDate are the same in each rows which should be the minDate where it first came in the database and the maxDate is the date where it's completed.</p>
",0,1,1,2025-09-24T04:21:26+00:00,2,115,True
79774712,3058288,,mysql,Cleanup MySQL tables after duplicate records where created and are referenced by second table,"<p>After a long period of this going wrong we found out that there is a error in our data set.</p>
<p>In the manufacturers table, manufacturers are often added multiple times and the product in the product table is referencing the duplicate ID's for the manufacturer.</p>
<p>This question is only about fixing these tables, we already prevent this from happening again.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>manufacturers_id</th>
<th>manufacturers_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Manufacturer #1</td>
</tr>
<tr>
<td>2</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>3</td>
<td>Manufacturer #3</td>
</tr>
<tr>
<td>4</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>5</td>
<td>Manufacturer #3</td>
</tr>
<tr>
<td>6</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>7</td>
<td>Manufacturer #1</td>
</tr>
<tr>
<td>8</td>
<td>Manufacturer #3</td>
</tr>
<tr>
<td>9</td>
<td>Manufacturer #2</td>
</tr>
</tbody>
</table></div>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>products_id</th>
<th>manufacturers_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>7</td>
<td>7</td>
</tr>
<tr>
<td>8</td>
<td>8</td>
</tr>
<tr>
<td>9</td>
<td>9</td>
</tr>
</tbody>
</table></div>
<p>We need to achieve two things:</p>
<ol>
<li>Remove the duplicate entries from the table manufacturers and keep the first entry</li>
<li>Update the product table where the duplicate manufacturers ID's are being replaced by the remaining first Id for that manufacturer</li>
</ol>
<p>Each step can be done manually but the quantity of different manufacturers and products makes this not suitable for a manual task.
And I am lacking the needed query knowledge, so help would be welcome.</p>
<p>This would be the desired result:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>manufacturers_id</th>
<th>manufacturers_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Manufacturer #1</td>
</tr>
<tr>
<td>2</td>
<td>Manufacturer #2</td>
</tr>
<tr>
<td>3</td>
<td>Manufacturer #3</td>
</tr>
</tbody>
</table></div>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>products_id</th>
<th>manufacturers_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>5</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
</tr>
<tr>
<td>8</td>
<td>3</td>
</tr>
<tr>
<td>9</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>Table structure:</p>
<pre><code>CREATE TABLE manufacturers 
(
    `manufacturers_id` int(11) NOT NULL,
    `manufacturers_name` varchar(32) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci;

ALTER TABLE manufacturers
    ADD PRIMARY KEY (`manufacturers_id`),
    ADD KEY `IDX_MANUFACTURERS_NAME` (`manufacturers_name`);
  
INSERT INTO manufacturers (`manufacturers_id`, `manufacturers_name`) 
VALUES (1, 'Manufacturer #1'),
       (2, 'Manufacturer #2'),
       (3, 'Manufacturer #3'),
       (4, 'Manufacturer #2'),
       (5, 'Manufacturer #3'),
       (6, 'Manufacturer #2'),
       (7, 'Manufacturer #1'),
       (8, 'Manufacturer #3'),
       (9, 'Manufacturer #2');

CREATE TABLE `products` 
(
    `products_id` int(11) NOT NULL,
    `manufacturers_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 COLLATE=utf8mb3_unicode_ci;

ALTER TABLE `products`
    ADD PRIMARY KEY (`products_id`);
 
INSERT INTO `products` (`products_id`, `manufacturers_id`) 
VALUES (1, 1),
       (2, 2),
       (3, 3),
       (4, 4),
       (5, 5),
       (6, 6),
       (7, 7),
       (8, 8),
       (9, 9);
</code></pre>
",4,5,1,2025-09-25T10:55:01+00:00,2,129,True
79775928,23473660,,mysql,Solving Deadlock Issue with MySQL REPLACE INTO,"<p>MySQL's REPLACE INTO causes deadlocks during high-concurrency batch data insertion. What locks does REPLACE INTO acquire during execution, and why do deadlocks occur?</p>
<p><strong>Known Clues:</strong></p>
<ol>
<li>The scenario where data already exists before insertion is ruled out.</li>
</ol>
<blockquote>
<p>However, concurrent updates to the inserted data by other threads (before or after insertion) are possible, though updates are single-row operations.
2. Table structure (200 million rows):</p>
</blockquote>
<pre><code>CREATE TABLE downlink_record (
  id bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT 'Auto-increment primary key',
  message_id varchar(128) NOT NULL,
  template_id varchar(20) NOT NULL,
  address varchar(256) NOT NULL,
  sign varchar(64) DEFAULT NULL,
  sign_id varchar(64) DEFAULT NULL,
  order_id varchar(256) DEFAULT NULL,
  sms_type int(11) DEFAULT NULL,
  amount int(11) DEFAULT NULL,
  report_num int(11) DEFAULT NULL,
  agent_name varchar(128) DEFAULT NULL,
  account_name varchar(128) DEFAULT NULL,
  team_id varchar(64) DEFAULT NULL,
  country_name varchar(20) DEFAULT NULL,
  router_name varchar(128) DEFAULT NULL,
  operator_name varchar(128) DEFAULT NULL,
  error_code varchar(128) DEFAULT NULL,
  send_status int(11) DEFAULT NULL,
  verify_status int(11) DEFAULT NULL,
  retry_count int(11) DEFAULT NULL,
  create_time bigint(20) DEFAULT NULL,
  deliver_time bigint(20) DEFAULT NULL,
  report_time bigint(20) DEFAULT NULL,
  receive_time bigint(20) DEFAULT NULL,
  verify_time bigint(20) DEFAULT NULL,
  last_update_time bigint(20) DEFAULT NULL,
  content text,
  extra_info text,
  PRIMARY KEY (id),
  UNIQUE KEY idx_message_id_retry_count (message_id, retry_count),
  KEY idx_message_id (message_id(120)),
  KEY idx_team_id (team_id),
  KEY idx_order_agent (order_id(191), agent_name),
  KEY idx_deliver_time (deliver_time),
  KEY idx_receive_time (receive_time),
  KEY idx_msg_addr_temp_status (address, template_id, send_status),
  KEY idx_create_time (create_time),
  KEY idx_template_id_send_status_create_time (template_id, send_status, create_time)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
</code></pre>
<ol start=""3"">
<li>All deadlocks involve the idx_message_id_retry_count index. Deadlock log:</li>
</ol>
<pre><code>2025-09-24 18:53:59 0x7fd24dbff700
*** (1) TRANSACTION:
TRANSACTION 55063751429, ACTIVE 0 sec inserting
mysql tables in use 1, locked 1
LOCK WAIT 4 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2
MySQL thread id 670729, OS thread handle 140553453934336, query id 6781831678 10.142.167.7 sms_x update
REPLACE INTO sms_downlink_record (...) VALUES (...) -- Truncated; no duplicate data between REPLACE operations
*** (1) WAITING FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 75 page no 16582916 n bits 344 index idx_message_id_retry_count of table sms.sms_downlink_record trx id 55063751429 lock_mode X insert intention waiting

*** (2) TRANSACTION:
TRANSACTION 55063751430, ACTIVE 0 sec inserting, thread declared inside InnoDB 4998
mysql tables in use 1, locked 1
4 lock struct(s), heap size 1136, 5 row lock(s), undo log entries 3
MySQL thread id 671750, OS thread handle 140541224285952, query id 6781831679 10.142.210.13 sms_x update
REPLACE INTO sms_downlink_record (...) VALUES (...) -- Truncated; no duplicate data between REPLACE operations
*** (2) HOLDS THE LOCK(S):
RECORD LOCKS space id 75 page no 16582916 n bits 344 index idx_message_id_retry_count of table sms.sms_downlink_record trx id 55063751430 lock_mode X
*** (2) WAITING FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 75 page no 16582916 n bits 344 index idx_message_id_retry_count of table sms.sms_downlink_record trx id 55063751430 lock_mode X insert intention waiting

*** WE ROLL BACK TRANSACTION (1)
</code></pre>
<p>I tried running a concurrent replace into script locally, but I was never able to reproduce the issue. This deadlock problem occurred in our company's centrally managed database with access control, and I cannot arbitrarily write data to the production environment.</p>
<p>I would like to know under what circumstances a GAP lock or X lock will be added to replace into</p>
",2,2,0,2025-09-26T12:27:56+00:00,1,62,True
79777856,22007794,,mysql,Laravel migrations on production with large dataset take too long and cause deployment timeout,"<p>I'm facing an issue while deploying code to my production server using GitHub Actions (CI/CD).</p>
<ul>
<li>I have 3 environments: <code>dev</code>, <code>staging</code>, and <code>production</code>.</li>
<li>On the dev server, migrations run fine because there’s little data.</li>
<li>On production, each table has around 100k–500k rows.</li>
</ul>
<p>When I deploy, the pipeline runs <code>php artisan migrate</code> to apply all pending migrations.
Some migrations run quickly, but when it hits a migration that alters a large table, the process hangs and eventually times out:</p>
<pre><code>Use the composer fund command to find out more!
Executing Command: cd /home/***/htdocs/backend.tudu.tech/releases/2025-09-29-06-08-28-main &amp;&amp; php8.2 artisan migrate ...

INFO  Running migrations.

2025_09_20_115652_add_height_to_questions_table ............. 786,165ms DONE
2025_09_23_103339_add_dark_to_questions_table ................ 10,031ms DONE
2025_09_23_114036_add_model_to_users_table The process &quot;cd /home/***/htdocs/backend.tudu.tech/releases/2025-09-29-06-08-28-main &amp;&amp; php8.2 artisan migrate&quot; exceeded the timeout of 3600 seconds.
Error: Process completed with exit code 1
</code></pre>
<p>At this point, the deployment fails and the site goes down because the migration is still running or locked.</p>
<p>My questions are:</p>
<ul>
<li>How can I safely run migrations on large production tables without hitting timeouts or locking the database for so long?</li>
<li>Is there a recommended approach for handling schema changes on large datasets in Laravel (e.g., adding columns, altering columns) so that deployments don’t fail?</li>
<li>Should I separate &quot;heavy&quot; migrations from the automated CI/CD pipeline?</li>
</ul>
<p>Additional context:</p>
<ul>
<li><p>Laravel 10</p>
</li>
<li><p>PHP 8.1.32</p>
</li>
<li><p>Database: MySQL</p>
</li>
<li><p>mysql 8.0.42-0</p>
</li>
<li><p>Example migration that caused the issue:</p>
</li>
</ul>
<pre class=""lang-php prettyprint-override""><code>public function up(): void
{
    Schema::table('users', function (Blueprint $table) {
        $table-&gt;string('device_model')-&gt;nullable();
    });
}
</code></pre>
<p>For Future users:
I have removed the migrations from CI/CD and it works fine.</p>
",3,3,0,2025-09-29T07:41:20+00:00,0,169,False
79780906,12980093,,mysql,Why correlated scalar is 10x times slower in MySQL comparing to PG,"<p>Let's create a table with 3000 rows</p>
<pre><code>create table tt(id int, txt text);

insert into tt
with recursive r(id) as
(select 1 union all select id + 1 from r where id &lt; 3e3)
select id, concat('name', id)
from r;
</code></pre>
<p>The same query in both databases results in very different performance:</p>
<pre><code>select sum(id), 
       sum((select count(*) 
            from tt t1 
            where t1.id = t2.id)) cnt
from tt t2
</code></pre>
<p><strong>MYSQL</strong></p>
<pre><code>mysql&gt; explain analyze
    -&gt; select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
    -&gt; from tt t2\G
*************************** 1. row ***************************
EXPLAIN: -&gt; Aggregate: sum(t2.id), sum((select #2))  (cost=602 rows=1) (actual time=7542..7542 rows=1 loops=1)
    -&gt; Table scan on t2  (cost=302 rows=3000) (actual time=0.025..2.75 rows=3000 loops=1)
-&gt; Select #2 (subquery in projection; dependent)
    -&gt; Aggregate: count(0)  (cost=62.5 rows=1) (actual time=2.51..2.51 rows=1 loops=3000)
        -&gt; Filter: (t1.id = t2.id)  (cost=32.5 rows=300) (actual time=1.25..2.51 rows=1 loops=3000)
            -&gt; Table scan on t1  (cost=32.5 rows=3000) (actual time=0.00256..2.31 rows=3000 loops=3000)

1 row in set, 1 warning (7.54 sec)
</code></pre>
<p><strong>PG</strong></p>
<pre><code>postgres=# explain analyze
postgres-# select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
postgres-# from tt t2;
                                                   QUERY PLAN
-----------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=163599.50..163599.51 rows=1 width=40) (actual time=684.339..684.340 rows=1 loops=1)
   -&gt;  Seq Scan on tt t2  (cost=0.00..47.00 rows=3000 width=4) (actual time=0.013..0.223 rows=3000 loops=1)
   SubPlan 1
     -&gt;  Aggregate  (cost=54.50..54.51 rows=1 width=8) (actual time=0.227..0.227 rows=1 loops=3000)
           -&gt;  Seq Scan on tt t1  (cost=0.00..54.50 rows=1 width=0) (actual time=0.113..0.223 rows=1 loops=3000)
                 Filter: (id = t2.id)
                 Rows Removed by Filter: 2999
 Planning Time: 0.663 ms
 Execution Time: 684.512 ms
(9 rows)
</code></pre>
<p>As you can see the difference is ~7.0 seconds vs ~0.7 seconds.</p>
<p>So it both cases it does not unnest subquery and executes it 3000 times.</p>
<p>But in MySQL one execution takes 2+ ms while in PG it takes 0.2 ms.</p>
<p><strong>Question asked to understand this difference and not to make query faster.</strong></p>
<p><strong>Obviously we can create an index or rewrite to explicit join.</strong></p>
<pre><code>select sum(t1.id), sum(cnt) cnt
from tt t2
join (select id, sum(1) cnt from tt group by id) t1 on t1.id = t2.id;
</code></pre>
<p>PS. Both RDBMS have default settings.</p>
<pre><code>mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 8.0.43    |
+-----------+
1 row in set (0.00 sec)

postgres=# select version();
                          version
------------------------------------------------------------
 PostgreSQL 15.1, compiled by Visual C++ build 1914, 64-bit
(1 row)
</code></pre>
<p><strong>UPDATE</strong></p>
<p>As requested in the comments - adding comparison on Ubuntu.</p>
<p><strong>Still 10x difference.</strong></p>
<p><strong>MYSQL</strong></p>
<pre><code>mysql&gt; explain analyze
    -&gt; select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
    -&gt; from tt t2\G
*************************** 1. row ***************************
EXPLAIN: -&gt; Aggregate: sum(t2.id), sum((select #2))  (cost=600 rows=1) (actual time=5416..5416 rows=1 loops=1)
    -&gt; Table scan on t2  (cost=300 rows=3000) (actual time=0.0244..5.38 rows=3000 loops=1)
-&gt; Select #2 (subquery in projection; dependent)
    -&gt; Aggregate: count(0)  (cost=60.3 rows=1) (actual time=1.79..1.79 rows=1 loops=3000)
        -&gt; Filter: (t1.id = t2.id)  (cost=30.3 rows=300) (actual time=0.906..1.79 rows=1 loops=3000)
            -&gt; Table scan on t1  (cost=30.3 rows=3000) (actual time=0.00668..1.56 rows=3000 loops=3000)

1 row in set, 1 warning (5.41 sec)
</code></pre>
<p><strong>PG</strong></p>
<pre><code>postgres=# explain analyze
select sum(id), sum((select count(*) from tt t1 where t1.id = t2.id)) cnt
from tt t2;
                                                   QUERY PLAN                                                    
-----------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=163599.50..163599.51 rows=1 width=40) (actual time=526.720..526.721 rows=1 loops=1)
   -&gt;  Seq Scan on tt t2  (cost=0.00..47.00 rows=3000 width=4) (actual time=0.012..0.301 rows=3000 loops=1)
   SubPlan 1
     -&gt;  Aggregate  (cost=54.50..54.51 rows=1 width=8) (actual time=0.175..0.175 rows=1 loops=3000)
           -&gt;  Seq Scan on tt t1  (cost=0.00..54.50 rows=1 width=0) (actual time=0.087..0.173 rows=1 loops=3000)
                 Filter: (id = t2.id)
                 Rows Removed by Filter: 2999
 Planning Time: 0.078 ms
 Execution Time: 526.766 ms
(9 rows)
</code></pre>
",0,4,4,2025-10-02T12:16:14+00:00,2,223,True
79781557,17218429,,mysql,MySQL: Problem with extracting first character from text field with Regular Expressions,"<p>I'm not able to get the first character of a string with the dot operator – it returns the rest of the string, not the character:</p>
<pre><code>CREATE TABLE test (
  name VARCHAR(10)
);
INSERT INTO test (name) VALUES ('abc');
INSERT INTO test (name) VALUES ('DEF');

SELECT
  REGEXP_REPLACE(name, '^(.)', '$1')
FROM
  test;
</code></pre>
<p>If the field name contains 'abc' it will return 'abc' instead of 'a' like in PHP or Java.</p>
<p>MySQL example: <a href=""https://onecompiler.com/mysql/43yhkkk3e"" rel=""nofollow noreferrer"">https://onecompiler.com/mysql/43yhkkk3e</a></p>
<p>Note: I'm aware that I could use LEFT(name, 1) here, but the actual RegEx pattern is more complex.</p>
",0,0,0,2025-10-03T08:15:16+00:00,3,66,True
79783029,243872,,mysql,Syntax error 1064 with LOAD DATA ... FIELDS OPTIONALLY ENCLOSED BY &#39;&quot;&#39;,"<p>My SQL statement in MySQL 5.7.44, Windows 10:</p>
<pre class=""lang-sql prettyprint-override""><code>drop table if exists test;
create table test (col1 VARCHAR(64), col2 VARCHAR(80) );
LOAD DATA LOCAL INFILE &quot;t.csv&quot; INTO TABLE test FIELDS OPTIONALLY ENCLOSED BY '&quot;' ESCAPED BY '&quot;' COLUMNS TERMINATED BY ';' LINES TERMINATED BY '\r\n' ;
</code></pre>
<p>The data:</p>
<pre class=""lang-none prettyprint-override""><code>E605;
E507;&quot;A string
spanning several lines &quot;some Umlauts in &quot;&quot;üöä&quot;&quot;  to show&quot;&quot;
escaping
last line&quot;
E600;&quot;once
again&quot;
</code></pre>
<p>As hexdump:</p>
<pre class=""lang-none prettyprint-override""><code>00000000  45 36 30 35 3b 0d 0a 45  35 30 37 3b 22 41 20 73  |E605;..E507;&quot;A s|
00000010  74 72 69 6e 67 0d 0a 73  70 61 6e 6e 69 6e 67 20  |tring..spanning |
00000020  73 65 76 65 72 61 6c 20  6c 69 6e 65 73 20 22 22  |several lines &quot;&quot;|
00000030  73 6f 6d 65 20 55 6d 6c  61 75 74 73 20 69 6e 20  |some Umlauts in |
00000040  22 22 c3 bc c3 b6 c3 a4  22 22 20 20 74 6f 20 73  |&quot;&quot;......&quot;&quot;  to s|
00000050  68 6f 77 22 22 0d 0a 65  73 63 61 70 69 6e 67 22  |how&quot;&quot;..escaping&quot;|
00000060  22 0d 0a 6c 61 73 74 20  6c 69 6e 65 22 0d 0a 45  |&quot;..last line&quot;..E|
00000070  36 30 30 3b 22 6f 6e 63  65 0d 0a 61 67 61 69 6e  |600;&quot;once..again|
00000080  22 0d 0a                                          |&quot;..|
00000083
</code></pre>
<p>The error occurs when I add the <code>FIELDS</code> related keywords <code>FIELDS OPTIONALLY ENCLOSED BY '&quot;' ESCAPED BY '&quot;'</code>. Intention is to have <code>VARCHAR</code>s either without <code>&quot;</code> and <code>VARCHAR</code>s framed in <code>&quot;</code>. But the outer framing <code>&quot;</code>s should not appear in the data.</p>
",0,1,1,2025-10-05T14:57:29+00:00,1,107,True
79785140,505306,,mysql,Using `with` to find the instructors with the highest salary,"<p>I am learning SQL and was testing the <code>with</code> expression syntax when I ran into this error for my SQL query.  I want to get the ids and names of all instructors from a university databse with the maximum salary. This database is from the book Database System Concepts by Silberschatz et al.</p>
<p>The schema of the instructor relation is <code>instructor(ID, name, dept_name, salary)</code> where the last attribute salary is a numeric type, MySQL Workbench is complaining that</p>
<blockquote>
<p>Error Code: 1054. Unknown column 'max_salary.value' in 'where clause'</p>
</blockquote>
<p>Here is my code</p>
<pre><code>-- MySQL workbench complains with the above error!
with max_salary(value) as 
             (select max(salary) 
              from instructor)
select ID, name
from instructor
where max_salary.value = instructor.salary;
</code></pre>
<p>I know how to rewrite this without the <code>with</code> clause, as follows
but I was having trouble with the syntax of <code>with</code> for this particular problem.</p>
<pre><code>-- This works!
select ID, name
from instructor
where instructor.salary = (select max(salary) from 
                           instructor )

</code></pre>
",0,1,1,2025-10-08T06:38:56+00:00,1,109,True
79786562,469352,"Kensington, MD",mysql,Is there an option to run a large query without causing it to pollute the InnoDB cache?,"<p>I want to run a bunch of large background queries where I don't want these lower priority queries to fill up the InnoDB cache with rows that will flush out cached rows needed by higher priority interactive users.</p>
<p>Is there an option to do this?  Eg. run this query without causing it to flush the InnoDB cache.</p>
",3,3,0,2025-10-09T14:53:06+00:00,1,70,True
79790370,22248427,,mysql,Why are these simple MYSQL update statements failing,"<p>I have been coding php for years, but today I have encountered a problem I have never seen before.  Some very simple update statements are failing -- or rather instead of updating, they  are replacing existing data with blanks.</p>
<p>This is not a simple debugging issue. I have rewritten these statements several times over the last three hours and they never work -- except to remove existing data when I click submit on the script.</p>
<p>When I echo the thisID variable and the rating variable after the REQUEST and UPDATE statements, the updated variable information is present. It looks like everything is in order to update the rating field appropriately.  However, when the Select statement runs after the UPDATE attempts, it brings up blanks for the rating field (and any other field that I might try to update with this script).</p>
<p>The rating field is of type INT(3) and I am always entering integers of 3 digits or less in my attempt to get this code to work.  I have the same problem with another field in the table that is called title.  The title field is VAR(255) and yet I have been unable to write a script that will update that field by adding text to it.  I write such code all the time, so I'm not sure what is going wrong.</p>
<p>I don't seem to be using any reserved words for field names.</p>
<p>I have been able to manually add data to the rating and title fields in phpMyAdmin.  However, when I run my script (below), these manually changed fields revert to blank fields.</p>
<p>So basically I have a table called pics that can be updated in phpMyAdmin but not using php code.  The php code can access the data, but any attempts to update the data results in data loss.</p>
<p>I tried writing a prepared statement as well. It failed as well and in the same way.  The prepared statement is shown at the very bottom of this post.</p>
<p>When I look at the page source in my browser, I see that the form contains the numeric data for the $thisID field, as it should, in a hidden field.  This script is it, by the way: there are no other forms on the page. As stated, I have modified the script to try updating the title field, but with no luck.</p>
<pre><code>&lt;?php   
$thisID=$_REQUEST['thisID'];
$rating=$_REQUEST['rating'];
    
$link-&gt;query(&quot;UPDATE pics SET rating='$rating' WHERE ID='$thisID'&quot;);
$k=$link-&gt;affected_rows;
echo &quot;K=&quot;.$k.&quot; WHERE ID=&quot;.$thisID.&quot; AND rating=&quot;.$rating.&quot;&lt;BR&gt;&quot;;

$sql = &quot;SELECT * FROM pics ORDER BY rating DESC&quot;;
$result = mysqli_query($link, $sql);

while($row = mysqli_fetch_assoc($result)) {
    $rating=$row['rating'];
    $thisID=$row['ID'];
    ?&gt;
    &lt;form name=&quot;&lt;?php echo $thisID ?&gt;&quot; method=&quot;post&quot;&gt;
      rating: &lt;input type=&quot;text&quot; name=&quot;rating&quot; size=3 value=&quot;&lt;?php echo $rating ?&gt;&quot;&gt;
      &lt;input type=&quot;hidden&quot; name=&quot;thisID&quot; value=&quot;&lt;?php echo $thisID ?&gt;&quot;&gt;
      &lt;input type=&quot;submit&quot; name=&quot;submit&quot; value=&quot;submit&quot;&gt;
    &lt;/form&gt;
&lt;?php
}
?&gt;
</code></pre>
<p>Prepared statement also tried:</p>
<pre><code>$stmt=$link-&gt;prepare(&quot;UPDATE pics SET rating=? WHERE ID=?&quot;);
$stmt-&gt;bind_param(&quot;dd&quot;,$rating,$thisID);
$stmt-&gt;execute();
</code></pre>
",-3,1,4,2025-10-14T15:49:23+00:00,1,282,False
79795103,810702,,mysql,MySQL USE command not working from PowerShell,"<p>Here is a snippet from a PowerShell script:</p>
<pre><code>$sql.CommandText = &quot;CREATE DATABASE IF NOT EXISTS ``$dbName``;&quot;
$rowsInserted = $sql.ExecuteNonQuery()

$sql.CommandText = &quot;USE ``$dbName``;&quot;
$rowsInserted = $sql.ExecuteNonQuery()
</code></pre>
<p>The <code>CREATE DATABASE</code> command works fine.  The <code>USE</code> command does not.  I get no error message from the Powershell ISE.  <code>$rowsInserted</code> is 1 when I execute the <code>CREATE DATABASE</code> command and 0 when I execute the <code>USE</code> command.</p>
<p>I am running the ISE as an administrator.  I have created the database outside of PowerShell and executed the USE command first.  Still doesn't work.   The database name is as simple as &quot;test&quot;.</p>
<p>All the commands work from MySQL Workbench and from a command window.</p>
<p>Any help would be much appreciated.</p>
",0,1,1,2025-10-20T16:40:23+00:00,2,96,True
79795265,2661251,"Atlanta, GA",mysql,Generic Date Add SQL that works in both MySQL and H2,"<pre><code>select DATE_ADD(CURRENT_DATE(), INTERVAL -30 DAY)
</code></pre>
<p>works in MySQL.</p>
<pre><code>SELECT DATEADD(‘DAY’, -30, CURRENT_DATE())
</code></pre>
<p>works in H2(2.2.224).</p>
<p>I am looking for a generic SQL solution that works in both the databases.</p>
<p>I tried</p>
<pre><code>SELECT CURRENT_DATE() - INTERVAL :days DAY
</code></pre>
<p>which worked in MySQL but didn't work in H2.</p>
",2,2,0,2025-10-20T20:36:00+00:00,2,101,True
79795528,1941545,,mysql,How to get the number of rows from multiple related tables with one query,"<p>I have four tables, one primary and three dependents, let's call them A, B, C and D.<br>
All tables have the column <code>ID</code>, with <code>ID</code> being the primary key for A and the foreign key for B-D.<br>
B-D also have a column called <code>timestamp</code> and (non-unique) indices combining <code>ID</code> and <code>timestamp</code> (in this order).</p>
<p>For a given subset of ID's from 'A', how do I get the number of rows in the tables B-D, optionally filtered by a range of timestamps?<br>
Preferably without using dependent subqueries, because these are (supposedly) 'very inefficient'.</p>
<p>What I've tried is something like this:</p>
<pre><code>SELECT A.`ID`,
       (COUNT(`B`.`timestamp`) + COUNT(`C`.`timestamp`) + COUNT(`D`.`timestamp`)) as `eventCount`
FROM `A`
LEFT JOIN `B` ON A.`ID` = `B`.`ID`
LEFT JOIN `C` ON A.`ID` = `C`.`ID`
LEFT JOIN `D` ON A.`ID` = `C`.`ID`
GROUP BY A.`ID`
</code></pre>
<p>However this yields a ridiculously high number akin to a cartesian product (10296 instead of 26 + 12 + 11, as the actual data for a specific row would be).<br>
I have considered using <code>COUNT(DISTINCT...)</code>, but am worried that it would also filter out duplicate timestamps, which are perfectly possible.</p>
<p>I have added <a href=""https://www.db-fiddle.com/f/bYucazioSy4ffXiR9m7vch/0"" rel=""nofollow noreferrer"">a fiddle</a> for you to try your queries against.
Additionally I want to note, that I do not, in fact, have 'a small set of data', I am talking about multiple 10ks of rows, which is why I am worried about efficiency to begin with.</p>
<p>However the query itself should have little to do with 'how many rows' there are.</p>
",3,3,0,2025-10-21T07:02:30+00:00,5,177,True
79799408,15152743,,mysql,Is it possible to force mysql server authentication using django.db.backends.mysql?,"<p>it's my first question on stack overflow because I can't find relevant information in Django documentation. Is it possible to force mysql server authentication with ssl using django.db.backends.mysql? I have checked its implementation in Django Github and it seems it supports only 3 ssl arguments: ca, cert and key. What I need is equivalent of --ssl-mode=VERIFY_IDENTITY. Has anyone found some workaround for this problem?
Here is my current configuration. TLS channel is working as expected, but identity of MySQL server is not validated.</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': env('DB_NAME'),
        'USER': env('DB_USER'),
        'PASSWORD': env('DB_PASSWORD'),
        'HOST': env('DB_HOST'),
        'PORT': env('DB_PORT'),
        'CONN_MAX_AGE': 600,
        'OPTIONS':{
            'ssl':{
                'ca': env('CA_CERT'),
                'cert': env('CERT'),
                'key': env('KEY')
            }
        }
    }
}
</code></pre>
",1,1,0,2025-10-25T09:21:43+00:00,1,70,True
79799532,17866323,,mysql,bash script concat() in prepared statement for mysql,"<p>This gives a syntax error:</p>
<pre><code>read -p &quot;entry: &quot; entry
sql=&quot;select concat('entry ', id) from mytbl where id = ?&quot;;
$mysql_conn &quot;prepare stmnt from '${sql}'; set @id='${entry}';
 execute stmnt using @id;deallocate prepare stmnt&quot;;
</code></pre>
<p>It works if used directly in mysql, but not through a bash script. If only <code>id</code> is selected it works. But <code>concat()</code> throws it off. It will also work if a direct query is made without a prepared statement. What gives?</p>
<p>Also tried:</p>
<pre><code>select concat('entry ',?)
</code></pre>
<p>and pass <code>@id</code> but it failed too.</p>
",3,4,1,2025-10-25T13:37:45+00:00,1,120,True
79799610,30826480,,mysql,Dictionary memory allocated and buffer pool in mysql,"<p>What is the dictionary memory and where does it stores, is it uses the memory of buffer pool or a seperate memory, does buffer pool stores metadata pages also or only user table and indexes, in both the older and new version of mysql</p>
",-1,0,1,2025-10-25T16:29:44+00:00,1,67,False
79800009,10695304,,mysql,Spatial index will not be used &#39;cause it has no SRID attribute,"<p>I am very confused now, because I cannot get this to work. I have a <code>tbl_address</code> which already has <code>longitude</code> and <code>latitude</code>. I want to add a spatial index for my search. So I want to add a location POINT column in my database. I tried this one:</p>
<pre><code>alter table tbl_address
add column location POINT GENERATED ALWAYS AS (ST_SRID(POINT(longitude, latitude), 4326)) STORED NOT NULL;
</code></pre>
<p>But when I then try to add my SI, I receive this warning back:</p>
<pre><code>CREATE SPATIAL INDEX idx_location ON tbl_address (location);
0 row(s) affected, 1 warning(s): 3674 The spatial index on column 'location' will not be used by the query optimizer since the column does not have an SRID attribute. Consider adding an SRID attribute to the column. Records: 0  Duplicates: 0  Warnings: 1
</code></pre>
<p>I get that just my entries do have the <code>4326</code> <code>srid</code>, but if I try to add it to the column itself (POINT <code>SRID</code> <code>4326</code>) I am receiving a syntax error on <code>GENERATED ALWAYS AS</code>.</p>
<p>How can I make it work? It would be great to have location updated when longitude/latitude changes. Do I really need to implement a trigger? I am using MySQL 8.</p>
<p>Thanks!</p>
<p>Edit:
My mysql version is 8.0.43-0ubuntu0.22.04.1 and this is the test table I am using:</p>
<pre><code>CREATE TABLE IF NOT EXISTS tbl_address
(
address_id int not null auto_increment,
latitude DECIMAL(9,2) NOT NULL default 0,
longitude DECIMAL(9,2) NOT NULL default 0,
location POINT not null SRID 4326,
Primary Key (address_id)
);
</code></pre>
<p>Not sure why it is working for you guys, maybe it is an issue with this specific version?</p>
",4,4,0,2025-10-26T11:28:22+00:00,1,158,True
79802698,11476399,"Rolle, Suisse",mysql,Mysql create a view for Power bi,"<p>I would like to create a view in mysql who show the place of my order as text. This place is a json object (example <code>{&quot;sap_hold&quot;: 1, &quot;in_licensing&quot;: 1}</code>). I would like to add a column in my view called <strong>software_log_places_text</strong>.</p>
<p>Power BI seems able to read this but when I try to add the value in a table I get the folowing error :</p>
<pre><code>OLE DB or ODBC error: [Expression.Error] We couldn't fold the expression to the data source. Please try a simpler expression.
</code></pre>
<p><a href=""https://i.sstatic.net/gwkgifKI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gwkgifKI.png"" alt=""enter image description here"" /></a></p>
<pre><code>SELECT
    *,
    CAST(CONVERT(software_log_places USING utf8mb4) AS CHAR(5000)) AS software_log_places_text
from `order`
</code></pre>
",0,0,0,2025-10-28T10:57:28+00:00,0,65,False
79802986,6507913,,mysql,MySQL order by id but prioritize an empty value of a certain column,"<p>Mainly, I want to order the ids in descending direction but also, how can I sort parent product/items (the ones with empty ean) first and their child rows below them?</p>
<p>Here's an example data from the database:</p>
<pre><code>| id   | ean           | name                                                        |
| 1005 | 4064722314889 | Camera Protector Series 15T - Model: 15T - Qty: 2           |
| 1004 | 4064722314896 | Camera Protector Series 15T - Model: 15T Pro - Qty: 2       |
| 1003 |               | Camera Protector Series 15T                                 |
| 1002 | 4064722314162 | Smartwatch Case Series GT6 Pro - Color: Black - Size: 46 mm |
| 1001 |               | Smartwatch Case Series GT6 Pro                              |

and so on...
</code></pre>
<p>Now, when I query it, I want to group it by id (desc) but also make sure to prioritize the row with an empty ean. Here's the desired result</p>
<pre><code>| id   | ean           | name                                                        |
| 1003 |               | Camera Protector Series 15T                                 |
| 1005 | 4064722314889 | Camera Protector Series 15T - Model: 15T - Qty: 2           |
| 1004 | 4064722314896 | Camera Protector Series 15T - Model: 15T Pro - Qty: 2       |
| 1001 |               | Smartwatch Case Series GT6 Pro                              |
| 1002 | 4064722314162 | Smartwatch Case Series GT6 Pro - Color: Black - Size: 46 mm |
</code></pre>
<p>and so on...</p>
<p>Thanks in advance!</p>
",-2,0,2,2025-10-28T15:19:44+00:00,1,75,False
79803168,31767397,,mysql,Join Query with an Aggregate Function (number of bowl games won for colleges that have won at least once),"<p>I'm working through my Data Management Applications labs and I'm stumped on one particular lab. I'm definitely missing something incredibly obvious, and I'd rather look dumb on the Internet than look dumb in front of my professor lol.</p>
<p>The lab's instructions are: <strong>&quot;Write a SELECT statement listing the college name, college city, college state, and the number of bowl game wins for colleges that have won at least one bowl game.&quot;</strong> Here are the table outlines:</p>
<p>&quot;The College table has the following columns:</p>
<ul>
<li>CollegeID - integer, primary key</li>
<li>Name - variable-length string</li>
<li>City - variable-length string</li>
<li>State - two character string</li>
</ul>
<p>The BowlGame table has the following columns:</p>
<ul>
<li>BowlGameID - integer, primary key</li>
<li>Bowl - variable-length string</li>
<li>Stadium - variable-length string</li>
<li>City - variable-length string</li>
<li>State - two character string</li>
<li>WinningCollegeID - integer, foreign key referencing CollegeID&quot;</li>
</ul>
<p>Everything I've tried has been some variation of this:</p>
<pre><code>SELECT Name, C.City, C.State, COUNT(WinningCollegeID) AS Wins
FROM College C
INNER JOIN BowlGame
ON CollegeID = WinningCollegeID
GROUP BY Name
HAVING COUNT(WinningCollegeID) &gt;= 1;
</code></pre>
<p>I keep getting an error that my GROUP BY clause is incompatible with the expressions in my SELECT statement, but I can't figure out why. I tried combinations of non-aggregated columns in the GROUP BY clause, but that always produced an error too. I think this is where I'm messing up. Most of the MySQL documentation for GROUP BY frankly went over my head. Any help is appreciated, thanks in advance ('v')/</p>
",2,2,0,2025-10-28T18:52:26+00:00,1,81,True
79804271,13667729,,mysql,Performance issue with ST_CONTAINS,"<p>I have a performance issue with a query in MySQL. I need to compare location data and am trying to use ST_CONTAINS in a join. However, I am having a performance issue as it is quite slow, taking around 16 seconds to retrieve 1,000 records. I need to process several thousand records in the query. I am searching the internet for the best solution. I have already tried using other checking functions, ST_Within and MBRContains. I added a spatial index for the geo column, setting the same SRID, but unfortunately the index did not want to use itself. I forced its use with USE INDEX, but this made the query even slower. I created a separate point column to get rid of Point(address.lat, address.lng), but to no avail. Is there anything else I can do? Has anyone encountered this problem or has any ideas for a solution, or can point out what I am doing wrong? Additionally, I am including the explain queries.</p>
<p>Time execute: 12s</p>
<pre><code>SELECT * FROM table_localities AS address
LEFT JOIN table_province AS province ON (ST_CONTAINS(province.geo, Point(address.lat, address.lng)))
WHERE province.delivery_area_id = :id
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>partitions</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>filtered</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>address</td>
<td></td>
<td>ALL</td>
<td>fk_delivery_area_id</td>
<td></td>
<td></td>
<td></td>
<td>1850</td>
<td>50.0</td>
<td>Using where</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>region</td>
<td></td>
<td>ref</td>
<td>fk_delivery_area_id,idx_geo</td>
<td>fk_delivery_area_id</td>
<td>8</td>
<td>const</td>
<td>300</td>
<td>100.0</td>
<td>Using where</td>
</tr>
</tbody>
</table></div>
<p>Time execute: much more than 16s</p>
<pre><code>SELECT * FROM table_localities AS address
LEFT JOIN table_province AS province USE INDEX (idx_geo) ON (ST_CONTAINS(province.geo, Point(address.lat, address.lng)))
WHERE province.delivery_area_id = :id
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>select_type</th>
<th>table</th>
<th>partitions</th>
<th>type</th>
<th>possible_keys</th>
<th>key</th>
<th>key_len</th>
<th>ref</th>
<th>rows</th>
<th>filtered</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>address</td>
<td></td>
<td>ALL</td>
<td>fk_delivery_area_id</td>
<td></td>
<td></td>
<td></td>
<td>1850</td>
<td>50.0</td>
<td>Using where</td>
</tr>
<tr>
<td>1</td>
<td>SIMPLE</td>
<td>region</td>
<td></td>
<td>ALL</td>
<td>idx_geo</td>
<td></td>
<td></td>
<td></td>
<td>21699</td>
<td>100.0</td>
<td>Range checked for each record (index map: 0x20)</td>
</tr>
</tbody>
</table></div>
",2,2,0,2025-10-29T22:29:22+00:00,0,152,False
79804599,11672197,,mysql,Architecture design for multilingual descriptions for several classes,"<p>I am creating an E-Commerce app. It has <code>Category</code> and <code>Product</code> classes. Both have multilingual descriptions, so, tables with <code>title</code>, <code>description</code>, <code>meta-*</code>, etc. and unique key (id, language_id).</p>
<p>To add translations to a <code>Review</code> class, a review should have descriptions. I cannot decide between these options:</p>
<ol>
<li>Create separate <code>CategoryDescription</code>, <code>ProductDescription</code>, <code>ReviewDescription</code> classes/tables. This is fast, but those are practically identical and I need to write methods for each.</li>
<li>Create a <code>Description</code> table/class that holds descriptions for any class. But: I don't know how to efficiently connect them to <code>Category</code>, <code>Product</code> and <code>Review</code> classes/tables. May not be scalable.</li>
</ol>
<p>What are your thoughts?</p>
<p>How exactly would you implement a solution?</p>
<p><a href=""https://stackoverflow.com/q/26765175/multi-language-database-with-default-fallback"">Multi language database, with default fallback</a> is a similar question, but it only concerns descriptions for one class. Here several classes should have multilingual descriptions.</p>
",3,0,0,2025-10-30T09:40:59+00:00,1,120,True
79807261,31801658,,mysql,ERROR 1064 (42000): You have an error in your SQL syntax; Getting error for adding foreign key after creating table,"<p>I created 2 tables students and course, in course I want to add <code>student_id</code> as foreign key but I am adding the key after creating the table and it is showing error. Need help in solving this thing.</p>
<pre><code>mysql&gt; create table students
(student_id INT PRIMARY KEY, 
first_name VARCHAR(60) NOT NULL, 
last_name VARCHAR(60) NOT NULL, 
email VARCHAR(100));
Query OK, 0 rows affected (0.02 sec)
</code></pre>
<pre><code>mysql&gt; INSERT INTO students (student_id, first_name, last_name, email) 
`VALUES (1, 'ana', 'smith', 'anasmith@gmail.com'),
(2, 'ben', 'brown', 'benbrown@gmail.com'), 
(3, 'cirus', 'smith', 'cirussmith@gmail.com');
Query OK, 3 rows affected (0.01 sec)
Records: 3  Duplicates: 0  Warnings: 0`
</code></pre>
<p><strong>Method1</strong></p>
<pre><code>`mysql&gt; ALTER TABLE course FOREIGN KEY (student_id) REFERENCES students(student_id);`
</code></pre>
<blockquote>
<p>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FOREIGN KEY (student_id) REFERENCES students(student_id)' at line 1</p>
</blockquote>
<p><strong>Method2</strong>
(I added ` on columns )</p>
<pre><code>`mysql&gt; ALTER TABLE course FOREIGN KEY (`student_id`) REFERENCES students(`student_id`);`
</code></pre>
<blockquote>
<p>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FOREIGN KEY (<code>student_id</code>) REFERENCES students(<code>student_id</code>)' at line 1</p>
</blockquote>
",-8,0,8,2025-11-02T16:56:43+00:00,1,123,True
79807706,13904658,,mysql,Doctrine ORM metadata for special id to represent no relationship on column that cannot be null in mysql,"<p>Hi I'm trying to improve a doctrine entity for it to be able to use doctrine relationships while still reflect the current schema in the database. As per the doctrine recommended practices it is better to map the relationships than have the foreign keys as a literal column that means the entity has the same integers as the foreign key column.</p>
<p>I have a table lets say it is table of rules whether not not a discount should apply to a product the mysql database schema looks something like this:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE discount_rule (
      business_id int not null,
      category_id int not null default 0, -- zero in the id is used to represent it applies across all categories
      brand_id    int not null default 0,
      ..., -- bunch of other properties
      PRIMARY KEY (business_id, category_id, brand_id)
);
</code></pre>
<p>currently the entity looks something like this:</p>
<pre class=""lang-php prettyprint-override""><code>class DiscountRule
{
    #[Column(type: 'integer')]
    #[Id]
    private int $businessId;

    #[Column(type: 'integer')]
    #[Id]
    private int $categoryId;

    #[Column(type: 'integer')]
    #[Id]
    private int $brandId;

    ...
}
</code></pre>
<p>when I want it to look something like this:</p>
<pre class=""lang-php prettyprint-override""><code>class DiscountRule
{
    #[ManyToOne(...)]
    #[Id]
    private Business $businessId;

    #[ManyToOne(...)]
    #[Id]
    private ?Category $categoryId;

    #[ManyToOne(...)]
    #[Id]
    private ?Brand $brandId;

    ...
}
</code></pre>
<p>Now using null to represent the abscence of a related entity seems to be what doctrine normally does here, however mysql does not allow nullable primary keys. I'm happy to move to using a unique constraint but again mysql doesn't actually constrain any columns with a null value, so we cannot guard against there being two rules created for &quot;Category=1 Brand=Any&quot;. Losing the db level constraint is sadly not an option as there are old parts of the code that directly query the db and do not go through doctrine.</p>
<p>Using a proxy reference doesn't feel right as <code>-&gt;getReference(Category::class, 0)</code> is not an entity and treating it as if it where could mean this invalid reference blow up in unexpected places in the code.</p>
<p>What I'm really looking for here is someway to tweak the doctrine mapping so what is a null in the entity class is dehydrated to a 0 in the database and visa versa. So the schema doesn't have to change but I can use the entity with relationship. If the above is not possible I'd be open to schema changes so long as the unique constraint is not lost.</p>
<p>The current code seems to be working though loading such an entity will have a relationship to a Proxy object with the id=0 which feels like it will blow up with an Exception somewhere unexpected so null feels like it would be much safer mapping.</p>
",1,1,0,2025-11-03T09:44:20+00:00,0,50,False
79807965,31598049,,mysql,An alias is giving different result in MySQL,"<p>Can someone explain why I'm getting two different results from the below two queries?</p>
<p>Q1:</p>
<pre><code>SELECT match_no, COUNT(*) as &quot;number of cards&quot;
FROM player_booked
GROUP BY match_no
ORDER BY &quot;number of cards&quot; DESC
LIMIT 1
</code></pre>
<p>R1:
match_no    number of cards
1           4</p>
<p>Q2:</p>
<pre><code>SELECT match_no, COUNT(*) as &quot;number of cards&quot;
FROM player_booked
GROUP BY match_no
ORDER BY COUNT(*) DESC
LIMIT 1
</code></pre>
<p>R2:
match_no    number of cards
51          10</p>
<p>The second result(R2) is the right one in this case but I don't understand why the alias is not giving the same result. What I'm missing?</p>
",1,2,1,2025-11-03T14:28:23+00:00,1,77,True
79808347,920731,,mysql,MySQL query to calculate based on 2 sets of data,"<p>I have a MySQL database that I have the following type of data records. It's a little more in depth than this but its good enough for the sample question.</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Ticket Number</th>
<th>Event (Start or Stop)</th>
<th>Location</th>
<th>Date</th>
</tr>
</thead>
</table></div>
<p>For each Ticket Number there will be 2 records (which represent a start location and an end location) There might be many records in the database.</p>
<p>Here is what I'd like to accomplish:</p>
<ul>
<li>Search for all records on a specific date.</li>
<li>Loop through the records and for every Stop record, I would need to locate the associated Ticket Number record that has the event Start.</li>
<li>Then I will do some calculations or display some info based on the Start and Stop information.</li>
</ul>
<p>As I am thinking about this I might have set the database up incorrectly. Where I have separate rows for each ticket &amp; start - stop entry I might (or should have made it 1 entry and update the start record with the stop data when I get it.</p>
<p>I am doing this in PHP also.</p>
",0,0,0,2025-11-03T22:01:47+00:00,6,146,True
79808801,23370832,,mysql,Create a trigger to update a column on a table after deleting an entry in another table,"<p>I'm making a very simple webpage for an animal shelter (school project) and I want to create a trigger where, if I delete a row in he 'adoptions' HTML table, it sets the situation of the adopted animal from 'adopted' to 'shelter'. Before adding the logic to the webpage, I'm doing tests in phpmyadmin and it says &quot;column id_animal in WHERE clause is unknwon&quot; and it doesn't work.</p>
<p>I want to match the id in &quot;id_animal&quot; to the id in the table &quot;animal&quot;</p>
<p>So far I've tried this:</p>
<pre><code>CREATE TRIGGER devolver_a_refugio2
AFTER DELETE ON adopciones
FOR EACH ROW
UPDATE animal SET situacion = 'refugio' WHERE id_animal = id;

--to delete just in cause

DROP TRIGGER IF EXISTS devolver_a_refugio2;

--my tables

Create table animal(
id INT AUTO_INCREMENT PRIMARY KEY,
foto VARCHAR (255),
nombre VARCHAR (50), 
especie ENUM ('perro', 'gato'),
raza VARCHAR (50),
sexo ENUM ('macho', 'hembra'),
cumple DATE,
ingreso DATE,
caso_especial ENUM ('ninguno','urgente', 'conjunta', 'tratamiento_medico', 'anciano') DEFAULT 'ninguno',
situacion ENUM ('refugio', 'acogida', 'adoptado') DEFAULT 'refugio',
descripcion TEXT);


CREATE TABLE adopciones (

id_adopcion INT AUTO_INCREMENT PRIMARY KEY,
fecha DATE DEFAULT CURRENT_DATE,
nombre_adoptante VARCHAR (255), 
primap_adoptante VARCHAR (255), 
segap_adoptante VARCHAR (255),
dni VARCHAR (9),
telefono VARCHAR(50),
direccion VARCHAR (255),
id_animal INT,
nombre_animal VARCHAR (50),
FOREIGN KEY (id_animal) REFERENCES animal(id));

</code></pre>
<p>What am I doing wrong? thank you in advance</p>
",4,5,1,2025-11-04T10:39:23+00:00,3,98,True
79809965,30452259,,mysql,dbt-core having issues with establishing connection with mysql,"<p>I have a dbt-core setup to connect to mysql database which is installed on a localhost on my personal machine. The connectivity from dbt-core to mysql was working fine until I recently upgraded dbt-core to the latest version.</p>
<p>As the latest dbt-core is incompatible with the latest mysql plugin, I have reverted dbt-core to the previous setup and then I face this weird issue that when I execute “dbt run” or “dbt snpashot” it runs without any error, but it is not connecting to the database.</p>
<p>I have verified the database is up and running on the localhost, I have verified profile.yml and db credentials, I have done a complete “dbt clean” and even deleted existing python virtual environment and created new but still no luck.</p>
<p>I see that “dbt run” or “dbt snapshot” is trying to create a new connection with mysql and then stopped without any errors and no further tracing.</p>
<pre><code>(dbt-venv) PS C:\Users\krish\OneDrive\Desktop\Learning\dbt_repo\mysql&gt; **dbt snapshot --select tmp_snapshot --debug**
11:01:12 Sending event: {‘category’: ‘dbt’, ‘action’: ‘invocation’, ‘label’: ‘start’, ‘context’: [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002362EBE6510&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002363179C050&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002363179C190&gt;]}
11:01:12 Running with dbt=1.7.19
11:01:12 running dbt with arguments {‘printer_width’: ‘80’, ‘indirect_selection’: ‘eager’, ‘log_cache_events’: ‘False’, ‘write_json’: ‘True’, ‘partial_parse’: ‘True’, ‘cache_selected_only’: ‘False’, ‘profiles_dir’: ‘C:\Users\krish\.dbt’, ‘version_check’: ‘True’, ‘warn_error’: ‘None’, ‘log_path’: ‘C:\Users\krish\OneDrive\Desktop\Learning\dbt_repo\mysql\logs’, ‘debug’: ‘True’, ‘fail_fast’: ‘False’, ‘use_colors’: ‘True’, ‘use_experimental_parser’: ‘False’, ‘no_print’: ‘None’, ‘quiet’: ‘False’, ‘warn_error_options’: ‘WarnErrorOptions(include=, exclude=)’, ‘invocation_command’: ‘dbt snapshot --select tmp_snapshot --debug’, ‘log_format’: ‘default’, ‘introspect’: ‘True’, ‘target_path’: ‘None’, ‘static_parser’: ‘True’, ‘send_anonymous_usage_stats’: ‘True’}
11:01:13 Sending event: {‘category’: ‘dbt’, ‘action’: ‘project_id’, ‘label’: ‘90307415-c08a-480c-ac91-a19cc1dc28b3’, ‘context’: [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023631857950&gt;]}
11:01:13 Sending event: {‘category’: ‘dbt’, ‘action’: ‘adapter_info’, ‘label’: ‘90307415-c08a-480c-ac91-a19cc1dc28b3’, ‘context’: [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002363194B020&gt;]}
11:01:13 Registered adapter: mysql=1.7.0
11:01:13 checksum: a8d43cdff37d0f32d7fbe4c5c25bea4bb6c86e162f8ce8fdb1a38cdcf7495431, vars: {}, profile: , target: , version: 1.7.19
11:01:13 Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
11:01:13 Partial parsing enabled, no changes found, skipping parsing
11:01:13 Sending event: {‘category’: ‘dbt’, ‘action’: ‘load_project’, ‘label’: ‘90307415-c08a-480c-ac91-a19cc1dc28b3’, ‘context’: [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023631B22B50&gt;]}
11:01:13 Sending event: {‘category’: ‘dbt’, ‘action’: ‘resource_counts’, ‘label’: ‘90307415-c08a-480c-ac91-a19cc1dc28b3’, ‘context’: [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236319873E0&gt;]}
11:01:13 Found 12 models, 8 snapshots, 1 test, 24 sources, 0 exposures, 0 metrics, 491 macros, 0 groups, 0 semantic models
11:01:13 Sending event: {‘category’: ‘dbt’, ‘action’: ‘runnable_timing’, ‘label’: ‘90307415-c08a-480c-ac91-a19cc1dc28b3’, ‘context’: [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023632B57070&gt;]}
11:01:13
11:01:13 Acquiring new mysql connection ‘master’
11:01:13 Acquiring new mysql connection ‘list_schemas’
11:01:13 Using mysql connection “list_schemas”
11:01:13 On list_schemas: /* {“app”: “dbt”, “dbt_version”: “1.7.19”, “profile_name”: “mysql”, “target_name”: “dev”, “connection_name”: “list_schemas”} */
select distinct schema_name
from information_schema.schemata
**11:01:13 Opening a new connection, currently in state init**
(dbt-venv) PS C:\Users\krish\OneDrive\Desktop\Learning\dbt_repo\mysql&gt;
</code></pre>
",0,0,0,2025-11-05T11:21:47+00:00,0,71,False
79811581,18919038,,mysql,SQLAlchemy Base.metadata.create_all(bind=engine) doesn&#39;t create table,"<p>So I have been trying to create 2 tables using MySQL and SQLAlchemy, but it keeps firing error: <code>sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1146, &quot;Table 'chatbot_db.sessions' doesn't exist&quot;)</code>
I also finds others answer in stackoverflow but I still can't fix my problems.</p>
<p>models.py:</p>
<pre><code>from sqlalchemy import Column, Integer, String, Text, ForeignKey, DateTime, func
from sqlalchemy.orm import relationship, declarative_base

Base = declarative_base()

class Session(Base):
    __tablename__ = &quot;sessions&quot;
    id = Column( Integer, primary_key=True)
    title = Column(String())
    transcript = Column(Text)
    summary = Column(Text)
    created_at = Column(DateTime, default=func.now())
    questions = relationship(&quot;Question&quot;, back_populates=&quot;session&quot;)

class Question(Base):
    __tablename__ = &quot;questions&quot;
    id = Column( Integer, primary_key=True)
    session_id = Column( Integer, ForeignKey(&quot;sessions.id&quot;))
    prompt = Column(Text)
    qtype = Column(String())
    created_at = Column(DateTime, default=func.now())
    session = relationship(&quot;Session&quot;, back_populates=&quot;questions&quot;)
</code></pre>
<p>database.py:</p>
<pre><code>import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from dotenv import load_dotenv

load_dotenv()

MYSQL_HOST = os.getenv(&quot;MYSQL_HOST&quot;, &quot;localhost&quot;)
MYSQL_PORT = int(os.getenv(&quot;MYSQL_PORT&quot;, 3306))
MYSQL_USER = os.getenv(&quot;MYSQL_USER&quot;, &quot;root&quot;)
MYSQL_PASSWORD = os.getenv(&quot;MYSQL_PASSWORD&quot;, &quot;1111&quot;)
MYSQL_DB = os.getenv(&quot;MYSQL_DB&quot;, &quot;chatbot_db&quot;)

DB_URL = f&quot;mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}&quot;

engine = create_engine(DB_URL, echo=True)
SessionLocal = sessionmaker(bind=engine)
Base = declarative_base()
</code></pre>
<p>main.py:</p>
<pre><code>from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .routes import generate

app = FastAPI(title=&quot;Short Question Backend&quot;)

origins = [&quot;http://localhost:3000&quot;]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=[&quot;*&quot;],
    allow_headers=[&quot;*&quot;],
)
from .database import Base, engine
from .models import Session, Question
Base.metadata.create_all(bind=engine)

app.include_router(generate.router, prefix=&quot;/api&quot;, tags=[&quot;AI&quot;])
</code></pre>
<p>any help would be deeply appreciated, thank you</p>
",1,1,0,2025-11-06T17:13:39+00:00,0,42,False
79813151,14456870,,mysql,MySQL dump from Ionos server not compatible with dockered MySQL db,"<p>I am trying to set up a development and testing environment for the software of my new employer.</p>
<p>The data is quite complex and due to some shortcuts/bad architecture decisions of the company that built the software, fake data won't do. So our plan is to download an SQL dump of the production db, anonymize some sensitive information with a faker script and then load it into a locally running docker container.</p>
<p>But when I try that, I get different errors such as:</p>
<blockquote>
<p>'You have an error in your SQL syntax. Check the manual that
corresponds to your MySQL server version for the right syntax to use
near...'</p>
</blockquote>
<p>So basically the dump which is from our hosting provider IONOS, is not compatible with my docker MySQL db. But both dbs, my dockered db and the one running on IONOS shared hosting, are version 8.0.36.</p>
<p>I have done the SQL dump via the web UI. We do not dare to download it via terminal.</p>
<p>Does anyone know of that problem and might suggest a fix?</p>
",0,0,0,2025-11-08T11:40:27+00:00,8,141,False
79818502,31868866,,mysql,Should I generally use a LIMIT clause when testing my SQL queries to a database?,"<p>I am learning SQL from a class online.</p>
<p>I wanted to ask if it's generally best to use LIMIT in my queries to make them run faster, or at least until I have them tested and pulling the right data.</p>
<p>If yes, where should I put it in the order of things? Does it need to go at the very end, or does it depend on what kind of SQL software I'm using?</p>
<p>Thanks a lot for your help!</p>
",0,0,0,2025-11-13T06:33:18+00:00,8,143,True
79818691,18554369,,mysql,Best way to get a number of list elements whose sum of values equal a constant value,"<p>I have a list which consists of numeric values.</p>
<p>In Python, it is written as:</p>
<pre class=""lang-py prettyprint-override""><code>[959, 3480, 9346, 8367, 2847, 8330, 6946, 9586, 9722, 2775]
</code></pre>
<p>Or in the case of MySQL:</p>
<pre class=""lang-sql prettyprint-override""><code>create table testtb(n int);
insert into testtb values(959),(3480),(9346),(8367),(2847),(8330),(6946),(9586),(9722),(2775);
</code></pre>
<p>I would like to find the elements with a sum value of 43219 from the list.</p>
<p>I believed SQL might not be the best choice so I chose programming and finished it.</p>
<p>Here is how I did it in Python using the following script:</p>
<pre class=""lang-py prettyprint-override""><code>import random
ls=[959, 3480, 9346, 8367, 2847, 8330, 6946, 9586, 9722, 2775]
length=len(ls)

flag=True
while flag:
  new_ls=random.sample(ls,length) #get a list with the same elements but in different order
  print(f'\n\na new list with the following combination begins: {new_ls}')
  total=0
  index=0
  for n in new_ls:
    total=total+n
    if total&gt;43219:  #there is no need to keep on using the current list
      break
    elif total==43219:  #found the answer
      print(f'\nfound the combination:{new_ls[:index+1]}')
      print(f'it has a length of {index+1}')
      print(f'the sorted list is:{sorted(new_ls[:index+1])}')
      flag=False
      break
    index+=1

</code></pre>
<p>After running the script, I got the answer after some failed combinations:</p>
<pre class=""lang-none prettyprint-override""><code>a new list with the following combination begins: [8330, 9346, 8367, 9722, 959, 9586, 2847, 3480, 6946, 2775]


a new list with the following combination begins: [6946, 2847, 2775, 959, 8367, 8330, 3480, 9346, 9586, 9722]


a new list with the following combination begins: [2775, 3480, 959, 8367, 9586, 9722, 8330, 6946, 9346, 2847]

found the combination:[2775, 3480, 959, 8367, 9586, 9722, 8330]
it has a length of 7
the sorted list is:[959, 2775, 3480, 8330, 8367, 9586, 9722]
</code></pre>
<p>Of course, the similar could be done in MySQL. A temporary table can be created using:</p>
<pre class=""lang-sql prettyprint-override""><code>select n from testtb order by rand();
</code></pre>
<p>Then use a cursor to get the value from each row and add them one by one. And compare the sum to the constant value.</p>
<p>Either way, the problem is that the resolution depends on too much randomness.  Is there some better way to achieve this in Python or MySQL?</p>
",1,0,0,2025-11-13T09:55:14+00:00,6,130,True
79818971,17161228,,mysql,Can I make MyDAC update a blob field faster?,"<p>So I'm uploading around 50MB into a longblob in a local mysql server.</p>
<p>Using LoadFromStream, setting the param value is near-instantaneous but the execution of the UPDATE statement takes around 2 seconds.</p>
<p>I can't find any resources around this subject so I'm querying the people here : is there any known tips to make this go faster, short of simply giving the task off to a thread ?</p>
<p>I'm open to any suggestions around this topic. Many thanks.</p>
<p>For reference, I'm setting the param value like this :</p>
<pre><code>Stream := TMemoryStream.Create();
Stream.WriteBuffer(tmpBytes, Length(tmpBytes)); // tmpBytes contains around 50MB
Stream.Position := 0;
query.ParamByName('pValue').LoadFromStream(Stream, ftBlob);
Stream.Free();
</code></pre>
",1,0,0,2025-11-13T14:07:07+00:00,5,181,False
79819276,31873402,,mysql,AWS Glue Script Scanning Entire Table Despite Date Filter,"<p>I have written a small Glue script that fetches some data between two dates, but I found that it scans the entire table instead of just the data within the specified time range. I also tried creating an index on my SQL database, but it didn’t make any difference.</p>
<pre><code>SELECT * 
FROM employee_management.employees 
WHERE created_at &gt;= '2025-11-11 23:00:00'
  AND created_at &lt; '2025-11-12 23:00:00';
</code></pre>
",1,1,0,2025-11-13T18:00:40+00:00,1,65,True
79820507,28884281,,mysql,How to make it so a parent NEEDS its children to exist in MySQL?,"<p>I am designing a DB where basically parent can't exist without two children. E.g: Entity set Marriage, CAN'T exist without entity set Man and entity set Woman, but neither Man or Woman need Marriage Entity set to exist. (Lets imagine only Man and Woman marriages so its less complex). Also, lets imagine Marriage can have many men or many women, but men and women can only have ONE marriage. I need help on the query, how do I enforce this?</p>
<pre><code>CREATE TABLE Marriage(
    marriageId INT
)

CREATE TABLE Man(
    manId INT,
    marriageId INT,
    FOREIGN KEY (marriageId) REFERENCES Marriage(marriageId) 
        ON DELETE SET NULL
)
CREATE TABLE Woman(
    womanId INT,
    marriageId INT,
    FOREIGN KEY (marriageId) REFERENCES Marriage(marriageId) 
        ON DELETE SET NULL
)
</code></pre>
<p>This is basically what I have now. (other attributes don't matter right now).</p>
<p>In summary my question is, how can I enforce the TOTAL participation in both ends from marriage?</p>
",0,0,0,2025-11-14T23:42:35+00:00,9,149,True
79821892,20197103,,mysql,How to Automate Extraction and Standardization of Multilingual Name/Username Data Without Modifying the Original Database?,"<p>We’re working with a <code>user</code> table populated from multiple SSO sources, and the data has inconsistencies that hinder generating standardized names. I need a persistent, automated way to extract and process this data <em>without altering the original database</em>:</p>
<h3>Context &amp; Problem Details</h3>
<p>The core issues we’re facing with the data:</p>
<ul>
<li><strong>Name fields (<code>firstname</code>/<code>lastname</code>)</strong>:
<ul>
<li>Mixed Chinese/English text in the same field.</li>
<li>Unstandardized non-local names (e.g., &quot;Tanaka Yuki&quot; vs. &quot;Yuki Tanaka&quot; for Japanese names; inconsistent formatting for Latin-script names like &quot;T??n Mahir Md A???F&quot;).</li>
<li>Embedded titles (&quot;Mr./Ms./Mrs.&quot;), special characters (brackets, quotes), and encoding garble (e.g., &quot;é™³å®¶æ‚ ???&quot;).</li>
<li>Preferred names are mixed into <code>firstname</code>/<code>lastname</code> (no dedicated field), causing redundancy.</li>
</ul>
</li>
<li><strong>Username field</strong>: Contains meaningless emojis (e.g., <code>:)</code>) and non-standard username (e.g., &quot;ohoj_1234&quot;).</li>
</ul>
<h3>Relevant Table Structure (Simplified)</h3>
<p>The <code>user</code> table has these key fields (others omitted):</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE `user` (
  `id` bigint(10) NOT NULL AUTO_INCREMENT,
  `username` varchar(100) NOT NULL DEFAULT '', -- Has emojis/non-standard values
  `firstname` varchar(100) NOT NULL DEFAULT '', -- Mixed names, titles, etc.
  `lastname` varchar(100) NOT NULL DEFAULT '',  -- Same issues as firstname
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
</code></pre>
<h3>Goal</h3>
<p>We need to extract and process this data to generate a standardized <code>fullname</code> field, with rules:</p>
<ul>
<li><strong>No preferred name</strong>: <code>firstname + lastname</code> or <code>lastname + firstname</code> (order depends on language: e.g., Chinese = &quot;Last+First&quot;, English = &quot;First+Last&quot;).</li>
<li><strong>With preferred name</strong>: Append the extracted preferred name to the above (e.g., &quot;Last+First (Preferred)&quot; or similar).</li>
</ul>
<h3>Requirements for the Solution</h3>
<ul>
<li><strong>Automated &amp; persistent</strong>: Process new data as it’s imported (not a one-time fix).</li>
<li><strong>Non-destructive</strong>: Leave the original database untouched—process extracted copies.</li>
<li><strong>Handles edge cases</strong>: Clean special characters/emojis, fix encoding, standardize multilingual name order, and extract preferred names from <code>firstname</code>/<code>lastname</code>.</li>
</ul>
<h3>What I’ve Tried (So Far)</h3>
<p>I’ve researched regex for cleaning special characters, but I’m stuck on:</p>
<ul>
<li>Effectively parsing multilingual names.</li>
<li>Reliably extracting preferred names (often embedded as duplicates or in brackets, but patterns aren’t consistent).</li>
<li>Automating the pipeline to trigger on new data imports.</li>
<li>Removing Class ID and Student ID , which they may exist in either <code>firstname</code> and <code>lastname</code> field  or merge with firstname / lastname base on our current observation. And The ClassID+StudnetID have inconsistent format from different school like 1A01, 1As11111, 1A 01 (2025-2026).</li>
</ul>
",0,0,0,2025-11-17T03:46:46+00:00,1,66,True
79824104,11195677,,mysql,Make MySQL evaluate subquery first,"<pre class=""lang-sql prettyprint-override""><code>SELECT    tt.trans_type_name AS transaction_type,
          trans.transaction_time,
          a.trans_action_name AS transaction_action_name,
          trans.transaction_notes
FROM(SELECT Cast(added_on AS CHAR) AS transaction_time,
            notes                  AS transaction_notes,
            trans_type_id          AS trans_type,
            trans_action_id        AS action_id
     FROM   transactions
     WHERE  added_by = 'service_app'
     AND    notes LIKE %(domain)s
     AND    added_on BETWEEN %(transactionstartdate)s 
                         AND %(transactionenddate)s) trans
LEFT JOIN transaction_types tt
ON        trans.trans_type = tt.trans_type_id
LEFT JOIN transaction_actions a
ON        trans.action_id = a.trans_action_id
ORDER BY  trans.transaction_time DESC 
</code></pre>
<p>This is a query I rewritten because the older query had the error:</p>
<blockquote>
<p>The SELECT would examine more than MAX_JOIN_SIZE rows; check your WHERE and use SET SQL_BIG_SELECTS=1 or SET MAX_JOIN_SIZE=# if the SELECT is okay</p>
</blockquote>
<p>The new query(written above) is more faster but I am still getting the <code>MAX_JOIN</code> error. The thing is, I cannot set <code>SET SQL_BIG_SELECTS=1</code>. My wish is evaluating the subquery first because for the values I plug, there are only 3 rows. The join needs to only happen for the 3 rows since it is just replacing the ID with it's corresponding value.</p>
<p>Is there a way? Thanks in advance.</p>
<p>Edit:
Some more clarity - 3 tables <strong>transactions</strong>, <strong>transaction_types</strong> and <strong>transaction_actions</strong>.
The base table is the <strong>transactions</strong> which has more than million rows. It has two IDs which references the other two tables. I want to select the names instead of IDs so joining it.</p>
",1,3,2,2025-11-19T05:58:00+00:00,1,153,False
79827801,1592429,,mysql,Deleting large data without stopping the active mysql server,"<p>I'm using a table of approximately 1TB in a MySQL database. This table also has a monthly partition. We store the last two months of data in this table and regularly truncate the data from the previous two months. However, when we do this without stopping the server, CPU usage increases significantly, and the server becomes unreachable until the truncate is complete. What needs to be done to delete this without stopping the system and without encountering any problems?</p>
<p>server: amazon ec2 c6gn.12xlarge 96GB RAM 48vCPU</p>
",1,0,0,2025-11-23T10:02:50+00:00,3,88,True
79831830,21440119,,mysql,MySQL SELECT … FOR UPDATE causing table lock during high traffic while generating sequential transaction IDs,"<p>I have a PHP + MySQL project where I need to generate <strong>sequential transaction IDs</strong>.<br />
Each transaction can have <strong>multiple items</strong>, so all rows for that transaction must share the same <code>txn_in</code> value.</p>
<h3><strong>Tables</strong></h3>
<h4><code>stock_inward</code></h4>
<pre><code>ID | txn_in  | material_code | insert_dt
1  | TXN001  | MT001         | 2025-01-13 14:09:08
2  | TXN001  | MT002         | 2025-01-13 14:09:08
3  | TXN001  | MT003         | 2025-01-13 14:09:08
4  | TXN002  | MT002         | 2025-01-13 15:02:37
5  | TXN003  | MT009         | 2025-01-14 11:01:25
6  | TXN003  | MT006         | 2025-01-14 11:01:25
</code></pre>
<h4><code>txn_allot</code></h4>
<pre><code>ID | module   | prefix | session | last_number
1  | STORE_IN | MIN    | 25-26   | 3
</code></pre>
<p><strong>Problem</strong></p>
<p>To generate the next transaction number (<code>TXN004</code>, <code>TXN005</code>, etc.) I am using:</p>
<pre><code>SELECT last_number FROM txn_allot 
WHERE module='STORE_IN' 
FOR UPDATE;
</code></pre>
<p>Then I increment the number and update <code>last_number</code>.</p>
<p>This works, but when multiple users are entering data at the same time,<br />
<strong>the <code>txn_allot</code> table becomes locked</strong>, causing big delays until the lock is released.</p>
<h3><strong>Question</strong></h3>
<p>What is the best way to safely generate sequential, unique transaction IDs <strong>without causing table-level locking</strong> when multiple concurrent requests occur?</p>
<h3><strong>Notes:</strong></h3>
<ul>
<li><p>MySQL database (InnoDB)</p>
</li>
<li><p>PHP backend</p>
</li>
<li><p>Requirement: transaction IDs must be unique and sequential (no duplicates)</p>
</li>
</ul>
",0,0,0,2025-11-27T15:07:07+00:00,7,109,True
79834114,6557243,,mysql,Parsing MySQL Client Handshake response message,"<p>I'm trying to parse the <a href=""https://dev.mysql.com/doc/dev/mysql-server/9.5.0/page_protocol_connection_phase_packets_protocol_handshake_response.html"" rel=""nofollow noreferrer"">Handshake response packet</a> from MySQL client/server protocol</p>
<p>For the task I'm connecting to the application via mysql cli client:</p>
<pre><code>mysql --version
mysql  Ver 9.5.0 for macos15.4 on arm64 (Homebrew)
</code></pre>
<p>I'm sending 3 different commands:</p>
<pre><code>mysql -h127.0.0.1 -u root --port=5555 -p --database testtestestest
mysql -h127.0.0.1 -u root --port=5555 -p --database test
mysql -h127.0.0.1 -u root --port=5555 -p
</code></pre>
<p>Part of the code is attempting to get the database name, if the <code>CLIENT_CONNECT_WITH_DB</code> flag is set to 1.</p>
<p>The code looks as follows:</p>
<pre><code>const (
    CLIENT_CONNECT_WITH_DB = 1 &lt;&lt; 3
)
func parseHandshakeResponse(input []byte) {
    fmt.Printf(&quot;raw: %08b %08b %08b %08b\n&quot;,
        input[0], input[1], input[2], input[3])
    clientCaps := binary.LittleEndian.Uint32(input[:4])
    fmt.Printf(&quot;caps: %b&quot;, clientCaps)
    // not related stuff ...
    // if clientCaps&amp;CLIENT_CONNECT_WITH_DB &gt; 0 { &lt;------- this fails if the database is testtestestest and it shouldn't
         databaseName = extractDBName(input, usernameEnd) // extractDBName is working fine 
    //}
// .... other sutff
}
</code></pre>
<p>The problem I'm facing is that with the 3 commands, I get those 3 different clientCaps:</p>
<pre><code>raw: 11011000 00000000 00000000 00000010
caps: 10000000000000000011011000username: root, database: test 
raw: 11100010 00000000 00000000 00000010
caps: 10000000000000000011100010username: root, database: testtestestest 
raw: 11010011 00000000 00000000 00000010
caps: 10000000000000000011010011username: root, database: caching_sha2_password 
</code></pre>
<p>If my understanding is correct the 4th bit must be 1 if the <code>CLIENT_CONNECT_WITH_DB</code> is set(which it should for <code>testtestestest</code>), but this is not the case. When the database name is <code>testtestestest</code>, the if condition fails and the database name is not retrieved(unless forced). The code works until the database name is 12 chars long(so the code detects the <code>CLIENT_CONNECT_WITH_DB</code> as set when <code>len(db_name) &lt;= 11</code>.</p>
",2,2,0,2025-11-30T18:39:44+00:00,1,70,True
79835052,897531,,mysql,Intermittent &quot;Error establishing a database connection” Errors,"<p>I have three WordPress sites hosted on a Bluehost dedicated server. Multiple times per week, one or more of the sites will show database connection errors:</p>
<pre class=""lang-none prettyprint-override""><code>Error establishing a database connection
</code></pre>
<p>There doesn’t seem to be any clear pattern to why this is happening. It’s not always the same site. Sometimes it’s two of the sites, sometimes it’s just one. One of the sites is medium sized, and the other two are currently small. The site(s) will go down at random times. No changes have been made to <code>wp-config.php</code>.</p>
<p>I’ve always been able to run <code>repair table</code> on all of the tables, and that brings the site back online, but I really need a solution that prevents this from happening in the first place. I’ve checked everything, including error logs. Bluehost support has not been helpful. Does anyone know what could be causing this or anything I should check?</p>
",0,0,0,2025-12-01T16:10:23+00:00,1,66,False
79836793,1346234,Lithuania,mysql,DataGrip export remote database,"<p>I want to export remote database into my local database.</p>
<p>I have 2 projects in DataGrip. One is Docker container, containing MySql. Another is remove database in AWS connected via SSH.</p>
<p>When doing <code>Import/Export</code> &gt; <code>Export using &quot;mysqldump&quot;</code> I get error:</p>
<blockquote>
<p>mysqldump: Got error: 1045: Access denied for user 'root'@'127.0.0.1' (using password: YES) when trying to connect</p>
</blockquote>
<p>How can I export remote database, if mysqldump is trying to connect to localhost?</p>
",0,0,0,2025-12-03T10:59:17+00:00,0,47,False
79837422,26564077,,mysql,Why don&#39;t my MySQL usernames match when I use them in JDBC?,"<p>I’m having trouble understanding how MySQL users actually work.<br />
When I run:</p>
<p><code>SELECT USER();</code> MySql shows: <code>root@localhost</code> .<br />
But my username when i log in is <em><strong>ade</strong></em>. so when I to set a connection using <code>DriverManager. getConnnection()</code>, I try both usernames but it gives me:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.RuntimeException: java.sql.SQLException: Access denied for user 'ade'@'localhost' (using password: YES)
    at jdbcpractices.Test.main(Test.java:18)
</code></pre>
<p>or</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.RuntimeException: java.sql.SQLException: Access denied for user 'root@localhost'@'localhost' (using password: YES)
    at jdbcpractices.Test.main(Test.java:18)
</code></pre>
<p>It only works when using the user <code>&quot;root&quot;</code>. (my implementation below.)</p>
<pre><code>public class Test {
    public static void main(String[] args) {
        String user = &quot;root&quot;; //ade
        String password = &quot;quesadilla&quot;;
        String URL = &quot;jdbc:mysql://localhost:3306/ademysql&quot;;

        try {
            Connection conn = DriverManager.getConnection(URL, user, password);
            System.out.println(&quot;Connected to database&quot;);
        } catch (SQLException e) {
            throw new RuntimeException(e);
        }

    }
}
</code></pre>
<p>So what’s the difference between them? Isn’t it supposed to work with <code>ade</code> or <code>root@localhost</code> as well? I’m confused about how this works underneath. Please give me some guidelines or help me understand this confusion. Thanks in advance!!</p>
",3,3,0,2025-12-04T00:09:55+00:00,0,76,False
79839096,5661749,,mysql,How to increase undo tablespaces on aws rds mysql innodb_undo_tablespaces,"<p>I have the issue that my undo history keeps growing and purging does not catch up.</p>
<p>This number constantly increases:
SELECT count FROM information_schema.innodb_metrics WHERE name = 'trx_rseg_history_len';</p>
<p>I already made sure that transactions are not the issue and tried many different combinations of purge threads and purge batch size. Nothing helped.</p>
<p>I now found out that the fact that I only have two innodb_undo_tablespaces could be the reason why more purge threads do not help.</p>
<p>I would therefore like to increase the undo tablespace count. Unfortunately this seems to be a non-modifiable parameter in aws rds and I cannot change it on an existing instance and also not for a new database.</p>
<p>Also creating them manually via:
<code>CREATE UNDO TABLESPACE undo_003 ADD DATAFILE 'undo_003.ibu';</code> does not seem to be an option since even the admin account in rds does not have that permission.</p>
<p>Is there any way to have more then 2 undo tablespaces with aws rds? I am actually using one of the largest available instances (r8g.24xlarge) so instance size should not be the reason for that restriction.</p>
<p>If not: Any other idea why purge throughput does not increase when comparing 1 vs 4 vs 16 threads?</p>
",0,0,0,2025-12-05T16:13:16+00:00,0,13,False
79840539,31435869,,mysql,MySQL: Show last 3 subscription plans with amounts per student in separate columns using PhpMyAdmin,"<p>I am working on a <strong>student subscription database</strong> in MySQL using PhpMyAdmin. I have the following tables:</p>
<p><strong>Students Table:</strong><br />
| student_id | student_name |</p>
<p><strong>Plans Table:</strong><br />
| plan_id | plan_name |</p>
<p><strong>Subscriptions Table:</strong><br />
| subscription_id | student_id | plan_id | amount_paid | payment_date |</p>
<p>I want to write a query that returns <strong>one row per student</strong> showing:</p>
<ul>
<li><p>Student name</p>
</li>
<li><p>Their <strong>last 3 subscription plan names</strong> (ordered by <code>payment_date</code> descending)</p>
</li>
<li><p>Corresponding <strong>amounts paid</strong></p>
</li>
</ul>
<p>For example, the output should look like this:</p>
<p>| student_name | plan_1 | amount_1 | plan_2 | amount_2 | plan_3 | amount_3 |</p>
<p>I tried the following query:</p>
<pre><code>SELECT 
    s.student_name,
    p1.plan_name AS plan_1, sub1.amount_paid AS amount_1,
    p2.plan_name AS plan_2, sub2.amount_paid AS amount_2,
    p3.plan_name AS plan_3, sub3.amount_paid AS amount_3
FROM students s
LEFT JOIN subscriptions sub1 ON s.student_id = sub1.student_id
LEFT JOIN plans p1 ON sub1.plan_id = p1.plan_id
LEFT JOIN subscriptions sub2 ON s.student_id = sub2.student_id
LEFT JOIN plans p2 ON sub2.plan_id = p2.plan_id
LEFT JOIN subscriptions sub3 ON s.student_id = sub3.student_id
LEFT JOIN plans p3 ON sub3.plan_id = p3.plan_id
WHERE sub1.payment_date = (SELECT MAX(payment_date) FROM subscriptions WHERE student_id = s.student_id)
   OR sub2.payment_date = (SELECT MAX(payment_date) FROM subscriptions WHERE student_id = s.student_id)
   OR sub3.payment_date = (SELECT MAX(payment_date) FROM subscriptions WHERE student_id = s.student_id);
</code></pre>
<p><strong>Problem:</strong></p>
<ol>
<li><p>The query duplicates joins and does not correctly pick the <strong>last 3 subscriptions</strong>.</p>
</li>
<li><p>The <code>WHERE</code> condition only selects the latest subscription, so <code>plan_2</code> and <code>plan_3</code> are often wrong or NULL.</p>
</li>
<li><p>Hardcoding <code>plan_1</code>, <code>plan_2</code>, <code>plan_3</code> is <strong>not scalable</strong>.</p>
</li>
</ol>
<p><strong>Questions:</strong></p>
<ol>
<li><p>How can I write a query that correctly returns the <strong>last 3 subscription plans with amounts per student</strong> in a <strong>single row</strong>?</p>
</li>
<li><p>Is there a way to do this <strong>dynamically</strong> without hardcoding columns for each subscription?</p>
</li>
</ol>
",-1,0,1,2025-12-08T00:12:53+00:00,0,28,False
79842361,32026814,,mysql,How can I optimize this MySQL spatial query using MBRIntersects and ST_DISTANCE_SPHERE?,"<p>I’m working with a MySQL table (about <strong>1.25 million rows</strong>). I need to retrieve rows where both the origin and destination fall within radius filters around two given coordinates. Using a large Origin and Destination radius (250 miles), the query takes between 4-7 seconds.</p>
<p>Here is the query:</p>
<p>Note: The <code>?</code> placeholders represent parameters from a prepared statement (Java).</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
    Created, Last_Modified, Net_Freight_Charges, Quote_Currency, Origin_Expected_Date, 
    ST_DISTANCE_SPHERE(Origin_Geolocation, ST_SRID(Point(?, ?), ?)) AS Distance_From_Origin_Meters, 
    ST_DISTANCE_SPHERE(Destination_Geolocation, ST_SRID(Point(?, ?), ?)) AS Distance_From_Destination_Meters 
FROM carrierquote 
WHERE Origin_Expected_Date IS NOT NULL 
AND Origin_Expected_Date &gt;= ? 
AND Origin_Expected_Date &lt;= ? 
AND (Equipment_Category = ? OR Mode_Equipment_Category = ?) 
AND Is_Accepted = ? 
AND Is_Haz_Mat = ? 
AND MBRIntersects(Origin_Geolocation, ST_GeomFromText(?, ?)) 
AND MBRIntersects( Destination_Geolocation, ST_GeomFromText(?, ?)) 
AND ST_DISTANCE_SPHERE(Origin_Geolocation, ST_SRID(Point(?, ?), ?)) &lt;= ? 
AND ST_DISTANCE_SPHERE(Destination_Geolocation, ST_SRID(Point(?, ?), ?)) &lt;= ? 
ORDER BY Origin_Expected_Date DESC
</code></pre>
<p><strong>Indexes I already have:</strong></p>
<ul>
<li><p><code>SPATIAL INDEX IDX_Origin_Geolocation (Origin_Geolocation)</code></p>
</li>
<li><p><code>SPATIAL INDEX IDX_Destination_Geolocation (Destination_Geolocation)</code></p>
</li>
<li><p><code>INDEX IDX_Origin_Expected_Date (Origin_Expected_Date)</code></p>
</li>
</ul>
<p><strong>What EXPLAIN shows:</strong></p>
<ul>
<li><p>MySQL picks <strong>only one</strong> spatial index (usually <code>Destination_Geolocation</code>)</p>
</li>
<li><p>Initial row estimate with a 250-mile bounding box is ~34,000 rows</p>
</li>
<li><p>After applying the actual spherical distance filters, only <strong>~1,200 rows</strong> remain</p>
</li>
<li><p>Full output: Select Type: SIMPLE Type: range Key: IDX_Destination_Geolocation (Destination_Geolocation) Rows:
36916 Filtered: 0.0 Extra: Using where; Using filesort</p>
</li>
</ul>
<p><strong>Full EXPLAIN ANALYZE output:</strong></p>
<pre><code>INFO  - -&gt; Sort: carrierquote.Origin_Expected_Date DESC  (cost=38441 rows=36916) (actual time=2655..2656 rows=1179 loops=1)
    -&gt; Filter: (*All of the WHERE clause filters*)  (cost=38441 rows=36916) (actual time=56.2..2654 rows=1179 loops=1)
        -&gt; Index range scan on carrierquote using IDX_Destination_Geolocation over (Destination_Geolocation unprintable_geometry_value)  (cost=38441 rows=36916) (actual time=0.349..2426 rows=163927 loops=1)
</code></pre>
<p>Using Index(Is_Accepted, Is_Haz_Mat, Origin_Expected_Date) shows no improvement. <strong>Explain Analyze output</strong>:</p>
<pre><code>INFO  - -&gt; Filter: (*All of the WHERE Clause filters*)  (cost=1.96 rows=0.19) (actual time=123..3000 rows=77 loops=1)
    -&gt; Index range scan on carrierquote using idx_filters over (Is_Accepted = 1 AND Is_Haz_Mat = 0 AND '2025-08-17' &lt;= Origin_Expected_Date &lt;= '2025-12
-18') (reverse), with index condition: ((carrierquote.Is_Haz_Mat = 0) and (carrierquote.Is_Accepted = 1) and (carrierquote.Origin_Expected_Date is not
null) and (carrierquote.Origin_Expected_Date &gt;= DATE'2025-08-17') and (carrierquote.Origin_Expected_Date &lt;= DATE'2025-12-18'))  (cost=1.96 rows=1) (act
ual time=0.0708..1675 rows=231360 loops=1)
</code></pre>
<p>I also tried moving the spatial calculations to before the query, which also showed no improvement in speed:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH
params AS (
    SELECT ST_SRID(Point(?, ?), ?) AS origin_point,
    ST_SRID(ST_MakeEnvelope(Point(?,?), Point(?,?)),?) AS origin_bb,
    ST_SRID(Point(?, ?), ?) AS destination_point,
    ST_SRID(ST_MakeEnvelope(Point(?,?), Point(?,?)),?) AS destination_bb
       )
      
SELECT Created, Last_Modified, Origin_Expected_Date, Net_Freight_Charges, Quote_Currency
FROM carrierquote
CROSS JOIN params p
WHERE Origin_Expected_Date IS NOT NULL
AND Origin_Expected_Date &gt;= ?
AND Origin_Expected_Date &lt;= ?
AND (Equipment_Category = ? OR Mode_Equipment_Category = ?)
AND Is_Accepted = ?
AND Is_Haz_Mat = ?
AND MBRIntersects(Origin_Geolocation, p.origin_bb)
AND MBRIntersects(Destination_Geolocation, p.destination_bb)
</code></pre>
<p>Any advice on reducing the row count earlier in the query, improving the spatial filtering, or rewriting the query for better performance would be greatly appreciated.</p>
",0,0,0,2025-12-09T21:11:27+00:00,0,80,False
79842442,31952442,,mysql,How to condense multiple boolean columns into one column in laravel?,"<p>How do I design a scalable user badges system without creating many boolean columns?</p>
<p>I'm building a badge system where users earn specific badges by completing certain tasks.</p>
<pre class=""lang-php prettyprint-override""><code>Schema::create('user_badges', function (Blueprint $table) {
    $table-&gt;integer('user_id', true, true);
    $table-&gt;boolean('Badge1');
    $table-&gt;boolean('Badge2');
    $table-&gt;boolean('Badge3');
    $table-&gt;boolean('Badge4');
    $table-&gt;boolean('Badge5');
    $table-&gt;boolean('Badge6');
    $table-&gt;boolean('Badge7');
    $table-&gt;boolean('Badge8');
});
</code></pre>
<p>This approach doesn’t scale well — every new badge requires adding a new boolean column, which gets messy fast.</p>
<p>What is a better database structure for storing user badges?</p>
<p>Why would an array or a more dynamic structure be a better fit?</p>
",0,0,0,2025-12-09T23:12:17+00:00,5,162,True
79842476,4497062,,mysql,Setting database encoding to latin,"<p>I'm trying to switch the default encoding of the database from UTF-8 to latin. I'm on Windows and VertrigoServ, I've edited the file my.ini under MySQL directory like this:</p>
<p>Under <code>[mysqld]</code> section, added the lines:</p>
<pre><code>character-set-server = latin1

collation-server = latin1_swedish_ci 
</code></pre>
<p>Under <code>[mysql]</code> :</p>
<pre><code>default-character-set=latin1
</code></pre>
<p>and</p>
<pre><code>
[client]

default-character-set = latin1

[mysqld_safe]

default-character-set=latin1

</code></pre>
<p>However, the in phpMyAdmin's home, Server charset is set to UTF-8 and the strings are stored as UTF-8 (as I can see with a PHP script using <code>mb_detect_encoding()</code> function).  Server connection collation is <code>latin1_swedish_ci</code> and the field data type is <code>varchar(255)  latin1_swedish_ci</code>.</p>
<p>What am I missing?</p>
",-1,0,1,2025-12-10T00:52:54+00:00,1,86,True
79843057,32030135,,mysql,MySQL returns NULL in 8.0.37 but works in 5.7,"<p><strong>ISSUE:</strong> When migrating from MySQL 5.7 to 8.0, the query started returning NULL instead of data for the same compressed data in a LONGBLOB field.</p>
<p><strong>Context:</strong></p>
<ul>
<li><p>There is a table <code>my_table</code> with a column <code>my_column LONGBLOB NOT NULL</code></p>
</li>
<li><p>The data was presumably compressed using <code>COMPRESS()</code></p>
</li>
<li><p>When executing the query <code>SELECT my_column FROM ...</code>, I get different results for 5.7 i have field, on 8.0 is null</p>
</li>
</ul>
<p>How can I solve this problem? I can't seem to find any documentation on the topic. Where can I read about it? I'd like a query that is compatible with both versions, if possible.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE my_table (
    id INTEGER PRIMARY KEY,
    my_column LONGBLOB NOT NULL
);

-- QUERY (same for diffrent version)
SELECT 
    id,
    my_column AS test_uncompress,
    LENGTH(my_column) AS blob_length,
    HEX(LEFT(my_column, 4)) AS magic_header
FROM my_table
WHERE id = 1;
</code></pre>
<p>Results:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>ver. MySQL</th>
<th>test_uncompress</th>
<th>blob_length</th>
<th>magic_header</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.7**</td>
<td>{&quot;valid&quot;: &quot;json&quot;}</td>
<td>2543</td>
<td>4F</td>
</tr>
<tr>
<td>8.0**</td>
<td>NULL</td>
<td>1140</td>
<td>B4</td>
</tr>
</tbody>
</table></div>
",3,3,0,2025-12-10T14:24:03+00:00,1,136,False
79843530,767434,,mysql,Redmine / Mysql 8 (in docker) TLS/SSL error: self-signed certificate in certificate chain,"<p>I setup a new redmine installation with mysql 8 - but I get SSL / TLS certificate errors.</p>
<p>I have no clue whats going wrong in this basic setup - normally I use postgres ...</p>
<p>Here is my compose file:</p>
<pre><code>services:
  redmine:
    image: redmine:6.1.0-alpine
    environment:
      REDMINE_DB_MYSQL: redmine-mysql
      REDMINE_DB_USERNAME: redmine
      REDMINE_DB_PASSWORD: xxxx
      REDMINE_SECRET_KEY_BASE: aaaaaaaaaaaaaa
      REDMINE_PLUGINS_MIGRATE: 'true'
      REDMINE_DB_DATABASE: redmine_default
    volumes:
      - /opt/container-pv-data/redmine/redmine-data:/usr/src/redmine/files
    ports:
      - 9880:3000/tcp
  redmine-mysql:
    image: mysql:8.0.44-bookworm
    environment:
      MYSQL_USER: redmine
      MYSQL_PASSWORD: xxxx
      MYSQL_ROOT_PASSWORD: yyyy
      MYSQL_DATABASE: redmine_default
    volumes:
      - /opt/container-pv-data/redmine/mysql_conf:/etc/mysql/conf.d
      - /opt/container-pv-data/redmine/mysql_data:/var/lib/mysql
</code></pre>
<p>I also tried to set the mysql command - but that also didnt fix the issue:</p>
<pre><code>command: mysqld --default-authentication-plugin=mysql_native_password --ssl=off --tls-version=''
</code></pre>
<p>My Error in Redmine during startup:</p>
<pre><code>.....
ActiveRecord::ConnectionNotEstablished: TLS/SSL error: self-signed certificate in certificate chain (ActiveRecord::ConnectionNotEstablished)
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:35:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter.new_client'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:145:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter#connect'
.....
.....
ActiveRecord::ConnectionNotEstablished: TLS/SSL error: SSL is required, but the server does not support it (ActiveRecord::ConnectionNotEstablished)
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:35:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter.new_client'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:145:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter#connect'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:154:in 'block in ActiveRecord::ConnectionAdapters::Mysql2Adapter#reconnect'
/usr/local/bundle/gems/activesupport-7.2.2.2/lib/active_support/concurrency/null_lock.rb:9:in 'ActiveSupport::Concurrency::NullLock#synchronize'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:151:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter#reconnect'
.....
.....
Caused by:
Mysql2::Error::ConnectionError: TLS/SSL error: SSL is required, but the server does not support it (Mysql2::Error::ConnectionError)
/usr/local/bundle/gems/mysql2-0.5.7/lib/mysql2/client.rb:92:in 'Mysql2::Client#connect'
/usr/local/bundle/gems/mysql2-0.5.7/lib/mysql2/client.rb:92:in 'Mysql2::Client#initialize'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:25:in 'Class#new'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:25:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter.new_client'
/usr/local/bundle/gems/activerecord-7.2.2.2/lib/active_record/connection_adapters/mysql2_adapter.rb:145:in 'ActiveRecord::ConnectionAdapters::Mysql2Adapter#connect'
</code></pre>
<p>Please give me some hints how to fix or disable mysql ssh / tls -&gt; I dont need it ...</p>
<p>Thanks!!</p>
<p><strong>EDIT:</strong> I really dont understand the downvotes - the referenced posts don't fix the issue!!!</p>
<p>The real fix in acctual mysql 8.0.44 version is using &quot;--skip-ssl&quot;</p>
<pre><code> --ssl               Enable SSL for connection (automatically enabled with
                      other flags).
                      (Defaults to on; use --skip-ssl to disable.)`
</code></pre>
",-3,0,3,2025-12-11T00:49:21+00:00,0,52,False
79843544,1200713,North America,mysql,PHP Nested MySQL Query (Recursive?),"<p>I'm playing around with GnuCash, with a database stored on MySql.</p>
<p>The goal is to display this data on a web page using PHP.
<a href=""https://i.sstatic.net/GPyzrUPQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GPyzrUPQ.png"" alt=""enter image description here"" /></a></p>
<p>The database table that I'm trying to query is:</p>
<pre><code>Field           Type           Collation           Null    Key     Default  Extra   Privileges                       Comment  
--------------  -------------  ------------------  ------  ------  -------  ------  -------------------------------  ---------
guid            varchar(32)    utf8mb4_0900_ai_ci  NO      PRI     (NULL)           select,insert,update,references           
name            varchar(2048)  utf8mb3_general_ci  NO              (NULL)           select,insert,update,references           
account_type    varchar(2048)  utf8mb3_general_ci  NO              (NULL)           select,insert,update,references           
commodity_guid  varchar(32)    utf8mb4_0900_ai_ci  YES             (NULL)           select,insert,update,references           
commodity_scu   int            (NULL)              NO              (NULL)           select,insert,update,references           
non_std_scu     int            (NULL)              NO              (NULL)           select,insert,update,references           
parent_guid     varchar(32)    utf8mb4_0900_ai_ci  YES             (NULL)           select,insert,update,references           
code            varchar(2048)  utf8mb3_general_ci  YES             (NULL)           select,insert,update,references           
description     varchar(2048)  utf8mb3_general_ci  YES             (NULL)           select,insert,update,references           
hidden          int            (NULL)              YES             (NULL)           select,insert,update,references           
placeholder     int            (NULL)              YES             (NULL)           select,ins
</code></pre>
<h2>ert,update,references</h2>
<pre><code>CREATE TABLE `accounts` (
  `guid` varchar(32) NOT NULL,
  `name` varchar(2048) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci NOT NULL,
  `account_type` varchar(2048) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci NOT NULL,
  `commodity_guid` varchar(32) DEFAULT NULL,
  `commodity_scu` int NOT NULL,
  `non_std_scu` int NOT NULL,
  `parent_guid` varchar(32) DEFAULT NULL,
  `code` varchar(2048) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci DEFAULT NULL,
  `description` varchar(2048) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci DEFAULT NULL,
  `hidden` int DEFAULT NULL,
  `placeholder` int DEFAULT NULL,
  PRIMARY KEY (`guid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
</code></pre>
<p>GNU Cash Stores Account (Parent Record / Children Reccords) in a Hierarchical Data in MySQL.</p>
<p>My PHP Code is:</p>
<pre><code>&lt;?php

$sql = &quot;SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`name` = 'Root Account';&quot;;
echo &quot;$sql&lt;br \&gt;\n&quot;;
include &quot;get_results_set.inc&quot;;

$row_id = 1;
$column = 1;
$acct_name = &quot;&quot;;

if (mysqli_num_rows($result_set) &gt;= 1 ){
    
    $record = mysqli_fetch_assoc($result_set);
        
        $acct_guid = $record['guid'] ?? null;
        $acct_name = $record['name'] ?? null;
        
        echo &quot;$column\t$row_id\t$acct_guid\t$acct_name\n&lt;br /&gt;&quot;;
        $row_id++;
        
    }
    
    $sql = &quot;SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`parent_guid` = '$acct_guid' ORDER BY a.`name`;&quot;;
    echo &quot;$sql&lt;br \&gt;\n&quot;;
    include &quot;get_results_set.inc&quot;;
    $result_set1 = $result_set;
    $column++;
        
    while($record = mysqli_fetch_assoc($result_set1)){
        
        $acct_guid = $record['guid'] ?? null;
        $acct_name = $record['name'] ?? null;
        
        echo &quot;$column\t$row_id\t$acct_guid\t$acct_name\n&lt;br /&gt;&quot;;
        $row_id++;
        
        ADDING THE NEXT QUERY HERE FOR THE CHILDREN RECORDS DOES NOT WORK    
    }
</code></pre>
<p>So far, the webpage results are:</p>
<pre><code>SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`name` = 'Root Account';
1 1 4bd6de0c4a704764853f508553791205 Root Account
SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`parent_guid` = '4bd6de0c4a704764853f508553791205' ORDER BY a.`name`;
2 2 6a038fc8720140e6adf6b6b61140c3b0 Assets
2 3 353496de854e4787b1a264a71cd931d5 Equity
2 4 20b10c6c0e084a5e82813930d32f13d1 Expenses
2 5 83ccdd2b18784e53ab31e7f1a716b0d2 Imbalance-USD
2 6 9a52b17893a3455e9fabe69416405a26 Income
2 7 806023758d874482a17d838824f06bb2 Liabilities
2 8 5cb805e3da624d49939d9977ca3844b8 Orphan-USD 
</code></pre>
<p>However, I'm unable to query for the accounts listed under &quot;Assets&quot;, (or the rest of the children accounts the code just abruptly stops).</p>
<p>Can I query in PHP within another query?</p>
<p>This code:</p>
<pre><code>    &lt;?php

$user=&quot;gnucash&quot;;
$host=&quot;localhost&quot;;
$pwd=&quot;gnucash&quot;;
$db=&quot;gnucash&quot;;

function getChildren($parent, $column, $row_id) {
    
    $user=&quot;gnucash&quot;;
    $host=&quot;localhost&quot;;
    $pwd=&quot;gnucash&quot;;
    $db=&quot;gnucash&quot;;
    
    $sql = &quot;SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`parent_guid` = '$parent' ORDER BY a.`name`;&quot;;
    echo &quot;$sql&lt;br \&gt;\n&quot;;
    include &quot;get_results_set.inc&quot;;
    
    if (mysqli_num_rows($result_set) &gt;= 1 ){
        
        $record = mysqli_fetch_assoc($result_set);
        
        $acct_guid = $record['guid'] ?? null;
        $acct_name = $record['name'] ?? null;
        
        echo &quot;$column\t$row_id\t$acct_guid\t$acct_name\n&lt;br /&gt;&quot;;
        $row_id++;
        
        getChildren($acct_guid, $column, $row_id);
    }
}

$sql = &quot;SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`name` = 'Root Account';&quot;;
echo &quot;$sql&lt;br \&gt;\n&quot;;
include &quot;get_results_set.inc&quot;;

$row_id = 1;
$column = 1;
$acct_name = &quot;&quot;;

if (mysqli_num_rows($result_set) &gt;= 1 ){
    
    $record = mysqli_fetch_assoc($result_set);
        
        $acct_guid = $record['guid'] ?? null;
        $acct_name = $record['name'] ?? null;
        
        echo &quot;$column\t$row_id\t$acct_guid\t$acct_name\n&lt;br /&gt;&quot;;
        $row_id++;
        getChildren($acct_guid, $column, $row_id);
        
    }
    

?&gt;
</code></pre>
<p>Produces the following results (it does only the first record of the recordset, and stops):</p>
<pre><code>SELECT a.`guid`, a.`name` FROM `accounts` a WHERE a.`name` = 'Root Account'; 
1 1 4bd6de0c4a704764853f508553791205 Root Account 

SELECT a.`guid`, a.`name` FROM `accounts` a 
WHERE a.`parent_guid` = '4bd6de0c4a704764853f508553791205' ORDER BY a.`name`; 
1 2  6a038fc8720140e6adf6b6b61140c3b0 Assets 

SELECT a.`guid`, a.`name` FROM `accounts` a 
WHERE a.`parent_guid` ='6a038fc8720140e6adf6b6b61140c3b0' ORDER BY a.`name`; 
1 3 7e5f2d4e89f04bac9fec8fc9ac79c409 Accounts Receivable 

SELECT a.`guid`, a.`name` FROM `accounts` a 
WHERE a.`parent_guid` = '7e5f2d4e89f04bac9fec8fc9ac79c409' ORDER BY a.`name`;
</code></pre>
",-2,1,3,2025-12-11T01:19:42+00:00,0,87,False
79844829,4963334,,mysql,Peewe new version 3.15.6 autocommit,"<p>In old peewe version 2.8.1 we had some manula quirks like below</p>
<pre><code>    def configure_proxy:
        proxy.obj.require_commit = False
        proxy.obj.autocommit = True        
        proxy.obj.commit_select = False
        proxy.obj.connect_kwargs[&quot;autocommit&quot;] = True 
</code></pre>
<p>In new peewe version I understand peewe relies on autocommit status sof underlying MYSQL driver which is PYMYSQL in this case and we have set it to true by overriding the _connect method like below</p>
<pre><code>def _connect(self):
    if mysql is None:
        raise ImproperlyConfigured('MySQL driver not installed!')
    conn = mysql.connect(db=self.database, autocommit=True,
                         **self.connect_params)
    return conn

def _set_autocommit(self, autocommit):
    &quot;&quot;&quot;Prevent Peewee from toggling autocommit mode&quot;&quot;&quot;
     pass

def  configure_proxy:
   proxy.obj.require_commit = False     
   proxy.obj.commit_select = False
     
</code></pre>
<p>In order to keep the behaviour same as old peewe version  with peewe 3.15.6 we opted for above changes.</p>
<p>Question</p>
<ol>
<li>is above approach is right?</li>
</ol>
",0,0,0,2025-12-12T11:23:42+00:00,1,45,False
79844959,23479712,,mysql,How can I ensure the uniqueness of folder names in a hierarchical SQL table if the parent column is NULL?,"<p>I am trying to create a table in MySQL to store a file hierarchy. In this table, the <code>parent_folder_id</code> column will reference the <code>id</code> of the parent folder. If a folder does not have a parent, the value of this column will be <code>NULL</code>, which means the folder is the root folder. Each user can have only one folder with a given name under the same parent.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE folders
  (
     id               INT NOT NULL auto_increment,
     user_id          INT NOT NULL,
     parent_folder_id INT DEFAULT NULL,
     folder_name      VARCHAR(255) NOT NULL,
     PRIMARY KEY (id),
     FOREIGN KEY (user_id) REFERENCES users(id),
     FOREIGN KEY (parent_folder_id) REFERENCES folders(id),
     CONSTRAINT uq_user_folder_name UNIQUE KEY (user_id, parent_folder_id,
     folder_name)
  );
</code></pre>
<p>The problem occurs when <code>parent_folder_id</code> is <code>NULL</code>. In this case, MySQL does not perform the comparison as expected, and a user may end up with an infinite number of root folders. How can this be prevented?</p>
",2,2,0,2025-12-12T14:21:44+00:00,2,115,True
79846734,20563954,,mysql,How to connect to MySQL database with SQLAlchemy when running the application and MySQL running as a Docker container?,"<p>I am trying to run an application as its Docker image. It needs to access the database on the server. I am trying to run everything on a remote server and it is a Linux server. I am using docker compose.</p>
<p>The docker-compose is:</p>
<pre class=""lang-yaml prettyprint-override""><code>services:
  mysql_service:
    image: mysql
    container_name: mysql_server
    restart: always
    ports:
      - &quot;3307:3306&quot;
    volumes:
      - mysql_data:/var/lib/mysql

  tests:
    build:
      dockerfile: dockerfile 
    container_name: tests
    volumes:
      - .:/run_docker_app 
    depends_on:
      - ollama
      - mysql_service
    command: [&quot;pytest&quot;, &quot;--disable-warnings&quot;, &quot;-rP&quot;, &quot;tests&quot;]
</code></pre>
<p>I build the app and run tests using the following in a GitHub workflow file. There are no other configs that I am adding to the Docker containers.</p>
<pre class=""lang-bash prettyprint-override""><code>docker compose build --no-cache 
docker compose up -d mysql_service 
docker compose run --rm tests
docker compose logs tests
</code></pre>
<p>The file which the tests use have the following code to create and connect to the database.</p>
<pre class=""lang-py prettyprint-override""><code>from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, scoped_session, declarative_base
from sqlalchemy import Column, Integer, String, DateTime
import datetime

DATABASE_URI = (&quot;mysql+pymysql://pyuser:pss0@mysql_service:3306/db&quot;)

engine = create_engine(DATABASE_URI, pool_pre_ping=True)
db_session = scoped_session(sessionmaker(bind=engine))

Base = declarative_base()
Base.query = db_session.query_property()
</code></pre>
<p>How to do I connect to this service running on the same container network? My issue is not that Docker can not run a service on a port. The issue is the MySQL container can not be accessed through SQLAlchemy on the same network.</p>
<p>I am modifying the database as a user that has all privileges to the database. I connect this user to the database using SQLAlchemy engine. The URI used for this is as follows. This does not connect to the docker image on the server.</p>
<pre><code>(&quot;mysql+pymysql://pyuser:pss@mysql_server:3306/db&quot;)
</code></pre>
<p>Error on this one</p>
<pre class=""lang-none prettyprint-override""><code>E   sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'mysql_service' (\[Errno -3\] Temporary failure in name resolution)&quot;)

E   (Background on this error at: &lt;https://sqlalche.me/e/20/e3q8)&gt;
</code></pre>
<p>I have tried restarting docker.
mysql conf. is as follows:</p>
<pre class=""lang-ini prettyprint-override""><code>#
# The MySQL database server configuration file.
#
# One can use all long options that the program supports.
# Run program with --help to get a list of available options and with
# --print-defaults to see which it would actually understand and use.
#
# For explanations see
# http://dev.mysql.com/doc/mysql/en/server-system-variables.html
# Here is entries for some specific programs
# The following values assume you have at least 32M ram
[mysqld]
#
# * Basic Settings
#
user            = mysql
# pid-file      = /var/run/mysqld/mysqld.pid
# socket        = /var/run/mysqld/mysqld.sock
# port          = 3306
# datadir       = /var/lib/mysql
# If MySQL is running as a replication slave, this should be
# changed. Ref https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_tmpdir
# tmpdir                = /tmp
#
# Instead of skip-networking the default is now to listen only on
# localhost which is more compatible and is not less secure.
bind-address            = 127.0.0.1
mysqlx-bind-address     = 127.0.0.1
#
# * Fine Tuning
#
key_buffer_size         = 16M
# max_allowed_packet    = 64M
# thread_stack          = 256K
# thread_cache_size       = -1
# This replaces the startup script and checks MyISAM tables if needed
# the first time they are touched
myisam-recover-options  = BACKUP
# max_connections        = 151
# table_open_cache       = 4000
#
# * Logging and Replication
#
# Both location gets rotated by the cronjob.
#
# Log all queries
# Be aware that this log type is a performance killer.
# general_log_file        = /var/log/mysql/query.log
# general_log             = 1
#
# Error log - should be very few entries.
#
log_error = /var/log/mysql/error.log
#
# Here you can see queries with especially long duration
# slow_query_log                = 1
# slow_query_log_file   = /var/log/mysql/mysql-slow.log
# long_query_time = 2
# log-queries-not-using-indexes
#
# The following can be used as easy to replay backup logs or for replication.
# note: if you are setting up a replication slave, see README.Debian about
#       other settings you may need to change.
# server-id             = 1
# log_bin                       = /var/log/mysql/mysql-bin.log
# binlog_expire_logs_seconds    = 2592000
max_binlog_size   = 100M
# binlog_do_db          = include_database_name
# binlog_ignore_db      = include_database_name
</code></pre>
",0,1,1,2025-12-13T20:28:30+00:00,0,31,False
79846829,15753749,,mysql,Failed to initialize JPA EntityManagerFactory: Unable to build Hibernate SessionFactory,"<p>I'm new to Java programming and while trying basic programming, I tried to connect java with Mysql Db where i am facing below issue,</p>
<p>Failed to initialize JPA EntityManagerFactory: Unable to build Hibernate SessionFactory</p>
<p>Below is the application property file</p>
<pre><code>spring.datasource.url=jdbc:mysql://localhost:3306/ems
spring.datasource.username = root
spring.database.password = Mysql@123

spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQLDialect
spring.jpa.hibernate.ddl-auto = update
</code></pre>
<p>below is the employee java class file</p>
<pre><code>package net.javaguides.ems.entity;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;

@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
@Entity
@Table(name=&quot;employees&quot;)
public class Employee {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = &quot;first_name&quot;)
    private String firstName;

    @Column(name = &quot;last_name&quot;)
    private String lastName;

    @Column(name = &quot;email_id&quot;, nullable = false, unique = true)
    private String email;
}
</code></pre>
<p>my sql connection are good where i am getting below in terminal while trying to connect</p>
<pre><code>Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 13
Server version: 9.5.0 MySQL Community Server - GPL

Copyright (c) 2000, 2025, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
</code></pre>
<p>while running project , I am getting as below</p>
<pre><code> 2025-12-13T19:04:59.536-06:00  WARN 2013 --- [           main] org.hibernate.orm.deprecation            : HHH90000025: MySQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-12-13T19:04:59.542-06:00  INFO 2013 --- [           main] org.hibernate.orm.connections.pooling    : HHH10001005: Database info:
    Database JDBC URL [undefined/unknown]
    Database driver: undefined/unknown
    Database dialect: MySQLDialect
    Database version: 8.0
    Default catalog/schema: unknown/unknown
    Autocommit mode: undefined/unknown
    Isolation level: NONE [default NONE]
    JDBC fetch size: undefined/unknown
    Pool: DatasourceConnectionProviderImpl
    Minimum pool size: undefined/unknown
    Maximum pool size: undefined/unknown
2025-12-13T19:04:59.837-06:00  INFO 2013 --- [           main] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-12-13T19:04:59.847-06:00  INFO 2013 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-12-13T19:05:00.864-06:00  WARN 2013 --- [           main] org.hibernate.orm.jdbc.error             : HHH000247: ErrorCode: 1045, SQLState: 28000
2025-12-13T19:05:00.865-06:00  WARN 2013 --- [           main] org.hibernate.orm.jdbc.error             : Access denied for user 'root'@'localhost' (using password: NO)
2025-12-13T19:05:00.873-06:00 ERROR 2013 --- [           main] j.LocalContainerEntityManagerFactoryBean : Failed to initialize JPA EntityManagerFactory: Unable to build Hibernate SessionFactory  [persistence unit: default] ; nested exception is org.hibernate.exception.AuthException: Unable to open JDBC Connection for DDL execution [Access denied for user 'root'@'localhost' (using password: NO)] [n/a]
2025-12-13T19:05:00.874-06:00  WARN 2013 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/hibernate/autoconfigure/HibernateJpaConfiguration.class]: Unable to build Hibernate SessionFactory  [persistence unit: default] ; nested exception is org.hibernate.exception.AuthException: Unable to open JDBC Connection for DDL execution [Access denied for user 'root'@'localhost' (using password: NO)] [n/a]
2025-12-13T19:05:00.877-06:00  INFO 2013 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2025-12-13T19:05:00.885-06:00  INFO 2013 --- [           main] .s.b.a.l.ConditionEvaluationReportLogger : 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-12-13T19:05:00.894-06:00 ERROR 2013 --- [           main] o.s.boot.SpringApplication               : Application run failed

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/hibernate/autoconfigure/HibernateJpaConfiguration.class]: Unable to build Hibernate SessionFactory  [persistence unit: default] ; nested exception is org.hibernate.exception.AuthException: Unable to open JDBC Connection for DDL execution [Access denied for user 'root'@'localhost' (using password: NO)] [n/a]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1817) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:525) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:333) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:371) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:331) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:201) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:966) ~[spring-context-7.0.1.jar:7.0.1]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:620) ~[spring-context-7.0.1.jar:7.0.1]
    at org.springframework.boot.web.server.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143) ~[spring-boot-web-server-4.0.0.jar:4.0.0]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:765) ~[spring-boot-4.0.0.jar:4.0.0]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:454) ~[spring-boot-4.0.0.jar:4.0.0]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:321) ~[spring-boot-4.0.0.jar:4.0.0]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1374) ~[spring-boot-4.0.0.jar:4.0.0]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363) ~[spring-boot-4.0.0.jar:4.0.0]
    at net.javaguides.ems.EmsBackendApplication.main(EmsBackendApplication.java:10) ~[classes/:na]
Caused by: jakarta.persistence.PersistenceException: Unable to build Hibernate SessionFactory  [persistence unit: default] ; nested exception is org.hibernate.exception.AuthException: Unable to open JDBC Connection for DDL execution [Access denied for user 'root'@'localhost' (using password: NO)] [n/a]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:428) ~[spring-orm-7.0.1.jar:7.0.1]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:396) ~[spring-orm-7.0.1.jar:7.0.1]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:409) ~[spring-orm-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1864) ~[spring-beans-7.0.1.jar:7.0.1]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1813) ~[spring-beans-7.0.1.jar:7.0.1]
    ... 15 common frames omitted
Caused by: org.hibernate.exception.AuthException: Unable to open JDBC Connection for DDL execution [Access denied for user 'root'@'localhost' (using password: NO)] [n/a]
    at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:87) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:34) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:115) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:101) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:72) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:37) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcConnection(ImprovedExtractionContextImpl.java:61) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcDatabaseMetaData(ImprovedExtractionContextImpl.java:68) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.getJdbcDatabaseMetaData(InformationExtractorJdbcDatabaseMetaDataImpl.java:34) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.processTableResultSet(InformationExtractorJdbcDatabaseMetaDataImpl.java:72) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.extract.internal.AbstractInformationExtractorImpl.getTables(AbstractInformationExtractorImpl.java:535) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.getTablesInformation(DatabaseInformationImpl.java:120) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.internal.GroupedSchemaMigratorImpl.performTablesMigration(GroupedSchemaMigratorImpl.java:70) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.performMigration(AbstractSchemaMigrator.java:230) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:109) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:267) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.lambda$process$5(SchemaManagementToolCoordinator.java:142) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at java.base/java.util.HashMap.forEach(HashMap.java:1430) ~[na:na]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:139) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.boot.internal.SessionFactoryObserverForSchemaExport.sessionFactoryCreated(SessionFactoryObserverForSchemaExport.java:35) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.internal.SessionFactoryObserverChain.sessionFactoryCreated(SessionFactoryObserverChain.java:33) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:350) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:436) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1459) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:66) ~[spring-orm-7.0.1.jar:7.0.1]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:433) ~[spring-orm-7.0.1.jar:7.0.1]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:416) ~[spring-orm-7.0.1.jar:7.0.1]
    ... 19 common frames omitted
Caused by: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: NO)
    at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:121) ~[mysql-connector-j-9.5.0.jar:9.5.0]
    at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114) ~[mysql-connector-j-9.5.0.jar:9.5.0]
    at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:840) ~[mysql-connector-j-9.5.0.jar:9.5.0]
    at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:416) ~[mysql-connector-j-9.5.0.jar:9.5.0]
    at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:237) ~[mysql-connector-j-9.5.0.jar:9.5.0]
    at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:180) ~[mysql-connector-j-9.5.0.jar:9.5.0]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:144) ~[HikariCP-7.0.2.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:373) ~[HikariCP-7.0.2.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:210) ~[HikariCP-7.0.2.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:488) ~[HikariCP-7.0.2.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:576) ~[HikariCP-7.0.2.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:97) ~[HikariCP-7.0.2.jar:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111) ~[HikariCP-7.0.2.jar:na]
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:138) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:499) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:44) ~[hibernate-core-7.1.8.Final.jar:7.1.8.Final]
    ... 41 common frames omitted


Process finished with exit code 1
</code></pre>
<p>Its in mac.</p>
",-1,0,1,2025-12-14T01:13:41+00:00,1,144,True
79847000,32051267,,mysql,How to exclude records based on conditions in related tables in SQL?,"<p>I have two tables: Patient and Encounters. They are related through an ID for each patient, with the ID appearing as &quot;Id&quot; in the Patient table and as &quot;Patient&quot; in the Encounters table. I need to get the full name and ID of those patients who have never had an 'Encounter Inpatient', with that data being stored in the Description column of the Encounters table. I thought about excluding the patients this way, but it only shows the rows with a value in Description different from that, and not the patients who have never had that value, since several patients have multiple entries in Encounters, each with a different Description.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT 
   p.Id, p.First, p.Last 
FROM datapatient.Encounters e 
LEFT JOIN datapatient.Patients p 
ON e.Patient=p.Id 
WHERE NOT e.Description=&quot;Encounter Inpatient&quot; 
GROUP BY p.Id;
</code></pre>
",-1,1,2,2025-12-14T12:07:06+00:00,1,114,True
79848526,32061251,,mysql,Search not working with Date filter and Pagination,"<p>I am working on an e-commerce web project using vanilla PHP, HTML, and Bootstrap CSS. I have implemented both an admin panel and client/user side. However, I’m facing an issue in the orders.php file, where I am using a Date filter along with search and pagination for the records from the orders table.</p>
<p>The search functionality was working fine before I added the Date filter. After adding the Date filter, the combination of the search, pagination, and date filtering doesn't seem to be functioning correctly.</p>
<p>I need help understanding where I might have gone wrong and how to properly implement this functionality.</p>
<p>PHP Logic to Count Total Records:</p>
<pre class=""lang-php prettyprint-override""><code>
&lt;?php

$limit = isset($_GET['limit']) ? intval($_GET['limit']) : 4;
if ($limit &lt; 1 || $limit &gt; 10)
  $limit = 4;

$curr = isset($_GET['page']) ? intval($_GET['page']) : 1;
if ($curr &lt; 1)
  $curr = 1;

$input = isset($_GET['search']) ? $_GET['search'] : &quot;&quot;;

$from = isset($_GET['from']) ? $_GET['from'] : &quot;&quot;;
$to = isset($_GET['to']) ? $_GET['to'] : &quot;&quot;;

if (!empty($from) &amp;&amp; !empty($to)) {

  $startDate = date('Y-m-d', strtotime($from));
  $endDate = date('Y-m-d', strtotime($to));

  $fullEndDate = &quot;{$endDate} 23:59:59&quot;;

  $count_sql = &quot;SELECT COUNT(*) AS total FROM orders WHERE created_at BETWEEN '$startDate' AND '$fullEndDate'&quot;;
} elseif ($input === &quot;&quot;) {
  $count_sql = &quot;SELECT COUNT(*) AS total FROM orders&quot;;
} else {
  $count_sql = &quot;SELECT COUNT(*) AS total FROM orders WHERE uname LIKE '%{$input}%' OR email LIKE '%{$input}%' OR address LIKE '%{$input}%'&quot;;
}
$res_count = $conn-&gt;query($count_sql);
$total = $res_count-&gt;fetch_assoc()['total'];

$total_pages = ceil($total / $limit);
$offset = ($curr - 1) * $limit;

$prev = max(1, $curr - 1);
$next = min($total_pages, $curr + 1);
</code></pre>
<p>PHP Logic to Display Records:</p>
<pre class=""lang-php prettyprint-override""><code>
&lt;?php

$product_sql = &quot;&quot;;

$from = isset($_GET['from']) ? $_GET['from'] : &quot;&quot;;
$to = isset($_GET['to']) ? $_GET['to'] : &quot;&quot;;

if (!empty($from) &amp;&amp; !empty($to)) {

  $startDate = date('Y-m-d', strtotime($from));
  $endDate = date('Y-m-d', strtotime($to));

  $fullEndDate = &quot;{$endDate} 23:59:59&quot;;

  $product_sql = &quot;SELECT * FROM orders WHERE created_at BETWEEN '$startDate' AND '$fullEndDate' ORDER BY created_at DESC LIMIT $limit OFFSET $offset &quot;;
} elseif ($input === &quot;&quot;) {
  $product_sql = &quot;SELECT * FROM orders ORDER BY oid DESC LIMIT $limit OFFSET $offset &quot;;
} else {
  $product_sql = &quot;SELECT * FROM orders WHERE uname LIKE '%{$input}%' OR email LIKE '%{$input}%' OR address LIKE '%{$input}%' ORDER BY oid DESC LIMIT $limit OFFSET $offset &quot;;
}
$res_product = $conn-&gt;query($product_sql);
</code></pre>
<p>HTML Form for Setting Limit of Records:</p>
<pre class=""lang-html prettyprint-override""><code>
        &lt;form method=&quot;get&quot; class=&quot;d-flex align-items-center&quot;&gt;
            &lt;label class=&quot;me-2&quot;&gt;Limit:&lt;/label&gt;
            &lt;select class=&quot;form-select form-select-sm me-2&quot; name=&quot;limit&quot; style=&quot;width:80px;&quot;&gt;
              &lt;?php for ($i = 1; $i &lt;= 7; $i++) { ?&gt;
                &lt;option value=&quot;&lt;?= $i ?&gt;&quot; &lt;?= $i == $limit ? &quot;selected&quot; : &quot;&quot; ?&gt;&gt;&lt;?= $i ?&gt;&lt;/option&gt;
              &lt;?php } ?&gt;
            &lt;/select&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;page&quot; value=&quot;&lt;?= $curr ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;search&quot; value=&quot;&lt;?= $input ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;from&quot; value=&quot;&lt;?= $startDate ?? &quot;&quot; ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;to&quot; value=&quot;&lt;?= $endDate ?? &quot;&quot; ?&gt;&quot;&gt;
            &lt;button class=&quot;btn btn-primary btn-sm&quot;&gt;Set&lt;/button&gt;
        &lt;/form&gt;
</code></pre>
<p>HTML Form for Search Functionality:</p>
<pre class=""lang-html prettyprint-override""><code>
        &lt;form method=&quot;get&quot; class=&quot;p-3&quot;&gt;
          &lt;div class=&quot;input-group mb-3&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;limit&quot; value=&quot;&lt;?= $limit ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;page&quot; value=&quot;&lt;?= $curr ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;from&quot; value=&quot;&lt;?= $startDate ?? &quot;&quot; ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;to&quot; value=&quot;&lt;?= $endDate ?? &quot;&quot; ?&gt;&quot;&gt;

            &lt;input type=&quot;text&quot; name=&quot;search&quot; class=&quot;form-control&quot; placeholder=&quot;Search Orders&quot; value=&quot;&lt;?= $input ?&gt;&quot;&gt;

            &lt;button class=&quot;input-group-text&quot;&gt;Search&lt;/button&gt;
          &lt;/div&gt;
        &lt;/form&gt;
</code></pre>
<p>HTML Form for Date Filter Functionality:</p>
<pre class=""lang-html prettyprint-override""><code>
        &lt;form method=&quot;get&quot; class=&quot;d-flex align-items-center&quot;&gt;
            &lt;label class=&quot;me-2&quot;&gt;From:&lt;/label&gt;

            &lt;input type=&quot;hidden&quot; name=&quot;limit&quot; value=&quot;&lt;?= $limit ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;page&quot; value=&quot;&lt;?= $curr ?&gt;&quot;&gt;
            &lt;input type=&quot;hidden&quot; name=&quot;search&quot; value=&quot;&lt;?= $input ?&gt;&quot;&gt;

            &lt;input type=&quot;date&quot; name=&quot;from&quot;
              value=&quot;&lt;?php echo isset($_GET['from']) ? htmlspecialchars($_GET['from']) : ''; ?&gt;&quot;&gt;
            &lt;label class=&quot;me-2&quot;&gt;To:&lt;/label&gt;
            &lt;input type=&quot;date&quot; name=&quot;to&quot; value=&quot;&lt;?php echo isset($_GET['to']) ? htmlspecialchars($_GET['to']) : ''; ?&gt;&quot;&gt;

            &lt;button class=&quot;btn btn-primary btn-sm&quot; name=&quot;filter&quot;&gt;Filter&lt;/button&gt;
        &lt;/form&gt;
</code></pre>
<p>pagination code:</p>
<pre class=""lang-php prettyprint-override""><code>
&lt;?php if ($total &gt; 0) { ?&gt;
          &lt;div class=&quot;card-footer d-flex justify-content-between align-items-center&quot;&gt;
            &lt;ul class=&quot;pagination pagination-sm mb-0&quot;&gt;

              &lt;?php if ($curr &gt; 1) { ?&gt;
                &lt;li class=&quot;page-item&quot;&gt;&lt;a class=&quot;page-link&quot;
                    href=&quot;?limit=&lt;?= $limit ?&gt;&amp;page=&lt;?= $prev ?&gt;&amp;search=&lt;?= $input ?&gt;&amp;from=&lt;?= $startDate ?? &quot;&quot; ?&gt;&amp;to=&lt;?= $endDate ?? &quot;&quot; ?&gt;&quot;&gt;&amp;laquo;&lt;/a&gt;
                &lt;/li&gt;
              &lt;?php } ?&gt;

              &lt;?php for ($p = 1; $p &lt;= $total_pages; $p++) { ?&gt;
                &lt;li class=&quot;page-item&quot;&gt;
                  &lt;a class=&quot;page-link&quot;
                    href=&quot;?limit=&lt;?= $limit ?&gt;&amp;page=&lt;?= $p ?&gt;&amp;search=&lt;?= $input ?&gt;&amp;from=&lt;?= $startDate ?? &quot;&quot; ?&gt;&amp;to=&lt;?= $endDate ?? &quot;&quot; ?&gt;&quot;&gt;&lt;?= $p ?&gt;&lt;/a&gt;
                &lt;/li&gt;
              &lt;?php } ?&gt;

              &lt;?php if ($curr &lt; $total_pages) { ?&gt;
                &lt;li class=&quot;page-item&quot;&gt;&lt;a class=&quot;page-link&quot;
                    href=&quot;?limit=&lt;?= $limit ?&gt;&amp;page=&lt;?= $next ?&gt;&amp;search=&lt;?= $input ?&gt;&amp;from=&lt;?= $startDate ?? &quot;&quot; ?&gt;&amp;to=&lt;?= $endDate ?? &quot;&quot; ?&gt;&quot;&gt;&amp;raquo;&lt;/a&gt;
                &lt;/li&gt;
              &lt;?php } ?&gt;

            &lt;/ul&gt;
          &lt;/div&gt;
        &lt;?php } ?&gt;

</code></pre>
",-3,0,3,2025-12-16T12:22:19+00:00,0,50,False
79849315,32040109,,mysql,How can I recover my existing databases safely without loosing the data,"<p>I'm trying to start MySQL in XAMPP on my Windows machine, but it immediately stops with the following error:</p>
<blockquote>
<p>10:57:03 [mysql] Attempting to start MySQL app...
10:57:03 [mysql] Status change detected: running<br />
10:57:05 [mysql] Status change detected: stopped<br />
10:57:05 [mysql] Error: MySQL shutdown unexpectedly.<br />
10:57:05 [mysql] This may be due to a blocked port, missing
dependencies, improper privileges, a crash, or a shutdown by another method.                                                                                       &gt;10:57:05 [mysql] Press the Logs button to view error logs and check the Windows Event Viewer for more clues.</p>
</blockquote>
",-3,0,3,2025-12-17T10:50:42+00:00,0,59,False
79849335,18159572,,mysql,Case Expression in Where clause,"<p>Isn't it much easier to write this:</p>
<pre><code>SELECT *
FROM your_table
WHERE CASE
  WHEN CURTIME() BETWEEN '11:01:00' AND '15:00:00' THEN created_at BETWEEN CONCAT(CURDATE(), ' 11:01:00') AND CONCAT(CURDATE(), ' 15:00:00')
  WHEN CURTIME() BETWEEN '15:01:00' AND '17:00:00' THEN created_at BETWEEN CONCAT(CURDATE(), ' 15:01:00') AND CONCAT(CURDATE(), ' 17:00:00')
 ELSE created_at BETWEEN CONCAT(DATE_SUB(CURDATE(), INTERVAL 1 DAY), ' 17:01:00') AND CONCAT(CURDATE(), ' 11:00:00')
 END;
</code></pre>
<p>than this?</p>
<pre><code>SELECT *
FROM your_table
WHERE created_at BETWEEN
(
    CASE
        WHEN CURTIME() BETWEEN '11:01:00' AND '15:00:00' THEN CONCAT(CURDATE(), ' 11:01:00')
        WHEN CURTIME() BETWEEN '15:01:00' AND '17:00:00' THEN CONCAT(CURDATE(), ' 15:01:00')
        ELSE CONCAT(DATE_SUB(CURDATE(), INTERVAL 1 DAY), ' 17:01:00')
    END
)
AND
(
    CASE
        WHEN CURTIME() BETWEEN '11:01:00' AND '15:00:00' THEN CONCAT(CURDATE(), ' 15:00:00')
        WHEN CURTIME() BETWEEN '15:01:00' AND '17:00:00' THEN CONCAT(CURDATE(), ' 17:00:00')
        ELSE CONCAT(CURDATE(), ' 11:00:00')
    END
);
</code></pre>
<p>I get the same results.<br>
On the first one, I can add all the statements I want, not just the <code>created_at</code>, for example adding <code>AND user_id = 1</code>.</p>
<p>Can someone please explain why the 2nd is better.</p>
",0,0,0,2025-12-17T11:03:09+00:00,8,89,True
79849478,31378419,,mysql,Duplicate primary key errors when applying binlog after mydumper/myloader restore,"<p>I am implementing a <strong>full + incremental restore</strong> workflow using <strong>mydumper / myloader</strong> and MySQL binary logs.</p>
<hr />
<h3>Backup step</h3>
<p>I take a snapshot-consistent backup using <code>mydumper</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>mydumper \
  -u Admin \
  -h 127.0.0.1 \
  -a \
  --database=db1 \
  --threads=4 \
  --trx-tables \
  --events \
  --routines \
  --triggers \
  --compress \
  --outputdir=/backup/17122025 \
  --verbose=3
</code></pre>
<p>After the dump finishes, I read the binlog coordinates from the generated <code>metadata</code> file:</p>
<pre class=""lang-bash prettyprint-override""><code>BINLOG_FILE=$(awk -F= '/SOURCE_LOG_FILE/ {gsub(/[ &quot;#]/,&quot;&quot;,$2); print $2}' metadata)
BINLOG_POS=$(awk -F= '/SOURCE_LOG_POS/  {gsub(/[ #]/,&quot;&quot;,$2); print $2}' metadata)
</code></pre>
<p>Example from metadata:</p>
<pre><code>SOURCE_LOG_FILE = &quot;binlog.000524&quot;
SOURCE_LOG_POS  = 337111
</code></pre>
<hr />
<h3>Restore step</h3>
<p>I restore the full backup into a new database using <code>myloader</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>myloader \
  -u Admin \
  -h 127.0.0.1 \
  -a \
  --directory=/backup/17122025 \
  --database=db1_restore \
  --threads=4 \
  --overwrite-tables \
  --verbose=3
</code></pre>
<p>At this point, the restored database looks correct.
For example, a table has <strong>400 rows</strong> with primary keys <code>1–400</code>.</p>
<hr />
<h3>Incremental (binlog) replay</h3>
<p>After the backup was taken, I insert additional rows into the source database.</p>
<p>To apply those changes to the restored database, I run:</p>
<pre class=""lang-bash prettyprint-override""><code>mysqlbinlog /Data/mysql/binlog.000524 \
  --start-position=337111 \
  --database=db1 \
| sed 's/`db1`/`db1_restore`/g' \
| mysql \
    --show-warnings \
    -u Admin \
    -h 127.0.0.1 \
    -p \
    db1_restore
</code></pre>
<hr />
<h3>Problem</h3>
<p>While applying the binlog, MySQL fails with duplicate primary key errors, for example:</p>
<blockquote>
<p>ERROR 1062 (23000) at line 56: Duplicate entry '401' for key 'tablename.PRIMARY'</p>
</blockquote>
<p>This happens even though:</p>
<ul>
<li>The restored table has rows <code>1–400</code></li>
<li>The binlog row image shows an insert for PK <code>401</code></li>
<li>The table’s <code>AUTO_INCREMENT</code> value is already higher than <code>401</code></li>
</ul>
<p>The binlog clearly contains row-based events like:</p>
<pre class=""lang-none prettyprint-override""><code>### INSERT INTO `db1_restore`.`tablename`
### SET
###   @1=401
</code></pre>
<hr />
<h3>Question</h3>
<p>What is the correct way to apply MySQL binlogs <strong>after a mydumper/myloader restore</strong> without hitting duplicate primary key errors?</p>
<hr />
<h3>Environment</h3>
<ul>
<li>MySQL 8.4.x</li>
<li>Binlog format: ROW</li>
<li>Backup tool: mydumper 0.21.x</li>
<li>Restore tool: myloader 0.21.x</li>
<li>Engine: InnoDB</li>
</ul>
",-1,0,1,2025-12-17T12:59:04+00:00,0,29,False
79851228,8953248,"Waterloo, ON, Canada",mysql,Is there a function (python or PHP) that will return a string that has properly quoted user input suitable for MySQL?,"<p>When updating a MySQL database with python3 or PHP, one ensures that the user-supplied values are appropriately protected by using something like:</p>
<pre><code>statement = &quot;INSERT INTO t1 (c1, c2, c3, c4 ) VALUES ( ?, ?, ?, ?)&quot;  
values = (v1, v2, v3, v4)  
db.execute(statement, values)  
  
</code></pre>
<p>Is there an equivalent function that simply returns the SQL statement as a string (e.g. what <code>mysqldump</code> produces)?<br />
And ideally, without having to connect to an actual database.</p>
<p>The goal is to write an interactive data input program that generates the appropriately quoted SQL independently of access to the database.<br />
(Security (e.g. manually tampering with the output) is not a concern in this situation.)</p>
<h3>Answer:</h3>
<p>No, and for a good reason.
There is little need for such a function, and any process that requires it is poorly designed.</p>
<p>In my case:</p>
<ul>
<li>I was hoping to keep the data collection entirely isolated from the database, and then later feed the results to MySQL to perform the update.</li>
<li>I was also trying to make it conceptually simple by having the collected data in a form suitable for giving directly to MySQL.</li>
</ul>
<p>The first is not necessarily a bad idea.
But the second <em>is</em>:</p>
<ul>
<li>It doesn't use the appropriate tools that are already provided in the libraries.</li>
<li>It requires the first part of the process to be aware of the second part.</li>
</ul>
<p>A much better approach is to have the collected data in an implementation-independent form (e.g. a text file of tab separated fields), and to have a second program that converts that data and passes it to MySql using the standard tools (&quot;prepare&quot; and &quot;execute&quot;).
Doing so also means that should I later switch to a different database system, the data collection process wouldn't need any changes.</p>
<p>Thanks to @Bill Karwin for pointing out what should have been obvious.</p>
",1,0,0,2025-12-19T15:40:21+00:00,15,74,True
79852033,29862443,,mysql,Extract Date from File Name MYSQL,"<p>I have a number of files that have the date embedded in them. I want to extract the date into another column. I have read a number of posts that seem to concentrate on extracting the part of a date from an existing date/time. The names vary in length.</p>
<p>Can you please help with this but please explain what the recommendation means so I can apply it to all the files.</p>
<p>By this I mean 'left' the numbering, etc</p>
<p>obviously I am a newbie and would like to understand the procedure.</p>
<p>Thank you in advance</p>
<p>Phil</p>
",-3,0,3,2025-12-20T23:25:39+00:00,0,54,False
79852591,30140016,,mysql,How do I choose a file destination in html/php?,"<p>I am using a html file type where the user can upload images. I am trying to determine the file location.<br />
I want it to go to the current file (where the php file is in) and then database\images.<br />
I am use mysql to upload data and I have a post-images table which contains a post_id (the forieng key from post table) and image_path which should point to the database/images so I can access it later. How do I do this?</p>
<pre class=""lang-php prettyprint-override""><code>if($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;POST&quot;) {
// [...]
$fileUpload = $_FILES[&quot;file-upload&quot;];
// [...]
 $sqlPostID = &quot;SELECT id FROM posts WHERE title = '$postTitle'&quot;;
            $result = mysqli_query($conn, $sqlPostID);

            if (mysqli_num_rows ($result) &gt; 0) {
                $row = mysqli_fetch_assoc($result);
                echo &quot;&lt;script type='text/javascript'&gt;alert(\&quot;$row[id]\&quot;);&lt;/script&gt;&quot;;

                $sqlpost_images = &quot;INSERT INTO post_images (post_id, image_path) VALUES ('$row[id]', '$fileUpload')&quot;;

                try {
                    mysqli_query($conn, $sqlpost_images);
                    echo &quot;&lt;script type='text/javascript'&gt;alert(\&quot;Images uploaded successfully.\&quot;);&lt;/script&gt;&quot;;

                } catch (mysqli_sql_exception $e) {
                    echo &quot;&lt;script type='text/javascript'&gt;alert(\&quot;Having difficulties connecting to database\&quot;);&lt;/script&gt;&quot;;
                }
// [...]
}
</code></pre>
<p>Please let me know if you know or need more info. I am trying to learn databases with php and mysql for web development. Thanks</p>
",0,0,0,2025-12-22T03:30:16+00:00,5,120,True
79854149,1136807,,mysql,Using REGEXP instead of AND condition,"<p>I am writing a query where the user can enter a <code>comma(,)</code> separated value which would be used as an <code>AND</code> condition on a single column - lets say <code>address</code> - for refined search.</p>
<p>For example:</p>
<pre class=""lang-sql prettyprint-override""><code>/**
 * Search: Gurugram, Haryana, India
 */

SELECT * FROM users
WHERE (
    address LIKE '%Gurugram%'
    AND address LIKE '%Haryana%'
    AND address LIKE '%India%'
)
</code></pre>
<p>Although, the query is producing the desired result as required, but I was wondering if it could be used with <code>REGEXP</code> instead of adding multiple <code>AND</code> conditions, something like:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM users
WHERE address REGEXP 'Gurugram,Haryana,India'
-- OR -- 
WHERE address REGEXP 'Gurugram+Haryana+India'
-- OR --
</code></pre>
<p>Is it possible this way?</p>
",3,3,0,2025-12-24T05:49:42+00:00,1,89,True
79854560,6225921,,mysql,Do SQL databases apply WHERE filters before joins?,"<pre><code>from a in xx 
join b in yy on a.someprop equals b.someprop 
join c in zz on a.someprop1 equals c.someprop 
where a.someprop2.Contains(strval)
</code></pre>
<p>I’m trying to understand how SQL query execution works with joins and <code>WHERE</code> filters.</p>
<p>From my current understanding, when a query includes multiple tables, the database first joins all the tables and only applies the <code>WHERE</code> clause afterward. That sounds inefficient, since it means unnecessary rows are processed before filtering.</p>
<p>However, I’ve read that modern SQL optimizers may reorder operations — for example, by applying filters early (predicate pushdown) so fewer rows are joined, which should be more efficient.</p>
<p>How can I verify the actual execution order (e.g., whether filtering happens before or after joins)?</p>
",0,0,0,2025-12-24T19:23:13+00:00,3,79,True
79854851,281965,Israel,mysql,MySQL throw ERROR 3098 .. table does not comply .. external plugin when FK exists,"<p>So I understand that in a group replication I must have either:</p>
<ul>
<li><p>Primary Key, OR</p>
</li>
<li><p>a NOT NULL UNIQUE column that will act as PK ( InnoDB choose it automatically )</p>
</li>
</ul>
<p>In my case I do have an explicit PK, like so:</p>
<pre><code>mysql&gt; DESCRIBE tableA;
+-------+--------------+------+-----+---------+----------------+
| Field | Type         | Null | Key | Default | Extra          |
+-------+--------------+------+-----+---------+----------------+
| id    | int unsigned | NO   | PRI | NULL    | auto_increment |
| name  | varchar(20)  | YES  | UNI | NULL    |                |
+-------+--------------+------+-----+---------+----------------+
2 rows in set (0.00 sec)

mysql&gt; DESCRIBE tableB;
+-------------+--------------+------+-----+-------------------+-------------------+
| Field       | Type         | Null | Key | Default           | Extra             |
+-------------+--------------+------+-----+-------------------+-------------------+
| id          | int unsigned | NO   | PRI | NULL              | auto_increment    |
| tableA_id   | int unsigned | NO   | MUL | NULL              |                   |
| col1        | varchar(10)  | YES  |     | NULL              |                   |
| col2        | text         | YES  |     | NULL              |                   |
| created     | datetime     | YES  |     | CURRENT_TIMESTAMP | DEFAULT_GENERATED |
+-------------+--------------+------+-----+-------------------+-------------------+
5 rows in set (0.00 sec)
</code></pre>
<ol>
<li>When I insert rows into tableA - they are added just fine.</li>
<li>When I then insert rows into tableB - they throw the following error</li>
</ol>
<pre><code>ERROR 3098 (HY000): 
The table does not comply with the requirements by an external plugin.
</code></pre>
<p>So i started debugging <code>tableB</code>, noticed that when I remove the <code>FOREIGN KEY</code></p>
<pre><code>CONSTRAINT `tableA_id_ibfk_1` FOREIGN KEY (`tableA_id`) REFERENCES `tableA` (`id`) ON DELETE CASCADE
</code></pre>
<p>it works. but why FK is bothering the group replication ?</p>
<p>I need the <code>FOREIGN KEY</code> ... any thoughts ?</p>
",0,0,0,2025-12-25T12:21:46+00:00,1,48,False
79854886,32102950,,mysql,create database in server(truenas scale),"<p>I am running a server using TrueNAS SCALE. I want to save all of my shop's invoices directly to the server and be able to view them from my computer. I am familiar with SQL for database management. Could you please describe the best way to set this up?</p>
",0,0,0,2025-12-25T13:45:57+00:00,2,53,False
79683611,28821514,,sqlite,Runtime error while execute sql file by sqlite3 &quot;unclosed [&quot;,"<p>I have sql file generated by DB browser:</p>
<pre><code>BEGIN TRANSACTION;
CREATE TABLE IF NOT EXISTS &quot;NB&quot; (
    &quot;phone&quot; TEXT CHECK(&quot;phone&quot; REGEXP '^(\+\d\d)?[\s-]?')
);
INSERT INTO &quot;NB&quot; VALUES ('+33 123 456 789');
</code></pre>
<p>When I tried to execute it with sqlite3: <code>sqlite3 nb.db &lt;NB.sql</code> I get an error: <code>Runtime error near line 5: unclosed '['</code>.</p>
<p>What is wrong and why I get this error?</p>
",2,2,0,2025-06-29T09:03:08+00:00,2,77,True
79684027,28821514,,sqlite,Сonfusion with module versions in lua: 5.1 module version into 5.3 interpreter,"<p>I need to use a SQLite database with Latex. After some searching, I came to the conclusion that the best way is to use <code>LuaLatex</code> with the <code>luasql-sqlite3</code> module. The installation required me to install <code>git</code>, <code>sqlite3</code>, <code>libsqlite3-dev</code> packages and the <code>luarocks</code> package manager. After all this, the <code>luasql-sqlite3</code> module was installed with the <code>luarocks</code> package manager and it was even found and enabled in terminal. But it did not want to be enabled from the <code>Tex</code> file: <code>LuaLatex</code> did not see it.</p>
<p>Using the <code>luarocks show luasql</code> command, I was able to find out that this module was installed in the <code>/usr/loca/lib/lua/5.1/luasql/sqlite3.so</code> folder. It is also clear that it uses <code>lua &gt;= 5.1 (using 5.1.1)</code>.</p>
<p>To specify the exact location of the library, I added the following line to the Tex file: <code>\directlua{package.cpath = &quot;/usr/local/lib/lua/5.1/luasql/sqlite3.so;&quot; .. package.cpath}</code>. But the import fails with an error: <code>undefiend symbol</code>. I found that such an error sometimes appears when importing <code>5.1</code> libraries into the <code>5.3</code> interpreter.</p>
<p>What should I do next? Is there a way to download a version of <code>luasql</code> for the <code>5.3</code> interpreter? Is there a way to explicitly tell <code>LuaLatex</code> which version of the interpreter to use?</p>
<p>Console output:</p>
<pre><code>...e/texmf-dist/tex/luatex/lualibs/lualibs-basic-merged.lua:404: error loading
 module 'luasql.sqlite3' from file '/usr/local/lib/lua/5.1/luasql/sqlite3.so':
    /usr/local/lib/lua/5.1/luasql/sqlite3.so: undefined symbol: lua_tointeger
stack traceback:
    [C]: in local 'f'
    ...e/texmf-dist/tex/luatex/lualibs/lualibs-basic-merged.lua:404: in local 'loo
kup'
    ...e/texmf-dist/tex/luatex/lualibs/lualibs-basic-merged.lua:429: in function &lt;
...e/texmf-dist/tex/luatex/lualibs/lualibs-basic-merged.lua:419&gt;
    [C]: in function 'require'
    [\directlua]:1: in main chunk.
l.19 }}
</code></pre>
<p><code>\usepackage{luapackageloader}</code> didn't help.</p>
<p>P.S. I am using TEX Studio.</p>
<p><strong>Upgrade</strong></p>
<p>To check it I add <code>print(_VERSION)</code> and <code>print(&quot;&lt;-------------------------&quot;)</code> to my Tex file and type <code>lua -v</code> in terminal. I got confirmation: <code>LuaLatex</code> use <code>Lua 5.3</code> version, but terminal use <code>Lua 5.1</code> version. That why <code>luarock</code> installed <code>luasql-sqlite3</code> for lua 5.1. Determine version of installed library can be got by <code>luarock list</code>.</p>
<p>The problem was resolved by removing <code>lua5.1</code>, <code>lua5.2</code> (and related <code>*-dev</code>, <code>*-0</code> packages), installation <code>lua5.3</code>, <code>liblua5.3-0</code>, <code>liblia5.3-dev</code> and reinstalling <code>luasql-sqlite3</code>:</p>
<pre><code>sudo luarocks install luasql-sqlite3 SQLITE_INCDIR=/usr/include
</code></pre>
<p>And I was wrong, <code>\usepackage{luapackageloader}</code> helps.</p>
<p>Before closing the question I will wait some time, maybe somebody will have better solution.</p>
",0,0,0,2025-06-29T20:24:24+00:00,1,76,True
79684334,16365326,,sqlite,sqlcipher_page_cipher aftre sqlite3_key()... with includet sqlite3 sqlcipher,"<p>I integrated sqlite to my project with help library <code>#include &lt;sqlite3.h&gt;</code>... Open and Connect work correct... next example my code:</p>
<pre><code>#include &lt;sqlite3.h&gt;

int callback(void *notUsed, int colCount, char **columns, char **colNames) {
    ///........
}

int main {
    sqlite3 *db;
    int res = sqlite3_open(&quot;test.db&quot;, &amp;db);
    if (res != SQLITE_OK) {
        printf(&quot;Cannot open database\n&quot;);
        exit(1);
    }
    sqlite3_exec(db, &quot;SELECT * FROM test_tables;&quot;, callback, NULL, NULL);
    if (db != NULL) {
        sqlite3_close(db);
    }
    return 0;
}
</code></pre>
<p>After encrypting the database (see screenshot, key type = 'passphrase' page size = '1024') I have connected sqlcipher to the project <code>-lsqlcipher -lsqlite3 -lssl -lcrypto -lcrypt32 -lwsock32 -lws2_32</code>.
<a href=""https://i.sstatic.net/Eru5PoZP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Eru5PoZP.png"" alt=""enter image description here"" /></a>
Set <code>SQLITE_HAS_CODEC 1</code> and <code>use sqlite_key()</code> after <code>sqlite_open()</code>... example my code:</p>
<pre><code>#include &lt;sqlite3.h&gt;
#define SQLITE_HAS_CODEC 1

int callback(void *notUsed, int colCount, char **columns, char **colNames) {
    ///........
}

int main {
    sqlite3 *db;
    int res = sqlite3_open(&quot;test.db&quot;, &amp;db);
    res = sqlite3_key(db, &quot;abc&quot;, 3); // i trying to write 3 and 4...
    if (res != SQLITE_OK) {
        printf(&quot;Cannot open database\n&quot;);
        exit(1);
    }
    sqlite3_exec(db, &quot;SELECT * FROM test_tables;&quot;, callback, NULL, NULL);
    if (db != NULL) {
        sqlite3_close(db);
    }
    return 0;
}
</code></pre>
<p>I trying use <code>pragma key='%password%'</code> and <code>pragma page_size=%page size%</code> but i always get next error:</p>
<pre><code>2025-06-29 22:53:57.796: sqlcipher_page_cipher: hmac check failed for pgno=1
2025-06-29 22:53:57.797: sqlite3Codec: error decrypting page 1 data: 1
2025-06-29 22:53:57.797: sqlcipher_codec_ctx_set_error 1
</code></pre>
<p>ALWAYS! No matter what I write, nothing changes. Has anyone come across this? Does anyone know what's going on? Thank all in advance</p>
<p>I was trying to connect to an encrypted database and expected to succeed.</p>
",1,1,0,2025-06-30T06:46:41+00:00,1,115,True
79687673,29933763,,sqlite,How can I Properly Get Around Competing PORT Connections to Gather SQL Data?,"<p>I have one plotly-dash file, which collects data from a database that two other files are publishing to, one that collects data on power and the other on current. The two data collection files use modbus to communicate with a device plugged into a physical usb port, COM4, then publish with sqlite3 to tables.</p>
<p>Note: I ran into some problems with having two separate files trying to get data from the device, so I used sockets so that only the power data gathering file is gathering data from the device, before sending the current data to the current data gathering file so they sort out their respective data independently.</p>
<p>There is one physical port for both of the functions to use, which is where problems start. While I can get and publish data, if I try to simultaneously gather power and current data, they cancel each other out and return 0 or invalid values. I get this in the terminal (Im using VS Code) when the power data gathering sequence is triggered on the 10th minute.</p>
<pre><code>Collecting Data [0]
Collecting Data [0]
3.45.3
3.45.3
Opened database successfully
Table created successfully
Starting on Active Power 1
Connected to COM4
Opened database successfully
Table created successfully
Starting on Active Power 1
Could not connect to COM4
Warning: avg_over_2hsec(25) returned None
[0]Active Power 1 Done
Starting on Active Power 2
Could not connect to COM4
Warning: avg_over_2hsec(27) returned None
[0]Active Power 2 Done
Starting on Active Power 3
Could not connect to COM4
Warning: avg_over_2hsec(29) returned None
[0]Active Power 3 Done
Starting on Active Power Total
Could not connect to COM4
Warning: avg_over_2hsec(43) returned None
[0]Active Power Total Done
Starting on Current 1
Could not connect to COM4
[0]Current 1 Done
Starting on Current 2
Could not connect to COM4
[0]Current 2 Done
Starting on Current 3
Could not connect to COM4
[0]Current 3 Done
Starting on Current Total
Could not connect to COM4
[0]Current Total Done
[0]Sent current data to Process B.
</code></pre>
<p>Here is the relevant code from the power data gathering file:</p>
<pre><code>def data_gathering(n):
with sqlite3.connect('power.db', timeout = 5, check_same_thread = False) as conn:
    conn.execute('PRAGMA journal_mode=WAL;')
    print(&quot;Opened database successfully&quot;)

    #conn.execute('DROP TABLE IF EXISTS POWER;')

    conn.execute('''
    CREATE TABLE IF NOT EXISTS POWER (
        TIME INT PRIMARY KEY,
        APW1 INT NOT NULL,
        APW2 INT NOT NULL,
        APW3 INT NOT NULL,
        APWTOT INT NOT NULL
    );
    ''')
    conn.commit()

    print(&quot;Table created successfully&quot;)
     
    time_stamp = int(time.time())
    
    print(&quot;Starting on Active Power 1&quot;)
    val1 = avg_over_2hsec(25)
    if val1 is None:
        print(&quot;Warning: avg_over_2hsec(25) returned None&quot;)
        appow1 = 0  
    else:
        appow1 = round(250 * val1)
    
    print(f&quot;[{n}]Active Power 1 Done&quot;)

    print(&quot;Starting on Active Power 2&quot;)
    val2 = avg_over_2hsec(27)
    if val2 is None:
        print(&quot;Warning: avg_over_2hsec(27) returned None&quot;)
        appow2 = 0 
    else:
        appow2 = round(250 * val2)
    print(f&quot;[{n}]Active Power 2 Done&quot;)

    print(&quot;Starting on Active Power 3&quot;)
    val3 = avg_over_2hsec(29)
    if val3 is None:
        print(&quot;Warning: avg_over_2hsec(29) returned None&quot;)
        appow3 = 0  
    else:
        appow3 = round(250 * val3)
    print(f&quot;[{n}]Active Power 3 Done&quot;)

    print(&quot;Starting on Active Power Total&quot;)
    valt = avg_over_2hsec(43)
    if valt is None:
        print(&quot;Warning: avg_over_2hsec(43) returned None&quot;)
        appowtot = 0  
    else:
        appowtot = round(250 * valt)
    print(f&quot;[{n}]Active Power Total Done&quot;)
    
    conn.execute(&quot;INSERT OR REPLACE INTO POWER (TIME, APW1, APW2, APW3, APWTOT) VALUES (?, ?, ?, ?, ?)&quot;,
         (time_stamp, appow1, appow2, appow3, appowtot));
    conn.commit()

    #CURRENT GATHERING PORTION 

    time_stamp = int(time.time())
    
    print(&quot;Starting on Current 1&quot;)
    value = avg_over_short(17)
    if value is None:
        value = 0  # or some default fallback value
    curr1 = round(250 * value)
    print(f&quot;[{n}]Current 1 Done&quot;)

    print(&quot;Starting on Current 2&quot;)
    value2 = avg_over_short(19)
    if value2 is None:
        value2 = 0  # or some default fallback value
    curr2 = round(250 * value2)
    print(f&quot;[{n}]Current 2 Done&quot;)

    print(&quot;Starting on Current 3&quot;)
    value3 = avg_over_short(21)
    if value3 is None:
        value3 = 0  # or some default fallback value
    curr3 = round(250 * value3)        
    print(f&quot;[{n}]Current 3 Done&quot;)

    print(&quot;Starting on Current Total&quot;)
    valuet = avg_over_short(23)
    if valuet is None:
        valuet = 0  # or some default fallback value
    currtot = round(250 * valuet)
    print(f&quot;[{n}]Current Total Done&quot;)

    current_data = {
    'TIME': time_stamp,
    'CURRY1': curr1,
    'CURRY2': curr2,
    'CURRY3': curr3,
    'CURRYTOT': currtot
}
    send_curr_data(current_data)
    print(f&quot;[{n}]Sent current data to Process B.&quot;)


def send_curr_data(current_data):
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect(('localhost', 9876))  # Connect to process B
            message = json.dumps(current_data)
            s.sendall(message.encode('utf-8'))
    except ConnectionRefusedError:
        print(&quot;gatheringcurrentdata.py not running or connection refused&quot;)

    except Exception as e:
        print(f&quot;Error: {e}&quot;)


def save_to_db(data):
    with sqlite3.connect('current_full.db') as conn:
        cursor = conn.cursor()
        # Insert current_data, with timestamp, CURRY1, CURRY2, 
        cursor.execute(&quot;INSERT INTO CURRENT (TIME, CURRY1, CURRY2, CURRY3, CURRYTOT) VALUES (?, ?, ?, ?, ?)&quot;,
                       (data['TIME'], data['CURRY1'], data['CURRY2'], data['CURRY3'],     data['CURRYTOT']))
        conn.commit()   

if __name__ == '__main__':
    n = 0
    freeze_support()

    now = datetime.now()
    sec_after_hour = now.minute * 60 + now.second
    sec_til_cleanstart = 600 - (sec_after_hour % 600)
    print(&quot;Waiting to start on xy:00,&quot;, sec_til_cleanstart, &quot;seconds left&quot;)
    time.sleep(sec_til_cleanstart)


    while True:
        nowDT = datetime.now()
        if nowDT.minute % 10 &gt;= 8:
            wait_secs = ((10- nowDT.minute % 10) % 10) * 60 - nowDT.second
            print(f&quot;[DataGathering] Outside write window. Sleeping {wait_secs} seconds.&quot;)
            time.sleep(wait_secs)
            continue

        now = time.time()


        print(&quot;Collecting Data&quot;, [n], flush=True)
        p = Process(target=data_gathering, args=(n,))
        p.start()

        p.join(timeout=480)

        if p.is_alive():
            print(&quot;Function timed out and was stopped.&quot;, flush=True)
            p.terminate()
            print(&quot;Terminate called&quot;)
            p.join()
            print(&quot;Process joined after termination&quot;, flush=True)  
        else:
            print(&quot;Data collected in time&quot;, flush=True) 
            try:
                with sqlite3.connect(&quot;power.db&quot;, timeout = 5, check_same_thread = False) as conn:
                    conn.execute(&quot;ATTACH DATABASE 'power_full.db' AS power_full&quot;)

                    conn.execute('''
                        CREATE TABLE IF NOT EXISTS power_full.POWER (
                            TIME INT PRIMARY KEY,
                            APW1 INT NOT NULL,
                            APW2 INT NOT NULL,
                            APW3 INT NOT NULL,
                            APWTOT INT NOT NULL
                        );
                        ''')
                
                    conn.execute('''
                        INSERT OR IGNORE INTO power_full.POWER
                                SELECT * FROM main.POWER
                    ''')

                    conn.commit()
                    print(&quot;Data copied across files&quot;)
        
            except Exception as e:#
                print(&quot;Failed to copy data across files&quot;)
    
            elapsed_time = time.time() - now
            remaining_time = 600 - elapsed_time
            if remaining_time &gt; 0:
                print (f&quot;Sleeping for&quot;, [remaining_time])
                time.sleep(remaining_time)
        
        n += 1  
</code></pre>
<p>And here's the current gathering file data code:</p>
<pre><code>def run_current_data_server(host='localhost', port=9876):
    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    server.bind((host, port))
    server.listen()
    print(f&quot;[Server] Listening on {host}:{port} for current data...&quot;)

    while True:
        conn, addr = server.accept()
        with conn:
            data_bytes = b''
            while True:
                chunk = conn.recv(1024)
                if not chunk:
                    break
                data_bytes += chunk
            try:
                data = json.loads(data_bytes.decode('utf-8'))
                save_to_db(data)
                print(f&quot;[Server] Received and saved current data: {data}&quot;)
            except Exception as e:
                print(f&quot;[Server] Error processing data: {e}&quot;)

def save_to_db(data):
    with sqlite3.connect('current_full.db') as conn:
        cursor = conn.cursor()
        cursor.execute(&quot;&quot;&quot;
            CREATE TABLE IF NOT EXISTS CURRENT (
                TIME INT PRIMARY KEY,
                CURRY1 INT NOT NULL,
                CURRY2 INT NOT NULL,
                CURRY3 INT NOT NULL,
                CURRYTOT INT NOT NULL
            );
        &quot;&quot;&quot;)
        cursor.execute(&quot;INSERT OR IGNORE INTO CURRENT (TIME, CURRY1, CURRY2, CURRY3, CURRYTOT)     VALUES (?, ?, ?, ?, ?)&quot;,
                       (data['TIME'], data['CURRY1'], data['CURRY2'], data['CURRY3'],     data['CURRYTOT']))
        conn.commit()

if __name__ == '__main__':
    run_current_data_server()
</code></pre>
<p>If you have any ideas on how I could get around the fighting over com port, please share them!</p>
",0,0,0,2025-07-02T15:15:23+00:00,0,50,False
79690604,29055850,,sqlite,I am using panda and tksheet to create a ledger table which I want to be able to edit by clicking on a cell. Cells don&#39;t respond to mouse,"<p>my code is as follows -:</p>
<pre class=""lang-py prettyprint-override""><code>import sqlite3
import pandas as pd
import tkinter as tk
from tksheet import Sheet
from tkinter import messagebox

# Connect to SQLite database
conn = sqlite3.connect(&quot;your_ledger.db&quot;)
cursor = conn.cursor()

# Load joined data from Journal, Ledger, Fund, and Chart
def load_data():
    query = &quot;&quot;&quot;
        SELECT
            Ledger.Id AS LedgerId,
            Journal.Id AS JournalId,
            Journal.User_date,
            Journal.Description,
            Journal.Posted,
            Ledger.Amount,
            Fund.Id AS Fund_Id,
            Fund.Name AS Fund_Name,
            Chart.Id AS Account_Id,
            Chart.Name AS Account_Name
        FROM Ledger
        JOIN Journal ON Ledger.Tran_id = Journal.Id
        LEFT JOIN Fund ON Ledger.Fund_Id = Fund.Id
        LEFT JOIN Chart ON Ledger.Account_Id = Chart.Id
        WHERE Journal.Deleted = 0
    &quot;&quot;&quot;
    df = pd.read_sql_query(query, conn)
    return df

# Save updated data from sheet to DB in one transaction
def save_data():
    try:
        new_data = pd.DataFrame(sheet.get_sheet_data(), columns=columns)

        with conn:  # Transaction context
            for _, row in new_data.iterrows():
                # Update Journal
                conn.execute(&quot;&quot;&quot;
                    UPDATE Journal
                    SET User_date = ?, Description = ?, Posted = ?
                    WHERE Id = ?
                &quot;&quot;&quot;, (row['User_date'], row['Description'], int(row['Posted']), row['JournalId']))
                
                # Update Ledger
                conn.execute(&quot;&quot;&quot;
                    UPDATE Ledger
                    SET Amount = ?, Fund_Id = ?, Account_Id = ?
                    WHERE Id = ?
                &quot;&quot;&quot;, (row['Amount'], row['Fund_Id'], row['Account_Id'], row['LedgerId']))

        messagebox.showinfo(&quot;Success&quot;, &quot;Changes saved successfully.&quot;)
    except Exception as e:
        messagebox.showerror(&quot;Error&quot;, str(e))

# GUI setup
root = tk.Tk()
root.title(&quot;Ledger Entry Form&quot;)

df = load_data()
columns = df.columns.tolist()

sheet = Sheet(root,
              data=df.values.tolist(),
              headers=columns)
sheet.pack(expand=True, fill=&quot;both&quot;)

save_button = tk.Button(root, text=&quot;Save Changes&quot;, command=save_data)
save_button.pack(pady=10)

root.mainloop()
</code></pre>
<p>As you can see I am loading the database query into a datafile and then into a sheet. When the button is pressed, the sheet is converted into a datafile and then   the database is updated with its contents</p>
<p>The table does not respond to mouse clicks at all. I tried asking copilot, which suggested I set editable to True in the sheet constructor. This didn't work. It then suggested I enable the bindings in the sheet shown in my code. This didn't work either. I am at a loss, and don't understand. Could you help me?</p>
<hr />
<p>This code seems to work, I don't know why though. It's not the editable=True arg ; Still not saving when I press the button:-</p>
<pre><code>import sqlite3
import pandas as pd
import tkinter as tk
from tksheet import Sheet
from tkinter import messagebox

# Connect to SQLite database
conn = sqlite3.connect(&quot;your_ledger.db&quot;)
cursor = conn.cursor()

# Load joined data from Journal, Ledger, Fund, and Chart
def load_data():
    query = &quot;&quot;&quot;
        SELECT
            Ledger.Id AS LedgerId,
            Journal.Id AS JournalId,
            Journal.User_date,
            Journal.Description,
            Journal.Posted,
            Ledger.Amount,
            Fund.Id AS Fund_Id,
            Fund.Name AS Fund_Name,
            Chart.Id AS Account_Id,
            Chart.Name AS Account_Name
        FROM Ledger
        JOIN Journal ON Ledger.Tran_id = Journal.Id
        LEFT JOIN Fund ON Ledger.Fund_Id = Fund.Id
        LEFT JOIN Chart ON Ledger.Account_Id = Chart.Id
        WHERE Journal.Deleted = 0
    &quot;&quot;&quot;
    df = pd.read_sql_query(query, conn)
    return df

# Save updated data from sheet to DB in one transaction
def save_data():
    try:
        new_data = pd.DataFrame(sheet.get_sheet_data(), columns=columns)

        with conn:  # Transaction context
            for _, row in new_data.iterrows():
                # Update Journal
                conn.execute(&quot;&quot;&quot;
                    UPDATE Journal
                    SET User_date = ?, Description = ?, Posted = ?
                    WHERE Id = ?
                &quot;&quot;&quot;, (row['User_date'], row['Description'], int(row['Posted']), row['JournalId']))
                
                # Update Ledger
                conn.execute(&quot;&quot;&quot;
                    UPDATE Ledger
                    SET Amount = ?, Fund_Id = ?, Account_Id = ?
                    WHERE Id = ?
                &quot;&quot;&quot;, (row['Amount'], row['Fund_Id'], row['Account_Id'], row['LedgerId']))

        messagebox.showinfo(&quot;Success&quot;, &quot;Changes saved successfully.&quot;)
    except Exception as e:
        messagebox.showerror(&quot;Error&quot;, str(e))

# GUI setup
root = tk.Tk()
root.title(&quot;Ledger Entry Form&quot;)

df = load_data()
columns = df.columns.tolist()

sheet = Sheet(root,
              data=df.values.tolist(),
              headers=columns,
              editable=True)  # Ensure cell editing is enabled
sheet.enable_bindings((&quot;single_select&quot;,  # Allow single cell selection
                       &quot;edit_cell&quot;  # Enable cell editing
                       ))  # Right-click popup menu
sheet.pack(expand=True, fill=&quot;both&quot;)

save_button = tk.Button(root, text=&quot;Save Changes&quot;, command=save_data)
save_button.pack(pady=10)

root.mainloop()
</code></pre>
",-1,1,2,2025-07-04T19:16:58+00:00,1,124,False
79692045,30978356,,sqlite,String as x-axis in Grafana xy plot,"<p>My goal is to plot a xy scatter plot in Grafana. X-axis is 'setting', and the potential values are 'off', 'idle', 'standby', and 'on'. On the y-axis I have 'performance' and the data type is real. My issue is that xy scatter plot only supports numeric values and not strings on the axis, <a href=""https://grafana.com/docs/grafana/latest/panels-visualizations/visualizations/xy-chart/"" rel=""nofollow noreferrer"">https://grafana.com/docs/grafana/latest/panels-visualizations/visualizations/xy-chart/</a>.</p>
<p>My data source is sqlite db, and the data is currently stored as</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>setting</th>
<th>performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>'idle'</td>
<td>11.4</td>
</tr>
<tr>
<td>2</td>
<td>'off'</td>
<td>2.5</td>
</tr>
<tr>
<td>3</td>
<td>'idle'</td>
<td>10.6</td>
</tr>
</tbody>
</table></div>
<p>I've tried to transform 'setting' to enum in Grafana (<a href=""https://grafana.com/docs/grafana/latest/panels-visualizations/query-transform-data/transform-data/"" rel=""nofollow noreferrer"">https://grafana.com/docs/grafana/latest/panels-visualizations/query-transform-data/transform-data/</a>) but it was not possible to use enum on x-axis.</p>
<p>Is there any other way to transform the string to numerical value in Grafana?
Or is there a better way to store the data in the database?</p>
",1,1,0,2025-07-06T19:35:59+00:00,1,103,True
79695093,14041392,"Quezon Province, Philippines",sqlite,How to properly modify the data model and update sqlite db in .NET 9 in Blazor web app?,"<p>My issue is somewhat related to these question threads (but not applicable due to being outdated and with a different overall context):</p>
<ul>
<li><p><a href=""https://stackoverflow.com/q/51158189/14041392"">Error CS1061 “...Does Not Contain Definition and No Extension Method...accepting a first argument of type ” could be found</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/q/63876058/14041392"">C# Error CS1061 'object' does not contain a definition for 'parameters'</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/q/2839946/14041392"">Getting error: CS1061</a></p>
</li>
</ul>
<p>In VS Code using C# and a Blazor web app (on .NET 9.0), more specifically, my error messages are as follows.</p>
<p>My thought process is by right-clicking the <code>{MyBlazorWebAppName}</code> project file (<code>{MyBlazorWebAppName}.csproj</code>) and selecting <code>Open in Integrated Terminal</code>, as I believe I need to update the database with my changes to the data model.</p>
<p>However, after executing the command:</p>
<pre><code>{folder directory}\MyBlazorWebAppName\MyBlazorWebAppName&gt; dotnet ef database update
</code></pre>
<p>I got this build failed message:</p>
<blockquote>
<p>Build started...<br />
Build failed. Use dotnet build to see the errors.</p>
</blockquote>
<p>Moreover, by executing <code>dotnet build</code> or <kbd>Ctrl-Shift+P</kbd>  .NET: Build, I got this error message:</p>
<blockquote>
<p>error CS1061: 'Admin' does not contain a definition for 'FullName' and no accessible extension method 'FullName' accepting a first argument of type 'Admin' could be found (are you missing a using directive or an assembly reference?)</p>
</blockquote>
<p>What have I done before encountering those errors:</p>
<p>In my <code>Models/Admin.cs</code> - old data model:</p>
<pre class=""lang-cs prettyprint-override""><code>using System.ComponentModel.DataAnnotations;

namespace MyBlazorWebAppName.Models;

public class Admin
{
    [Key]
    public string? UserID { get; set; }
    public string? Username { get; set; }
    public string? FullName { get; set; }
    public string? EmailAdd { get; set; }
    public string? RoleType { get; set; }
}
</code></pre>
<p>With changes applied:</p>
<pre class=""lang-cs prettyprint-override""><code>using System.ComponentModel.DataAnnotations;

namespace MyBlazorWebAppName.Models;

public class Admin
{
    [Key]
    public string? UserID { get; set; }
    public string? Username { get; set; }
    public string? GivenName { get; set; }
    public string? MiddleName { get; set; }
    public string? FamilyName { get; set; }
    public string? EmailAdd { get; set; }
    public string? RoleType { get; set; }
}
</code></pre>
<p>As you may have noticed, I replaced <code>FullName</code> with <code>GivenName</code>, <code>MiddleName</code>, and <code>FamilyName</code>.</p>
<p>I'm uncertain how to apply my data model changes to my SQLite database.</p>
<p>Note: I've followed and completed the tutorial for this particular section: <a href=""https://learn.microsoft.com/en-us/aspnet/core/blazor/tutorials/movie-database-app/part-2?view=aspnetcore-9.0&amp;pivots=vsc"" rel=""nofollow noreferrer"">Build a Blazor movie database app (Part 2 - Add and scaffold a model)</a></p>
",1,1,0,2025-07-09T04:52:52+00:00,2,135,True
79695857,1713450,,sqlite,"How to join two tables, concat_group, then group by count of that concat_group value?","<p>I have data that looks like this:</p>
<p><code>serialized_book</code>:</p>
<pre><code>id | author | title
---+--------+---------------------------
0  | foo    | i am foo.
1  | bar    | the derivabloviating . . .
...
</code></pre>
<p><code>chapter_pub_date</code>:</p>
<pre><code>id | chapter | year | month | day | book_id
---+---------+------+-------+-----+---------
0  | 1       | 2024 | 12    | 10  | 0
1  | 2       | 2025 | 1     | 5   | 0
...
27 | 1       | 2024 | 12    | 10  | 1
...
</code></pre>
<p>I want a query that yields unique authors-who-published-that-month (I know the data isn't fully normalized since I don't have a separate table for &quot;author&quot;).</p>
<p>So something like this:</p>
<pre><code>month   | unique_author_count
--------+--------------------
2024-12 | 2
2025-1  | 1
...
</code></pre>
<p>The best I've got so far is</p>
<pre class=""lang-sql prettyprint-override""><code>select 
    w.author, concat(p.year, &quot;-&quot;, p.month) as yr_mo 
from 
    serialized_book w
join 
    chapter_pub_date p on w.id = p.book_id
group by 
    w.author, yr_mo
order by 
    yr_mo desc
</code></pre>
<p>This returns a list of every unique <code>(author, year-month-they-published-something)</code>:</p>
<pre><code>author | yr_mo
-------+----------
foo    | 2024-12
foo    | 2025-1
bar    | 2024-12
...
</code></pre>
<p>But I'm stuck going from this to</p>
<pre><code>yr_mo   | count_unique_author_per_yr_mo
--------+-------------------------------
2024-12 | 2
...
</code></pre>
<p>I think it's either going to require some kind of <code>partition</code> or a sub-query, but when I look at examples in the docs and on SO, I can't find any that feature joins, which complicates the sub-query solution.</p>
",2,2,0,2025-07-09T15:04:00+00:00,1,60,True
79697269,6141238,,sqlite,"When reading a database table with polars, how do I avoid a SchemaError?","<p>I have a large <code>table_to_load</code> in a database file <code>my_database.db</code> that I am trying to read into a Python program as a <code>polars</code> DataFrame.  Here is the code that does the reading:</p>
<pre><code>import polars as pl

conn = sqlite3.connect('my_database.db')
df = pl.read_database(connection=conn, query='SELECT * FROM table_to_load', infer_schema_length=None)
conn.close()
</code></pre>
<p>When I run this code, <code>pl.read_database</code> throws the error, &quot;polars.exceptions.SchemaError: failed to determine supertype of i64 and binary.&quot;  The traceback is at the end of the post.  I have several closely related questions about this:</p>
<ol>
<li>Each column of <code>table_to_load</code> verifiably already has one of the three <a href=""https://docs.pola.rs/api/python/dev/reference/api/polars.read_database.html"" rel=""nofollow noreferrer"">sqlite3 datatypes</a> <code>integer</code>, <code>real</code>, and <code>text</code>.  Can I instruct <code>polars</code> to just use a reasonable analog of each datatype?  For example, <code>i64</code> seems like a reasonable analog of <code>integer</code>.</li>
<li>Why does <code>polars</code> struggle to determine the datatype if it is reading the whole column as instructed by the parameter setting <code>infer_schema_length=None</code>, and is there any way to resolve this?  (It <a href=""https://docs.pola.rs/api/python/dev/reference/api/polars.read_database.html"" rel=""nofollow noreferrer"">looks</a> like there is also a <code>schema_overrides</code> parameter, but this would be challenging to use in my application because my table columns are not static.)</li>
<li>In the traceback, why does <code>polars</code> not reveal the column or columns that are generating the SchemaError, and is there a way to request that it do so?  In my case, this would be helpful because <code>table_to_load</code> has hundreds of columns.</li>
</ol>
<hr />
<p>The traceback of the SchemaError:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\user0\AppData\Local\Programs\Python\Python312\test_module.py&quot;, line 115, in &lt;module&gt;
    main_function()
  File &quot;c:\Users\user0\AppData\Local\Programs\Python\Python312\test_module.py&quot;, line 54, in main_function
    df = pl.read_database(connection=conn, query='SELECT * FROM table_to_load', infer_schema_length=None)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\io\database\functions.py&quot;, line 251, in read_database  
    ).to_polars(
      ^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\io\database\_executor.py&quot;, line 543, in to_polars      
    frame = frame_init(
            ^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\io\database\_executor.py&quot;, line 300, in _from_rows     
    return frames if iter_batches else next(frames)  # type: ignore[arg-type]
                                       ^^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\io\database\_executor.py&quot;, line 283, in &lt;genexpr&gt;
    DataFrame(
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\dataframe\frame.py&quot;, line 377, in __init__
    self._df = sequence_to_pydf(
               ^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\_utils\construction\dataframe.py&quot;, line 461, in sequence_to_pydf
    return _sequence_to_pydf_dispatcher(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\functools.py&quot;, line 909, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\_utils\construction\dataframe.py&quot;, line 674, in _sequence_of_tuple_to_pydf
    return _sequence_of_sequence_to_pydf(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user0\AppData\Local\Programs\Python\Python312\Lib\site-packages\polars\_utils\construction\dataframe.py&quot;, line 590, in _sequence_of_sequence_to_pydf
    pydf = PyDataFrame.from_rows(
           ^^^^^^^^^^^^^^^^^^^^^^
polars.exceptions.SchemaError: failed to determine supertype of i64 and binary


  [1]: https://www.sqlite.org/datatype3.html
</code></pre>
",0,0,0,2025-07-10T15:10:29+00:00,2,676,True
79698938,1100221,,sqlite,"SQL Query to sum 2 columns from 1 table, and subtract result from a column in a different table","<p>I have 2 tables (not related, no primary/foreign keys, no nulls in any column)</p>
<p>Table: MonthlyEarnings</p>
<pre><code>| Month | Year | DebitAmount | CreditAmount |
| ----- | ---- | ----------- | ------------ |
| July  | 2025 | 0           | 250.45       |
| July  | 2025 | 0           | 468.35       |
| July  | 2025 | 110         | 0            |
| July  | 2025 | 0           | 130.87       |
| July  | 2025 | 28          | 0            |
</code></pre>
<p>Table: StartOfMonthBalance</p>
<pre><code>| Month | Year | BalanceAmount |
| ----- | ---- | ------------- |
| July  | 2025 | 35608.65      |
</code></pre>
<p>I want to write a query in SQL that returns the starting balance from StartOfMonthBalance, total expenses, total earnings from MonthlyEarnings and the ending balance calculated as starting balance + total earnings - total expenses.</p>
<p>The query I have in MS Access is as below:</p>
<pre><code>SELECT
    SUM(M.DebitAmount) AS TotalExpenses, 
    SUM(M.CreditAmount) AS TotalEarnings, 
    (SELECT S.BalanceAmount FROM StartOfMonthBalance S WHERE S.Month = &quot;July&quot; AND S.Year = 2025) AS StartingBalance, 
    (StartingBalance + TotalEarnings - TotalExpenses) AS EndingBalance 
FROM MonthlyEarnings M
WHERE M.Month = &quot;July&quot; AND M.Year = 2025
</code></pre>
<p>How can I write a similar query for SQLite?</p>
<p>When I try, this is the error I get:</p>
<blockquote>
<p>Execution finished with errors. Result: no such column: StartingBalance At line 1: SELECT SUM(M.DebitAmount) AS TotalExpenses, SUM(M.CreditAmount) AS TotalEarnings, (SELECT S.BalanceAmount FROM StartOfMonthBalance S WHERE S.Month = &quot;July&quot; AND S.Year = 2025) AS StartingBalance, (StartingBalance + TotalEarnings - TotalExpenses) AS EndingBalance FROM MonthlyEarnings M WHERE M.Month = &quot;July&quot; AND M.Year = 2025</p>
</blockquote>
",-1,0,1,2025-07-12T01:16:43+00:00,0,37,False
79699458,23141860,,sqlite,Why is my SQLite database not creating a column for password?,"<p>I am building an app in expo as a personal project and I am trying to create a users table that will store names, emails, usernames, and passwords. The login screen and database run when the app starts and I am able to type in the app to input a password but the login always fails. When I check the error log in my VS Code terminal, I see an error saying there is not a password column.</p>
<p>This is the full error message:</p>
<blockquote>
<p>ERROR  Registration error: [Error: Calling the 'prepareAsync' function has failed<br />
→ Caused by: Error code 1: table users has no column named password]</p>
</blockquote>
<p>Here is my current code for the <code>initDB</code> function for reference:</p>
<pre><code>export async function initDB() {
    try {
        db = await SQLite.openDatabaseAsync('app.db');

        await db.execAsync(`
            DROP TABLE IF EXISTS users;
                CREATE TABLE users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                email TEXT UNIQUE,
                password TEXT NOT NULL
            );
        `);

        console.log('Database initialized and table created');
    } catch (error) {
        console.error('Error initializing database:', error);
    }
}
</code></pre>
<p>I have tried many workarounds for this, including deleting the table and rerunning <code>initDB</code> when the code launches, just clearing the app's workspace, and my current work around is using</p>
<pre><code>DROP TABLE IF EXISTS users;
</code></pre>
<p>to start the <code>initDB</code> SQL statement. However, all these workarounds have resulted in the same error.</p>
<p>I am pretty new to expo and react native as a whole, so I would appreciate any insight into this problem.</p>
",0,0,0,2025-07-12T17:24:52+00:00,1,76,False
79701324,1100221,,sqlite,"Could not load file or assembly &#39;Microsoft.Data.Sqlite, Version=9.0.7.0&#39;","<p>In Visual Studio 2022, I have an installer project. I went by what was suggested here:
<a href=""https://stackoverflow.com/questions/46915404/the-type-initializer-for-sqlite-sqliteconnection-threw-an-exception/56636305#56636305"">The type initializer for &#39;SQLite.SQLiteConnection&#39; threw an exception</a></p>
<p>When I run the project in Visual Studio, there are no issues.
When I install the application on the same machine on which the project was developed, and run the .exe, I am getting this error:</p>
<blockquote>
<p>Could not load file or assembly 'Microsoft.Data.Sqlite, Version=9.0.7.0, Culture=neutral' or one of its dependencies. The system cannot find the file specified.</p>
</blockquote>
<p>The database file exists in the application startup folder.</p>
<pre><code>using (SqliteConnection connection = 
       new SqliteConnection(&quot;Data Source=&quot; + Application.StartupPath + @&quot;\savings.db&quot;))
</code></pre>
<p>In my installer project, I have:</p>
<p><a href=""https://i.sstatic.net/82jRC29T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82jRC29T.png"" alt=""enter image description here"" /></a></p>
<p>After installation, this is what is in <code>C:\Program Files\</code>:</p>
<p><a href=""https://i.sstatic.net/TpNUQSKJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpNUQSKJ.png"" alt=""Installed folder structure"" /></a></p>
<p>RESOLVED: I included all the dependencies into the target folder and it is working now:
<a href=""https://i.sstatic.net/cW2jLKLg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cW2jLKLg.png"" alt=""Updated target folder"" /></a></p>
",0,0,0,2025-07-14T20:16:16+00:00,0,108,False
79703897,12129383,,sqlite,Electron.js form input fields freeze for a few seconds after inserting data,"<p>I'm building a desktop app using Electron.js with a Bootstrap-based form that includes input fields and dropdowns.</p>
<p>Everything works fine initially, but after I insert data once or while filling the form, all input fields freeze for 2–3 seconds. Dropdowns still work during this time, but the text input fields stop accepting input temporarily.</p>
<p>What's happening:
After submitting the form once or sometimes while typing,</p>
<p>Input fields become unresponsive for 2–3 seconds,</p>
<p>Then everything works again.</p>
<p>ALl forms in application got freeze</p>
<p>No setTimeout or sleep functions that could be delaying the UI.</p>
",1,1,0,2025-07-16T18:50:20+00:00,0,66,False
79707043,30410699,,sqlite,How to make concurrent writes in SQLite with FastAPI + SQLAlchemy without &quot;database is locked&quot; error?,"<p>What Happens
I trigger <code>/session_a</code> and <code>/session_b</code> almost simultaneously (e.g. with <code>Postman</code> or <code>curl</code>).</p>
<p><code>/session_b</code> usually succeeds.</p>
<p><code>/session_a</code> fails on <code>db.commit()</code> with</p>
<pre class=""lang-none prettyprint-override""><code>sqlite3.OperationalError: database is locked.
</code></pre>
<p>I found that the issue is caused by <em>session A</em> reaching commit earlier than <em>session B</em>, but session B is already holding the write lock — which means A will inevitably fail with a database is locked error.</p>
<p>Reproducible Code</p>
<pre class=""lang-py prettyprint-override""><code>import asyncio
import sqlite3
import threading
import time
import uuid
from loguru import logger
 
from fastapi import FastAPI, Depends
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
 
DATABASE_URL = &quot;sqlite:///./test_locked.db&quot;
 
engine = create_engine(
    DATABASE_URL, connect_args={&quot;check_same_thread&quot;: False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=True, bind=engine)
Base = declarative_base()
 
app = FastAPI()
 
class Item(Base):
    __tablename__ = &quot;items&quot;
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
 
Base.metadata.create_all(bind=engine)
 
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
 
@app.post(&quot;/session_a&quot;)
async def session_a(db: Session = Depends(get_db)):
    # 会话A启动事务
    logger.info(&quot;A start&quot;)
    uuid_str = str(uuid.uuid4())
    item = Item(name=f&quot;session_a{uuid_str}&quot;)
    db.add(item)
    await asyncio.sleep(0.5)  # 模拟长事务，占用锁
    logger.info(f&quot;A commit {uuid_str}&quot;)
    db.commit()
    return {&quot;status&quot;: &quot;A committed&quot;}
 
@app.post(&quot;/session_b&quot;)
async def session_b(db: Session = Depends(get_db)):
    logger.info(&quot;B start&quot;)
    # 会话B尽快获取锁
    await asyncio.sleep(0.1)
    uuid_str = str(uuid.uuid4())
    item = Item(name=f&quot;session_b{uuid_str}&quot;)
    db.add(item)
    db.flush()
    logger.info(f&quot;B flush {uuid_str}&quot;)
    await asyncio.sleep(1)
    db.commit()
    logger.info(f&quot;B commit {uuid_str}&quot;)
    return {&quot;status&quot;: &quot;B committed&quot;}
 
if __name__ == &quot;__main__&quot;:
    import uvicorn
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)
</code></pre>
<p>Request code</p>
<pre class=""lang-py prettyprint-override""><code>import requests
import threading
import time

def test_session_a():
    res = requests.post(&quot;http://127.0.0.1:8000/session_a&quot;, timeout=10)
    print(f&quot;session_a: {res.status_code}&quot;)

def test_session_b():
    res = requests.post(&quot;http://127.0.0.1:8000/session_b&quot;, timeout=10)
    print(f&quot;session_b: {res.status_code}&quot;)


th1 = threading.Thread(target=test_session_a)
th2 = threading.Thread(target=test_session_b)
 
th1.start()
time.sleep(0.1)
th2.start()
 
th1.join()
th2.join()
</code></pre>
<p>I think it's illogical that <code>db.commit()</code> is synchronously blocking. Using <code>StaticPool</code> seems to solve the issue, but it's unsafe. At the same time, I'm afraid to set <code>autoflush=False</code> in the project, as it might cause some endpoints to crash. What should I do?</p>
",1,1,0,2025-07-19T07:45:53+00:00,1,237,True
79709014,25493371,,sqlite,Correct design for SQL table in SQLite3: how to optimize substring search?,"<p>I am using <code>SQLite3</code> in C++ where I need to store text data. There are 7 columns in the table. I need to filter data by 7 columns. I need to check for equality of 6 columns to a specific value, and I need to search for a substring by the last column. I use a regular index for the first 6 columns and the search (SELECT query) works quite fast. However, for the 7th column, I use <code>LIKE %value%</code> to search for a substring. However, <code>LIKE</code> does not use indexes. How can I efficiently implement searching by this field? Note that <code>WHERE</code> filtering by fields can contain all 7 columns, or just one (which uses LIKE).</p>
<p><strong>UPDATE</strong>
I found out that full text search works well in such cases, but full text search does not work with a regular table, as far as I know. What solution do you recommend?</p>
<p>My texts are not long (up to 100 characters) and there are no spaces. Example: ThisIsMyExampleString. In that case I'm not sure that full text search will work here</p>
",0,0,0,2025-07-21T11:49:00+00:00,0,172,False
79709204,8473360,,sqlite,How can I add a Root CA certificate to Google Chrome’s local SSL store via script on Windows?,"<p>Since a recent version, Google Chrome maintains its own “local store” for SSL root certificates, allowing users to trust certificates without installing them at the Windows OS level. You can view this store at:
&quot;chrome://certificate-manager/localcerts&quot;</p>
<p>User‑added certificates appear under “User Certificates” at:
&quot;chrome://certificate-manager/localcerts/usercerts&quot;</p>
<p>I’d like to automate adding a certificate to Chrome’s local store via command line using a .bat script.</p>
<p>I located the file:
&lt; Chrome Profile Directory &gt;\Default\ServerCertificate</p>
<p>It’s an SQLite3 database that stores certificates in DER format.
Using the sqlite3 command, I inserted my DER‑encoded certificate into the ServerCertificate table.
After restarting Chrome, I see that one certificate appears under Local Store (chrome://certificate-manager/localcerts), but it does not show up under User Certificates (&quot;chrome://certificate-manager/localcerts/usercerts&quot;).</p>
<p>And of course, the certificate does not work, I must be missing a file to edit.</p>
<p>Which file(s) or registry entries does Chrome update when you manually import a Root CA into its local store? In other words, what else besides the ServerCertificate SQLite database needs to be modified so that a batch script can replicate the manual Import.</p>
",0,0,0,2025-07-21T14:22:30+00:00,0,239,False
79709330,24003111,,sqlite,viewing inside database in docker volume,"<p>I wrote a program with nodejs and I setup docker for it. The programs database is stored in a volume here is the code from compose.yaml</p>
<pre><code>
services:
  server:
    build:
      context: .
    environment:
      NODE_ENV: production
      EMAIL_USER: ${EMAIL_USER}
      EMAIL_PASSWORD: ${EMAIL_PASSWORD}
      USERNAME1: ${USERNAME1}
      USERNAME2: ${USERNAME2}
      USERPASSWORD: ${USERPASSWORD}
      EMAIL_RECIPIENT: ${EMAIL_RECIPIENT}
    ports:
      - 3000:3000
    volumes:
      - db-data:/usr/src/app/data

volumes:
  db-data:

</code></pre>
<p>how do I access the database in the container. I cant reach it with WSL console. WSL uses Alpine Linux and the database is sqlite.</p>
",1,1,0,2025-07-21T16:04:32+00:00,2,160,False
79713553,2946144,,sqlite,SQLite is missing as DataSource in Visual Studio 2019,"<p>I used to generate and print reports in Visual Studio 2015 using SQLite databases. Creating reports with RDLC was very easy — I could design them and print directly on A4 paper. ORM use was like blessing. However, after Visual Studio 2015, no newer edition of Visual Studio provides an option to add SQLite as a data source. As a result, I can no longer use ORM and insert SQLite tables into the report designer or use them in expression textboxes.</p>
<p>In Visual Studio 2015, I could easily add SQLite via &quot;Add Data Source&quot;, but that option is now missing. Even .xsd files don't seem to support SQLite anymore. What should I do now? <strong>How to Add SQLite as DataSource in Visual Studio 2019 -2022.</strong></p>
<p><a href=""https://i.sstatic.net/M6zKsF5p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6zKsF5p.png"" alt=""No support for adding SQLite database in Datasource in Visual Studio 2017-2022"" /></a></p>
<p>Visual Studio 2015 allowed adding SQLite easily through the “Add Data Source” option, and it worked perfectly with .xsd datasets. But now, even .xsd doesn’t support SQLite properly.
<a href=""https://i.sstatic.net/Ucs0bsED.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ucs0bsED.jpg"" alt=""Removed support for SQLite in Dataset (.xsd) in Visual Studio 2017-2022"" /></a></p>
<p>I even tried installing the <strong>SQLite Data Provider</strong> bundle to bring back support, but it didn’t help. In any version of Visual Studio after 2015, I still can't add SQLite as a data source in the RDLC report designer. This has made it very difficult to work with reports, since I can’t insert SQLite tables or use them in the expression textboxes like I used to. I <strong>can't use ORM</strong> either.
<a href=""https://i.sstatic.net/LhTAiHud.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LhTAiHud.jpg"" alt=""No support for SQLite in Report deisgner (.rdlc files) in Visual Studio 2017-2022"" /></a></p>
<p>To make things worse, Visual Studio 2015 is also nearing its end of extended support cycle. I'm not sure what to do next.</p>
",0,0,0,2025-07-24T14:41:28+00:00,0,51,False
79715574,22809135,,sqlite,On Delete set null not behaving as expected with sqlite,"<p>I'm using sqlite and I have a table
<code>Sessions (sessionId INTEGER, ...)</code>
And a table
<code>Orders (orderId INTEGER, ...,sessionID INTEGER, Foreign key sessionID references Session on delete set null)</code>
But when I delete a session that has orders I get an error that says that I'm violationg a foreign key constarint.
Did this happen to anyone else ?</p>
",1,1,0,2025-07-26T09:48:24+00:00,1,30,True
79720513,2066459,"Whitehouse Station, NJ",sqlite,Having clause removing rows. Is there a workaround?,"<p>Sqlites having clause is not acting as I expect. if I query 2 columns and use one in the group by, I don't get all the rows. I can get around the problem putting the having in a in clause, but should I have to? Code below.</p>
<pre><code>create table test (
one, two );

insert into test values('1','A');
insert into test values('2','B');
insert into test values('3','B');

select one,two
from test
group by two
having count(*) &gt; 1;

select 'next result';
select one,two
from test
where two in (
select two from test
group by two
having count(*) &gt; 1);

drop table test;


</code></pre>
<p>output</p>
<pre><code>2|B
next result
2|B
3|B
</code></pre>
",1,1,0,2025-07-30T20:46:42+00:00,2,65,True
79720945,9852762,,sqlite,Proper way to locate the position of a single newly inserted row based on order by clause immediately after insert?,"<p>After inserting a single row of data into a table (always a single row), I'd like to return to the application code the relative position of that row based on an order by clause. Thus, the application code sends SQLite a row to insert and SQLite returns its position.</p>
<p>Using <code>last_insert_rowid()</code> and the window function <code>row_number()</code> appears to work but that's just my hackish guess. Are there better, more efficient methods?</p>
<p>If I copied this correctly, this example runs in the SQLite Fiddle.</p>
<pre><code>create table data (key,value);

insert into data 
values (5, 'e'), (3, 'c'), (8, 'h');

select rowid, * from data;

+-------+-----+-------+
| rowid | key | value |
+-------+-----+-------+
| 1     | 5   | e     |
| 2     | 3   | c     |
| 3     | 8   | h     |
+-------+-----+-------+

insert into data values (1, 'a');

select * from data order by value asc;

+-----+-------+
| key | value |
+-----+-------+
| 1   | a     |
| 3   | c     |
| 5   | e     |
| 8   | h     |
+-----+-------+

select d.*
from 
    (select rowid as row_id, row_number() over (order by value asc) pos, * 
     from data) d
where d.row_id = last_insert_rowid();

+--------+-----+-----+-------+
| row_id | pos | key | value |
+--------+-----+-----+-------+
| 4      | 1   | 1   | a     |
+--------+-----+-----+-------+

insert into data values (2,'b');

select * from data order by value desc;

+-----+-------+
| key | value |
+-----+-------+
| 8   | h     |
| 5   | e     |
| 3   | c     |
| 2   | b     |
| 1   | a     |
+-----+-------+

select d.*
from 
    (select rowid as row_id, row_number() over (order by value desc) pos, * 
     from data) d
where d.row_id = last_insert_rowid();

+--------+-----+-----+-------+
| row_id | pos | key | value |
+--------+-----+-----+-------+
| 5      | 4   | 2   | b     |
+--------+-----+-----+-------+
</code></pre>
",2,3,1,2025-07-31T07:45:28+00:00,2,114,True
79722041,,,sqlite,how i can populate struct type only in returning joined column from sql,"<p>I have a table named tasks where have relationship one-many to another table (projects), i want to query tasks with projects but only return selected column from the projects table. how i can do that in Go</p>
<p>here's task model:</p>
<pre><code>type Task struct {
    Id        int
    Name      string
    Status    string
    Priority  string
    ProjectId int
    Project
    CreatedAt time.Time
}
</code></pre>
<p>here's project model :</p>
<pre><code>type Project struct {
    Id        int
    Name      string
    About     string
    Features  string
    CreatedAt time.Time
}

</code></pre>
<p>here's get tasks example :</p>
<pre><code>

func (ts *TaskStore) GetTasks(filter FilterTask, sort Sort) ([]Task, error) {

    var tasks []Task

    query := `SELECT t.id,t.name,t.priority,t.status,project_id,t.created_at,p.name FROM tasks t  LEFT JOIN projects p ON p.id = t.project_id WHERE t.priority LIKE CONCAT(&quot;%&quot;,?,&quot;%&quot;)  AND t.status LIKE CONCAT(&quot;%&quot;,?,&quot;%&quot;)  ORDER BY t.created_at ` + sort.String() + `
    `

    res, err := ts.Db.Query(query, filter.Priority, filter.Status)
    if err != nil {
        return tasks, fmt.Errorf(&quot;unable to get values: %W&quot;, err)
    }

    defer res.Close()

    for res.Next() {
        var task Task

        err := res.Scan(
            &amp;task.Id,
            &amp;task.Name,
            &amp;task.Priority,
            &amp;task.Status,
            &amp;task.ProjectId,
            &amp;task.CreatedAt,
        )
        if err != nil {
            return tasks, err
        }

        tasks = append(tasks, task)
    }

    return tasks, nil
}

</code></pre>
<p>from the query i only select name from projects but what if i later want to selected
another column or some tasks dont have a project then field project
in tasks will return zero-value of the project</p>
",1,1,0,2025-08-01T03:40:35+00:00,1,112,True
79723902,5841406,"Bath, UK",sqlite,"Accessing sqlite fts5 from a Windows client, in particular for backup purposes","<p>I have a large fts5 virtual table (currently about 90GB).</p>
<p>Because the sqlite ODBC driver, which I use (version 3.43.2) as my main sqlite scripting client, doesn't support fts5, I populate and use fts5 either via sqlitespy (1.9.16) or via python (3.11.2)'s shipped sqlite3 support, which includes fts5.</p>
<p>To achieve a database backup I would normally use the ODBC driver to .clone a database and/or .dump the tables. With fts5 present I am reduced to either doing a file zip or doing a line by line clone of the fts5 virtual table via python:</p>
<pre><code>import sqlite3

con1 = sqlite3.connect('source.db3')
con2 = sqlite3.connect('target.db3')
con1.text_factory = lambda b: b.decode(errors = 'ignore')
counter = 0
for line in con1.iterdump():
    try:
        if 'INSERT INTO &quot;ftstable&quot;' in line or 'CREATE VIRTUAL TABLE' in line:
            counter = counter + 1
            if counter % 1000000 == 0:
                print(str(counter))
            con2.execute(line)
    except Exception as e:
        print(line)
        print(str(e))
        continue
con2.commit()
con1.close()
con2.close()
</code></pre>
<p>I used an answer here: <a href=""https://stackoverflow.com/questions/6677540/how-do-i-dump-a-single-sqlite3-table-in-python"">how do i dump a single sqlite3 table in python?</a></p>
<p>The 'text_factory' line was required because there is some corrupt data in my database which was causing the script to crash with a decode error. I used an answer here: <a href=""https://stackoverflow.com/questions/22751363/sqlite3-operationalerror-could-not-decode-to-utf-8-column"">sqlite3.OperationalError: Could not decode to UTF-8 column</a></p>
<p>Note that iterdump() picks up fts5's 'underlying' ordinary tables twice: once when dumping the virtual table, which is fine, and then again as ordinary CREATE TABLEs + INSERTs, which will fail if you try to run them after successfully populating the the virtual table.</p>
<p>I can't find any references anywhere on the web to fts5 backups.</p>
<p>Does anyone have a better fts5 backup solution?</p>
",0,0,0,2025-08-03T09:03:45+00:00,0,55,False
79726283,4627124,,sqlite,Drop or cast zero default date in schema in sqlite-to-postgres migration using pgloader,"<p>I use pgloader to upload 11 databases from BIRD-DEV dataset into Postgres.</p>
<pre><code>pgloader &quot;$sqlite_path&quot; &quot;postgresql:///$pg_db_name&quot;
</code></pre>
<p>9 databases are uploaded correctly, but 2 give fatal error because of a zero default date in the schema:</p>
<pre><code>2025-08-05T14:30:41.188003Z ERROR Database error 22008: date/time field value out of range: &quot;0000-00-00&quot;
QUERY: CREATE TABLE races
(
  raceid    bigserial,
  year      bigint default '0',
  round     bigint default '0',
  circuitid bigint default '0',
  name      text default '',
  date      date default '0000-00-00',
  time      text,
  url       text
);
2025-08-05T14:30:41.188003Z FATAL Failed to create the schema, see above.
</code></pre>
<p>Is it possible to solve this problem on the level of pgloader without editing the database file ?</p>
",0,0,0,2025-08-05T14:38:35+00:00,0,70,False
79727413,1695672,Ireland,sqlite,Setting the counter of one table equal that of another doesn&#39;t work as expected,"<p>I have two tables in a SQLite database:</p>
<pre><code>CREATE TABLE IF NOT EXISTS &quot;DVDsBluRays&quot; (
        &quot;dbd_Title&quot;     TEXT,
        &quot;dbd_DateBought&quot;        TEXT,
        &quot;dbd_LastSeen&quot;  TEXT,
        &quot;dbd_Notes&quot;     TEXT,
        &quot;dbd_Category&quot;  TEXT,
        &quot;dbd_Type&quot;      TEXT,
        &quot;dbd_Length&quot;    INTEGER,
        &quot;dbd_MyRating&quot;  TEXT,
        &quot;dbd_Cost&quot;      TEXT,
        &quot;dbd_Extras&quot;    TEXT,
        &quot;dbd_Quality&quot;   TEXT,
        &quot;dbd_IsBluRay&quot;  TEXT,
        &quot;dbd_DiscType&quot;  TEXT,
        &quot;dbd_fnm_Counter&quot;       INTEGER,
        &quot;dbd_Counter&quot;   INTEGER NOT NULL UNIQUE,
        PRIMARY KEY(&quot;dbd_Counter&quot; AUTOINCREMENT)
);
CREATE TABLE IF NOT EXISTS &quot;FilmNames&quot; (
        &quot;fnm_FilmName&quot;  TEXT UNIQUE,
        &quot;fnm_dbd_Counter&quot;       INTEGER,
        &quot;fnm_Counter&quot;   INTEGER NOT NULL UNIQUE,
        PRIMARY KEY(&quot;fnm_Counter&quot; AUTOINCREMENT)
);
</code></pre>
<p>These are their contents:</p>
<p><code>FilmNames</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Christmas carol</td>
<td></td>
<td>795</td>
</tr>
<tr>
<td>13th warrior, the</td>
<td></td>
<td>904</td>
</tr>
<tr>
<td>16 blocks</td>
<td></td>
<td>1747</td>
</tr>
<tr>
<td>1408</td>
<td></td>
<td>2050</td>
</tr>
<tr>
<td>17 again</td>
<td></td>
<td>2323</td>
</tr>
<tr>
<td>2012</td>
<td></td>
<td>2364</td>
</tr>
<tr>
<td>21 Jump Street</td>
<td></td>
<td>3012</td>
</tr>
<tr>
<td>2 guns</td>
<td></td>
<td>3922</td>
</tr>
<tr>
<td>A bad idea gone wrong</td>
<td></td>
<td>4134</td>
</tr>
<tr>
<td>21 bridges</td>
<td></td>
<td>4623</td>
</tr>
<tr>
<td>1917</td>
<td></td>
<td>4986</td>
</tr>
<tr>
<td>A Christmas carol (1984)</td>
<td></td>
<td>5386</td>
</tr>
</tbody>
</table></div>
<p><code>DVDsBluRays</code>:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>dbd_Title</th>
<th>dbd_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>13th warrior, the</td>
<td>5</td>
</tr>
<tr>
<td>Abyss, the</td>
<td>10</td>
</tr>
<tr>
<td>A bug's life</td>
<td>80</td>
</tr>
<tr>
<td>A few good men</td>
<td>156</td>
</tr>
<tr>
<td>A murder of crows</td>
<td>307</td>
</tr>
<tr>
<td>16 blocks</td>
<td>567</td>
</tr>
<tr>
<td>12:01</td>
<td>571</td>
</tr>
<tr>
<td>17 again</td>
<td>720</td>
</tr>
<tr>
<td>2001: A space odyssey</td>
<td>736</td>
</tr>
<tr>
<td>2012</td>
<td>836</td>
</tr>
<tr>
<td>A fish called Wanda</td>
<td>915</td>
</tr>
</tbody>
</table></div>
<p>For the film names that they have in common I want to set <code>fnm_dbd_Counter</code> to <code>dbd_Counter</code>.  This shows the film names in common:</p>
<pre><code>select fnm_FilmName, fnm_Counter, dbd_Counter
from (select * from FilmNames) fnm
Join DVDsBluRays dbd on fnm.fnm_FilmName = dbd.dbd_Title
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>13th warrior, the</td>
<td>904</td>
<td>5</td>
</tr>
<tr>
<td>16 blocks</td>
<td>1747</td>
<td>567</td>
</tr>
<tr>
<td>17 again</td>
<td>2323</td>
<td>720</td>
</tr>
<tr>
<td>2012</td>
<td>2364</td>
<td>836</td>
</tr>
</tbody>
</table></div>
<p>When I try to set the <code>fnm_dbd_Counter</code> equal to <code>dbd_Counter</code> the outcome is not what I expect:</p>
<pre><code>update FilmNames
set fnm_dbd_Counter = dbd_Counter 
from (select * from FilmNames) fnm
Join DVDsBluRays dbd on fnm.fnm_FilmName = dbd.dbd_Title
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Christmas carol</td>
<td>836</td>
<td>795</td>
</tr>
<tr>
<td>13th warrior, the</td>
<td>836</td>
<td>904</td>
</tr>
<tr>
<td>16 blocks</td>
<td>836</td>
<td>1747</td>
</tr>
<tr>
<td>1408</td>
<td>836</td>
<td>2050</td>
</tr>
<tr>
<td>17 again</td>
<td>836</td>
<td>2323</td>
</tr>
<tr>
<td>2012</td>
<td>836</td>
<td>2364</td>
</tr>
<tr>
<td>21 Jump Street</td>
<td>836</td>
<td>3012</td>
</tr>
<tr>
<td>2 guns</td>
<td>836</td>
<td>3922</td>
</tr>
<tr>
<td>A bad idea gone wrong</td>
<td>836</td>
<td>4134</td>
</tr>
<tr>
<td>21 bridges</td>
<td>836</td>
<td>4623</td>
</tr>
<tr>
<td>1917</td>
<td>836</td>
<td>4986</td>
</tr>
<tr>
<td>A Christmas carol (1984)</td>
<td>836</td>
<td>5386</td>
</tr>
</tbody>
</table></div>
<p>They're all set to the dbd_Counter of one of the DVDs/Blu-rays, even the ones that don't match.  I would have expected:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>fnm_FilmName</th>
<th>fnm_dbd_Counter</th>
<th>fnm_Counter</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Christmas carol</td>
<td></td>
<td>795</td>
</tr>
<tr>
<td>13th warrior, the</td>
<td>5</td>
<td>904</td>
</tr>
<tr>
<td>16 blocks</td>
<td>567</td>
<td>1747</td>
</tr>
<tr>
<td>1408</td>
<td></td>
<td>2050</td>
</tr>
<tr>
<td>17 again</td>
<td>720</td>
<td>2323</td>
</tr>
<tr>
<td>2012</td>
<td>836</td>
<td>2364</td>
</tr>
<tr>
<td>21 Jump Street</td>
<td></td>
<td>3012</td>
</tr>
<tr>
<td>2 guns</td>
<td></td>
<td>3922</td>
</tr>
<tr>
<td>A bad idea gone wrong</td>
<td></td>
<td>4134</td>
</tr>
<tr>
<td>21 bridges</td>
<td></td>
<td>4623</td>
</tr>
<tr>
<td>1917</td>
<td></td>
<td>4986</td>
</tr>
<tr>
<td>A Christmas carol (1984)</td>
<td></td>
<td>5386</td>
</tr>
</tbody>
</table></div>
",2,2,0,2025-08-06T14:21:48+00:00,1,86,True
79731335,31247329,,sqlite,How to refresh items on a page without page refresh in Flask,"<p>I'm trying to add a script which directs me to the same page with refreshed content.</p>
<p>I'm using an sqlite3 database for my code and when i try it only works with the page refresh. This is one of my SQL structures.</p>
<pre><code>CREATE TABLE classes (
    model_id TEXT NOT NULL,
    class_number INTEGER,
    class_name TEXT,
    FOREIGN KEY(model_id) REFERENCES models(model_id)
);
</code></pre>
<p>The problem is this is my flask route i use this kind of structure:</p>
<pre><code>@app.route(&quot;/model/&lt;model_id&gt;&quot;,methods=[&quot;POST&quot;,&quot;GET&quot;])
@login_required
def view_model(model_id):
...
with sqlite3.connect(&quot;database.db&quot;) as con:
.
.
.
return render_template(&quot;train.html&quot;,  
   categories=CATEGORIES, 
   model_id=model_id, 
   model=model,
   classes=classes,
   images=images,
   istrained=istrained,
   test_image=test_image
   )
images=images)
</code></pre>
<p>And when ijust need to change the SQL just like in this route i need to either redirect(f&quot;/model/{model_id}&quot;) or return &quot;&quot;,402.In the first case all the page reloads and since the sql change it successfully reloads all the page.Also when i try the second one nothing applies since the classes is defined in the first route.</p>
<pre><code>@app.route(&quot;/add_class/&lt;model_id&gt;&quot;,methods=[&quot;POST&quot;,&quot;GET&quot;])
@login_required
def add_class(model_id):
    with sqlite3.connect(&quot;database.db&quot;) as con:
        con.row_factory = sqlite3.Row
        db = con.cursor()
        result = db.execute(&quot;SELECT MAX(class_number) FROM classes WHERE model_id = ?&quot;, (model_id,)).fetchone()
        max_num = result[0] if result and result[0] is not None else 0
        default_num = int(max_num +1)
        default_name = &quot;Class &quot; + str(max_num + 1)
        db.execute(&quot;INSERT INTO classes (model_id,class_name,class_number) VALUES(?,?,?)&quot;,
        (model_id,default_name,default_num))
        con.commit()
    return redirect(f&quot;/model/{model_id}&quot;)
</code></pre>
<p>I need to find a way to change the content but without all the page reloading.And want to know are there any ways to solve this problem without javascript.</p>
",1,2,1,2025-08-10T15:44:38+00:00,2,151,True
79732653,12323191,"St. Louis, MO, USA",sqlite,"Swift SQLite3 Not updating, but cannot get the exact error","<p>I had this code working at one time but all of a sudden, it's not working anymore.</p>
<p>I'm creating a table with the following:</p>
<pre class=""lang-swift prettyprint-override""><code>// Create Stock Table
func createStockTable() {
    let createTableString = &quot;&quot;&quot;
    CREATE TABLE IF NOT EXISTS Stocks (
        id INTEGER PRIMARY KEY,
        stockName STRING,
        status INT,
        imgName STRING,
        prevClose DOUBLE,
        curPrice DOUBLE,
        yield DOUBLE,
        noShares INT, 
        capitalization DOUBLE,
        lastUpdated String
    );
    &quot;&quot;&quot;
    var createTableStatement: OpaquePointer? = nil
    if sqlite3_prepare_v2(db, createTableString, -1, &amp;createTableStatement, nil) == SQLITE_OK {
        if sqlite3_step(createTableStatement) == SQLITE_DONE {
            print(&quot;Stock table is created successfully&quot;)
        } else {
            print(&quot;Stock table creation failed.&quot;)
        }
        sqlite3_finalize(createTableStatement)
    }
}
</code></pre>
<p>This is working just fine.</p>
<p>I have the following insert function to insert the rows:</p>
<pre class=""lang-swift prettyprint-override""><code>// add entries to Stock table
func insertStocks(id: Int, stockName: String, status: Int, imgName: String, prevClose: Double, curPrice: Double, yield: Double, noShares: Int, capitalization: Double, lastUpdated: String) -&gt; Bool {
    let stocks = getAllStocks()
    for stock in stocks {
        if stock.id == id {
            return false
        }
    }
    let insertStatementString = &quot;INSERT INTO Stocks (id, stockName, status, imgName, prevClose, curPrice, yield, noShares, capitalization, lastUpdated) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);&quot;
    var insertStatement: OpaquePointer? = nil
    if sqlite3_prepare_v2(db, insertStatementString, -1, &amp;insertStatement, nil) == SQLITE_OK {
        sqlite3_bind_int(insertStatement,1,Int32(id))
        sqlite3_bind_text(insertStatement, 2, (stockName as NSString).utf8String, -1, nil)
        sqlite3_bind_int(insertStatement, 3, Int32(status))
        sqlite3_bind_text(insertStatement, 4, (imgName as NSString).utf8String, -1, nil)
        sqlite3_bind_double(insertStatement, 5, Double(prevClose))
        sqlite3_bind_double(insertStatement, 6, Double(curPrice))
        sqlite3_bind_double(insertStatement, 7, Double(yield))
        sqlite3_bind_int64(insertStatement,8, Int64(noShares))
        sqlite3_bind_double(insertStatement, 9, Double(capitalization))
        sqlite3_bind_text(insertStatement, 10, (lastUpdated as NSString).utf8String, -1, nil)
        if sqlite3_step(insertStatement) == SQLITE_DONE {
            print(&quot;Stock Entry was created successfully&quot;)
            sqlite3_finalize(insertStatement)
            return true
        } else {
            print(&quot;Stock Entry Insert failed&quot;)
            return false
        }
    } else {
        print(&quot;INSERT Statement has failed&quot;)
        return false
    }
}
</code></pre>
<p>When I run the following, it gives me an error that the stock entry insert failed. I cannot seem to determine why.</p>
<pre class=""lang-swift prettyprint-override""><code>func addStocks() {
    db.insertStocks(id: 1, stockName: &quot;Tulsa Motors&quot;, status: 1, imgName: &quot;Tulsa_logo&quot;, prevClose: 125.18, curPrice: 125.18, yield: 0.025, noShares: 14357698, capitalization: myFunc.calcMarketCap(curPrice: 128.18, noShares: 14357698), lastUpdated: &quot;2025-05-01 17:00:00&quot;)
}
</code></pre>
<p>[edit]
Tim has raised the question on the creation of the db object, so I'm posting that code as well:</p>
<pre><code>let dataPath: String = &quot;MyDB&quot;
    let functions = Functions()
    var db: OpaquePointer?
    
    // Create DB
    func openDatabase()-&gt;OpaquePointer?{
        let filePath = try! FileManager.default.url(for: .documentDirectory, in: .userDomainMask, appropriateFor: nil, create: false).appendingPathComponent(dataPath)
        
        var db: OpaquePointer? = nil
        if sqlite3_open(filePath.path, &amp;db) != SQLITE_OK{
            debugPrint(&quot;Cannot open DB.&quot;)
            return nil
        }
        else{
            print(&quot;DB successfully created.&quot;)
            return db
        }
    }
</code></pre>
<p>The only issue I can possibly see is the <code>INT32</code> and <code>INT64</code> but those worked before so I'm not sure why that would no longer be working.</p>
<p>Suggestions?</p>
",1,2,1,2025-08-12T02:34:10+00:00,1,168,True
79735305,1564944,,sqlite,PHP Warning: Unable to load dynamic library &#39;pdo_sqlite&#39; when running php artisan serve in Laravel 11,"<p>I'm encountering a warning when I try to run my Laravel 11 project using the command php artisan serve. The error message is as follows:</p>
<pre><code>PHP Warning:  PHP Startup: Unable to load dynamic library 'pdo_sqlite' (tried: /usr/lib/php/20220829/pdo_sqlite (/usr/lib/php/20220829/pdo_sqlite: cannot open shared object file: No such file or directory), /usr/lib/php/20220829/pdo_sqlite.so (/usr/lib/php/20220829/pdo_sqlite.so: undefined symbol: php_pdo_unregister_driver)) in Unknown on line 0
</code></pre>
<p>I am using <strong>PHP 8.2, Laravel 11, Debian 12</strong></p>
<p>I checked my <strong>php.ini</strong> at <strong>/etc/php/8.2/apache2</strong> file and confirmed that the <strong>extension=pdo_sqlite</strong> line is uncommented.</p>
<p>I verified that the <strong>pdo_sqlite</strong> extension is installed by running <strong>php -m | grep pdo.</strong></p>
<p>Also to note, I am but using mysql for my laravel project.</p>
<p>What could be causing this warning, and how can I resolve the issue with the pdo_sqlite extension?</p>
",1,1,0,2025-08-14T11:00:52+00:00,1,103,True
79738195,5009319,USA,sqlite,Conversion between text and julian days in SQLITE is one day off,"<p>When I convert a text date to Julian, and then convert from Julian back to text, the result is one day off.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT CAST(julianday('2025-08-17') AS INTEGER);
2460904
SELECT strftime('%Y-%m-%d',2460904);
2025-08-16
</code></pre>
<p>Why is the text representation of the date one day earlier than the original date even though I use the same Julian value <code>2460904</code>?</p>
",1,1,0,2025-08-18T00:40:45+00:00,1,41,True
79739726,1805979,In between,sqlite,db:schema:load fails with &quot;Unknown key: :size in schema.rb&quot;,"<p>I’ve upgraded from Rails 6.1 to 8.0. My app uses SQLite for the test database. After the upgrade my CI (GitHub Actions) fails on <code>bin/rails db:create db:schema:load</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>Run bin/rails db:create db:schema:load

Created database 'db/test.sqlite3'
bin/rails aborted!
ArgumentError: Unknown key: :size. Valid keys are: :limit, :precision, :scale, :default, :null, :collation, :comment, :primary_key, :if_exists, :if_not_exists, :as, :type, :stored (ArgumentError)
        raise ArgumentError.new(&quot;Unknown key: #{k.inspect}. Valid keys are: #{valid_keys.map(&amp;:inspect).join(', ')}&quot;)
</code></pre>
<p>My schema.rb has lines like:</p>
<pre class=""lang-rb prettyprint-override""><code>t.text &quot;content&quot;, size: :long
</code></pre>
<p>This worked in the older version but in Rails 8.0 <code>size</code> is no longer valid. I created a migration to remove the <code>size</code> key and replace it with <code>limit</code> but Rails still dumps <code>size: :long</code> into schema.rb for large text columns under SQLite. This means the dumper is emitting a key that the loader rejects.</p>
<ul>
<li>Ruby 3.3.9</li>
<li>Rails 8.0.2</li>
<li>SQLite3 gem 2.0.2</li>
<li>RSpec for tests 8.0.2</li>
</ul>
<p>Is this an intentional change in Rails 8? Is it documented? What’s the recommended fix? Manually editing schema.rb doesn’t help because <code>db:schema:dump</code> brings <code>size: :long</code> back. Writing a migration with <code>limit</code> instead doesn’t stop <code>size</code> from appearing again with SQLite.</p>
<p>Should I switch my schema format to SQL (<code>config.active_record.schema_format = :sql</code>) or use another database in test instead of SQLite? I'd rather keep things simple. Or is there a Rails 8 -compatible way to keep schema.rb working with SQLite? LLMs recommend switching from schema.rb to structure.sql, but I'd like to continue with the simplicity of schema.rb.</p>
",1,1,0,2025-08-19T09:43:16+00:00,0,134,False
79740248,1100221,,sqlite,Update multiple rows in a SQLite DB - add a number to the value in multiple rows,"<p>I have a table in SQLite:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Id</th>
<th style=""text-align: left;"">Month</th>
<th style=""text-align: left;"">Year</th>
<th style=""text-align: left;"">StartingBalance</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">Jan</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">50000.00</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: left;"">Feb</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">53105.67</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: left;"">Mar</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">53405.32</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: left;"">Apr</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">54216.89</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: left;"">May</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">57020.93</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td style=""text-align: left;"">Jun</td>
<td style=""text-align: left;"">2007</td>
<td style=""text-align: left;"">59002.23</td>
</tr>
<tr>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">...</td>
<td style=""text-align: left;"">...</td>
</tr>
<tr>
<td style=""text-align: left;"">99</td>
<td style=""text-align: left;"">Jul</td>
<td style=""text-align: left;"">2025</td>
<td style=""text-align: left;"">141215.38</td>
</tr>
<tr>
<td style=""text-align: left;"">100</td>
<td style=""text-align: left;"">Aug</td>
<td style=""text-align: left;"">2025</td>
<td style=""text-align: left;"">145002.68</td>
</tr>
</tbody>
</table></div>
<p>I need to add a value of 55 to every StartingBalance value from Apr 2007 to Aug 2025. How can I do this in a single query?</p>
<p>Thanks.</p>
",0,1,1,2025-08-19T17:52:53+00:00,1,78,True
79743145,25493371,,sqlite,Executing SELECT query in one thread and INSERT and UPDATE in another thread in SQLite3,"<p>Is it safe to execute SELECT query in one thread and INSERT and UPDATE in another thread in SQLite3 in C++? Will it cause data race/UB?</p>
<p>I create a connection to the database as follows</p>
<pre><code>sqlite3* db;
sqlite3_open(&quot;path/to/example.db&quot;, &amp;db);
</code></pre>
",1,1,0,2025-08-22T08:25:16+00:00,2,100,True
79744333,11222817,,sqlite,Unexpected SQLite SELECT statement results within Delphi,"<p>I am receiving unexpected results from an SQL <code>SELECT</code> statement in my Delphi 12.3 app. I believe the SQL statement is properly formatted, however, I get no records.</p>
<p>I tested the following SQL statements and they returned the appropriate results:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM ExpGenItem;

SELECT * FROM ExpGenItem INDEXED BY idx_pdate;
</code></pre>
<p>When I executed the following SQL statement, I receive no results (I should get 5-6):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * 
FROM ExpGenItem INDEXED BY idx_pdate 
WHERE ExpDate BETWEEN '07/01/2025' AND '07/15/2025';
</code></pre>
<p>The <code>ExpDate</code> column in the SQL datebase is of type <code>DATE</code>, and the application code uses two <code>TDateTimePicker</code> controls to generate the start/end dates for the SQL statement.</p>
<p>Here is the code that builds the SQL statement and executes it. Where <code>RepStateDate</code> and <code>RepEndDate</code> are both <code>TDateTime</code> types. The function <code>etStrQuote()</code> simply appends the single quote mark (') to the date strings.</p>
<pre class=""lang-pascal prettyprint-override""><code>sqltxt:='SELECT * FROM ExpGenItem INDEXED BY idx_pdate WHERE ';
sqltxt:=sqltxt+'ExpDate BETWEEN '+etStrQuote(DateToStr(RepStartDate))+' ';
sqltxt:=sqltxt+'AND '+etStrQuote(DateToStr(RepEndDate))+';';

dmData1.ExpItemQuery.SQL.Text:=sqltxt;
dmData1.ExpItemQuery.Open;
</code></pre>
",2,2,0,2025-08-23T14:35:15+00:00,1,153,True
79746466,48445,New Zealand,sqlite,"In Delphi 12, using FireDAC, is there a way I can set the field length of a string automatically","<p>We are porting an application to a SQLite database (that we can't change - it's PowerSync if that helps) that has a number of views, and when we do a select from one of these views, all the text fields are 32k in length (FDQuery.FormatOptions.MaxStringSize).  We can limit this by adjusting the SQL (in about 1500 places) to something like</p>
<pre><code>SELECT id, MyField AS &quot;MyField::Text(32)&quot; FROM test;
</code></pre>
<p>I've been playing around with the Field Mapping rules on the connection (FDConnection.FormatOptions.MapRules), but I've been unable to restrict the length of the fields using it (although I can convert a string field to a date field).</p>
<p>Are there any other ways I can restrict the string length per field, without modifying every field in every select statement in the application?</p>
",0,0,0,2025-08-26T06:12:20+00:00,0,161,False
79747771,12338765,,sqlite,Creating an r-tree index table for bounding box,"<p>I'm currently working on setting up a SQLite database using GRDB to store a very simple location-based model (I've simplified the model for the purpose of this question)</p>
<pre><code>struct ExampleModel: Codable, Identifiable, FetchableRecord, PersistableRecord {
    let id: String
    let longitude: Double
    let latitude: Double
}
</code></pre>
<p>However, I also need to create an r-tree index table to allow users to pass a bounding box and retrieve all of the persisted elements within this bounding box</p>
<pre><code>struct BoundingBox {
    let minX: Double
    let maxX: Double
    let minY: Double
    let maxY: Double
}
</code></pre>
<p>It's the first time I'm having to implement a functionality like this and I'm totally lost on how to get this r-tree index table up and running. What would the database schema look like and how would I even get a bounding box if all I have is a model with a specific location? I'd appreciate some tips!</p>
",1,1,0,2025-08-27T09:21:31+00:00,1,121,True
79751742,27964927,,sqlite,MAUI: SQLite database &quot;removed&quot; after app closes in release mode,"<p>Why does the SQLite database file get deleted when the release mode app closes?</p>
<p>My SQLite database works perfectly during the app lifecycle but gets completely deleted when the app is closed only in release mode. Debug mode works fine and the database persists correctly.</p>
<p>I have used GenAI to try help and before removing it I tried to set up platform specific locations to try help- however no luck. Looking at <a href=""https://learn.microsoft.com/en-us/dotnet/maui/data-cloud/database-sqlite?view=net-maui-9.0"" rel=""nofollow noreferrer"">this</a> there seems to be no need for this. I am very lost on this!</p>
<p>Andrio path: <code>/data/data/com.better.mobileapp/files/BetterSQLite.db3</code></p>
<p>Environment:</p>
<ul>
<li>.NET MAUI 8.0</li>
<li>SQLite-net-pcl 1.8.116</li>
<li>Target platforms: Android API 34, iOS 17+</li>
</ul>
<p>What works:</p>
<ul>
<li>Database initializes successfully on app start</li>
<li>All tables are created properly</li>
<li>Database operations (CRUD) work perfectly during app session</li>
<li>File permissions are correct (verified via file explorer on Android)</li>
<li>Database file exists and has expected size (~140KB with data)</li>
</ul>
<p>What fails:</p>
<ul>
<li>After closing the app completely and reopening, database file is gone</li>
<li>No error messages or exceptions thrown</li>
<li><strong>Directory still exists but the .db3 file is deleted.</strong></li>
</ul>
<p>This behaviour happens on both Android and iOS.
I have tried to debug via Device logs when you close the app, the logs close with it - unless you can force it open?</p>
<p><strong>Minimal Reproduction project if anyone is keen to have a look</strong></p>
<p><em>Minimal Reproduction Environment:</em></p>
<ul>
<li>.NET MAUI 9.0</li>
<li>SQLite-net-pcl 1.9.172</li>
</ul>
<p><a href=""https://github.com/LewieDun/SQLiteApp"" rel=""nofollow noreferrer"">GitHuh Minimal Reproduction Repo</a></p>
<hr />
<p>Project Code:</p>
<pre><code>public static async Task InitializeAsync()
{
     if (_connection != null) return;

     await _semaphore.WaitAsync();

     try
     {
         if (_connection != null) 
             return; // Double-check after acquiring lock

         string dbPath = Path.Combine(FileSystem.AppDataDirectory, &quot;BetterSQLite.db3&quot;);

         //// Delete the database file if it exists (uncomment to enable)
         //if (File.Exists(dbPath))
         //    File.Delete(dbPath);

         _connection = new SQLiteAsyncConnection(dbPath);

         // Create the tables if they don't exist
         await _connection.CreateTablesAsync&lt;...,...&gt;();

         _logger?.LogInformation(&quot;SQLite database initialized successfully at {DbPath}&quot;, dbPath);
     }
     catch (Exception ex)
     {
         _logger?.LogError(ex, &quot;Failed to initialize SQLite database&quot;);
         throw new InvalidOperationException(&quot;Failed to initialize database&quot;, ex);
     }
     finally
     {
         _semaphore.Release();
     }
}
</code></pre>
<p>Android Studio file view:</p>
<p><a href=""https://i.sstatic.net/KBj4K9Gy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KBj4K9Gy.png"" alt=""Android Studio File View"" /></a></p>
",2,2,0,2025-08-31T12:12:18+00:00,0,220,False
79751991,26411637,,sqlite,How to make GitHub detect SQL?,"<p>In my backend project with Java and Spring Boot I want to document the SQL code uses the ORM, there is in the root of my project <code>docs/sql/</code>, for now I have the database schema. But I GitHub doesn't detect the SQL code. I saw in many repos detects SQL and other languages at same time, but I can't get it. <a href=""https://github.com/FrancoBujakiewicz/latte"" rel=""nofollow noreferrer"">https://github.com/FrancoBujakiewicz/latte</a></p>
<p>I tried those options:</p>
<pre><code> *.sql linguist-detectable
 *.sql linguist-language=SQL # also tried lowercase: 'sql'
 *.sql text

 query/** linguist-detectable
 query/** linguist-language=SQL
 query/** linguist-documentation=false

 # Kotlin is detected properly. 
 # By default in my repo will be Gradle build files 
 # (I'm using Kotlin DSL for Gradle build files), 
 # but with this are detected as Kotlin

 *.kts linguist-detectable=true
 *.kts linguist-language=Kotlin

</code></pre>
<p>SQL code should follow some standart to be detected?</p>
<pre class=""lang-sql prettyprint-override""><code>
 CREATE TABLE action (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    description VARCHAR(255),
    name VARCHAR(75) NOT NULL UNIQUE,
    PRIMARY KEY (id)

 );

 CREATE TABLE brand (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    description VARCHAR(255),
    name VARCHAR(75) NOT NULL UNIQUE,
    PRIMARY KEY (id)

 );

 CREATE TABLE flavor (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    description VARCHAR(255),
    name VARCHAR(75) NOT NULL UNIQUE,
    available BOOLEAN NOT NULL,
    PRIMARY KEY (id)

 );

 CREATE TABLE product (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    description VARCHAR(255),
    name VARCHAR(75) NOT NULL UNIQUE,
    price NUMERIC(8,2) NOT NULL,
    stock INTEGER NOT NULL,
    brand_id BIGINT NOT NULL,
    PRIMARY KEY (id)

 );

 CREATE TABLE role (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    description VARCHAR(255),
    name VARCHAR(75) NOT NULL UNIQUE,
    PRIMARY KEY (id)

 );

 CREATE TABLE role_action (

    role_id BIGINT NOT NULL,
    action_id BIGINT NOT NULL,
    PRIMARY KEY (role_id, action_id)

 );

 CREATE TABLE size (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    description VARCHAR(255),
    name VARCHAR(75) NOT NULL UNIQUE,
    available BOOLEAN NOT NULL,
    price NUMERIC(8,2) NOT NULL,
    PRIMARY KEY (id)

 );

 CREATE TABLE user (

    id INTEGER,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    passwd_hash VARCHAR(60) NOT NULL UNIQUE,
    phone_number VARCHAR(30) NOT NULL UNIQUE,
    salt VARCHAR(50) NOT NULL UNIQUE,
    username VARCHAR(50) NOT NULL UNIQUE,
    role_id BIGINT NOT NULL,
    PRIMARY KEY (id)

 );

</code></pre>
",2,2,0,2025-08-31T20:13:51+00:00,1,90,True
79753597,31245684,,sqlite,Python 3.13.5 sqlite3 DeprecationWarning persists despite registering custom date adapter,"<p>I am building an application using Python 3.13.5 and the native <code>sqlite3</code> library. To correctly handle <code>datetime.date</code> objects and avoid the new <code>DeprecationWarning</code>, I am trying to implement the officially recommended custom adapter and converter pattern.</p>
<p>Despite registering a custom adapter for the <code>datetime.date</code> type, the <code>DeprecationWarning: The default date adapter is deprecated...</code> is still being triggered every time I execute an <code>INSERT</code> statement with a <code>date</code> object.</p>
<p>Interestingly, my custom <strong>converter</strong> (for reading <code>DATE</code> columns from the DB) seems to work correctly when I use the <code>detect_types=sqlite3.PARSE_DECLTYPES</code> flag. The problem seems to be exclusively with the <strong>adapter</strong> (for writing Python <code>date</code> objects to the DB), which appears to be ignored, causing <code>sqlite3</code> to fall back to its deprecated default method.</p>
<p>I have isolated the problem in the following minimal, self-contained script. This script does not use <code>pytest</code> or any external libraries.</p>
<pre><code>import sqlite3
from datetime import datetime, date
import sys

###############################################################################
print('\n')
print(f'=' * 25, '--- PYTHON AND SQLITE3 VERSION ---', '=' * 25)
print(f'Python Version: {sys.version}')
print(f'Sqlite3 Version: {sqlite3.sqlite_version_info}')
print(f'=' * 50)
###############################################################################

#################### --- ADAPTERS AND CONVERSORS --- ####################
def date_adapter(object_date: date) -&gt; str:
    'receives an object_date in the date adapter for adaptation to the new pattern of sqlite3'
    adapter_format_str = object_date.isoformat()
    return adapter_format_str

def date_conversor(object_bytes: bytes) -&gt; date:
    'receives an object_bytes from database for the converting in a object_date to python'
    convert_object_str = object_bytes.decode()
    adapter_format_date = datetime.strptime(convert_object_str, '%Y-%m-%d').date()
    return adapter_format_date

####### --- DATE OBJECT -&gt; BYTES (STR) OBJECT --- #######
sqlite3.register_adapter(datetime.date, date_adapter)
####### --- BYTES (STR) OBJECT -&gt; DATE OBJECT --- #######
sqlite3.register_converter('date', date_conversor)

####### --- CREATING AN DB WITHIN MEMORY --- #######
test_conn = sqlite3.connect(':memory:', detect_types = sqlite3.PARSE_DECLTYPES)
cursor = test_conn.cursor()
object_today = date.today()

###############################################################################
print(f'=' * 25, '--- TYPE INPUT ---', '=' * 25)
print(f'[DEBUG] Type Object Input in Database: {type(object_today)}')
print(f'=' * 50)
print('\n')
print(f'#' * 25, '--- WARNING HERE ---', '#' * 25)
###############################################################################

####### --- CREATING TABLE, INSERTING VALUE AND SELECTING THE OBJECT--- #######
cursor.execute('''
        CREATE TABLE IF NOT EXISTS test (
        id INTEGER PRYMARY KEY,
        date_today DATE NOT NULL
        )
    ''')
cursor.execute('''
        INSERT INTO test (date_today)
        VALUES (?)
    ''',
    (
        object_today,
    ))

cursor.execute('''
        SELECT *
        FROM test
    ''')
return_database = cursor.fetchall()
test_conn.commit()
test_conn.close()

###############################################################################
print(f'#' * 25, '--- WARNING HERE ---', '#' * 25)
print('\n')
print(f'=' * 25, '--- TYPE OUTPUT ---', '=' * 25)
print(f'[DEBUG] Type Object Output from Database: {type(return_database[0][1])}')
print(f'=' * 50)
###############################################################################
</code></pre>
<p>When I run this script, I get the following output:</p>
<pre><code>========================= --- PYTHON AND SQLITE3 VERSION --- =========================
Python Version: 3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]
Sqlite3 Version: (3, 49, 1)
==================================================
========================= --- TYPE INPUT --- =========================
[DEBUG] Type Object Input in Database: &lt;class 'datetime.date'&gt;
==================================================


######################### --- WARNING HERE --- #########################
c:\Users\Lider CPD\Desktop\Joao - Arquivos\Particularidades - Joao\Projetos Python\gestaofarma_simples\debug_data.py:50: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes
  cursor.execute('''
######################### --- WARNING HERE --- #########################


========================= --- TYPE OUTPUT --- =========================
[DEBUG] Type Object Output from Database: &lt;class 'datetime.date'&gt;
==================================================
</code></pre>
<p>What I've Tried
I have confirmed the column type in CREATE TABLE is DATE.</p>
<p>My adapter function (date_adapter) is correct and returns a string as required.</p>
<p>I have confirmed the problem exists both within my pytest test suite and in the minimal script above, so it is not a pytest-specific issue.</p>
<p>The registration is happening before any connection is made.</p>
<p>My Question
What am I missing in the registration or connection setup that is causing sqlite3 to ignore my custom registered adapter for datetime.date and fall back to the deprecated default? Is there a known issue or a change in behavior in Python 3.13.5 that is not explicitly covered in the main documentation &quot;recipes&quot;?</p>
<p><strong>--- EDIT: Solution ---
User LMC's answer below resolved the issue.</strong></p>
",5,5,0,2025-09-02T14:24:02+00:00,1,576,True
79756509,5217216,,sqlite,Correct way to handle attached SQLite databases with jOOQ,"<p>I'm trying to do a join on a table in an attached SQLite database (<a href=""https://sqlite.org/lang_attach.html"" rel=""nofollow noreferrer"">https://sqlite.org/lang_attach.html</a>) and I'm having trouble getting jOOQ to format the field names correctly. The query that jOOQ is generating is quoting the name of the attached database, but this causes the query to fail:</p>
<pre class=""lang-sql prettyprint-override""><code>select &quot;attached.MemberTransactions&quot;.time, &quot;attached.MemberTransactions&quot;.tableName, &quot;attached.MemberTransactions&quot;.id
from MemberTransactions as curr
  full outer join attached.MemberTransactions
    on (curr.time, curr.tableName, curr.id) = (&quot;attached.MemberTransactions&quot;.time, &quot;attached.MemberTransactions&quot;.tableName, &quot;attached.MemberTransactions&quot;.id)
where curr.id is null
order by &quot;attached.MemberTransactions&quot;.time asc
</code></pre>
<p>Removing the quotes fixes the query, i.e.:</p>
<pre class=""lang-sql prettyprint-override""><code>select attached.MemberTransactions.time, attached.MemberTransactions.tableName, attached.MemberTransactions.id
from MemberTransactions as curr
  full outer join attached.MemberTransactions
    on (curr.time, curr.tableName, curr.id) = (attached.MemberTransactions.time, attached.MemberTransactions.tableName, attached.MemberTransactions.id)
where curr.id is null
order by attached.MemberTransactions.time asc
</code></pre>
<p>This is how I'm generating the query:</p>
<pre class=""lang-java prettyprint-override""><code>try(Connection connection = DatabaseConnector.createConnection(baseFileName))
{
  connection.prepareStatement(String.format(&quot;ATTACH DATABASE \&quot;%s\&quot; AS %s&quot;, attachFileName, ATTACHED_DB_NAME)).execute();
  DSLContext context = DSL.using(connection, SQLDialect.SQLITE);
// ...
String otherTableName = String.format(&quot;%s.%s&quot;, DatabaseConnector.ATTACHED_DB_NAME, Membertransactions.MEMBERTRANSACTIONS.getName());
Membertransactions otherTable = Membertransactions.MEMBERTRANSACTIONS.as(otherTableName);
Membertransactions currTable = Membertransactions.MEMBERTRANSACTIONS.as(&quot;curr&quot;);
return context.select(otherTable.TIME, otherTable.TABLENAME, otherTable.ID)
  .from(currTable.fullOuterJoin(otherTableName).on(currTable.eq(otherTable)).asTable())
  .where(currTable.ID.isNull())
  .fetchInto(MembertransactionsRecord.class);
</code></pre>
<p>I've tried various workarounds, but I haven't found anything that fixes all of the field names. I can pass raw strings to <code>fullOuterJoin()</code> or <code>from()</code>, but <code>select()</code> can't take raw strings. I need to specifically select <code>otherTable</code>'s fields because otherwise, <code>fetchInto()</code> uses the null-valued side of the join; this means I can't just use <code>selectFrom()</code> with a raw string and drop the <code>orderBy()</code> call as a workaround. I can pass an unquoted qualified name from <code>DSL.unquotedName()</code> to the <code>as()</code> call that I'm assigning to <code>otherTable</code>, but then the field names in the query stop being qualified for some reason (i.e. instead of &quot;\&quot;attached.MemberTransactions\&quot;.*&quot;, I get &quot;MemberTransactions.*&quot;).</p>
<p>Is there any way I can force the field names to be inserted into the query as fully-qualified and unquoted? Or is there any official support for the SQLite attach statement that I missed which would handle this case correctly?</p>
",2,2,0,2025-09-05T07:36:39+00:00,2,80,True
79757096,1703510,,sqlite,Is sqlite upsert &quot;insert or ignore ... on conflict do update...&quot; a valid statement?,"<p>In Sqlite3...
I need to copy data from an unconstrained table to a more constrained table.  I need to drop illegal entries (null), replace matching existing entries (unique x), and add new entries.  As far as replacing matching, if there are multiple matches in the old table, I don't care which one of them ends up in the new table, its understood the new table is a subset of the old one.</p>
<p>Given tables like this:</p>
<p><code>create table  newTable ( x int not null unique, y int )</code></p>
<p><code>create table  oldTable ( x int, y int )</code></p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>oldTable</th>
<th></th>
<th></th>
<th>newTable</th>
<th></th>
<th></th>
<th>updated newTable</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>y</td>
<td></td>
<td>x</td>
<td>y</td>
<td></td>
<td>x</td>
<td>y</td>
</tr>
<tr>
<td>--------</td>
<td>--------</td>
<td></td>
<td>--------</td>
<td>--------</td>
<td></td>
<td>--------</td>
<td>--------</td>
</tr>
<tr>
<td>null</td>
<td>1</td>
<td></td>
<td>1</td>
<td>0</td>
<td></td>
<td>1</td>
<td>2 or 3</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>... is using a statement like this safe (defined/valid/deterministic/idiomatic/etc)?</p>
<p><code>insert or ignore into newTable (x,y) select (x,y) from oldTable where true on conflict (x) do update x=excluded.x, y=excluded.y where x=excluded.x</code></p>
<p>(side-note: &quot;where true&quot; is there to prevent ambiguity about the end of the select)</p>
<p>This statement works, and it does exactly what I need (only ignores null values, otherwise does the insert/update), and I know it's valid by this <a href=""https://sqlite.org/lang_insert.html"" rel=""nofollow noreferrer"">diagram</a>, but I don't see any clear documentation stating the order of precedence between ignore and on conflict.  I want to make sure that I'm not exploiting some undocumented loophole that'll be &quot;fixed&quot; in a future version of sqlite.  Mainly I just need to know if the &quot;conflict&quot; handler is guaranteed to take precedence over the generic &quot;ignore&quot; error handler.... or is there a better way to do that?</p>
<p>edit: added &quot;do&quot; to make the statement valid</p>
",2,2,0,2025-09-05T19:25:53+00:00,1,122,True
79758081,20839852,your moms bedroom,sqlite,Why does my `RoomDatabase` instance not have all the public methods?,"<p>I have a Kotlin project for desktop which uses Room DB.
The main function looks like this:</p>
<pre class=""lang-kotlin prettyprint-override""><code>import androidx.room.Database
import androidx.room.Room
import androidx.room.RoomDatabase
import androidx.sqlite.driver.bundled.BundledSQLiteDriver

@Database(version = 1)
abstract class MyRoomDatabase : RoomDatabase()

fun main() {
    val db = Room.databaseBuilder&lt;MyRoomDatabase&gt;(&quot;database.db&quot;)
        .setDriver(BundledSQLiteDriver())
        .build()

    db.runInTransaction {
        // something
    }
}
</code></pre>
<p>It creates an empty database without any DAOs or entities.</p>
<p>Then I try to run the <code>runInTransaction</code> method, but my IDE and the compiler can't find it.</p>
<p>However this method should exist, it is mentioned in the docs <a href=""https://developer.android.com/reference/androidx/room/RoomDatabase#runInTransaction(java.lang.Runnable)"" rel=""nofollow noreferrer"">here</a>.</p>
<p>It should be available for all instances of <code>RoomDatabase</code> and since <code>MyRoomDatabase</code> inherits from <code>RoomDatabase</code>, it should have that method.</p>
<p>Any idea why it is missing??</p>
<p>Here's my build file:</p>
<pre class=""lang-kotlin prettyprint-override""><code>plugins {
    kotlin(&quot;jvm&quot;) version &quot;2.2.0&quot;
    id(&quot;com.google.devtools.ksp&quot;) version &quot;2.2.0-2.0.2&quot;
    application
}

repositories {
    mavenCentral()
    google()
}

dependencies {
    implementation(&quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:1.7.3&quot;)
    testImplementation(kotlin(&quot;test&quot;))

    // Room DB
    val roomVersion = &quot;2.7.2&quot;
    implementation(&quot;androidx.room:room-runtime:$roomVersion&quot;)
    ksp(&quot;androidx.room:room-compiler:$roomVersion&quot;)
    implementation(&quot;androidx.sqlite:sqlite-bundled:2.5.2&quot;)
}

kotlin {
    jvmToolchain(23)
}

application {
    mainClass.set(&quot;MainKt&quot;)
}
</code></pre>
<p>I'm using version 2.7.2 of room which should definitely have that method implemented.</p>
<p>Also, a lot of the other public methods such as <code>clearAllTables</code> or <code>compileStatement</code> are missing.</p>
<p>And this is not because the database is empty or has no DAOs, it didn't work in my actual project which has data.</p>
<p>Hope you have any idea</p>
<p>Thank you and have a nice day</p>
<p>Edit: Replit project to reproduce the error
<a href=""https://replit.com/@axolotlKing07/Reproducing-Room-DB-method-missing?v=1"" rel=""nofollow noreferrer"">https://replit.com/@axolotlKing07/Reproducing-Room-DB-method-missing?v=1</a></p>
",2,2,0,2025-09-07T11:26:28+00:00,2,139,True
79764183,14017449,,sqlite,Microsoft.EntityFrameworkCore.Sqlite TOP and LIMIT SQL syntax error,"<p>In Entity Framework Core 9.0.9, the query builder fails with SQLite Syntax</p>
<pre><code>builder.Services.AddDbContext&lt;MyDbContext&gt;(fun options -&gt;
     options.UseSqlite(connectionString) |&gt; ignore
</code></pre>
<p>It generates this <code>SELECT</code> syntax:</p>
<pre><code>FETCH FIRST 10 ROWS ONLY
</code></pre>
<p>and</p>
<pre><code>OFFSET 10 ROWS
</code></pre>
<p>correct by ANSI/ISO SQL:2008</p>
<p>but in SQLite it causes an error:</p>
<blockquote>
<p>Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 1: 'near &quot;FETCH&quot;: syntax error'.</p>
</blockquote>
<p><code>FETCH FIRST … ROWS ONLY</code> → DB2, Oracle 12c+, PostgreSQL.<br />
<code>OFFSET … ROWS</code> → SQL Server (2012+), PostgreSQL, Oracle 12c+.<br />
<code>OFFSET … ROWS FETCH NEXT … ROWS ONLY;</code> Microsoft SQL Server.</p>
<p>In SQLite it must be</p>
<pre><code>LIMIT 10
LIMIT -1 OFFSET 50
LIMIT 10 OFFSET 50
</code></pre>
<p>My work-around in F#</p>
<pre><code>type MyQuerySqlGenerator(deps: QuerySqlGeneratorDependencies) =
    inherit QuerySqlGenerator(deps)
    override this.GenerateLimitOffset (x: SelectExpression): unit = 
   
        // Default SQLite: LIMIT &lt;fetch&gt; OFFSET &lt;offset&gt;
        // Custom: e.g., only LIMIT, or wrap in subquery
        //.Take(10) → LIMIT 10
        //.Skip(50) → LIMIT -1 OFFSET 50
        //.Skip(50).Take(1) → LIMIT 10 OFFSET 50

        match x.Limit with
        | null -&gt; 
            if x.Offset = null then 
                base.GenerateLimitOffset(x)   // no Take
            else 
                this.Sql.Append(&quot; LIMIT -1 OFFSET &quot;) |&gt; ignore
                this.Visit(x.Offset) |&gt; ignore
        | fetchExpr -&gt;
            this.Sql.Append(&quot; LIMIT &quot;) |&gt; ignore
            this.Visit(fetchExpr) |&gt; ignore
            if x.Offset &lt;&gt; null then 
                this.Sql.Append(&quot; OFFSET &quot;) |&gt; ignore
                this.Visit(x.Offset) |&gt; ignore

type MyQuerySqlGeneratorFactory(deps: QuerySqlGeneratorDependencies) =
    interface IQuerySqlGeneratorFactory with
        member _.Create() =
            upcast new MyQuerySqlGenerator(deps)

type MyDbContext(options: DbContextOptions&lt;ReadDbContext&gt;) =
    inherit DbContext(options)
    override _.OnConfiguring(optionsBuilder: DbContextOptionsBuilder) =
        optionsBuilder
            .ReplaceService&lt;IQuerySqlGeneratorFactory, MyQuerySqlGeneratorFactory&gt;()
        |&gt; ignore

</code></pre>
<p>My code has alread customized MyQuerySqlGenerator and I used &quot;patch&quot;.
Is there some other more elegant solution?</p>
",1,1,0,2025-09-14T08:39:45+00:00,0,72,False
79765404,3796660,USA,sqlite,Room Database - Secondary ORDER BY not working Java,"<p>I have a RecyclerView list of CardViews for tasks, etc.  Some of the CardViews have a due date attached to them, others have no due date.  I want to show the CardViews with due dates on top of the list, sorted by nearest to farthest due date.  This is working as expected.</p>
<p>I want the CardViews without due dates to show at the bottom of the list, and be sorted by their Card IDs from highest to lowest, but this is not working as expected.  The bottom of the list is showing the lowest CardView ID first and then higher number Cards below it.</p>
<p>Here is the Room code:</p>
<pre><code>@Query(&quot;SELECT * FROM cards_table WHERE cardType = :cardType0 ORDER BY &quot; +
    &quot;CASE WHEN cardDuedatentime = -1 THEN 1 END, cardId DESC, &quot; +
    &quot;CASE WHEN cardDuedatentime != -1 THEN cardDuedatentime END ASC &quot;
)       
List&lt;Card&gt; getFilteredCards0(String cardType0);      
</code></pre>
<p>The graphic shows the card numbers with Card #1 above #2.  I would like that to be reversed with Card #2 above Card #1, etc.</p>
<p>What am I missing here?</p>
<p><a href=""https://i.sstatic.net/zBuDY45n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zBuDY45n.png"" alt=""enter image description here"" /></a></p>
",1,1,0,2025-09-15T16:38:15+00:00,2,156,True
79766243,473259,"Oslo, Norway",sqlite,Python is creating a connection for each execution in executemany,"<p>I have the following code</p>
<pre><code>logger = get_console_logging_object()
def get_db_path() -&gt; str:
    &quot;&quot;&quot; get the path to the database file&quot;&quot;&quot;
    script_path = Path(__file__).parents[1]
    db_path = script_path.joinpath(&quot;data&quot;,&quot;verySensetivedata.db&quot;)
    logger.info(f&quot;Saving data to {db_path}&quot;)
    return db_path

def get_db_connection():
    &quot;&quot;&quot; returns the db connections&quot;&quot;&quot;
    return sqlite3.connect(get_db_path())

def init_sqlite():
    &quot;&quot;&quot; creates the table xxxx&quot;&quot;&quot;
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute(&quot;create table if not exists sfdata (...)&quot;)
        conn.commit()
        cur.close()
        conn.close()
    except sqlite3.OperationalError as err:
        logger.error(f&quot;Error in creating the sfdata table with error: {err}&quot;)
        cur.close()
        conn.rollback()
        conn.close()
        raise err

def write_data_to_db(data: list):
    &quot;&quot;&quot; writes all the data from openai to the database
    data-&gt; list   (account_name, opportunity, created_date, techs, roles)&quot;&quot;&quot;

    insert_sql = &quot;insert into sfdata (....... ) values(?,?,?,?,?)&quot;
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.executemany(insert_sql, data)
        conn.commit()
        cur.close()
        conn.close()
    except sqlite3.OperationalError as err:
        logger.error(f&quot;Error in inserting values into sfdata table with error: {err}&quot;)
        cur.close()
        conn.rollback()
        conn.close()
        raise err
</code></pre>
<p>when I execute the write_data_to_db() I get the log output &quot;Saving the data to ...&quot; for each element in the list, is the the correct behaviour? Is there something wrong with the code?</p>
<p>thanks,</p>
<p>es</p>
",0,0,0,2025-09-16T12:35:41+00:00,1,58,True
79766996,13247321,"Western Australia, Australia",sqlite,Problem using a TRESTResponseDataSetAdapter with Boolean fields and SQLite,"<p>I am writing a Firemonkey mobile app, and as the user could be without a signal at times, I need an SQLite local database to hold some data.
I am using Delphi's Rest components to call an endpoint and receive some JSON. I have a TRESTResponseDataSetAdapter connected, but I'm getting an error caused by the field &quot;Is_Admin&quot; which is a boolean field in the internet server database, and the integer field &quot;Is_Admin&quot; in the local SQLite database.
Is there some way to convert values before they are written to the SQLite table?</p>
",0,0,0,2025-09-17T07:26:39+00:00,0,21,False
79771295,23621351,,sqlite,SQLite3 Exec Failing with Assertion in checkWalModeFromQuery on Xcode 26,"<p>I'm encountering a runtime assertion failure when trying to execute a SQLite query using sqlite3_exec in Xcode 26. The error occurs specifically when attempting to set WAL (Write-Ahead Logging) mode and other database configurations.</p>
<p>Here is the code snippet that causes the problem:</p>
<pre class=""lang-objc prettyprint-override""><code>NSString *sql = @&quot;pragma journal_mode = wal; pragma synchronous = normal; create table if not exists manifest (key text, filename text, size integer, inline_data blob, modification_time integer, last_access_time integer, extended_data blob, primary key(key)); create index if not exists last_access_time_idx on manifest(last_access_time);&quot;;
char *error = NULL;
sqlite3_exec(_db, sql.UTF8String, NULL, NULL, &amp;error);
</code></pre>
<p>When this code executes, it triggers the following assertion failure:</p>
<blockquote>
<p>An abort signal terminated the process. Such crashes often happen because of an uncaught exception or unrecoverable error or calling the abort() function.</p>
<p>Assertion failed: (0), function checkWalModeFromQuery, file SQLiteDatabaseTracking.cpp, line 942.</p>
</blockquote>
<p>I tried disabling WAL mode, but it doesn’t seem to take effect. I removed <code>pragma journal_mode = wal; pragma synchronous = normal;</code></p>
",0,0,0,2025-09-22T06:28:26+00:00,0,77,False
79772776,357546,Switzerland,sqlite,"Django migration successfully applied, but the database is not modified","<p>I need to use a secondary SQLite database in a new Django project. This database is on the local filesystem but outside the Django folder. Its path is specified in a <code>.env</code> file at the root of the Django project.</p>
<p>I want Django to be able to manage migrations on that database, but I already have data in it, which I don't want to loose.</p>
<p>I was able to integrate the database into the Django project, and I see no error at any point. I can fetch data from the database via the Django shell. However, when I try to apply migrations, nothing happens: the database is not modified, but Django doesn't give me any error (in fact it says the migration has been applied).</p>
<p>Here's what I did:</p>
<ol>
<li>created an &quot;archiver&quot; app within Django</li>
<li>within this app, created a <code>routers.py</code> file with the following code:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>class ArchiverDbRouter:

    def db_for_read(self, model, **hints):
        if model._meta.app_label in ['archiver']:
            return 'archiver'
        return None

    def db_for_write(self, model, **hints):
        if model._meta.app_label in ['archiver']:
            return 'archiver'
        return None

    def allow_migrate(self, db, app_label, model_name=None, **hints):
        if app_label in ['archiver']:
            return db == 'archiver'
        return None

</code></pre>
<ol start=""3"">
<li>configured <code>settings.py</code> to use two databases. The idea is to keep the default database for everything Django, and then the &quot;archiver&quot; database for the &quot;archiver&quot; app.</li>
</ol>
<pre><code>import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()
USER_DIR = Path(os.getenv('USER_DIR', './user'))

(...)

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
    },
    'archiver': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': USER_DIR / 'data/app.db',
    }
}

DATABASE_ROUTERS = ['archiver.routers.ArchiverDbRouter']
</code></pre>
<ol start=""4"">
<li>generated my models with the <code>inspectdb</code> command:</li>
</ol>
<pre><code>python manage.py inspectdb --database archiver &gt; tempmodels.py
</code></pre>
<p>Then tweaked those models and saved them to <code>archiver/models.py</code>. I removed the <code>managed = False</code> properties in all models.</p>
<ol start=""5"">
<li>initialized the database</li>
</ol>
<pre><code>python manage.py migrate
</code></pre>
<p>This creates the &quot;default&quot; database file.</p>
<ol start=""6"">
<li>generated the migrations for archiver</li>
</ol>
<pre><code>python manage.py makemigrations archiver
</code></pre>
<p>The <code>0001_initial.py</code> migration file is created.</p>
<ol start=""7"">
<li>applied this migration with the <code>--fake</code> flag</li>
</ol>
<pre><code>python manage.py migrate archiver 0001 --fake
</code></pre>
<p>I can see the corresponding migration saved in the <code>django_migrations</code> table.</p>
<p>At this point, I can use the Django shell and access the actual data in my &quot;archiver&quot; database, which seems to confirm that the routing works correctly and the database is found by Django. E.g.</p>
<pre><code>&gt;&gt;&gt; q = State(account=&quot;test&quot;, name=&quot;test&quot;, value=&quot;test&quot;)
&gt;&gt;&gt; q.save()
</code></pre>
<p>Then I see that the new line (with the three &quot;test&quot; values) is present in the &quot;states&quot; table of m &quot;archiver&quot; database (using a third-party tool, HeidiSQL). I can also see that the modified date for the database file has been updated.</p>
<ol start=""8"">
<li><p>made some changes to my <code>models.py</code>, by removing a field that was never used in the <code>Post</code> model.</p>
</li>
<li><p>generated the migrations again</p>
</li>
</ol>
<pre><code>python manage.py makemigrations archiver
</code></pre>
<p>The migration file is created:</p>
<pre><code>from django.db import migrations

class Migration(migrations.Migration):

    dependencies = [
        ('archiver', '0001_initial'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='post',
            name='note',
        ),
    ]
</code></pre>
<ol start=""10"">
<li>applied the new migration</li>
</ol>
<pre><code>python manage.py migrate archiver
</code></pre>
<p>This gives me the output:</p>
<pre class=""lang-bash prettyprint-override""><code>Operations to perform:
  Apply all migrations: archiver
Running migrations:
  Applying archiver.0002_remove_post_note... OK
</code></pre>
<p>No error. I can see the corresponding migration saved in the <code>django_migrations</code> table.</p>
<p>HOWEVER, when I explore the archiver database (using HeidiSQL again), the &quot;note&quot; field is still present. Also, the &quot;modified date&quot; for the database file has NOT changed.</p>
<p>What am I missing?</p>
",1,1,0,2025-09-23T15:05:42+00:00,1,66,True
79773161,11222817,,sqlite,Record Selection in Main Dataset from a Sub Dataset in Delphi,"<p>The application I'm writing has a main form where all the database fields and <code>DBGrid</code> that contains the entire SQLite dataset (<code>ExpItemQuery</code>) reside. I have designed a database search functionality into the application using a separate Delphi form. The search form can find records based on a number of fields (project ID, dates, vendor, etc.). The search results are shown in a <code>DBGrid</code> using a different dataset query (<code>SearchQuery</code>) of the same main database records as the main form.</p>
<p>The problem I struggling with is how once the user selects the correct record from the Search Form can I bring it back to the main form, select the correct record to display on the main form. To make matters a bit more challenging, each of these datasets will have the same content (fields) from the database, but the record count and numbers will be completely different, so using them to locate the proper record won't work. I have looked into using the <code>CopyRecord</code> method of <code>TFDQuery</code> however, there is little to no documentation or examples to follow so I'm not sure its appropriate. I have also looked at similar methods <code>GotoCurrent</code> and <code>CloneCurosr</code> which I don't believe will work properly given the differences in the main form dataset and the subset dataset.</p>
<p>The only potential solution I can reasonably come up with is to copy the <code>SearchQuery</code> record information as selected by the user into a memory storage record, close the search form, search the main form dataset for the correct record using several key fields and display it.</p>
<p>I'm hoping that someone has a better solution to this problem that I've may have overlooked.</p>
",0,0,0,2025-09-23T23:42:05+00:00,0,103,False
79773349,27521706,,sqlite,Why does Streamlit throw &quot;ImportError: cannot import name &#39;get_all_transactions_by_user&#39; from &#39;db&#39;&quot; even though the function exists?,"<p>I’m building a personal finance management app in Streamlit with multiple pages (dashboard.py, transactions.py, budget.py, export.py). I have a db.py file that contains all database functions, including <code>get_all_transactions_by_user</code>.</p>
<p>When I try to import this function in export.py using:</p>
<pre class=""lang-py prettyprint-override""><code>from db import get_all_transactions_by_user
</code></pre>
<p>I get the following error:</p>
<pre class=""lang-none prettyprint-override""><code>ImportError: cannot import name 'get_all_transactions_by_user' from 'db'
</code></pre>
<p>Other functions in db.py can be imported without issues. I’ve verified the function exists, and I’ve checked for spelling mistakes and circular imports, but the problem persists.</p>
<p>To troubleshoot, I have:</p>
<ul>
<li>Verified that <code>get_all_transactions_by_user</code> is correctly defined in db.py.</li>
<li>Tried both <code>from db import get_all_transactions_by_user</code> and <code>import db</code> approaches.</li>
<li>Confirmed there are no circular imports in the app.</li>
<li>Restarted Streamlit server multiple times.</li>
</ul>
<p>Expected behavior: The page should import the function and allow me to fetch user transactions.</p>
<p>Actual behavior: The <code>ImportError</code> occurs, preventing the function from being imported.</p>
<p>Code snippets for context:</p>
<p>db.py:</p>
<pre><code>def get_all_transactions_by_user(user_id):
    # fetch transactions from SQLite
    pass
</code></pre>
<p>export.py:</p>
<pre><code>from db import get_all_transactions_by_user

transactions = get_all_transactions_by_user(current_user_id)
</code></pre>
",0,0,0,2025-09-24T06:44:24+00:00,1,132,False
79774762,31569631,,sqlite,"With SQLite, where is the as keyword optional, and what difference does adding it make, if any?","<p>Currently I am learning Python programming for manipulating SQLite database, and I am a bit confused with the following SQL statement which works.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * 
FROM student s, result r
WHERE s.studentid = r.studentid
</code></pre>
<p>When we give an alias to a table, we're taught to use the '<strong>as</strong>' keyword before an alias, and the above SQL statement should be like this one:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * 
FROM student as s, result as r
WHERE s.studentid = r.studentid
</code></pre>
<ul>
<li>Is the above SQL statement also correct?</li>
<li>What situation don't I need to add the keyboard '<strong>as</strong>'?</li>
<li>Adding the keyword 'as' depends on our preferences? We can choose to add it or omit it randomly?</li>
</ul>
",0,4,4,2025-09-25T11:36:26+00:00,2,148,True
79779530,5527374,,sqlite,Escaping weird path names with sqlite json_extract,"<p>I'm trying to figure out how to escape weird field values in a json_extract expression. Here's my table:</p>
<pre class=""lang-sql prettyprint-override""><code>create table records (
  record_pk text primary key,
  config json not null default '{}',

  check (
    json_valid(config) and
    json_type(config) = 'object'
  )
);
</code></pre>
<p>Note that the config field is always a JSON hash. Here's the value for config in one of the records:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;Joe O'Toole&quot;: {
    &quot;said \&quot;whatever\&quot; dude&quot;: {
      &quot;val u&quot;: true
    }
  }
}
</code></pre>
<p>Note that I purposefully used weird field names to test for edge cases. I can't figure out how to query for that record. The query must exactly query for this path:</p>
<pre class=""lang-none prettyprint-override""><code>Joe O'Toole
   said &quot;whatever&quot; dude
      val u
        true
</code></pre>
<p>Here's a query that doesn't work:</p>
<pre class=""lang-sql prettyprint-override""><code>select *
from records
where
  json_extract(
    config,
    '$.&quot;Joe O''Toole&quot;.&quot;said &quot;&quot;whatever&quot;&quot; dude&quot;.&quot;val u&quot;'
  ) is not null
</code></pre>
<p>How should that query be modified to find the record?</p>
<p>Here's my system information:</p>
<pre class=""lang-none prettyprint-override""><code>---- Ubuntu
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.5 LTS
Release:        22.04
Codename:       jammy
---- sqlite3
3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1
</code></pre>
",0,0,0,2025-09-30T21:33:41+00:00,1,68,False
79779601,29872478,,sqlite,how to avoid error loading lsqlite3 module for Love2D?,"<p>Important edit: it think that the issue might come from the fact that I'm using a Mac that uses an Arm64 processor. However, I'm not entirely sure since I don't know what that means.</p>
<p>I'm trying to use lua sqlite within love2D. I'm not sure how exactly to phrase this, but I should clarify that I'm trying to make it work specifically with love2d so that I can distribute it without anyone needing to install everything on their end. But i was led to <a href=""https://github.com/CentauriSoldier/SQLite3-for-Lua"" rel=""nofollow noreferrer"">this file</a> by the <a href=""https://love2d.org/wiki/SQLite3"" rel=""nofollow noreferrer"">official website</a>. I followed the instructions and copied it into my project folder, required it with <code>local sqlite3 = require(&quot;sqlite3&quot;)</code> (this is the main.lua:3 referenced in the error message) only for it to give this error message when attempting to load love:</p>
<pre><code>Error

error loading module 'lsqlite3' from file 'Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so':
dlopen(Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so, 0x0006): tried: 'Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OSUsers/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (no such file), '/Applications/love.app/Contents/MacOS/../Frameworks/Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (no such file), '/Applications/love.app/Contents/Frameworks/Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (no such file), '/usr/lib/Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (no such file, not in dyld cache), 'Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64e' or 'arm64')), '/Users/(name)/Documents/code/luasqlite test/Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/(name)/Documents/code/luasqlite test/Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (no such file), '/Users/(name)/Documents/code/luasqlite test/Users/(name)/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so' (fat file, but missing compatible architecture (have 'i386,x86_64', need 'arm64e' or 'arm64'))


Traceback

[love &quot;callbacks.lua&quot;]:228: in function 'handler'
[C]: at 0x01051ef318
[C]: in function 'require'
sqlite3.lua:279: in function 'init'
sqlite3.lua:1032: in main chunk
[C]: in function 'require'
main.lua:3: in main chunk
[C]: in function 'require'
[C]: in function 'xpcall'
[C]: in function 'xpcall'
</code></pre>
<p>At this point, all that I have in my main.lua file is</p>
<pre><code>local love = require &quot;love&quot;

local sqlite3 = require(&quot;sqlite3&quot;)

local db = sqlite3.open(&quot;Normal_Translations.db&quot;)
</code></pre>
<p>so I have to assume it's an issue with requiring sqlite3</p>
<p>This is my first time trying to do something like this, so I'm unsure what it means or how I can fix it. What can I do to prevent this issue from happening?</p>
<p>looking at the error message leads me to these lines from the lsqlite3.lua file:</p>
<pre><code>--require the dll/so
sqlite3 = require(&quot;lsqlite3&quot;);
</code></pre>
<p>and</p>
<pre><code>init();
</code></pre>
<p>which just seems to call the function the first line was within</p>
",1,1,0,2025-10-01T01:39:43+00:00,0,104,False
79780092,3544438,Far far and away,sqlite,Should I disallow backups of sqlite to the same physical media?,"<p>Looking at the Sqlite backup options
<a href=""https://oldmoe.blog/2024/04/30/backup-strategies-for-sqlite-in-production/"" rel=""nofollow noreferrer"">https://oldmoe.blog/2024/04/30/backup-strategies-for-sqlite-in-production/</a></p>
<p>I have a question that is orthogonal to the backup method used:
suppose on Windows I allow user to enter backup destination,
is it a good practice to check if the root volume and the backup volume reside on physically different media? Using some helper like</p>
<pre><code>using System;
using System.Runtime.InteropServices;

class VolumeChecker
{
    [DllImport(&quot;kernel32.dll&quot;, SetLastError = true)]
    static extern bool GetVolumeInformation(
        string lpRootPathName,
        System.Text.StringBuilder lpVolumeNameBuffer,
        int nVolumeNameSize,
        out uint lpVolumeSerialNumber,
        out uint lpMaximumComponentLength,
        out uint lpFileSystemFlags,
        System.Text.StringBuilder lpFileSystemNameBuffer,
        int nFileSystemNameSize);

    public static bool AreOnSameVolume(string path1, string path2)
    {
        string root1 = System.IO.Path.GetPathRoot(System.IO.Path.GetFullPath(path1));
        string root2 = System.IO.Path.GetPathRoot(System.IO.Path.GetFullPath(path2));

        GetVolumeInformation(root1, null, 0, out uint serial1, out _, out _, null, 0);
        GetVolumeInformation(root2, null, 0, out uint serial2, out _, out _, null, 0);

        return serial1 == serial2;
    }
}
</code></pre>
<p>or option #2: just warn that it's not smart to have the backup on the local disk
or option #3: do not check and hang them as much rope as they desire?</p>
",0,0,0,2025-10-01T13:04:51+00:00,1,46,True
79780200,29872478,,sqlite,Error loading lsqlite3 module for love2D due to &quot;symbol not being found&quot;,"<p>Follow-up to <a href=""https://stackoverflow.com/questions/79779601/how-to-avoid-error-loading-lsqlite3-module-for-love2d"">this question</a></p>
<p>My current project folder includes the main.lua file, a .db file, , a sqlite.lua file from <a href=""https://github.com/CentauriSoldier/SQLite3-for-Lua/tree/master"" rel=""nofollow noreferrer"">this somewhat old page</a>, and a lsqlite3.so file copied from my homebrew folder. After copying the .so file into my project folder instead of the one created by the sqlite.lua file, that fixed the original issue I had, but made me run into this error instead:</p>
<pre><code>Error

error loading module 'lsqlite3' from file 'Users/name/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so':
dlopen(Users/name/Library/Application Support/LOVE/luasqlite test/plugins/sqlite3/lsqlite3.so, 0x0006): symbol not found in flat namespace '_lua_isinteger'


Traceback

[love &quot;callbacks.lua&quot;]:228: in function 'handler'
[C]: at 0x010039f318
[C]: in function 'require'
sqlite3.lua:279: in function 'init'
sqlite3.lua:1032: in main chunk
[C]: in function 'require'
main.lua:3: in main chunk
[C]: in function 'require'
[C]: in function 'xpcall'
[C]: in function 'xpcall'
</code></pre>
<p>To restate the bigger issue from the previous question, I'm trying to add some form of lua sqlite that works with Love2D (and on Mac). What might have caused the error here and what, if anything, can I do about it?</p>
",0,0,0,2025-10-01T14:46:37+00:00,0,86,False
79780555,12741306,,sqlite,Issue with StoreKit 2 In App Purchases: &quot;unfinalized statements / unfinished backups&quot; SQLite Crash,"<p>I’m running into a persistent error while implementing <strong>StoreKit 2</strong> renewable subscriptions in my <strong>SwiftUI</strong> app.</p>
<p><strong><strong>Context</strong></strong></p>
<p>I have a two-screen flow:</p>
<ul>
<li><strong>Screen 1</strong>: user selects a subscription plan (monthly / yearly).</li>
<li><strong>Screen 2</strong>: user fills out personal information and taps <strong>Subscribe</strong> that triggers the purchase function.</li>
</ul>
<p>On first launch or the first couple of purchases (on both StoreKit's <strong>local testing</strong> and <strong>Sandbox testing</strong>), everything works fine. The App Store popup appears, the purchase goes through, and I get the transaction result.</p>
<p>But after a few runs (<strong>3rd or 4th purchase attempt onward</strong>), my app crashes the moment the App Store purchase popup appears.</p>
<p><strong><strong>Error Logs</strong></strong></p>
<p>When the crash happens, the console shows:</p>
<pre><code>unable to close due to unfinalized statements or unfinished backups
BUG IN CLIENT OF libsqlite3.dylib: database integrity compromised by API violation: vnode unlinked while in use: /private/var/mobile/Containers/Data/Application/D8D97A11-DF06-4EF2-AC55-138C4739A167/Library/d6d2e85a60f0480c4c17834eeb827a14_MPDB.sqlite
invalidated open fd: 21 (0x11)
BUG IN CLIENT OF libsqlite3.dylib: database integrity compromised by API violation: vnode unlinked while in use: ...
</code></pre>
<p><strong><strong>Observations</strong></strong></p>
<ul>
<li>The error only shows <strong>after some time</strong>, maybe due to multiple transactions and switching between plans for the same user, not on the very first purchases.</li>
<li>If I land on the purchase screen <strong>immediately after app launch</strong>, the purchase works. But if I wait a while before navigating to the purchase screen, the popup causes the app to crash.</li>
<li>I’m <strong>not using Core Data</strong> or my own SQLite database at all, so I assume this DB (<strong>MPDB.sqlite</strong>) is StoreKit’s internal persistence.</li>
</ul>
<p><strong><strong>Things I’ve tried so far</strong></strong></p>
<ul>
<li><strong>Cleaning StoreKit caches:</strong></li>
</ul>
<pre><code>rm -rf ~/Library/Developer/CoreSimulator/Devices/&lt;device_id&gt;/data/Containers/Data/Application/&lt;app_id&gt;/Library/Caches/storekit
</code></pre>
<ul>
<li><strong>Rebuilding from scratch</strong>, cleaning build folder.</li>
<li><strong>Switching between sandbox accounts</strong>, signing out/in again.</li>
<li><strong>Added</strong> <code>await transaction.finish()</code> after verified purchases.</li>
<li><strong>Added cleanup for unfinished transactions</strong> at app launch:</li>
</ul>
<pre class=""lang-swift prettyprint-override""><code>for await result in Transaction.unfinished {
if case .verified(let transaction) = result {
await transaction.finish()
}
}
</code></pre>
<ul>
<li>Tried both <strong>StoreKit Configuration file</strong> and <strong>Sandbox environment</strong>, but issue persists in both.</li>
</ul>
<p><strong><strong>Question</strong></strong></p>
<ul>
<li>Is this error <strong>StoreKit-specific</strong> (internal SQLite DB corruption) or something wrong in my implementation?</li>
<li>Why would it only appear <strong>after a few runs</strong> / with a <strong>delay before navigating</strong> to the purchase screen?</li>
<li>Am I missing something else in handling <strong>StoreKit 2 transactions</strong>?</li>
</ul>
<p><em>Screenshots of the errors are attached for context.</em></p>
<p>Any insights would be really appreciated... I’m stuck because I can’t tell if this is an <strong>Apple bug with StoreKit 2</strong> or something I’ve overlooked in my code.</p>
<p><strong><strong>Specs</strong></strong></p>
<ul>
<li><strong>Xcode</strong>: 16.4</li>
<li><strong>Build version</strong>: 16F6</li>
<li><strong>iOS version</strong>: 18.6.2</li>
<li>List item</li>
</ul>
<p><a href=""https://i.sstatic.net/xp7mHPiI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xp7mHPiI.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/CCpFngrk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CCpFngrk.png"" alt=""enter image description here"" /></a></p>
",0,0,0,2025-10-02T00:20:51+00:00,0,432,False
79784972,9901882,,sqlite,Calculate Duration in SQLite from M/D/YYYY H:MM:SS Text and Format as h:mm:ss,"<p>I have an SQLite table named trip where the started_at and ended_at times are stored as TEXT in a non-standard M/D/YYYY H:MM:SS (or HH:MM:SS) format. I need to write an SQLite query to calculate the duration and UPDATE the duration column.</p>
<p>The final format for the duration needs to be h:mm:ss (for hours 0-9) or hh:mm:ss (for hours 10+). The minute and second parts should always have two digits with a leading zero if necessary.</p>
<p>Sample of rows with duration (first 10):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>started_at</th>
<th>ended_at</th>
<th>duration</th>
</tr>
</thead>
<tbody>
<tr>
<td>1613335</td>
<td>5/21/2019 18:33</td>
<td>5/21/2019 18:40</td>
<td>0:07:03</td>
</tr>
<tr>
<td>1613639</td>
<td>5/21/2019 19:07</td>
<td>5/21/2019 19:12</td>
<td>0:04:57</td>
</tr>
<tr>
<td>1613708</td>
<td>5/21/2019 19:13</td>
<td>5/21/2019 19:15</td>
<td>0:01:14</td>
</tr>
<tr>
<td>1613867</td>
<td>5/21/2019 19:29</td>
<td>5/21/2019 19:36</td>
<td>0:06:58</td>
</tr>
<tr>
<td>1636714</td>
<td>5/24/2019 13:38</td>
<td>5/24/2019 13:41</td>
<td>0:03:06</td>
</tr>
<tr>
<td>1636780</td>
<td>5/24/2019 13:52</td>
<td>5/24/2019 14:13</td>
<td>0:21:27</td>
</tr>
<tr>
<td>1636856</td>
<td>5/24/2019 14:04</td>
<td>5/24/2019 14:34</td>
<td>0:30:43</td>
</tr>
<tr>
<td>1636912</td>
<td>5/24/2019 14:14</td>
<td>5/24/2019 14:15</td>
<td>0:01:05</td>
</tr>
<tr>
<td>1637035</td>
<td>5/24/2019 14:37</td>
<td>5/24/2019 14:43</td>
<td>0:05:33</td>
</tr>
<tr>
<td>1637036</td>
<td>5/24/2019 14:38</td>
<td>5/24/2019 15:09</td>
<td>0:31:54</td>
</tr>
</tbody>
</table></div>
<p>Sample of rows with empty duration (first 10):</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>started_at</th>
<th>ended_at</th>
<th>duration</th>
</tr>
</thead>
<tbody>
<tr>
<td>1821126</td>
<td>6/7/2019 2:17</td>
<td>6/7/2019 2:42</td>
<td></td>
</tr>
<tr>
<td>1821158</td>
<td>6/7/2019 2:20</td>
<td>6/7/2019 2:50</td>
<td></td>
</tr>
<tr>
<td>1821204</td>
<td>6/7/2019 2:23</td>
<td>6/7/2019 2:38</td>
<td></td>
</tr>
<tr>
<td>1821289</td>
<td>6/7/2019 2:30</td>
<td>6/7/2019 2:45</td>
<td></td>
</tr>
<tr>
<td>1821386</td>
<td>6/7/2019 2:40</td>
<td>6/7/2019 2:44</td>
<td></td>
</tr>
<tr>
<td>1821487</td>
<td>6/7/2019 2:49</td>
<td>6/7/2019 2:49</td>
<td></td>
</tr>
<tr>
<td>1821575</td>
<td>6/7/2019 2:58</td>
<td>6/7/2019 4:08</td>
<td></td>
</tr>
<tr>
<td>1823055</td>
<td>6/7/2019 13:30</td>
<td>6/7/2019 13:34</td>
<td></td>
</tr>
<tr>
<td>1823201</td>
<td>6/7/2019 13:57</td>
<td>6/7/2019 14:00</td>
<td></td>
</tr>
<tr>
<td>1823376</td>
<td>6/7/2019 14:28</td>
<td>6/7/2019 15:08</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>I tried something like:</p>
<pre><code>        UPDATE trip
        SET duration = 
            CASE 
                WHEN TRIM(started_at) &lt;&gt; '' AND TRIM(ended_at) &lt;&gt; '' THEN 
                    printf('%d:%02d:%02d', 
                        (julianday(ended_at) - julianday(started_at)) * 24, 
                        ((julianday(ended_at) - julianday(started_at)) * 24 * 60) % 60, 
                        ((julianday(ended_at) - julianday(started_at)) * 24 * 60 * 60) % 60)
                ELSE duration
            END
        WHERE TRIM(duration) = '';
</code></pre>
<p>but I am getting 00:00:00 for all cases</p>
",-4,0,4,2025-10-07T22:40:30+00:00,1,98,False
79787110,4627124,,sqlite,Referencing a non-existing column in a CTE,"<p>Referencing a non-existing column in a CTE does not give an error in sqlite3, even if it looks like that the CTE is evaluated in answer calculation.</p>
<p>Take this example:</p>
<pre><code>CREATE TABLE account (
    account_id INTEGER PRIMARY KEY,
    date DATE NOT NULL
);

CREATE TABLE disp (
    disp_id INTEGER PRIMARY KEY,
    client_id INTEGER NOT NULL,
    account_id INTEGER NOT NULL,
    FOREIGN KEY (account_id) REFERENCES account(account_id)
);

CREATE TABLE loan (
    loan_id INTEGER PRIMARY KEY,
    account_id INTEGER NOT NULL,
    amount INTEGER NOT NULL,
    FOREIGN KEY (account_id) REFERENCES account(account_id)
);

INSERT INTO account (account_id, date) VALUES (1, '950101');
INSERT INTO disp (disp_id, client_id, account_id) VALUES (1, 100, 1);
INSERT INTO loan (loan_id, account_id, amount) VALUES (1, 1, 50000);
</code></pre>
<p>and</p>
<pre><code>WITH biggest_loan AS (
    SELECT client_id FROM loan ORDER BY amount DESC LIMIT 1
)
SELECT account_id, date 
FROM account 
WHERE account_id IN (
    SELECT account_id 
    FROM disp 
    WHERE client_id = (SELECT client_id FROM biggest_loan)
);
</code></pre>
<p>give 1|950101. This query should fail, because loan.client_id does not exist. Can you explain what is happening ?</p>
<p>I am using SQLite 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1</p>
",2,2,0,2025-10-10T07:58:07+00:00,0,67,False
79792725,30306188,,sqlite,Why does my message not appear on the page even though the server log shows it was sent successfully in CivetWeb + SQLite?,"<p>I’m building a local web chat feature using <strong>CivetWeb (C++)</strong>, <strong>SQLite</strong>, and a frontend in plain HTML/JS.
When a message is sent, the server log shows:</p>
<pre><code>Attempting to send message: product_id=3, sender=1, receiver=Admin, message=1
</code></pre>
<p>So the <code>/send_message</code> endpoint is triggered correctly.
However, the new message does <strong>not appear on the chat page</strong> (<code>message_feature.html</code>), even though I can see it was inserted in the log.</p>
<p>Here’s what I have:</p>
<h4><strong>Frontend (message_feature.html)</strong></h4>
<pre class=""lang-js prettyprint-override""><code>async function loadConversation() {
  const res = await fetch(`/api/conversation?product_id=${productId}`);
  if (!res.ok) return;
  const msgs = await res.json();
  const box = document.getElementById('messages');
  box.innerHTML = '';
  msgs.sort((a, b) =&gt; new Date(a.created_at) - new Date(b.created_at));
  msgs.forEach(m =&gt; {
    const div = document.createElement('div');
    div.className = 'msg ' + (m.sender === 'Admin' ? 'received' : 'sent');
    div.innerHTML = `&lt;div&gt;${m.message}&lt;/div&gt;`;
    box.appendChild(div);
  });
}
</code></pre>
<h4><strong>Backend (C++ with CivetWeb)</strong></h4>
<pre class=""lang-cpp prettyprint-override""><code>// POST /send_message
mg_set_request_handler(ctx, &quot;/send_message&quot;, [](mg_connection *conn, void *) -&gt; int {
    if (strcmp(mg_get_request_info(conn)-&gt;request_method, &quot;POST&quot;) != 0)
        return 405;

    const struct mg_request_info *ri = mg_get_request_info(conn);
    int len = (int)ri-&gt;content_length;
    if (len &lt;= 0 || len &gt; 4096) len = 4096;
    std::vector&lt;char&gt; post_data(len + 1);
    int n = mg_read(conn, post_data.data(), len);
    post_data[n] = '\0';

    char product_id_str[32] = {0}, sender[128] = {0}, receiver[128] = {0}, message[1024] = {0};
    mg_get_var(post_data.data(), n, &quot;product_id&quot;, product_id_str, sizeof(product_id_str));
    mg_get_var(post_data.data(), n, &quot;sender&quot;, sender, sizeof(sender));
    mg_get_var(post_data.data(), n, &quot;receiver&quot;, receiver, sizeof(receiver));
    mg_get_var(post_data.data(), n, &quot;message&quot;, message, sizeof(message));

    fprintf(stderr, &quot;Attempting to send message: product_id=%s, sender=%s, receiver=%s, message=%s\n&quot;,
            product_id_str, sender, receiver, message);

    sqlite3 *db = safe_open(&quot;C:/Users/priva/OneDrive/Desktop/supplierbuyer/products.db&quot;);
    const char *sql = &quot;INSERT INTO messages (product_id, sender, receiver, message) VALUES (?, ?, ?, ?);&quot;;
    sqlite3_stmt *stmt = nullptr;
    sqlite3_prepare_v2(db, sql, -1, &amp;stmt, nullptr);
    sqlite3_bind_int(stmt, 1, atoi(product_id_str));
    sqlite3_bind_text(stmt, 2, sender, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 3, receiver, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 4, message, -1, SQLITE_TRANSIENT);
    sqlite3_step(stmt);
    sqlite3_finalize(stmt);
    sqlite3_close(db);

    mg_printf(conn, &quot;HTTP/1.1 201 Created\r\nContent-Length: 0\r\n\r\n&quot;);
    return 201;
}, nullptr);

// GET /api/conversation
mg_set_request_handler(ctx, &quot;/api/conversation&quot;, [](mg_connection *conn, void *) -&gt; int {
    const mg_request_info *ri = mg_get_request_info(conn);
    char product_id_str[32] = {0};
    if (ri-&gt;query_string)
        mg_get_var(ri-&gt;query_string, strlen(ri-&gt;query_string), &quot;product_id&quot;, product_id_str, sizeof(product_id_str));
    int product_id = atoi(product_id_str);

    sqlite3 *db = safe_open(&quot;C:/Users/priva/OneDrive/Desktop/supplierbuyer/products.db&quot;);
    const char *sql = R&quot;(
        SELECT sender, receiver, message, strftime('%Y-%m-%d %H:%M:%S', created_at)
        FROM messages WHERE product_id = ? ORDER BY id ASC;
    )&quot;;
    sqlite3_stmt *stmt = nullptr;
    sqlite3_prepare_v2(db, sql, -1, &amp;stmt, nullptr);
    sqlite3_bind_int(stmt, 1, product_id);

    std::string json = &quot;[&quot;;
    bool first = true;
    while (sqlite3_step(stmt) == SQLITE_ROW) {
        if (!first) json += &quot;,&quot;;
        first = false;
        const char *sender = (const char *)sqlite3_column_text(stmt, 0);
        const char *receiver = (const char *)sqlite3_column_text(stmt, 1);
        const char *message = (const char *)sqlite3_column_text(stmt, 2);
        const char *created_at = (const char *)sqlite3_column_text(stmt, 3);
        char buf[512];
        snprintf(buf, sizeof(buf),
            R&quot;({&quot;sender&quot;:&quot;%s&quot;,&quot;receiver&quot;:&quot;%s&quot;,&quot;message&quot;:&quot;%s&quot;,&quot;created_at&quot;:&quot;%s&quot;})&quot;,
            sender, receiver, message, created_at);
        json += buf;
    }
    json += &quot;]&quot;;
    sqlite3_finalize(stmt);
    sqlite3_close(db);

    mg_printf(conn, &quot;HTTP/1.1 200 OK\r\nContent-Type: application/json\r\n\r\n%s&quot;, json.c_str());
    return 200;
}, nullptr);

</code></pre>
<p>Messages are saved in the same <code>products.db</code> file under a <code>messages</code> table.
I also checked that <code>loadConversation()</code> runs after sending a message, but nothing shows in the chat box.</p>
<p>Here’s the relevant part of my message_feature.html code:</p>
<pre><code>document.getElementById('chatForm').addEventListener('submit', async e =&gt; {
  e.preventDefault();
  
  const fd = new FormData(e.target);
  const userRes = await fetch('/api/get_current_user');
  const { username } = await userRes.json();
  fd.append('sender', username);

  const body = new URLSearchParams(fd);

  // Send the message to the backend
  const res = await fetch('/send_message', { 
    method: 'POST',
    headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
    body: body
  });

  // I wait for the response before refreshing messages
  if (res.ok) {
    document.getElementById('message').value = '';
    loadConversation();  // ✅ Called right after successful response
  } else {
    alert('Failed to send message.');
  }
});
</code></pre>
<p>So <code>loadConversation()</code> is only called after the POST request returns successfully (status 201).
However, even then, the chat box doesn’t update — it still shows no messages, even though <code>/send_message</code> logs the insert on the server.</p>
<p>I also tried adding a small delay before calling <code>loadConversation()</code> (using <code>setTimeout(loadConversation, 1000)</code>), but it made no difference — the messages still don’t show up.</p>
<hr />
<h3><strong>Question:</strong></h3>
<p>Even though messages are inserted (the log shows they’re sent), they don’t appear when fetched from <code>/api/conversation</code>.
What could cause the frontend to not display them — is it a problem with the query, JSON response, or DOM rendering?</p>
<hr />
<h3><strong>What I Tried</strong></h3>
<ul>
<li><p>I verified that the <code>/send_message</code> endpoint works — it logs</p>
<pre><code>Attempting to send message: product_id=3, sender=1, receiver=Admin, message=1
</code></pre>
<p>every time I click <strong>Send</strong>.</p>
</li>
<li><p>I checked that the <strong>message is inserted</strong> into the SQLite database (<code>messages</code> table) manually using DB Browser.</p>
</li>
<li><p>I confirmed that <code>/api/conversation?product_id=3</code> returns a JSON array, but sometimes it’s <strong>empty</strong> or missing the new message.</p>
</li>
<li><p>I also added <code>console.log(msgs)</code> inside <code>loadConversation()</code> in <code>message_feature.html</code> — the console shows an empty array <code>[]</code> even though the message exists in the DB.</p>
</li>
<li><p>I tried calling <code>loadConversation()</code> immediately after sending (and also with a 3-second interval), but it still doesn’t update the chat box.</p>
</li>
</ul>
<h3><strong>What I Expected</strong></h3>
<p>I expected that after sending a message, the new message would:</p>
<ol>
<li>Be inserted into the <code>messages</code> table.</li>
<li>Appear immediately in the chat box under the correct side (<code>sent</code> or <code>received</code>).</li>
<li>Also appear in the admin view (<code>messageadmin.html</code>) after a few seconds (since it refreshes automatically).</li>
</ol>
<hr />
<p><strong>Full project source code:</strong><br />
👉 <a href=""https://github.com/misbagas/supplierbuyer"" rel=""nofollow noreferrer"">https://github.com/misbagas/supplierbuyer</a></p>
",0,2,2,2025-10-17T03:49:35+00:00,0,104,False
79797190,25560525,,sqlite,SQLite query to find a result using two tables which needs a match in two rows of one table,"<p>I have an SQLite database - and to simplify, it has two tables:-
<code>recipe</code> and <code>ingredients</code>.</p>
<p>The recipe database is comprised of</p>
<pre><code>recipeId (autoincrement, primary index)
title
</code></pre>
<p>and the ingredients:-</p>
<pre><code>id (autoincrement, primary index)
recipeId (relating to the recipeId in the recipe table)
ingredient
unit (e.g.pounds or litres)
amount (the quantity)
</code></pre>
<p>I am using the sqlite database browser in Ubuntu to allow me to craft and execute the scripts.</p>
<p>An example of the recipe table may contain the following data:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>recipeId</th>
<th>title</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Slime cake</td>
</tr>
<tr>
<td>2</td>
<td>Giraffe biscuits</td>
</tr>
<tr>
<td>3</td>
<td>Toad toasties</td>
</tr>
</tbody>
</table></div>
<p>and the ingredients:-</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>recipeId</th>
<th>ingredient</th>
<th>unit</th>
<th>amount</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>water</td>
<td>pint</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>porridge</td>
<td>g</td>
<td>100</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>egg</td>
<td></td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>2</td>
<td>egg</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>flour</td>
<td>g</td>
<td>100</td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>flour</td>
<td>g</td>
<td>150</td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>bread</td>
<td>slice</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>I am seeking e.g. to find the recipe title where the recipe requires 2 eggs and some water.
I tried things like:</p>
<pre><code>select title from recipe
   join ingredient on recipe.recipeId= ingredient.recipeId 
   where 
      (ingredient.ingredient = &quot;egg&quot; and ingredient.amount = 2) or ingredient.ingredient = &quot;water&quot; ;
</code></pre>
<p>That gives me any recipe where either it has 2 eggs or water, but it would be nice to see one row in the results, not two!</p>
<p>I cannot see how nested select statements would help - but my knowledge of SQLite ... is very limited, so any guidance would be appreciated, as I couldn't find an answer online (which means I most likely haven't phrased my questions adequately!).
Many thanks for your help.</p>
<p>Sadly, I was unable to leave a comment to @Guillaume Outters but, it worked, which I was most grateful for, but there was one anomaly.</p>
<p>I saw one result that had the recipe listed twice, and assuming I had two copies of the recipe, I listed the recipeId as well, but the duplicate was a genuine duplicate.</p>
<p>When I looked at the recipe, in the one recipe, it requires eggs three times (!!), but each time, with a different quantity - so it still should only have picked it up once.</p>
<p>It isn't a problem, in terms of what I was trying to achieve, but I remain puzzled as to why.</p>
<p>My thanks again!</p>
",2,2,0,2025-10-22T20:00:20+00:00,2,95,True
79797245,9186370,,sqlite,Laravel PHPUnit test: Multiple databases issue,"<p>In my Laravel application, I have set up two databases in a single server. All specific migrations and model classes have <code>$connection</code> property set up to use <code>blog</code> instance. The app works fine, no rollback issue on migrations.</p>
<p>Here is my <code>.env</code> file:</p>
<pre class=""lang-none prettyprint-override""><code>DB_CONNECTION=mysql
DB_HOST=db
DB_PORT=3306
DB_DATABASE=laravel
DB_USERNAME=laravel
DB_PASSWORD=laravel
DB_CHARSET=utf8mb4
DB_COLLATION=utf8mb4_unicode_ci

DB_BLOG_CONNECTION=mysql
DB_BLOG_HOST=db
DB_BLOG_PORT=3306
DB_BLOG_DATABASE=blog
DB_BLOG_USERNAME=laravel
DB_BLOG_PASSWORD=laravel
DB_BLOG_CHARSET=utf8mb4
DB_BLOG_COLLATION=utf8mb4_unicode_ci
</code></pre>
<p>Here is my <code>config/database.php</code> file:</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php

return [
    'default' =&gt; env('DB_CONNECTION', 'sqlite'),
    'connections' =&gt; [
        'sqlite' =&gt; [
            // not touched
        ],
        'mysql' =&gt; [
            // not touched
        ],
        'blog' =&gt; [
            // copied from the 'mysql'
            // changed 'driver':
            'driver' =&gt; env('DB_BLOG_CONNECTION', 'mysql'),
        ],
];
</code></pre>
<p>Now, I want to write some tests for the blog feature. Here is the <code>BlogTest.php</code> file:</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php

namespace Tests\Feature;

use App\Models\Blog;
use Illuminate\Foundation\Testing\RefreshDatabase;
use Tests\TestCase;

class BlogTest extends TestCase
{
    use RefreshDatabase;

    public function test_basic_structure_of_blogs_list(): void
    {
        Blog::factory()
            -&gt;withCategories(3)
            -&gt;count(3)
            -&gt;create();

        $this-&gt;getJson('/api/blog')
            -&gt;assertOk();
    }
}
</code></pre>
<p>Here is my <code>phpunit.xml</code> file:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;env name=&quot;DB_CONNECTION&quot; value=&quot;sqlite&quot;/&gt;
&lt;env name=&quot;DB_DATABASE&quot; value=&quot;:memory:&quot;/&gt;
&lt;env name=&quot;DB_BLOG_CONNECTION&quot; value=&quot;sqlite&quot;/&gt;
&lt;env name=&quot;DB_BLOG_DATABASE&quot; value=&quot;:memory:&quot;/&gt;
</code></pre>
<p>Whenever I try to run only that specific test (<code>php artisan test --filter BlogTest</code>) it works. But when I try to run all tests at once (<code>php artisan test --compact</code>) then it fails:</p>
<pre><code>FAILED  Tests\Feature\BlogTest &gt; basic structure of blogs list                                                                                                       QueryException   
  SQLSTATE[HY000]: General error: 1 no such table: blogs (Connection: blog, SQL: insert into &quot;blogs&quot; ...
</code></pre>
<p>But when I overwrite <code>setUp()</code> method to this</p>
<pre class=""lang-php prettyprint-override""><code>protected function setUp(): void
    {
        parent::setUp();

        $this-&gt;artisan('migrate:rollback');
        $this-&gt;artisan('migrate');

        // or only this also works
        $this-&gt;artisan('migrate:refresh');
    }
</code></pre>
<p>then it works.</p>
<p>My question is why <code>RefreshDatabase</code> trait doesn't take care of running all the migrations properly or it is normal to fail when you use multiple databases?<br />
Or even is it possible that I use in-memory SQLite for both of them and it causes an issue?</p>
",0,0,0,2025-10-22T21:41:25+00:00,0,140,False
79797328,7867195,"Limerick, Ireland",sqlite,aiosqlite and managing transactions,"<p>aiosqlite's own documentation is extremely minimal so I would appreciate help from  experts.</p>
<p>I have a chat client project and the backend is in Python/FastAPI, with all the relevant calls being <code>async def</code> and subsequent calls between modules all using async/await.</p>
<p>I need to use sqlite as the storage backend, necessitating the use of aiosqlite.</p>
<p>I need to ensure that all write transactions are serialized; sqlite, as far as I understand, does not do parallel writing properly. So if one coroutine (as I understand they're not exactly threads with async?) is writing, the other will have to wait until that writing is complete. (&quot;Does not scale&quot; does not matter, I'll do a Postgres version later). Or does sqlite actually do parallel independent write transactions fine now?</p>
<p>And, of course, I also need to ensure atomic write transactions, so that if something fails the transaction is rolled back.</p>
<p>Just how do I do this with aiosqlite? Create a new connection for every transaction, or keep a shared connection? Call some explicit &quot;begin&quot; before starting the writes, or not? Commit/rollback explicitly or expect automatic commit on leaving the scope/rollback on unhandles exception?</p>
",0,0,0,2025-10-23T01:21:15+00:00,1,137,False
79798310,268581,,sqlite,sqlite3 shell on Windows : extra carriage returns when using output redirection,"<h1>Demonstrate the issue</h1>
<p>Let's use <code>nvim</code> to create a text file:</p>
<pre><code>&gt; nvim C:\temp\in-file.txt
</code></pre>
<p>The contents of the text file:</p>
<pre><code>&gt; Get-Content C:\temp\in-file.txt
abc
bcd
cde
</code></pre>
<p>Now let's create a simple database that can store this content:</p>
<pre><code>&gt; sqlite3 C:\temp\test-1.db
-- Loading resources from C:\Users\dharm/.sqliterc
SQLite version 3.50.4 2025-07-30 19:33:53
Enter &quot;.help&quot; for usage hints.
sqlite&gt; CREATE TABLE test_table (id INTEGER PRIMARY KEY, content TEXT NOT NULL);
</code></pre>
<p>Let's insert the text file content:</p>
<pre><code>sqlite&gt; INSERT INTO test_table (content) VALUES (readfile('c:\temp\in-file.txt'));
sqlite&gt; SELECT * FROM test_table;
+----+---------+
| id | content |
+----+---------+
| 1  | abc     |
|    | bcd     |
|    | cde     |
+----+---------+
</code></pre>
<p>OK, looks good.</p>
<p>Now let's use <code>sqlite3</code> to output the <code>content</code> to the console:</p>
<pre><code>&gt; sqlite3 C:\temp\test-1.db -cmd '.mode list' &quot;SELECT content FROM test_table WHERE id = 1;&quot;
-- Loading resources from C:\Users\dharm/.sqliterc
abc
bcd
cde

</code></pre>
<p>And now let's redirect that output to a file:</p>
<pre><code>&gt; sqlite3 C:\temp\test-1.db -cmd '.mode list' &quot;SELECT content FROM test_table WHERE id = 1;&quot; &gt; C:\temp\out-5.txt
-- Loading resources from C:\Users\dharm/.sqliterc
</code></pre>
<p>Note that the file has extra blank lines:</p>
<pre><code>&gt; Get-Content C:\temp\out-5.txt
abc

bcd

cde


</code></pre>
<h1>Two carriage returns per line</h1>
<p>The <code>Format-Hex</code> command shows that each line has <code>0D 0D 0A</code>. I.e. it has two carriage returns.</p>
<pre><code>&gt; Format-Hex C:\temp\out-5.txt

   Label: C:\temp\out-5.txt

          Offset Bytes                                           Ascii
                 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
          ------ ----------------------------------------------- -----
0000000000000000 61 62 63 0D 0D 0A 62 63 64 0D 0D 0A 63 64 65 0D abc���bcd���cde�
0000000000000010 0D 0A 0D 0A                                     ����
</code></pre>
<h1>Workaround: <code>writefile</code> in <code>sqlite3</code>:</h1>
<p>Now, I can get this to work if I use <code>writefile</code> in <code>sqlite3</code> instead of output redirection:</p>
<pre><code>&gt; sqlite3 C:\temp\test-1.db -cmd '.mode list' &quot;SELECT writefile('c:\temp\out-5.txt', content) FROM test_table WHERE id = 1;&quot;
-- Loading resources from C:\Users\dharm/.sqliterc
15
&gt; Get-Content C:\temp\out-5.txt
abc
bcd
cde
</code></pre>
<h1>Question</h1>
<p>Is there a way to get this to work using output redirection instead of <code>writefile</code> from <code>sqlite3</code>?</p>
<p>I can use the <code>writefile</code> approach. But I'm curious if there's a way to get the output redirection approach to work.</p>
",4,4,0,2025-10-24T00:45:05+00:00,2,135,True
79799862,8167524,"Moscow, Россия",sqlite,How to include a recursive query in the main query?,"<p>I have a database in SQLite with these tables and columns:</p>
<ul>
<li><code>Products</code>: <code>id</code>, <code>name</code>, <code>number</code>, <code>price</code>, <code>category_id</code>.</li>
<li><code>Orders</code>: <code>id</code>, <code>client_id</code>, <code>datetime</code>, <code>sum</code>.</li>
<li><code>OrderProducts</code>: <code>order_id</code>, <code>product_id</code>, <code>product_count</code>.</li>
<li><code>Categories</code>: <code>id</code>, <code>parent_id</code>, <code>name</code>.</li>
<li><code>Clients</code>: <code>id</code>, <code>name</code>, <code>address</code>.</li>
</ul>
<p>I need to get the &quot;Top 5 most purchased products over the past month&quot; (by number of units in orders).</p>
<p>The report should include <code>Product Name</code>, <code>Level 1 Category</code>, and <code>total number of units sold</code>.</p>
<p>I wrote this SQL code, it works fine:</p>
<pre><code>SELECT Products.Name, COUNT(*) * Product_count AS Result_count, Categories.Parent_id
FROM Orders
JOIN OrderProducts ON Orders.Id = OrderProducts.Order_id
JOIN Products ON OrderProducts.Product_id = Products.Id
JOIN Categories ON Products.Category_id = Categories.Id
WHERE Orders.Created_at &gt; DATETIME('now', '-30 day')
GROUP BY Product_id
ORDER BY Result_count DESC 
LIMIT 5;
</code></pre>
<p>There is a separate query for to get all parents:</p>
<pre><code>WITH RECURSIVE owners (Id, Parent_Id, Name, Number) AS
(
    SELECT Id, Parent_Id, Name, Number
    FROM Categories c
    WHERE c.Id  = 9

    UNION ALL 

    SELECT c1.Id, c1.Parent_Id, c1.Name, c1.Number
    FROM (owners o 
    JOIN Categories c1 ON (o.Parent_Id = c1.Id))
) 
SELECT * 
FROM owners;
</code></pre>
<p>How do I:</p>
<ol>
<li>Get the <strong>top level</strong> only parent from the recursive query?</li>
<li>Combine two blocks of code</li>
</ol>
",4,5,1,2025-10-26T06:26:32+00:00,1,145,True
79799995,31745104,,sqlite,CapacitorSQLite “isConnection() is not implemented on Android” in Ionic v8 + Capacitor 7,"<p>I’m developing an app using <strong>Ionic v8</strong> and <strong>Capacitor 7</strong>, and I’m trying to integrate the <a href=""https://github.com/capacitor-community/sqlite"" rel=""nofollow noreferrer""><code>@capacitor-community/sqlite</code></a> plugin.</p>
<p>However, when I run the app on an <strong>Android device</strong>, I get the following error:</p>
<pre><code>Error: CapacitorSQLite.isConnection() is not implemented on Android
</code></pre>
<hr />
<h3>What I’ve Tried</h3>
<ul>
<li><p>Installed the plugin:</p>
<pre><code>npm install @capacitor-community/sqlite

</code></pre>
</li>
<li><p>Synced with Capacitor:</p>
<pre><code>npx cap sync

</code></pre>
</li>
<li><p>Verified that the plugin appears in the sync output.</p>
</li>
<li><p>Rebuilt the Android project:</p>
<pre><code>cd android &amp;&amp; ./gradlew clean &amp;&amp; cd ..
npx cap sync android
npx cap open android

</code></pre>
</li>
</ul>
<hr />
<h3>Environment</h3>
<ul>
<li><p><strong>Ionic Framework:</strong> v8</p>
</li>
<li><p><strong>Capacitor:</strong> v7</p>
</li>
<li><p><strong>@capacitor-community/sqlite:</strong> latest version (e.g., 7.0.2)</p>
</li>
<li><p><strong>Platform:</strong> Android</p>
</li>
</ul>
",0,0,0,2025-10-26T10:52:39+00:00,0,32,False
79802903,20983123,,sqlite,Does sqlite ignore WHERE clause after ON CONFLICT of UPSERT?,"<p>This is a minimal example:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE t(x PRIMARY KEY,y);
INSERT INTO t VALUES (1,2);
INSERT INTO t VALUES (1,3)
    ON CONFLICT(x) WHERE 0 DO UPDATE SET y = 9
    ON CONFLICT DO UPDATE SET y = 10;
SELECT * FROM t;
</code></pre>
<p>The result is confusingly <code>1|9</code> where <code>1|10</code> is expected from my sense.</p>
<p><a href=""https://sqlite.org/lang_upsert.html"" rel=""nofollow noreferrer"">sqlite docs</a> seems did not mention this where clause at all, although it is in the diagram.</p>
",4,4,0,2025-10-28T14:06:38+00:00,2,113,True
79804286,7491096,,sqlite,How can I retrieve data between certain times?,"<p>My data decoded from a radio receiver is stored in tables <code>Flights</code> and <code>Aircraft</code> with a common field <code>AircraftID</code> to enable a <code>JOIN</code>. There are fields for when the radio signal starts and ends (<code>StartTime</code> and <code>Endtime</code>). To select data between two points in time and anything transmitted during that time, this query works:</p>
<pre><code>SELECT ModeS, Registration, Manufacturer, Type, SerialNo, RegisteredOwners, UserTag, StartTime, EndTime, Callsign, FirstSquawk, LastSquawk, FirstIsOnGround, LastIsOnGround, FirstLat, LastLat, FirstLon, LastLon, FirstGroundSpeed, LastGroundSpeed, FirstAltitude, LastAltitude
FROM Flights
INNER JOIN Aircraft ON Aircraft.AircraftID = Flights.AircraftID
WHERE StartTime &gt; '2025-10-24 23:00' AND EndTime &lt; '2025-10-26 01:00';
</code></pre>
<p>I need to replace the last line so that it becomes less specific and I don't have to manually change the date every day (it runs once a day in the morning, to report for 24 hours of yesterday's data + 1 hour before and 1 hour after).<br />
Pseudo-code:</p>
<pre><code>WHERE StartTime &gt; 23:00 the day before yesterday, until EndTime &lt; 01:00 today
</code></pre>
<p>Attempt based on an example I found:</p>
<pre><code>WHERE StartTime &gt; DATE('now','-25 hours') AND EndTime &lt; DATE('now','+1 hour);
</code></pre>
<p>It would be easier if there weren't two different time fields involved.</p>
",0,0,0,2025-10-29T23:03:41+00:00,1,128,True
79804625,4696802,,sqlite,How to ensure proper destruction order using RAII for SQLite database?,"<p>In my class which holds a SQLite database handle I have:</p>
<pre><code>struct SQLitePreparedStatement
    {
        sqlite3_stmt* stmt = nullptr;
        ~SQLitePreparedStatement()
        {
            if (stmt)
                sqlite3_finalize(stmt);
        }
    };
    SQLitePreparedStatement create_table_statement{};
    SQLitePreparedStatement exists_statement{};
    SQLitePreparedStatement insert_statement{};
    SQLitePreparedStatement insert_or_replace_statement{};
    SQLitePreparedStatement get_value_statement{};
    SQLitePreparedStatement remove_value_statement{};
    SQLitePreparedStatement count_rows_prepared_stmt{};
    SQLitePreparedStatement get_row_id_stmt{};
    sqlite3* sqlite_db;
</code></pre>
<p>Until now I have simply called sqlite3_close on the database in the destructor, without calling finalize on the statements. I think this is wrong, and that you have to call finalize on each statement associated with the database handle BEFORE calling close on the database. This is why I wrapped the statements in a RAII struct. But if I call close on the database in the destructor this will happen BEFORE the statements have been destroyed. Is the solution here to wrap the database handle in yet another struct? And then make sure sqlite_db always comes BEFORE the statements in the declaration order and the statement destructors will run first. That's guaranteed, right? All of this seems very flimsy and prone to breakage.</p>
",0,0,0,2025-10-30T10:03:01+00:00,2,136,True
79805181,11501306,"McKinney, TX, USA",sqlite,How do I resolve a error global::System.Diagnostics.Debugger.Break(); in a .Net Maui application,"<p>I'm trying to configure a new Maui app to use EF and Sqlite following this guide: <a href=""https://learn.microsoft.com/en-us/dotnet/maui/data-cloud/database-sqlite?view=net-maui-9.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/maui/data-cloud/database-sqlite?view=net-maui-9.0</a></p>
<p>For a long time I got an error that a deps.json file didn't exist.  I fixed that by editing the project's <code>&lt;TargetFrameworks&gt;</code> and removing everything in that element.  I replace it with <code>&lt;TargetFrameworks&gt;net8.0-windows10.0.19041.0&lt;/TargetFrameworks&gt;</code> and got past that error.</p>
<p>Now its getting through build and deploy (based on the Build Output window) and stopping in App.g.i.cs on this</p>
<pre><code>#if DEBUG &amp;&amp; !DISABLE_XAML_GENERATED_BREAK_ON_UNHANDLED_EXCEPTION
            UnhandledException += (sender, e) =&gt;
            {
                if (global::System.Diagnostics.Debugger.IsAttached) global::System.Diagnostics.Debugger.Break();
            };
#endif
</code></pre>
<p>In the Debug output I see this <code>Exception thrown: 'System.InvalidOperationException' in Microsoft.Extensions.DependencyInjection.dll</code>  I've tried putting a Try/Catch around the <code>builder.services</code> in MauiProgram.cs.  I also put a try catch in a database startup Init method.  It never enters either Catch.  Here is the database class</p>
<pre><code> public class DatabaseBase
 {
     public SQLiteAsyncConnection database;

     public DbSet&lt;Movie&gt; Movies { get; set; }
     public DbSet&lt;Group&gt; Groups { get; set; }
     public DbSet&lt;VersionDbTables&gt; VersionDbTables { get; set; }

     public async Task Init()
     {
         try
         {
             if (database is not null)
                 return;

             database = new SQLiteAsyncConnection(Constants.DatabasePath, Constants.Flags);
             var result = await database.CreateTablesAsync&lt;Movie, Group&gt;(CreateFlags.None);
         }
         catch (Exception ex) { 
             Console.WriteLine(ex);
         }
     }
 }
</code></pre>
<p>And I have a singleton to create the service <code>builder.Services.AddSingleton&lt;DatabaseBase&gt;();</code> in <code>MauiProgram.cs</code>.</p>
<p>I don't know what to try next to troubleshoot this.</p>
<p>UPDATE:
I tried restoring the  and now I get a JIT debugger window. After I close the modal, if I click on OK, it tries to open a new VS and attach a debugger, which it can't.</p>
<p><a href=""https://i.sstatic.net/YFydRszx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YFydRszx.png"" alt=""enter image description here"" /></a></p>
<p>UPDATE:</p>
<p>I've also tried this <a href=""https://learn.microsoft.com/en-us/previous-versions/xamarin/xamarin-forms/data-cloud/data/entity-framework"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/previous-versions/xamarin/xamarin-forms/data-cloud/data/entity-framework</a> and I have the same debugger error as the previous screenshot.
I did notice its a Win32 error now, not EF.  Looking into that more.</p>
",0,0,0,2025-10-30T20:12:13+00:00,2,110,False
79805744,13076067,,sqlite,Appium Device Farm + Prisma (SQLite) — “The table main.Session does not exist” despite successful prisma db push,"<p>I'm trying to set up Appium Device Farm with Prisma on Windows 10, but even though Prisma generates successfully and prisma db push says the schema is in sync, the Appium plugin cannot connect to the database at runtime.</p>
<p>My setup:</p>
<pre><code>npm install -g appium@3.1.0
appium driver install --source=npm appium-uiautomator2-driver@6.1.0
appium plugin install --source=npm appium-device-farm@11.0.5
appium plugin install --source=npm appium-inspector-plugin@2025.8.2
</code></pre>
<p>Inside
<em>C:\Users\Andrei.appium\node_modules\appium-device-farm\prisma\schema.prisma</em>
I have the standard SQLite datasource:</p>
<pre><code>datasource db {
  provider = &quot;sqlite&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}
</code></pre>
<p>My Prisma CLI and client versions are both 5.22.0.</p>
<p>I generated Prisma successfully: <code>npx prisma generate</code></p>
<p>And pushed the schema: <code>npx prisma db push --force-reset</code></p>
<p>That created a file at: <code>C:\Users\Andrei\.appium\node_modules\appium-device-farm\prisma\dev.db</code></p>
<p>I set my DATABASE_URL globally: <code>file:C:/Users/Andrei/.appium/node_modules/appium-device-farm/prisma/dev.db</code></p>
<p>And also tried with a .env file inside the same directory.When I run Prisma Studio (npx prisma studio), I can see all models (User, Session, Node, etc.) as expected.</p>
<p>🚨 The problem: When I start Appium with the plugins:
<code>appium server -pa /wd/hub --use-plugins=device-farm,inspector</code>
I get this output (trimmed):</p>
<pre><code>[device-farm-main] Initializing storage
[device-farm-main] Initializing database
[device-farm-main] Creating new database: ./temp-appium/db.json
[device-farm-main] Database loaded
...
PrismaClientKnownRequestError:
Invalid `prisma.session.updateMany()` invocation:
The table `main.Session` does not exist in the current database.
...
Invalid `prisma.user.count()` invocation:
The table `main.User` does not exist in the current database.
...
Invalid `prisma.node.findFirst()` invocation:
The table `main.Node` does not exist in the current database.
</code></pre>
<p>So, even though the Prisma database exists and db push works,
Device Farm ignores it and falls back to <code>./temp-appium/db.json.</code></p>
<p>❓Question</p>
<p>How can I make Appium Device Farm actually use my Prisma SQLite database (dev.db) instead of falling back to temp-appium/db.json?</p>
<p>Is there a specific .env location, Appium startup flag, or plugin configuration needed for Device Farm to pick up Prisma’s DATABASE_URL on Windows?</p>
",1,1,0,2025-10-31T11:51:32+00:00,0,55,False
79807789,14459522,,sqlite,Serialization for database?,"<p>Lately I've been curious about databases so I am reading books and trying to do simple assignments to learn more about them. I read about pages which contain rows and that's all good but how do I serialize those pages back forth to disk.</p>
<p>I Google online and I come accross protobuffs but as far as I know that's an overkill for this. Can I get pointed to right direction, I come across various consideration like endianness for multiplatform serialization format.</p>
<p>Thank you,</p>
",0,0,0,2025-11-03T11:04:55+00:00,3,79,False
79808345,11501306,"McKinney, TX, USA",sqlite,How can I solve an EF Core error querying SQLite?,"<p>I just got my .Net 9 Maui app working with EF Core and SQLite.  I have one row of data in the database that looks like this:<a href=""https://i.sstatic.net/ZMi8oqmS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZMi8oqmS.png"" alt=""enter image description here"" /></a></p>
<p>I have a simple query that runs when the app loads.  The repository method is this</p>
<pre><code> public void GetMoviesAsync()
   {
       try
       {
           using (var movieContext = new MovieContext())
           {
               var movies = movieContext.Movies
                   .AsNoTracking()
                    .ToList&lt;Movie&gt;();

               //return movies ?? new List&lt;Movie&gt;();
           }
       }
       catch (Exception ex)
       {
           throw new Exception(ex.Message);
       }
   }
</code></pre>
<p>When the EF query runs, it throws an error: <code>'N' is an invalid start of a value. LineNumber: 0 | BytePositionInLine: 0.</code> which is being thrown from</p>
<pre><code>Exception thrown: 'System.Text.Json.JsonReaderException' in System.Text.Json.dll
Exception thrown: 'System.Text.Json.JsonReaderException' in Microsoft.EntityFrameworkCore.Relational.dll
</code></pre>
<p>I can only assume that the SQLite response comes back to EF as JSON?  I can't figure out what that bad character &quot;N&quot; is.</p>
<p>I also tried running it as raw sql <code>movieContext.Database.ExecuteSqlRaw(&quot;Select * From Movies&quot;);</code> which returned a -1, so no rows returned.</p>
<p>Here is the Movie class:</p>
<pre><code>  public class Movie
    {
        [Key]
        public int MovieId { get; set; }

        [Required]
        public string Title { get; set; }

        public DateOnly ReleaseDate { get; set; }
        public string Genre { get; set; }
        public string[] Tags { get; set; }
        public string MovieLocation { get; set; }
        public string ImageLocation { get; set; }

        public int GroupId { get; set; }
    }
</code></pre>
<p>And the data context</p>
<pre><code>   public class MovieContext : DbContext
   {

       public DbSet&lt;Movie&gt; Movies { get; set; }
       public DbSet&lt;Group&gt; Groups { get; set; }

       public MovieContext()
       {
           SQLitePCL.Batteries_V2.Init();
           this.Database.EnsureCreated();
       }

       protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) =&gt;
           optionsBuilder.UseSqlite(&quot;Data Source = D:\\Repos\\MovieManager\\MovieManager.db3&quot;);

   }

</code></pre>
",2,2,0,2025-11-03T21:56:18+00:00,1,89,True
79809024,30573484,,sqlite,Getting SQLite Syntax Error when saving model in Sequelize.js v7,"<p>So, I'm trying to learn full stack by making a list app, and I'm using
Sequelize.js for the database. So far I've got User and List models. I
recently switched to Sequelize v7.0.0-alpha.47. I can't quite remember,
but I believe it worked for a bit using v7; however, now I'm getting
this strange error:</p>
<pre class=""lang-none prettyprint-override""><code>Error: SQLITE_ERROR: near &quot;RETURNING&quot;: syntax error
    at new BaseError (/home/projects/listless-qzftb29s/node_modules/@sequelize/core/src/errors/base-error.ts:35:5)
    at new DatabaseError (/home/projects/listless-qzftb29s/node_modules/@sequelize/core/src/errors/database-error.ts:32:5)
    at SqliteQuery.formatError (/home/projects/listless-qzftb29s/node_modules/@sequelize/sqlite3/src/query.js:303:16)
    at executeSql (/home/projects/listless-qzftb29s/node_modules/@sequelize/sqlite3/src/query.js:197:20)
    at eval (/home/projects/listless-qzftb29s/node_modules/@sequelize/core/src/sequelize.js:380:16)
</code></pre>
<p>Here is the database main code. I believe the error is occurring from saving the
model:</p>
<pre class=""lang-ts prettyprint-override""><code>import { sequelize } from &quot;./models&quot;;
import User from &quot;./models/user.model&quot;;

await sequelize.sync();

const maxCodes = (
  await User.findOrCreate({ // &lt;&lt;&lt; error here
    where: { username: &quot;maxCodes&quot; },
  })
)[0];
</code></pre>
<p>I also tried it like this:</p>
<pre><code>import { sequelize } from &quot;./models&quot;;
import User from &quot;./models/user.model&quot;;

await sequelize.sync();

const [maxCodes, built] = (
  await User.findOrBuild({
    where: { username: &quot;maxCodes&quot; },
  })
);
if (built) maxCodes.save(); // async error w/out await, so it must occur at saving
</code></pre>
<p>I'm using <code>@sequelize/core@7.0.0-alpha.47</code> and
<code>@sequelize/sqlite3@7.0.0-alpha.47</code>.</p>
<p>Here's my user model:</p>
<pre class=""lang-ts prettyprint-override""><code>import { Model, DataTypes } from &quot;@sequelize/core&quot;;
import type {
  HasManyAddAssociationMixin,
  HasManyCountAssociationsMixin,
  HasManyCreateAssociationMixin,
  HasManyGetAssociationsMixin,
  HasManyHasAssociationMixin,
  HasManySetAssociationsMixin,
  HasManyAddAssociationsMixin,
  HasManyHasAssociationsMixin,
  HasManyRemoveAssociationMixin,
  HasManyRemoveAssociationsMixin,
  NonAttribute,
  CreationOptional,
  InferAttributes,
  InferCreationAttributes,
} from &quot;@sequelize/core&quot;;
import {
  Attribute,
  PrimaryKey,
  AutoIncrement,
  NotNull,
  HasMany,
} from &quot;@sequelize/core/decorators-legacy&quot;;

import List from &quot;./list.model&quot;;

class User extends Model&lt;InferAttributes&lt;User&gt;, InferCreationAttributes&lt;User&gt;&gt; {
  @Attribute(DataTypes.INTEGER)
  @PrimaryKey
  @AutoIncrement
  id!: CreationOptional&lt;number&gt;;

  @Attribute(DataTypes.STRING)
  @NotNull
  username!: string;

  @HasMany(() =&gt; List, {
    foreignKey: &quot;userId&quot;,
    inverse: {
      as: &quot;user&quot;,
    },
  })
  lists?: NonAttribute&lt;List[]&gt;;

  declare getLists: HasManyGetAssociationsMixin&lt;List&gt;; // Note the null assertions!
  declare addList: HasManyAddAssociationMixin&lt;List, number&gt;;
  declare addLists: HasManyAddAssociationsMixin&lt;List, number&gt;;
  declare setLists: HasManySetAssociationsMixin&lt;List, number&gt;;
  declare removeList: HasManyRemoveAssociationMixin&lt;List, number&gt;;
  declare removeLists: HasManyRemoveAssociationsMixin&lt;List, number&gt;;
  declare hasList: HasManyHasAssociationMixin&lt;List, number&gt;;
  declare hasLists: HasManyHasAssociationsMixin&lt;List, number&gt;;
  declare countLists: HasManyCountAssociationsMixin&lt;List&gt;;
  declare createList: HasManyCreateAssociationMixin&lt;List, &quot;userId&quot;&gt;;

  declare createdAt: CreationOptional&lt;Date&gt;;
  declare updatedAt: CreationOptional&lt;Date&gt;;
}

export default User;

</code></pre>
<p>Here is my List model code:</p>
<pre class=""lang-ts prettyprint-override""><code>import { Model, DataTypes } from &quot;@sequelize/core&quot;;
import type {
  NonAttribute,
  ForeignKey,
  CreationOptional,
  InferAttributes,
  InferCreationAttributes,
  BelongsToGetAssociationMixin,
  BelongsToCreateAssociationMixin,
  BelongsToSetAssociationMixin,
} from &quot;@sequelize/core&quot;;

import {
  Attribute,
  PrimaryKey,
  AutoIncrement,
  NotNull,
  BelongsTo,
} from &quot;@sequelize/core/decorators-legacy&quot;;

import User from &quot;./user.model&quot;;

class List extends Model&lt;InferAttributes&lt;List&gt;, InferCreationAttributes&lt;List&gt;&gt; {
  @Attribute(DataTypes.INTEGER)
  @PrimaryKey
  @AutoIncrement
  id!: CreationOptional&lt;number&gt;;

  @Attribute(DataTypes.STRING)
  @NotNull
  name!: string;

  @BelongsTo(() =&gt; User, {
    foreignKey: &quot;userId&quot;,
    inverse: {
      as: &quot;lists&quot;,
      type: &quot;hasMany&quot;,
    },
  })
  user!: NonAttribute&lt;User&gt;;

  @Attribute(DataTypes.INTEGER)
  @NotNull
  userId!: ForeignKey&lt;User[&quot;id&quot;]&gt;;

  // timestamps!
  // createdAt can be undefined during creation
  declare createdAt: CreationOptional&lt;Date&gt;;
  // updatedAt can be undefined during creation
  declare updatedAt: CreationOptional&lt;Date&gt;;

  declare getUser: BelongsToGetAssociationMixin&lt;User&gt;;
  declare createUser: BelongsToCreateAssociationMixin&lt;User&gt;;
  declare setUser: BelongsToSetAssociationMixin&lt;User, User[&quot;id&quot;]&gt;;
}

export default List;

</code></pre>
<p>And finally, my Sequelize config:</p>
<pre class=""lang-ts prettyprint-override""><code>import { Sequelize } from &quot;@sequelize/core&quot;;
import { SqliteDialect } from &quot;@sequelize/sqlite3&quot;;

import User from &quot;./user.model&quot;;
import List from &quot;./list.model&quot;;

const sequelize = new Sequelize({
  dialect: SqliteDialect,
  storage: &quot;data/database.sqlite&quot;,
  models: [User, List],
});

export { sequelize, Sequelize };
</code></pre>
<p>I honestly have no idea what is happening. <strong>Is this an error on my
part, or an error in Sequelize itself? Why am I even getting this
error?</strong> (Like what code is causing the error?)</p>
<p>If you want to see the full code (note: much more than just
server-side), you can check out the <a href=""https://stackblitz.com/edit/listless-qzftb29s?file=README.md"" rel=""nofollow noreferrer"">live demo</a>. If you think that the
error is coming from elsewhere in the code, instead of what I've shown
here, please comment and I'll review my question and code.</p>
",3,3,0,2025-11-04T14:28:13+00:00,1,108,True
79810374,2892357,"Poland, Wrocław",sqlite,What is the best practices to delete outdated data from the SQLite database on the server?,"<p>I have a Golang server (pet project) and each second I add 2879 bytes of data in database. At the moment I'm about a 1GB of data. I want to cleanup the database to avoid a moment when I'm run out of space and the ways I see to solve:</p>
<ol>
<li><p>Create and run a time scheduled goroutine on the same process as a server. It starts when server starts.</p>
</li>
<li><p>Run a cronjob/systemd timed service with a <code>sqlite3</code> command to cleanup the database.</p>
</li>
</ol>
<p>The p.1 is like a clean way to solve the problem - explicit solution, you never miss that you need to take care about the data removing in another place.</p>
<p>The p.2 - I don't like it because it's implicit. If the service goes wrong - you will run out of the space.</p>
<p>---</p>
<p>I also consider variants to put the data in another table (like <code>pending_removing</code>)</p>
<p>I'm here to listen you opinions.</p>
<p>Thanks!</p>
",0,0,0,2025-11-05T16:57:51+00:00,2,90,True
79814854,15373638,,sqlite,Electron Builder: Built app starts UI but server + SQLite database do not run in packaged versio,"<p>I have an Electron application that uses:</p>
<ul>
<li><p>React for the frontend</p>
</li>
<li><p>Node + Express backend inside <code>/Server</code></p>
</li>
<li><p>A SQLite database stored inside the same <code>/Server</code> folder</p>
</li>
</ul>
<p>During development, everything works correctly.<br />
Running:</p>
<pre><code>npm start
</code></pre>
<p>starts Electron, and the server is required inside <code>main.js</code> like this:</p>
<pre><code>require(&quot;./Server/index.js&quot;);
</code></pre>
<p>The React build is loaded from <code>client/build/index.html</code>, and API calls work normally.</p>
<hr />
<h3>Problem</h3>
<p>When I package the app using:</p>
<pre><code>npm run dist
</code></pre>
<p>The application UI opens, but the backend server does <strong>not</strong> run, and all API calls from React stay in <strong>Pending</strong>. The SQLite database is also not found.</p>
<p>So the packaged app runs <strong>only the UI</strong>, without the server.</p>
<p>Electron Builder also gives warnings like:</p>
<pre><code>asar usage is disabled — this is strongly not recommended
</code></pre>
<p>And if I enable <code>asar</code>, the SQLite DB becomes inaccessible.</p>
<hr />
<h3>What I understand so far</h3>
<ul>
<li><p>The server and <code>.sqlite</code> file cannot work properly when bundled inside <code>app.asar</code>.</p>
</li>
<li><p>They need to be unpacked to the real filesystem (e.g., <code>resources/server/</code>).</p>
</li>
<li><p>The server entry path should change depending on development vs production.</p>
</li>
</ul>
<hr />
<h3>Current <code>build</code> section in package.json</h3>
<pre><code>&quot;build&quot;: {
  &quot;appId&quot;: &quot;com.yourcompany.matjari&quot;,
  &quot;files&quot;: [
    &quot;main.js&quot;,
    &quot;Server/**&quot;,
    &quot;client/build/**&quot;
  ],
  &quot;asar&quot;: false
}
</code></pre>
<hr />
<h3>Question</h3>
<p><strong>What is the correct Electron Builder configuration to:</strong></p>
<ol>
<li><p>Include the <code>/Server</code> folder and the SQLite database in the packaged app</p>
</li>
<li><p>Ensure the server runs when the app is built (<code>.exe</code>)</p>
</li>
<li><p>Allow the database to be writable and accessible at runtime</p>
</li>
</ol>
<p>Should I use <code>&quot;asar&quot;: true</code> and <code>asarUnpack</code>?<br />
Should the database be moved to a different directory such as <code>resources/</code>?<br />
What is the recommended structure for Electron apps that include a backend server and SQLite?</p>
<hr />
<h3>Goal</h3>
<p>I want the packaged app (<code>.exe</code>) to:</p>
<ul>
<li><p>Start the Express server automatically</p>
</li>
<li><p>Load and use the SQLite database normally</p>
</li>
<li><p>Keep everything inside the installation folder (no external installs required)</p>
</li>
</ul>
<hr />
<p>Thanks in advance to anyone who has solved this setup before.</p>
",0,0,0,2025-11-09T14:51:26+00:00,0,61,False
79815549,550314,"Warrington, United Kingdom",sqlite,Can&#39;t create a Sqlite.Net In Memory Database with Shared Cache when using Dapper,"<p>How can I create an in-memory Sqlite database with shared cache?</p>
<p>If I use the connectionstring mentioned in <a href=""https://learn.microsoft.com/en-us/dotnet/standard/data/sqlite/in-memory-databases"" rel=""nofollow noreferrer"">MS Docs for Sqlite In-Memory</a></p>
<pre><code>Data Source=InMemorySample;Mode=Memory;Cache=Shared
</code></pre>
<p>Then I find that the in-memory database is <em>not</em> shared (I know this because the database creation script runs but tests report 'no such table' errors).</p>
<p>How do I make Sqlite in-memory shared cache work?</p>
<br/>
<p><strong>Reproduction</strong></p>
<p>For me, this code throws &quot;no such table&quot; with either of the alleged in-memory connection strings, but works with a file-backed db.</p>
<pre class=""lang-cs prettyprint-override""><code>void Main()
{
    // Works fine with file-backed db.
    if (File.Exists(&quot;temp.db&quot;)) File.Delete(&quot;temp.db&quot;);
    CreateAndSelectAndWriteLine(&quot;Data Source=temp.db&quot;);

    //Fails with either of the MS suggested datasource values
    CreateAndSelectAndWriteLine(&quot;Data Source=:memory:&quot;);
    CreateAndSelectAndWriteLine(
        &quot;Data Source=InMemorySample;Mode=Memory;Cache=Shared&quot;);
}

void CreateAndSelectAndWriteLine(string connectionString)
{
    using var conn = new SqliteConnection(connectionString);

    conn.Execute(&quot;Create Table HereIAm(id int)&quot;);

    Console.WriteLine(&quot;------------------------------------------&quot;);
    Console.WriteLine(connectionString);
    try
    {
        Console.WriteLine(
         conn.Query&lt;int&gt;(&quot;Select * from HereIAm&quot;)
             .Count());
    }
    catch (Exception e)
    {
        Console.WriteLine(e);
    }
    Console.WriteLine(&quot;-----------------------------------------&quot;);

</code></pre>
",1,1,0,2025-11-10T11:46:04+00:00,1,100,True
79819270,23593613,,sqlite,SqlAlchemy not using updated table name in join statement,"<p>I'm not exactly sure how to what is going on, but for our testing we rename the tables before we generate everything (ie schema.table_name is renamed to table_name_schema due to issues with how we have to test). I have a file of generated queries to be able to pull from as constants, and for a good while everything was working fine and as expected. I don't know what happened or why, but as of Monday, my tests are erroring out stating that a table doesn't exist. It labels the table as the old name and not the new name, but the other table within that select statement is using the new name, so I'm confused as to why one is correct and the other isn't.</p>
<p>When I run the tests, I have importlib reload the queries so that the new table names are there after our test database updates everything:</p>
<pre class=""lang-py prettyprint-override""><code>def setUp(self):
    self.now = TestConstants.TEST_TIME_WINDOW_CENTER
    self.database = TestDatabase()
    importlib.reload(queries)
</code></pre>
<p>Then my test runs this statement:</p>
<pre class=""lang-py prettyprint-override""><code>get_qc_sod_data = (
    select(QcSodTable)
    .join(
        LocalTimeTable,
        and_(
            QcSodTable.platformid == LocalTimeTable.platformid,
            QcSodTable.networktype == LocalTimeTable.networktype
        ),
        isouter=True,
    )
    .where(
        or_(QcSodTable.qc_flag.not_in((&quot;P&quot;, &quot;F&quot;)), QcSodTable.qc_flag == None),
        (QcSodTable.datetime + cast(literal(1.5) + ' DAYS', Interval)) - (cast(func.coalesce(LocalTimeTable.time_conv, 0) / 24 + ' HOURS', Interval)) &lt; func.now(),
    )
)
</code></pre>
<p>The error comes at the <code>LocalTimeTable</code> where it states the table does not exist but lists the incorrect name (<code>schema.table_name</code>) though the Select table lists the proper name as expected, so I'm not sure why the other is not correct. If I change <code>LocalTimeTable</code> to <code>LocalTimeTable.__ table__</code> then it passes fine</p>
<p>Is there something I'm missing here? Do I need to reload that table as well within my constants queries file?</p>
<p>EDIT:</p>
<p>SQLAlchemy version is 2.0.43</p>
<p>Here is the local time table:</p>
<pre class=""lang-py prettyprint-override""><code>class LocalTimeTableBase(Base):
    __abstract__ = True

    PLATFORMID_COLUMN_NAME = &quot;platformid&quot;
    NETWORKTYPE_COLUMN_NAME = &quot;networktype&quot;
    TIME_CONV_COLUMN_NAME = &quot;time_conv&quot;
    DISTRIBUTIONCODE_COLUMN_NAME = &quot;distributioncode&quot;
    SECURITYID_COLUMN_NAME = &quot;securityid&quot;

    platformid = Column(String, name=PLATFORMID_COLUMN_NAME, primary_key=True)
    networktype = Column(
        String, name=NETWORKTYPE_COLUMN_NAME, nullable=False, primary_key=True
    )
    time_conv = Column(Numeric, name=TIME_CONV_COLUMN_NAME)
    distributioncode = Column(String, name=DISTRIBUTIONCODE_COLUMN_NAME)
    securityid = Column(Integer, name=SECURITYID_COLUMN_NAME)


class LocalTimeTable(LocalTimeTableBase):
    __tablename__ = &quot;local_time&quot;
    __table_args__ = {&quot;schema&quot;: SCHEMA_NAME}
</code></pre>
",0,0,0,2025-11-13T17:57:48+00:00,0,76,False
79820952,26867731,,sqlite,"Best practice for localizing a large (16,000+ rows) SQLite database in android","<p>The core of my app is an SQLite database containing over 16,000 strings.<br />
My goal is to translate them into multiple languages in the most performant, memory-efficient, and maintainable way.</p>
<p>I understand how to handle standard UI strings using different strings.xml files. My question is specifically about the large, static dataset in the database.</p>
<p><strong>I have explored several approaches, but each seems to have a significant drawback:</strong></p>
<p><strong>Using XML String Resources (getIdentifier()):</strong><br />
Move all 16,000 meanings into <code>res/values/strings.xml</code> with names like <code>&lt;string name=&quot;word_1&quot;&gt;text&lt;/string&gt;</code>. Then, in my code, fetch the string dynamically using <code>context.getResources().getIdentifier(&quot;word_&quot; + id, &quot;string&quot;, context.getPackageName())</code>.<br />
The official documentation warns that getIdentifier() is very slow, and I'm worried about performance for this many strings.</p>
<p><strong>Normalized Database (Separate translations table):</strong><br />
Create a word_translations table with columns like word_id, language_code, translated_meaning. Then JOIN on the user's current language.<br />
This seems like the &quot;correct&quot; database design, but I'm trying to avoid adding complexity to the database schema and in-app migration logic if possible. I'm hoping to leverage Android's resource system more directly for easier management.</p>
<p><strong>Given a large, static dataset of 16,000+ items, what is the industry-standard, most robust pattern for localization in an Android app?</strong></p>
",0,0,0,2025-11-15T16:15:50+00:00,1,60,True
79822333,11092636,,sqlite,Connect to SQLCipher encrypted SQLite database with Rider&#39;s built-in plugin,"<p>I'm trying to connect to my database through Rider but it doesn't work since I encrypted my database with SQLCipher. Tutorials say I should add these fields in the &quot;Advanced&quot; tab but that doesn't work:</p>
<p><a href=""https://i.sstatic.net/DdRSuQg4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DdRSuQg4.png"" alt=""enter image description here"" /></a></p>
<p>I can connect using Microsoft.Data.Sqlite so it's not a file corruption problem, I just don't understand how to use the IDE (JetBrains Rider 2025.3.0.1) properly.</p>
<p>I get this error:</p>
<pre><code>Error encountered when performing Introspect schema main: [SQLITE_NOTADB] File opened that is not a database file (file is not a database).
[SQLITE_NOTADB] File opened that is not a database file (file is not a database).
</code></pre>
<p>which is similar to the error I have if I try to open the database through code <strong>without</strong> a password. But I did specify the (<strong>correct</strong>) password in the <code>Advanced</code> tab of the <code>Data Sources and Drivers</code> Rider SQL IDE.</p>
",0,1,1,2025-11-17T13:14:18+00:00,1,156,True
79827356,30985469,,sqlite,Does SQLite support calling from a TRIGGER a user-defined function that uses the WITH clause?,"<p>I have a database that implements a very simple tree structure with a table, hierarchy, which has two columns, parent and child.</p>
<pre class=""lang-sql prettyprint-override""><code> CREATE TABLE hierarchy (
     parent INTEGER,
     child  INTEGER,
     PRIMARY KEY(parent, child)
 );
</code></pre>
<p>I need to ensure that there are no cycles on the graph defined by the table's rows. For example, if hierarchy has (1, 2) and (2, 3), inserting (3, 1) or using UPDATE to change (2, 3) to (2, 1) should not be allowed. To do this, I figured the most appropiate way to do this would be defining two TRIGGERs (one for INSERT and another for UPDATE) that would check if the command would create a cycle in the database. I would also need recursion for this, so I would need the WITH clause.</p>
<p>However, according to section 5 of <a href=""https://sqlite.org/lang_with.html"" rel=""nofollow noreferrer"">SQLite's documentation on the WITH clause</a>, it cannot be used within a TRIGGER. Thus, I moved the WITH to a function defined in C++, which is <code>lookForCycles</code>, that is called from the TRIGGER.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TRIGGER prevent_cycles_insert 
     BEFORE INSERT ON hierarchy 
     FOR EACH ROW 
     BEGIN 
        SELECT
        CASE
           WHEN lookForCycles(NEW.parent, NEW.child) = 1 THEN
              RAISE(ABORT, 'Cycle detected in INSERT INTO hierarchy!')
        END;
     END;

CREATE TRIGGER prevent_cycles_update 
     BEFORE UPDATE ON hierarchy 
     FOR EACH ROW 
     BEGIN 
        SELECT
        CASE
           WHEN lookForCycles(NEW.parent, NEW.child) = 1 THEN
              RAISE(ABORT, 'Cycle detected in UPDATE hierarchy!')
        END;
     END; 
</code></pre>
<p><code>lookForCycles</code> simply executes the following SQLite command. Note that the parameters (the question marks) are set to the values of <code>lookForCycles</code> 's arguments using <code>sqlite3_bind_value</code> .</p>
<pre><code>WITH RECURSIVE ancestors(node) AS(
   SELECT ?
   UNION ALL
   SELECT h.parent
   FROM hierarchy h
   JOIN ancestors a ON h.child = a.node
)
SELECT 1 FROM ancestors WHERE node = ?;
</code></pre>
<p>I have tested this implementation, and it works as expected. For example, if I put the following rows into hierarchy: (1, 2) (1, 3), (2, 3) and (3, 4), trying to insert (4, 1) or updating (3, 4) to (3, 1) fails with &quot;Cycle detected in INSERT INTO/UPDATE hierarchy!&quot;.</p>
<p>What worries me is that, even if the WITH clause is used in a completely different context than the TRIGGER, as it is called from a C++ function, I don't know if it works now, but is not guaranteed to do the same for all cases. If someone with more knowledge about SQLite could tell me if this is how I should do it, it would be great!</p>
<p>Two annotations:</p>
<ol>
<li><p>I know I could implement this with recursive TRIGGERs and I have already thought about a possible algorithm, but I would rather avoid that option, as it would probably require using a temporary table and would be way more complex than simply using a WITH.</p>
</li>
<li><p>This also works, for some reason, even though I'm using WITH directly inside the TRIGGER:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TRIGGER prevent_cycles
BEFORE INSERT ON hierarchy
FOR EACH ROW
BEGIN
    WITH RECURSIVE ancestors(node) AS (
        SELECT NEW.parent
        UNION ALL
        SELECT h.parent
        FROM hierarchy h
        JOIN ancestors a ON h.child = a.node
    )
    SELECT
        CASE
            WHEN EXISTS (
                SELECT 1 FROM ancestors WHERE node = NEW.child
            ) THEN
                RAISE(ABORT, 'Cycle detected!')
        END;
END;
</code></pre>
</li>
</ol>
",1,0,0,2025-11-22T13:55:39+00:00,4,64,False
79829421,26650329,,sqlite,How do I make a custom field output different datatypes depending on file asking for data?,"<p>I'm building a molecule properties displaying website using django and sqlite. The database takes a numpy array and stores it as a BLOB. Currently, it just outputs the numpy array back out when asked for it. I'd like it to check what kind of file is asking the data, and output a numpy array if python is asking for it ( for instance for a data integrity test ), and a json object if requested by a javascript file ( like for molecule visualization ).</p>
<p>I'm not even sure if it's possible, so I guess my question is twofold. One, is it possible? Two, if it's not, as long as the data is read only, are there any major problems that might come up if I let inputs be as numpy arrays, and outputs be as json.</p>
<p>The custom field is defined as so:</p>
<p>The packages used:</p>
<pre class=""lang-py prettyprint-override""><code>import io
import json
import numpy as np
from django.db.models import BinaryField
from django.core.exceptions import ValidationError
from django.utils.translation import gettext_lazy as _
</code></pre>
<p>For the current numpy only implementation:</p>
<pre class=""lang-py prettyprint-override""><code># Stores numpy array as BLOB then returns BLOB as np array
class NpArrayField(BinaryField):
    &quot;&quot;&quot;
    Turns numpy array into raw binary data to store in database. When take from database, it outputs the numpy array.
    &quot;&quot;&quot;
    def from_db_value(self, value, expression, connection):

        if value is None:
            return None
        buffer = io.BytesIO(value)
        return np.load(buffer, allow_pickle=True) # returns numpy array
        

    def to_python(self, value):

        if value is None or isinstance(value, np.ndarray):
            return value
        
        # If it's bytes from DB or form
        if isinstance(value, (bytes, bytearray)):
            buffer = io.BytesIO(value)
            return np.load(buffer, allow_pickle=True)

        raise ValidationError(_(&quot;NpArrayField value must be a numpy.ndarray or a byte/bytearray&quot;))


    def get_prep_value(self, value):

        if value is None:
            return None

        if not isinstance(value, np.ndarray):
            raise ValidationError(_(&quot;NpArrayField value must be a numpy.ndarray&quot;))

        buffer = io.BytesIO()
        np.save(buffer, value, allow_pickle=True)
        return buffer.getvalue()
    

    def deconstruct(self):
        name, path, args, kwargs = super().deconstruct()
        return name, path, args, kwargs
</code></pre>
<p>An implementation which takes a numpy array (stored as a BLOB), and which outputs json:</p>
<pre class=""lang-py prettyprint-override""><code>class NpArrayEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, numpy.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self,obj)

class NpArrayField(BinaryField):
    &quot;&quot;&quot;
    Turns numpy array into json style raw binary data to store in database. When taken from the database, it outputs the json
    &quot;&quot;&quot;
    def from_db_value(self, value, expression, connection):

        if value is None:
            return None
        return value.decode('utf-8'))
        

    def to_python(self, value):
        if value is None:
            return None
        return value

    def get_prep_value(self, value):

        if value is None:
            return None

        if not isinstance(value, np.ndarray):
            raise ValidationError(_(&quot;NpArrayField value must be a numpy.ndarray&quot;))

        np_as_json = json.dumps({&quot;array&quot; : value}, cls=NpArrayEncoder)
        return bytes(np_as_json, 'utf-8')
    

    def deconstruct(self):
        name, path, args, kwargs = super().deconstruct()
        return name, path, args, kwargs
</code></pre>
<p>In case anyone was wondering, these arrays should be immutable, so it makes sense in this paradigm to store them as atomic units rather than as individual values.</p>
",2,3,1,2025-11-25T08:04:21+00:00,1,78,True
79829738,18278801,,sqlite,Heavy duplication when upserting with GORM generics in SQLite,"<p>I have several models in my application, which have associations between each other. I'm trying to recursively upsert them, but haven't been able to find a good solution that doesn't extensively rely on raw SQL or the non-generics API. These are my models:</p>
<p><strong>EDIT 2025-11-26</strong> Added <code>Enclosure</code>.</p>
<pre class=""lang-golang prettyprint-override""><code>// Feed represents an RSS channel
type Feed struct {
    gorm.Model
    Title       string `gorm:&quot;not null;&quot;`
    Description string `gorm:&quot;not null;&quot;`
    // The link to the HTML representation of the feed. Different from FetchFrom
    Link string `gorm:&quot;not null;&quot;`
    // The URL that this feed can be retrieved from. Different from Link
    FetchFrom string `gorm:&quot;not null;unique&quot;`
    Language  string
    // The time-to-live of the feed. Time in minutes that the reader should wait between each refresh
    TTL          int `gorm:&quot;default:60&quot;`
    ETag         string
    LastModified time.Time
    Items        []Item `gorm:&quot;constraint:OnDelete:CASCADE;&quot;`
}

// Item represents an RSS item/post
type Item struct {
    gorm.Model
    // The feed this item came from.
    Feed Feed
    // ID of the feed. Automatically creates a belongs-to relationship with Feed -&gt; Item.
    FeedID      uint   `gorm:&quot;index;not null;&quot;`
    GUID        string `gorm:&quot;not null;unique&quot;`
    Title       string
    Description string
    Link        *string
    Author      string
    PubDate     time.Time  `gorm:&quot;not null;&quot;`
    Read        bool       `gorm:&quot;not null;&quot;`
    Starred     bool       `gorm:&quot;not null;&quot;`
    Enclosure   *Enclosure `gorm:&quot;constraint:OnDelete:CASCADE;&quot;`
}

// Enclosure represents an RSS enclosure, usually media associated with an item
// See https://www.rssboard.org/rss-specification
type Enclosure struct {
    gorm.Model
    ItemID   uint `gorm:&quot;index&quot;`
    Item     Item
    URL      string `gorm:&quot;not null;&quot;`
    MimeType string `gorm:&quot;not null;&quot;`
    FilePath string `gorm:&quot;not null;&quot;`
}
</code></pre>
<p>And this is my upsert solution:</p>
<pre class=""lang-golang prettyprint-override""><code>func upsert[T any](db *gorm.DB, m T, where string, args ...any) error {
    if where == &quot;&quot; {
        return fmt.Errorf(&quot;expected non-empty where&quot;)
    }

    ctx := context.Background()

    w := gorm.G[T](db).Where(where, args...)
    rows, err := w.Updates(ctx, m)
    if err != nil {
        return fmt.Errorf(&quot;failed to update object: %w&quot;, err)
    }

    if rows == 0 {
        err := gorm.G[T](db).Create(ctx, &amp;m)
        if err != nil {
            return fmt.Errorf(&quot;failed to create object: %w&quot;, err)
        }
        return nil
    }
    return nil
}
</code></pre>
<p>When using the upsert on a Feed, it gets stored twice in the feeds table, one having fetch_from as provided and the other having an empty one. The items also appear to be stored twice each. When adding the <code>UNIQUE</code> constraint to <code>Item.GUID</code>, it throws several extremely long errors in this format:</p>
<pre class=""lang-sql prettyprint-override""><code>upsert.go:27 constraint failed: UNIQUE constraint failed: items.guid (2067)
[50.595ms] [rows:0] sql statement
</code></pre>
<p>At the end, there appear to be 2 relatively short errors:</p>
<pre class=""lang-sql prettyprint-override""><code>2025/11/25 19:13:01 .../server/database/upsert.go:27 constraint failed: UNIQUE constraint failed: items.guid (2067)
[47.266ms] [rows:1] INSERT INTO `feeds` (`created_at`,`updated_at`,`deleted_at`,`title`,`description`,`link`,`fetch_from`,`language`,`ttl`,`e_tag`,`last_modified`) VALUES (&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS 
Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;
RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Boar
d announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements a
nd Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple 
Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;
,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rss
board.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;e
n-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000
-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;)
,(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:
13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-
11-25 19:13:01.123&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.12
3&quot;,NULL,&quot;RSS Advisory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;),(&quot;2025-11-25 19:13:01.123&quot;,&quot;2025-11-25 19:13:01.123&quot;,NULL,&quot;RSS Advi
sory Board&quot;,&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,&quot;https://www.rssboard.org/&quot;,&quot;&quot;,&quot;en-us&quot;,10,&quot;&quot;,&quot;0000-00-00 00:00:00&quot;) ON CONFLICT DO NOTHING RETURNING `id`

2025/11/25 19:13:01 .../server/database/upsert.go:27 constraint failed: UNIQUE constraint failed: items.guid (2067)
[50.595ms] [rows:0] UPDATE `feeds` SET `updated_at`=&quot;2025-11-25 19:13:01.123&quot;,`title`=&quot;RSS Advisory Board&quot;,`description`=&quot;RSS Advisory Board announcements and Really Simple Syndication news&quot;,`link`=&quot;https://www.rssboard.org/&quot;,`fetch_f
rom`=&quot;http://feeds.rssboard.org/rssboard&quot;,`language`=&quot;en-us&quot;,`ttl`=10 WHERE (id = 0 OR fetch_from = &quot;http://feeds.rssboard.org/rssboard&quot;) AND `feeds`.`deleted_at` IS NULL
&gt; github.com/its-mrarsikk/fedup/server/database.upsert[go.shape.struct { gorm.io/gorm.Model; Title string &quot;gorm:\&quot;not null;\&quot;&quot;; Description string &quot;gorm:\&quot;not null;\&quot;&quot;; Link string &quot;gorm:\&quot;not null;\&quot;&quot;; FetchFrom string &quot;gorm:\&quot;not nu
ll;unique\&quot;&quot;; Language string; TTL int &quot;gorm:\&quot;default:60\&quot;&quot;; ETag string; LastModified time.Time; Items []github.com/its-mrarsikk/fedup/shared/rss.Item &quot;gorm:\&quot;constraint:OnDelete:CASCADE;\&quot;&quot; }]() ./server/database/upsert.go:27 (PC: 
0xde9642)
</code></pre>
<p>The one(s) above appear to contain all of the items it's about to store, spanning thousands of lines.</p>
<p>This all happens in one <code>upsert</code> call. Any help would be appreciated.</p>
<p><strong>EDIT 2025-11-26</strong> I'm using <code>github.com/glebarez/sqlite</code> for a driver.</p>
<p>Here's a complete reproducible example:</p>
<p>main.go - <a href=""https://bpa.st/7ZMA"" rel=""nofollow noreferrer"">https://bpa.st/7ZMA</a></p>
<p>go.mod - <a href=""https://bpa.st/4Z2A"" rel=""nofollow noreferrer"">https://bpa.st/4Z2A</a></p>
<p>I tested them and they replicate the issue.</p>
",1,1,0,2025-11-25T13:04:32+00:00,0,95,False
79830258,31946364,,sqlite,&#39;SQLite Error 19: &#39;FOREIGN KEY constraint failed&#39;.&#39; while try to insert data,"<p>I am trying to run my database code, but I keep getting this error and I've done whatever I can think of to fix it, whether it is random posts, or chatgpt, but I can't seem to figure it out. The comments are in Swedish so you can ignore them.</p>
<p>Error says it stems from this line of code:</p>
<pre class=""lang-cs prettyprint-override""><code>await db.Database.MigrateAsync();
</code></pre>
<p>Error details:</p>
<pre><code>Microsoft.EntityFrameworkCore.DbUpdateException
  HResult=0x80131500
  Message=An error occurred while saving the entity changes. See the inner exception for details.
  Source=Microsoft.EntityFrameworkCore.Relational
  StackTrace:
   at Microsoft.EntityFrameworkCore.Update.ReaderModificationCommandBatch.&lt;ExecuteAsync&gt;d__50.MoveNext()
   at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.&lt;ExecuteAsync&gt;d__9.MoveNext()
   at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.&lt;ExecuteAsync&gt;d__9.MoveNext()
   at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.&lt;ExecuteAsync&gt;d__9.MoveNext()
   at Microsoft.EntityFrameworkCore.Storage.RelationalDatabase.&lt;SaveChangesAsync&gt;d__8.MoveNext()
   at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.&lt;SaveChangesAsync&gt;d__113.MoveNext()
   at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.&lt;SaveChangesAsync&gt;d__117.MoveNext()
   at Microsoft.EntityFrameworkCore.DbContext.&lt;SaveChangesAsync&gt;d__63.MoveNext()
   at Microsoft.EntityFrameworkCore.DbContext.&lt;SaveChangesAsync&gt;d__63.MoveNext()
   at Program.&lt;&lt;Main&gt;$&gt;d__0.MoveNext() in [redacted]\Program.cs:line 39

  This exception was originally thrown at this call stack:
    Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(int, SQLitePCL.sqlite3)
    Microsoft.Data.Sqlite.SqliteDataReader.NextResult()
    Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(System.Data.CommandBehavior)
    Microsoft.Data.Sqlite.SqliteCommand.ExecuteReaderAsync(System.Data.CommandBehavior, System.Threading.CancellationToken)
    Microsoft.Data.Sqlite.SqliteCommand.ExecuteDbDataReaderAsync(System.Data.CommandBehavior, System.Threading.CancellationToken)
    Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReaderAsync(Microsoft.EntityFrameworkCore.Storage.RelationalCommandParameterObject, System.Threading.CancellationToken)
    Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReaderAsync(Microsoft.EntityFrameworkCore.Storage.RelationalCommandParameterObject, System.Threading.CancellationToken)
    Microsoft.EntityFrameworkCore.Update.ReaderModificationCommandBatch.ExecuteAsync(Microsoft.EntityFrameworkCore.Storage.IRelationalConnection, System.Threading.CancellationToken)

Inner Exception 1:
SqliteException: SQLite Error 19: 'FOREIGN KEY constraint failed'.

</code></pre>
<p>The entire <code>Program.cs</code> and other code is shown here:</p>
<pre class=""lang-cs prettyprint-override""><code>using EF_Core___Migrations___Seeding;
using Microsoft.EntityFrameworkCore;
using System.Runtime.InteropServices;

Console.WriteLine(&quot;DB: &quot; + Path.Combine(AppContext.BaseDirectory, &quot;shop.db&quot;));

// Säkerställ DB + migrations + Seed
using(var db = new ShopContext())
{
    // Migrate Async: Skapar databasen om det inte fnns
    // Kör bara om det inte finns några kategorier sen innan

    await db.Database.MigrateAsync();

    // Enkel seeding för databasen
    // Kör bara om det inte finns några kategorier sen innan
    if (!await db.Categories.AnyAsync())
    {
        db.Categories.AddRange(
            new Category { CategoryName = &quot;Books&quot;, CategoryDescription = &quot;All books we have!&quot; },
            new Category { CategoryName = &quot;Movies&quot;, CategoryDescription = &quot;All movies we have!&quot; }
            );
        await db.SaveChangesAsync();
        Console.WriteLine(&quot;Seeded db!&quot;);
    }

    if (!await db.Products.AnyAsync())
    {
        db.Products.AddRange(
            new Product { ProductName = &quot;Hammare&quot;, ProductDescription = &quot;Slå en spik&quot;, ProductPrice = 249, },
            new Product { ProductName = &quot;Tröja&quot;, ProductDescription = &quot;Alla tröjor vi har&quot;, ProductPrice = 130 }
            );
        await db.SaveChangesAsync();
        Console.WriteLine(&quot;Seeded db with products!&quot;);
    }

}

// CLI för CRUD; CREATE, READ, UPDATE, DELETE
while (true)
{
    Console.WriteLine(&quot;\n Commands: addproduct | productlist | categorylist | add | delete &lt;id&gt; | edit &lt;id&gt; | exit&quot;);
    Console.WriteLine(&quot;&gt; &quot;);
    var line = Console.ReadLine()?.Trim() ?? string.Empty;

    // hoppa över tomma rader
    if (string.IsNullOrEmpty(line))
    {
        continue;
    }
     
    if (line.Equals(&quot;exit&quot;, StringComparison.OrdinalIgnoreCase))
    {
        break; // Avsluta programmet, hoppa ur loopen
    }

    // Delar upp raden på mellanslag: t.ex &quot;eddit 2&quot; --&gt; [&quot;edit&quot;, &quot;2&quot;]
    var parts = line.Split(' ', StringSplitOptions.RemoveEmptyEntries);
    var cmd = parts[0].ToLowerInvariant();

    //Enkel switch för kommandotolkning
    switch (cmd)
    {
        case &quot;addproduct&quot;:
            await AddProductAsync();
            break;

        case &quot;productlist&quot;:
            await ProductListAsync();
            break;

        case &quot;categorylist&quot;:
            // Lista vår categories
            await ListAsync();
            break;

        case &quot;add&quot;:
            await AddAsync();
            break;

        case &quot;edit&quot;:
            // Körver id efter kommandot &quot;edit&quot;
            if (parts.Length &lt; 2 || !int.TryParse(parts[1], out var id))
            {
                Console.WriteLine(&quot;Usage: Edit &lt;id&gt;&quot;);
                break;
            }
            await EditAsync(id);
            break;

        case &quot;delete&quot;:
            // Radera en category
            if (parts.Length &lt; 2 || !int.TryParse(parts[1], out var idD))
            {
                Console.WriteLine(&quot;Usage: Delete &lt;id&gt;&quot;);
                break;
            }
            await DeleteAsync(idD);
            break;

        default:
            Console.Write(&quot;Unknown command: &quot;);
            break;
    }
}

static async Task DeleteAsync(int id)
{
    using var db = new ShopContext();

    // Hämta raden du vill ta bort igen
    var category = await db.Categories.FirstOrDefaultAsync((c =&gt; c.CategoryId == id));

    if (category == null)
    {
        Console.WriteLine(&quot;Category not found!&quot;);
        return;
    }

    db.Categories.Remove(category);

    try
    {
        await db.SaveChangesAsync();
        Console.WriteLine(&quot;Category deleted!&quot;);
    }
    catch (DbUpdateException exception)
    {
        Console.WriteLine(exception.Message);
    }
}

static async Task EditAsync(int id)
{
    using var db = new ShopContext();

    // Hämta radre vi vill uppdatera
    var category = await db.Categories.FirstOrDefaultAsync((x =&gt; x.CategoryId == id));

    if (category == null)
    {
        Console.WriteLine(&quot;Category not found&quot;);
        return;
    }

    // Visar nuvarande värden; Uppdatera namn för en specifik category
    Console.Write($&quot;{category.CategoryName} &quot;);
    var name = Console.ReadLine()?.Trim() ?? string.Empty;

    if (!string.IsNullOrEmpty(name))
    {
        category.CategoryName = name;
    }

    // Uppdatera description för en specifik category
    Console.Write($&quot;{category.CategoryDescription} &quot;);
    var description = Console.ReadLine()?.Trim() ?? string.Empty;

    if (!string.IsNullOrEmpty(description))
    {
        category.CategoryDescription = description;
    }

    // Uppdatera
    try
    {
        await db.SaveChangesAsync();
        Console.WriteLine(&quot;Edited!&quot;);
    }
    catch (DbUpdateException exception)
    {
        Console.WriteLine(exception.Message);
    }
}

// READ: lista alla categories
static async Task ListAsync()
{
    // Ny context per separation (bra praxis)
    using var db = new ShopContext();

    // AsNoTracking = snabbare får read-only scenarion (Ingen change tracking)
    var rows = await db.Categories.AsNoTracking().OrderBy(category =&gt; category.CategoryId).ToListAsync();
    Console.WriteLine(&quot;Id | Name | Description&quot;);

    foreach (var row in rows)
    {
        Console.WriteLine($&quot;{row.CategoryId} | {row.CategoryName} | {row.CategoryDescription}&quot;);
    }
}

// Lists all products
static async Task ProductListAsync()
{
    // Ny context per separation (bra praxis)
    using var db = new ShopContext();

    // AsNoTracking = snabbare får read-only scenarion (Ingen change tracking)
    var productRow = await db.Products.AsNoTracking().OrderBy(p =&gt; p.ProductId).ToListAsync();
    Console.WriteLine(&quot;ProductId | Product Name | Product Description&quot;);

    foreach (var row in productRow)
    {
        Console.WriteLine($&quot;{row.ProductId} | {row.ProductName} | {row.ProductDescription}&quot;);
    }
}

// CREATE: lägg till en ny category
static async Task AddAsync()
{
    Console.WriteLine(&quot;Name: &quot;);
    var name = Console.ReadLine()?.Trim() ?? string.Empty;

    // Enkel validering
    if (string.IsNullOrEmpty(name) || name.Length &gt; 100)
    {
        Console.WriteLine(&quot;Name is required (max 100).&quot;);
        return;
    }

    Console.WriteLine(&quot;Descriptions. (Optional): &quot;);
    var desc = Console.ReadLine()?.Trim() ?? string.Empty;

    using var db = new ShopContext();
    db.Categories.Add(new Category { CategoryName = name, CategoryDescription = desc });

    try
    {
        // Spara våra ändringar
        await db.SaveChangesAsync();
        Console.WriteLine(&quot;Category added!&quot;);
    }
    catch (DbUpdateException exception)
    {
        Console.WriteLine(&quot;DB Error (Maybe duplicate?)! &quot; + exception.GetBaseException().Message);
    }
}

static async Task AddProductAsync()
{
    Console.WriteLine(&quot;Product Name: &quot;);
    var name = Console.ReadLine()?.Trim() ?? string.Empty;

    // Enkel validering
    if (string.IsNullOrEmpty(name) || name.Length &gt; 100)
    {
        Console.WriteLine(&quot;Name is required (max 100 characters).&quot;);
        return;
    }

    Console.Write(&quot;Price &quot;);
    var stringPrice = Console.ReadLine()?.Trim() ?? string.Empty;

    if (string.IsNullOrEmpty(stringPrice) || !decimal.TryParse(stringPrice, out var price) || price &lt; 0)
    {
        Console.WriteLine(&quot;Price cannot be empty. Has to be a number!&quot;);
        return;
    }

    Console.WriteLine(&quot;Descriptions. (Optional): &quot;);
    var desc = Console.ReadLine();

    using var db = new ShopContext();
    db.Products.Add(new Product { ProductName = name, ProductPrice = price, ProductDescription = desc, });

    try
    {
        // Spara våra ändringar
        await db.SaveChangesAsync();
        Console.WriteLine(&quot;Product added!&quot;);
    }
    catch (DbUpdateException exception)
    {
        Console.WriteLine(&quot;DB Error (Maybe duplicate?)! &quot; + exception.GetBaseException().Message);
    }
}
</code></pre>
<p>Here is my other code in case they are related to the problem.</p>
<p>My <code>ShopContext.cs</code>:</p>
<pre class=""lang-cs prettyprint-override""><code>using System;
using System.Collections.Generic;
using System.ComponentModel.DataAnnotations;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Microsoft.EntityFrameworkCore;

namespace EF_Core___Migrations___Seeding
{
    // DbContext = &quot;Enheten&quot; som representerar databasen
    public class ShopContext : DbContext
    {
        // DB&lt;Category&gt; mappar till tabellen &quot;Category&quot; i databasen
        public DbSet&lt;Category&gt; Categories =&gt; Set&lt;Category&gt;();

        public DbSet&lt;Product&gt; Products =&gt; Set&lt;Product&gt;();

        // Här berättar vi för EF Core att vi vill använda SQLite och var filen ska finnas
        protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
        {
            var dbPath = Path.Combine(AppContext.BaseDirectory, &quot;shop.db&quot;);

            optionsBuilder.UseSqlite($&quot;Filename={dbPath}&quot;);
        }

        // OnModelCreating används för att finjustera modellen
        protected override void OnModelCreating(ModelBuilder modelBuilder)
        {
            modelBuilder.Entity&lt;Category&gt;(e =&gt;
            {
                // Sätter primärnyckel
                e.HasKey(x =&gt; x.CategoryId);

                // Säkerställer samma regler som data anotations (required + MaxLenght)
                e.Property(x =&gt; x.CategoryName)
                    .IsRequired().HasMaxLength(100);

                e.Property(x =&gt; x.CategoryDescription).HasMaxLength(250);

                // Skapar ett UNIQUE-index på CategoryName
                // Databasen tillåter inte två rader med samma CategoryName
                // Kanske inte vill ha två kategorier som heter &quot;Books&quot;
                e.HasIndex(x =&gt; x.CategoryName).IsUnique();
            });

            modelBuilder.Entity&lt;Product&gt;(e =&gt;
            {
                // Sätter primärnyckel (PK)
                e.HasKey(x =&gt; x.ProductId);

                // Säkerställer samma regler som data annotations (required + MaxLenght)
                e.Property(x =&gt; x.ProductName)
                    .IsRequired().HasMaxLength(100);

                e.Property(x =&gt; x.ProductPrice).IsRequired();

                e.Property(x =&gt; x.ProductDescription).HasMaxLength(250);

                // TODO: UNCOMMENT
                //  e.HasIndex(x =&gt; x.ProductName).IsUnique();

                e.HasOne(p =&gt; p.Category)
                .WithMany(c =&gt; c.Products)
                .HasForeignKey(p =&gt; p.CategoryId)
                .OnDelete(DeleteBehavior.Restrict);
            });
        }
    }
}
</code></pre>
<p><code>Product.cs</code>:</p>
<pre class=""lang-cs prettyprint-override""><code>using Microsoft.EntityFrameworkCore;
using System;
using System.Collections.Generic;
using System.ComponentModel.DataAnnotations;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EF_Core___Migrations___Seeding
{
    public class Product
    {
        // Primär nyckeln
        public int ProductId { get; set; }

        [MaxLength(250)]
        public string? ProductDescription { get; set; }

        [Required, MaxLength(100)]
        public string ProductName { get; set; } = null!;

        [Required]

        public decimal ProductPrice { get; set; }
        public int CategoryId { get; set; }
        public Category Category { get; set; } = null!;
    }
}
</code></pre>
<p>And lastly my <code>Category.cs</code>:</p>
<pre class=""lang-cs prettyprint-override""><code>using System;
using System.Collections.Generic;
using System.ComponentModel.DataAnnotations;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EF_Core___Migrations___Seeding
{
    //Enkel modellklass (entity) som EF Core mappa till en tabell &quot;Category&quot;
    public class Category
    {
        // Primärnycke. EF Core ser &quot;CategoryId&quot; och gör det till PK
        public int CategoryId { get; set; }

        // Required = får inte vara null (varken i C# eller i databasen)
        // MaxLength = genererar en kolumn med maxlängd 100 + används vid validering
        [Required, MaxLength(100)]
        public string CategoryName { get; set; } = null!;

        // Optional (Nullable '?') text med max 250 tecken
        [MaxLength(250)]
        public string? CategoryDescription { get; set; }

        public ICollection&lt;Product&gt; Products { get; set; } = new List&lt;Product&gt;();
    }
}
</code></pre>
",1,1,0,2025-11-26T01:25:25+00:00,1,126,True
79831183,17259466,,sqlite,Linking custom sqlite when building python,"<p>I am trying to link a custom version of sqlite3 when building python, but there is an error when compiling python :</p>
<p><code>/usr/bin/install: cannot stat 'Modules/_sqlite3.cpython-313-x86_64-linux-gnu.so': No such file or directory make: *** [Makefile:2325: sharedinstall] Error 1</code></p>
<p>It does not pick up the custom sqlite despite pointing out to it with th following environment variables:</p>
<pre><code>export CPPFLAGS=&quot;-I/opt/sqlite/include&quot;
export CFLAGS=&quot;-I/opt/sqlite/include&quot;
export C_INCLUDE_PATH=&quot;/opt/sqlite/include:${C_INCLUDE_PATH}&quot;

export LDFLAGS=&quot;-L/opt/sqlite/lib&quot;
export LIBRARY_PATH=&quot;/opt/sqlite/lib:${LIBRARY_PATH}&quot;
export LD_LIBRARY_PATH=&quot;/opt/sqlite/lib:${LD_LIBRARY_PATH}&quot;
export PKG_CONFIG_PATH=&quot;/opt/sqlite/lib/pkgconfig:${PKG_CONFIG_PATH}&quot;
</code></pre>
<p>How can I ensure that the custom sqlite is used and not the system version?</p>
<p>Edit: Here is the github link for the shell scripts that I am using to build sqlite and link it to python.</p>
<p><a href=""https://github.com/sherwin684/custom_sqlite_python"" rel=""nofollow noreferrer"">https://github.com/sherwin684/custom_sqlite_python</a></p>
",2,3,1,2025-11-26T21:53:51+00:00,0,84,False
79831882,11222817,,sqlite,Understanding Dataset Controls in Delphi While Using SQLite,"<p>My Delphi application uses SQLite for its database. My understanding is that the database connection control <code>TFDConnection</code> has a default row count of 50 records. If true, there should only be 50 records displayed in the <code>TDBGrid</code> control on screen at any one time. The next set of 50 (or less) would be shown once you pass the 50th record in the dataset by proceeding to the next or last record.</p>
<p>The problem is my application is apparently loading all the database records (currently 148) at one time which exceeds the default record count using <code>select * from tablename</code>. I have tried to setup a &quot;paging&quot; approach to solve the problem as well using the following:</p>
<pre class=""lang-pascal prettyprint-override""><code>data.query.last;
TotalRec:=data.query.RecordCount;
PageCnt:=TotalRec div 50;
RecRemain:=TotalRec mod 50;
</code></pre>
<p>Then using the following code to get the dataset based on a limit and offset:</p>
<pre class=""lang-pascal prettyprint-override""><code>data.query.SQL.Text:='select * FROM tablename LIMIT :limit OFFSET :ofs;';
data.query.ParamByName('limit').asInteger:=50;
data.query.ParamByName('ofs').asInteger:=ofs1;
data.query.open;
</code></pre>
<p>The variable ofs1 is initially zero but is incremented or decremented by 50. Even this approach is not limiting the dataset to 50.</p>
<p>Any help understanding why this is happening would be very helpful moving forward. Thanks.</p>
",0,0,0,2025-11-27T15:56:19+00:00,6,187,True
79832098,19773284,,sqlite,How can I set up an ETL process for DataFrames (dbt or alternative tools)?,"<p>Question: I'm currently working on a dashboard prototype and storing data from a 22-page PDF document as 22 separate DataFrames. These DataFrames should undergo an ETL process (especially data type conversions) on every import before being written to the database.</p>
<p>My initial approach was to use dbt seeds and/or models. However, dbt loads the seed CSV directly into the SQLite database without my prior transformations taking effect. I want to transform first and then load.</p>
<p>Setup:</p>
<p>* Data source: 22 DataFrames extracted from PDF tables</p>
<p>* Database: SQLite (for prototype only)</p>
<p>* ETL/ELT tool: preferably dbt, but not mandatory</p>
<p>* Language: Python  Problem: How can I set up an ETL workflow where DataFrames are processed and transformed directly without having to load them into dbt as CSV (seeds) beforehand?</p>
<p>Is there a way to integrate DataFrames directly into a dbt model process? If not, what alternative tools are suitable (e.g., Airflow, Dagster, Prefect, pandas-based ETL pipelines, etc.)?</p>
<p>Previous attempts:</p>
<p>* dbt seed: loads CSV directly into the database → transformations don't work</p>
<p>* dbt models: only work if the data is already in the database, which I want to avoid</p>
<p>* Currently: manual type conversions in Python (float, int, string, datetime)</p>
<p>Looking for: Best practices or tool recommendations for directly transforming DataFrames and then loading them into SQLite – with or without dbt.</p>
",0,0,0,2025-11-27T21:37:31+00:00,0,41,False
79832664,15577675,,sqlite,sqlite3.OperationalError because Python thinks my table name is a column,"<p>I am trying to access the table in my game which is called tbl[savename]. But it thinks my table name is a column.</p>
<p>My code is:</p>
<pre><code>    def save_data(self, data):
    conn = sqlite3.connect(&quot;dbInfo.db&quot;)
    identify = 0
    cur = conn.cursor()
    print(data[&quot;username&quot;])
    cur.execute(f&quot;&quot;&quot;UPDATE {data[&quot;username&quot;]}
                SET username = {data[&quot;username&quot;]}, shopname = {data[&quot;shopname&quot;]}, day = {data[&quot;day&quot;]}, money = {data[&quot;money&quot;]}, cash_flow = {data[&quot;cash_flow&quot;]}, character = {data[&quot;character&quot;]}
                WHERE id = ?&quot;&quot;&quot;)
    conn.commit()
    print(&quot;Saved data!&quot;)
    conn.close()
</code></pre>
<p>But I get the error:</p>
<pre><code> Exception in Tkinter callback
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\tkinter\__init__.py&quot;, line 1921, in __call__
    return self.func(*args)
  File &quot;U:\Year 12 CS\CompSciProject\CompSciProject\CompSciProject\code\Startup.py&quot;, line 130, in apply_settings
    self.SaveAndClose()
  File &quot;U:\Year 12 CS\CompSciProject\CompSciProject\CompSciProject\code\Startup.py&quot;, line 150, in SaveAndClose
    self.backend.save_data(data = self.player_info)
  File &quot;U:\Year 12 CS\CompSciProject\CompSciProject\CompSciProject\code\Startup_backend.py&quot;, line 56, in save_data
    cur.execute(f&quot;&quot;&quot;UPDATE {data[&quot;username&quot;]}
sqlite3.OperationalError: no such column: tblGeorge
</code></pre>
",-3,0,3,2025-11-28T14:42:47+00:00,0,46,False
79834382,16193835,,sqlite,Displayed data disappears after successfully adding data to sqlite database,"<p>Please help me! I'm building a simple stock search app using Flask (backend) and plain HTML/JS (frontend).
When the user enters a ticker, the app:</p>
<p>Send a GET request to my Flask backend (/search)</p>
<p>Display the stock data returned from the Tiingo API</p>
<p>After successful display, send a POST request to /add_to_history to store the ticker in a SQLite database</p>
<p>The GET request works correctly — the data displays as expected in my table.</p>
<p>The problem:
As soon as the POST request (/add_to_history) returns a 200 response, the table that was displaying the fetched stock data immediately disappears from the page. There are no errors in the console, and both network requests return status 200. The UI simply hides the content right after the POST call finishes.
Here is my index.html code</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&gt;
    &lt;title&gt;Stock App&lt;/title&gt;
    &lt;script&gt;
      window.stockData = {
        &quot;companyOutLook&quot;: null,
        &quot;stockSummary&quot;: null
      }
      function onBtnClick(event) {
        event.preventDefault();
        const inputField = document.getElementById('stockTicker');
        const tickerSymbol = inputField.value;
        
        fetch(`http://127.0.0.1:5000/search?ticker=${encodeURIComponent(tickerSymbol)}`)
        .then(response =&gt; {
          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
          }
          return response.json();
        })
        .then(data =&gt; {
          console.log('Data received:', data);
          // Check if response contains an error
          if (data.error) {

            hideTabsAndTable();
            showErrorMessage();
            return;
          }
          // Store data for later use
          if (Array.isArray(data) &amp;&amp; data.length &gt; 0) {
            window.stockData.companyOutLook = data[0];
            if (data.length &gt; 1) {
              window.stockData.stockSummary = data[1];
            }
            else {
              window.stockData.stockSummary = null;
            }
            
            // Show tabs and table when data is available
            hideErrorMessage();
            
            showTabsAndTable();
            showCompanyOutlook();
            // add to sqlite history
            fetch(`http://127.0.0.1:5000/add_to_history`, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
              },
              body: JSON.stringify({ticker: tickerSymbol}),
            })
            .then(response =&gt; {
              console.log('Response:', response);
              
            })
            .catch(error =&gt; {
              console.error('Error:', error);
            });
          } 
          else {
            window.stockData.companyOutLook = null;
            window.stockData.stockSummary = null;
            hideTabsAndTable();
            showErrorMessage();
          }
         
        })
        .catch(error =&gt; {
          hideTabsAndTable();
          showErrorMessage();
        });
      }

      function showTabsAndTable() {
        const tabs = document.querySelector('.tabs');
        const table = document.querySelector('.content');
        if (tabs) tabs.style.display = 'flex';
        if (table) table.style.display = 'table';
      }

      function hideTabsAndTable() {
        const tabs = document.querySelector('.tabs');
        const table = document.querySelector('.content');
        if (tabs) tabs.style.display = 'none';
        if (table) table.style.display = 'none';
      }

      function showCompanyOutlook() {
        const table = document.querySelector('.content');
        table.innerHTML = '';
        
        const rows = [
          'Company Name',
          'Ticker Symbol',
          'Exchange Code',
          'Start Date',
          'Description'
        ];
        const dataNames = [
          &quot;name&quot;,
          &quot;ticker&quot;,
          &quot;exchangeCode&quot;,
          &quot;startDate&quot;,
          &quot;description&quot;
        ] 
        if (window.stockData.companyOutLook) {
          rows.forEach((header, i) =&gt; {
            const row = document.createElement('tr');
            const headerCell = document.createElement('td');
            headerCell.className = 'content-header';
            headerCell.textContent = header;
            const dataCell = document.createElement('td');
            dataCell.className = 'content-data';
            const value = window.stockData.companyOutLook[dataNames[i]] ?? '';
            dataCell.textContent = value;
            row.appendChild(headerCell);
            row.appendChild(dataCell);
            table.appendChild(row);
          });
        }
        else {
          table.innerHTML = &quot;&lt;tr&gt;&lt;td colspan='2'&gt;&lt;p&gt;Error: No record has been found, please enter a valid symbol.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&quot;;
        }
        
        
        
        // Update active tab styling
        updateActiveTab(0);
      }

      function showStockSummary() {
        const table = document.querySelector('.content');
        table.innerHTML = '';
        
        const rows = [
          'Ticker Symbol',
          'Trading Day',
          'Previous Closing Price',
          'Opening Price',
          'High Price',
          'Low Price',
          'Last Price',
          'Change',
          'Change Percent',
          'Number of Shares Traded'
        ];
        if (window.stockData.stockSummary) {
          const summary = window.stockData.stockSummary;
          const last = Number(summary.last ?? summary.tngoLast ?? 0);
          const prevClose = Number(summary.prevClose ?? 0);
          const change = prevClose ? last - prevClose : 0;
          const changePercent = prevClose ? (change / prevClose) * 100 : 0;

          
          const tradingDay = summary.timestamp
            ? String(summary.timestamp).split('T')[0]
            : '';

          const data = [
            summary.ticker ?? '',
            tradingDay,
            prevClose || '',
            summary.open ?? '',
            summary.high ?? '',
            summary.low ?? '',
            last || '',
            (change &gt; 0 ? &quot;+&quot; + change.toFixed(2) : change &lt; 0 ? change.toFixed(2) : &quot;0&quot;) || '',
            (changePercent &gt; 0 ? &quot;+&quot; + changePercent.toFixed(2) + &quot;%&quot; : changePercent &lt; 0 ? changePercent.toFixed(2) + &quot;%&quot; : &quot;0%&quot;) || '',
            summary.volume ?? '',
          ];

          rows.forEach((header, i) =&gt; {
            const row = document.createElement('tr');
            const headerCell = document.createElement('td');
            headerCell.className = 'content-header';
            headerCell.textContent = header;
            const dataCell = document.createElement('td');
            dataCell.className = 'content-data';
            dataCell.id = `data-${i}`;
            dataCell.textContent = data[i];
            row.appendChild(headerCell);
            row.appendChild(dataCell);
            table.appendChild(row);
          });

          // add image to data 7 and 8
          const data7 = document.getElementById('data-7');
          const data8 = document.getElementById('data-8');
          if (data7 &amp;&amp; data7.textContent.includes(&quot;+&quot;)) {
            data7.innerHTML += &quot;&lt;img src='GreenArrowUP.png' alt='Green Arrow Up' width='10' height='10' /&gt;&quot;
          } else if (data7 &amp;&amp; data7.textContent.includes(&quot;-&quot;)) {
            data7.innerHTML += &quot;&lt;img src='RedArrowDown.png' alt='Red Arrow Down' width='10' height='10' /&gt;&quot;
          }
          if (data8 &amp;&amp; data8.textContent.includes(&quot;+&quot;)) {
            data8.innerHTML += &quot;&lt;img src='GreenArrowUP.png' alt='Green Arrow Up' width='10' height='10' /&gt;&quot;
          } else if (data8 &amp;&amp; data8.textContent.includes(&quot;-&quot;)) {
            data8.innerHTML += &quot;&lt;img src='RedArrowDown.png' alt='Red Arrow Down' width='10' height='10' /&gt;&quot;
          }

        }
        else {
          table.innerHTML = &quot;&lt;p&gt;Error: No record has been found, please enter a valid symbol.&lt;/p&gt;&quot;;
        }
        
        
        // Update active tab styling
        updateActiveTab(1);
      }

      function updateActiveTab(activeIndex) {
        const tabs = document.querySelectorAll('.tabs &gt; div');
        tabs.forEach((tab, index) =&gt; {
          if (index === activeIndex) {
            tab.classList.add('active');
          } else {
            tab.classList.remove('active');
          }
        });
      }

      function showErrorMessage() {
        const errorMessage = document.querySelector('.error-message');
        errorMessage.style.display = 'block';
      }
      function hideErrorMessage() {
        const errorMessage = document.querySelector('.error-message');
        errorMessage.style.display = 'none';
      }
      // Initialize tabs with click handlers
      document.addEventListener('DOMContentLoaded', function() {
        const tabs = document.querySelectorAll('.tabs &gt; div');
        tabs[0].addEventListener('click', showCompanyOutlook);
        tabs[1].addEventListener('click', showStockSummary);
        // Search History tab - no action for now
      });
      function clearInput() {
        const inputField = document.getElementById('stockTicker');
        inputField.value = '';
        window.stockData.companyOutLook = null;
        window.stockData.stockSummary = null;
        hideTabsAndTable();
        hideErrorMessage();
      }
    &lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=&quot;container&quot;&gt;
      &lt;h1&gt;Stock Search&lt;/h1&gt;
      &lt;form onsubmit=&quot;onBtnClick(event)&quot;&gt;
        &lt;span&gt;Enter Stock Ticker Symbol&lt;span class=&quot;required&quot;&gt;*&lt;/span&gt;&lt;/span&gt;
        &lt;input type=&quot;text&quot; id=&quot;stockTicker&quot; required&gt;
        &lt;div class=&quot;buttons&quot;&gt; 
          &lt;button type=&quot;submit&quot; &gt;Search&lt;/button&gt;
          &lt;button type=&quot;button&quot; onclick=&quot;clearInput()&quot;&gt;Clear&lt;/button&gt;&lt;/div&gt;
      &lt;/form&gt;
      
    &lt;/div&gt;
    &lt;div class=&quot;divider&quot;&gt;&lt;/div&gt;
    

    &lt;div class=&quot;error-message&quot; style=&quot;display: none;&quot;&gt;
      &lt;p&gt;Error: No record has been found, please enter a valid symbol.&lt;/p&gt;
    &lt;/div&gt;

    &lt;div class=&quot;tabs&quot; style=&quot;display: none;&quot;&gt;
      &lt;div&gt;Company Outlook&lt;/div&gt;
      &lt;div&gt;Stock Summary&lt;/div&gt;
      &lt;div&gt;Search History&lt;/div&gt;
    &lt;/div&gt;

    &lt;table class=&quot;content&quot; style=&quot;display: none;&quot;&gt;
      &lt;!-- Table rows will be dynamically generated by JavaScript --&gt;
       &lt;tbody id=&quot;table-body&quot;&gt;&lt;/tbody&gt;
    &lt;/table&gt;

  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Here is the app.py file</p>
<pre><code>from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import sqlite3
app = Flask(__name__)
CORS(app, resources={r&quot;/search*&quot;: {&quot;origins&quot;: &quot;*&quot;}, r&quot;/add_to_history*&quot;: {&quot;origins&quot;: &quot;*&quot;}})
API_KEY = &quot;SECRET_API_KEY&quot;
DB_NAME = &quot;search_history.db&quot;
@app.route('/')
def index():
  return &quot;Hello, World!&quot;


def init_db():
    with sqlite3.connect(DB_NAME) as conn:
        conn.execute('''CREATE TABLE IF NOT EXISTS SearchHistory (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )''')

# initialize the database
# run init_db() when the app starts
init_db()

@app.route('/history', methods=['GET'])
def get_search_history():
  with sqlite3.connect(DB_NAME) as conn:
    conn.row_factory = sqlite3.Row
    rows = conn.execute('SELECT ticker, timestamp FROM SearchHistory LIMIT 10').fetchall()
    results = [dict(row) for row in rows]
  return jsonify(results)


@app.route('/add_to_history', methods=['POST'])
def add_to_history():
  try:
    data = request.get_json()
    if not data:
      return jsonify({'error': 'No JSON data provided'}), 400
    
    ticker = data.get('ticker')
    if not ticker:
      return jsonify({'error': 'Ticker is required'}), 400
    
    with sqlite3.connect(DB_NAME) as conn:
      conn.execute('INSERT INTO SearchHistory(ticker) VALUES (?)', (ticker,))
      conn.commit()
    
    return jsonify({'message': 'Record added to history'}), 200
  except Exception as e:
    return jsonify({'error': str(e)}), 500

def get_history():
  with sqlite3.connect(DB_NAME) as conn:
    conn.row_factory = sqlite3.Row
    rows = conn.execute('SELECT ticker, timestamp FROM SearchHistory LIMIT 10').fetchall()
    results = [dict(row) for row in rows]
  return results


@app.route('/search', methods=['GET'])
def search():
  try:
    ticker = request.args.get('ticker', &quot;&quot;)
    # if not ticker is passed, return error message
    if not ticker:
      return jsonify({'error': &quot;No record has been found, please enter a valid symbol&quot; })

    # define a return object
    results = []
    # fetch company outlook tab
    url = f&quot;https://api.tiingo.com/tiingo/daily/{ticker}?token={API_KEY}&quot;
    response = requests.get(url)
    data = response.json()
    if data and data.get('detail', &quot;&quot;) != &quot;Not found.&quot;:
      results.append(data)
    # fetch stock summary tab
    url = f&quot;https://api.tiingo.com/iex/{ticker}?token={API_KEY}&quot;
    response = requests.get(url)
    data = response.json()
    if data and len(data) &gt; 0:
      results.append(data[0]) 
    # add to history if fetched successfully
    # if results:
    #   add_to_history(ticker)
    
    return jsonify(results)

  except Exception as e:
    return jsonify({'error': str(e) })
  
if __name__ == '__main__':
  app.run(debug=True)

</code></pre>
",2,2,0,2025-12-01T06:02:25+00:00,1,198,False
79837192,5000980,,sqlite,How do I fix this error when using SELECT in my ON CONFLICT ... DO statement in SQLite3?,"<p>I'm new to SQLite and trying to make an UPSERT statement that just updates everything in one table with everything from a different table. If there are conflicts, it should update with the new values. The tables have the same schema. When I do a regular <code>INSERT INTO</code> without conflicts, it works fine. And when I insert using <code>VALUES</code> it works fine. But when I use <code>SELECT</code> from the second table and <code>ON CONFLICT (col) DO</code> I get an error pointing at <code>DO</code>. This happens whether or not there is actually a conflict.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE my_table(
  col1 TEXT PRIMARY KEY,
  col2 INTEGER);
INSERT INTO my_table(col1, col2) VALUES('blue', 1);

CREATE TABLE new_data(
  col1 TEXT PRIMARY KEY,
  col2 INTEGER);
INSERT INTO new_data(col1, col2) VALUES('red', 3);

-- UPSERT using VALUES works
INSERT INTO my_table(col1, col2) 
  VALUES('blue', 2) 
  ON CONFLICT (col1) DO 
  UPDATE 
  SET col2 = excluded.col2;

SELECT * FROM my_table;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>blue</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<pre><code>-- INSERT with SELECT works
INSERT INTO my_table(col1, col2) 
  SELECT col1, col2
  FROM new_data;

SELECT * FROM my_table;
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
</tr>
</thead>
<tbody>
<tr>
<td>blue</td>
<td>2</td>
</tr>
<tr>
<td>red</td>
<td>3</td>
</tr>
</tbody>
</table></div>
<pre><code>-- INSERT with SELECT throws correct unique conflict
INSERT INTO my_table(col1, col2) 
  SELECT col1, col2 
  FROM new_data;
</code></pre>
<blockquote>
<p>Runtime error: UNIQUE constraint failed: my_table.col1 (19)</p>
</blockquote>
<pre><code>-- Using SELECT and ON CONFLICT DO together throws an error
INSERT INTO my_table(col1, col2) 
  SELECT col1, col2 
  FROM new_data 
  ON CONFLICT (col1) DO 
  UPDATE 
  SET col2 = excluded.col2;
</code></pre>
<blockquote>
<p>Parse error: near &quot;DO&quot;: syntax error<br />
SELECT col1, col2 FROM new_data ON CONFLICT (col1) DO UPDATE SET col2 = exclude<br />
error here ---^</p>
</blockquote>
<p>Please help! It's driving me crazy.
I saw suggestions for <code>INSERT OR REPLACE</code> instead of UPSERT, but it doesn't look like it works exactly the same way, and I think I need to use an <code>UPDATE</code> trigger later on.</p>
<p>My version of SQLite3 is 3.51.0. I already tried closing and restarting SQLite.<br />
I'm working in VSCode.</p>
",2,2,0,2025-12-03T18:09:01+00:00,1,74,True
79837626,5939888,,sqlite,SQLite not working after migrating form VS 2022 to VS 2026,"<p>I am migrating a large Windows Forms application from Visual Studio 2022 to Visual Studio 2026 (both Community Edition). The current version of the application, consisting of 13 C# projects,  is based on .NET 8.0.
All went quite smoothly, except for the use of SQLite which is referenced from one project only.
The program starts fine, but the first attempt to access the database gives me the following error:</p>
<p><code>Unable to load DLL 'e_sqlite3' or one of its dependencies: The specified module could not be found. (0x8007007E)</code></p>
<p>I’ve looked for the file “e_sqlite3.dll” and it was never there in my project. The relevant part of the original, VS 2022 csproj file is:</p>
<pre><code>  &lt;PropertyGroup Condition=&quot;'$(Configuration)|$(Platform)' == 'Debug|x64'&quot;&gt;
    &lt;OutputPath&gt;bin\x64\Debug\&lt;/OutputPath&gt;
  &lt;/PropertyGroup&gt;
  &lt;PropertyGroup Condition=&quot;'$(Configuration)|$(Platform)' == 'Release|x64'&quot;&gt;
    &lt;OutputPath&gt;bin\x64\Release\&lt;/OutputPath&gt;
  &lt;/PropertyGroup&gt;
  &lt;ItemGroup&gt;
    &lt;Reference Include=&quot;xxSecurity&quot;&gt;
      &lt;HintPath&gt;xxSecurity\xxSecurity.dll&lt;/HintPath&gt;
    &lt;/Reference&gt;
    &lt;Reference Include=&quot;System.Text.Json, Version=6.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51, processorArchitecture=MSIL&quot; /&gt;
    &lt;Reference Include=&quot;System.Web.Extensions&quot; /&gt;
    &lt;Reference Include=&quot;System.Web.Services&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;Content Include=&quot;soascape icon.ico&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;ProjectReference Include=&quot;..\..\ApiUseInfo\ApiUseInfo.csproj&quot; /&gt;
    &lt;ProjectReference Include=&quot;..\..\PluginHandler\PluginHandler.csproj&quot; /&gt;
    &lt;ProjectReference Include=&quot;..\..\SDEF\SDEF.csproj&quot; /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;PackageReference Include=&quot;ClosedXML&quot; Version=&quot;0.104.1&quot; /&gt;
    &lt;PackageReference Include=&quot;DocumentFormat.OpenXml&quot; Version=&quot;3.1.1&quot; /&gt;
    &lt;PackageReference Include=&quot;ExcelNumberFormat&quot; Version=&quot;1.1.0&quot; /&gt;
    &lt;PackageReference Include=&quot;FastMember&quot; Version=&quot;1.5.0&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.CSharp&quot; Version=&quot;4.7.0&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Identity.Client&quot; Version=&quot;4.66.1&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Identity.Client.Extensions.Msal&quot; Version=&quot;4.66.1&quot; /&gt;
    &lt;PackageReference Include=&quot;Microsoft.Office.Interop.Excel&quot; Version=&quot;15.0.4795.1001&quot; /&gt;
    &lt;PackageReference Include=&quot;SharpZipLib&quot; Version=&quot;1.4.2&quot; /&gt;
    &lt;PackageReference Include=&quot;SharpZipLib.NETStandard&quot; Version=&quot;1.0.7&quot; /&gt;
    &lt;PackageReference Include=&quot;System.ComponentModel.Annotations&quot; Version=&quot;5.0.0&quot; /&gt;
    &lt;PackageReference Include=&quot;System.Data.DataSetExtensions&quot; Version=&quot;4.5.0&quot; /&gt;
    &lt;PackageReference Include=&quot;System.Data.SQLite.Core&quot; Version=&quot;1.0.119&quot; /&gt;
    &lt;PackageReference Include=&quot;System.Data.SQLite.EF6&quot; Version=&quot;1.0.119&quot; /&gt;
    &lt;PackageReference Include=&quot;EntityFramework&quot; Version=&quot;6.5.1&quot; /&gt;
    &lt;PackageReference Include=&quot;System.Net.Http&quot; Version=&quot;4.3.4&quot; /&gt;
    &lt;PackageReference Include=&quot;System.Text.RegularExpressions&quot; Version=&quot;4.3.1&quot; /&gt;
    &lt;PackageReference Include=&quot;System.Web.Services.Description&quot; Version=&quot;8.0.0&quot; /&gt;
  &lt;/ItemGroup&gt;
</code></pre>
<p>After looking at Internet, I’ve found the following discussion:
<a href=""https://system.data.sqlite.org/home/info/9634ad17e6b169c81d8b617756ca0dad546d5e19"" rel=""nofollow noreferrer"">https://system.data.sqlite.org/home/info/9634ad17e6b169c81d8b617756ca0dad546d5e19</a></p>
<p>I’ve decided to give the suggestions from there a try, and installed the NuGet solution SQLitePCLRaw.core by Eric Sink (version 2.1.11). The application was able to read the database after that. I have not added any explicit reference to this library (using System.Data.SQLite; is the only using statement in my code)</p>
<p>I wander how is it possible that the program using the very same library System.Data.SQLite.Core works fine <strong>WITHOUT</strong> the file “e_sqlite3.dll” when built with VS 2022, and tries to references “e_sqlite3.dll” when built with VS 2026?</p>
<p>There seems to be some intelligent code in System.Data.SQLite.Core that takes different paths depending on some parameter not related to my application code.</p>
",-2,1,3,2025-12-04T07:21:54+00:00,0,168,False
79838369,19708417,,sqlite,ENOENT for node:sqlite without it being a dependency,"<p>I am currently making a npm package for a TypeScript Console Application Framework. I want this framework to offer the using developers a simple npm script &quot;tscaf build&quot; which takes all the commands etc. they defined and packs them into an executable. I am using pkg for this.</p>
<p>I have already figured out that the framework/packaging process itself is not the problem as it generally works. Whenever I try to use one specific service, which uses <code>fetch</code> calls, I get the following error:</p>
<pre><code>Error! Cannot read file, ENOENT
  node:sqlite
&gt; Error! ENOENT: no such file or directory, open 'E:\Creation\Coding\Projects\aniloader-cli\node:sqlite'
node:internal/errors:985
  const err = new Error(message);
              ^

Error: Command failed: &quot;E:\Creation\Coding\Projects\tscaf\node_modules\.bin\pkg.cmd&quot; &quot;E:\Creation\Coding\Projects\aniloader-cli\dist\index.js&quot; --target node18-win-x64 --output &quot;E:\Creation\Coding\Projects\aniloader-cli\target\tscaf&quot;
    at genericNodeError (node:internal/errors:985:15)
    at wrappedFn (node:internal/errors:539:14)
    at checkExecSyncError (node:child_process:925:11)
    at execSync (node:child_process:997:15)
    at executeBuild (E:\Creation\Coding\Projects\tscaf\dist\scripts\build.js:42:34)
    at Object.&lt;anonymous&gt; (E:\Creation\Coding\Projects\tscaf\dist\scripts\index.js:15:34)
    at Module._compile (node:internal/modules/cjs/loader:1761:14)
    at Object..js (node:internal/modules/cjs/loader:1893:10)
    at Module.load (node:internal/modules/cjs/loader:1481:32)
    at Module._load (node:internal/modules/cjs/loader:1300:12) {
  status: 2,
  signal: null,
  output: [ null, null, null ],
  pid: 5272,
  stdout: null,
  stderr: null
}

Node.js v24.11.1
</code></pre>
<p>It seems to be occuring in relation with my FetchWrapper. Now to my question: I searched all the Node.js docs but I could not find anything realted to <code>fetch</code> using sqlite or something similar. Neither do I, nor does any of my dependencies use node:sqlite directly. I do not understand why pkg tries to open/resolve the path.</p>
<p>When I run using <code>ts-node</code>, I do not get the error. I tried reinstalling Node.js and installing sqlite as a dependency, even though the path <code>project root/node:sqlite</code> looks pretty weird to me. Whenever I do not include the commands using <code>fetch</code> calls, I can package.</p>
<p>Thanks for any help!</p>
",-3,0,3,2025-12-04T21:19:54+00:00,0,46,False
79839625,13988248,,sqlite,I can&#39;t query by UUID,"<p>I'm having trouble querying UUID values stored as BLOB(16) in SQLite3 using Go (with <a href=""https://pkg.go.dev/github.com/doug-martin/goqu/v9@v9.19.0#section-readme"" rel=""nofollow noreferrer"">goqu</a>). I insert a UUID using <code>uuid.New().MarshalBinary()</code> and this works correctly. However, when I try to select the row by ID, the query always returns no rows, even though the value in the database matches the UUID I'm querying for (the hexadecimal representations are identical).</p>
<p>It seems as though SQLite or goqu is comparing the BLOB differently to what I expected, but I can't work out why a direct <code>WHERE id = ?</code> query doesn't match the 16-byte UUID.</p>
<p>The relevant code for inserting and querying is below.</p>
<p>Table:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE IF NOT EXISTS master (
            id BLOB(16) NOT NULL PRIMARY KEY, 
            username TEXT NOT NULL UNIQUE COLLATE NOCASE, 
            password BLOB NOT NULL
);
</code></pre>
<p>Insert:</p>
<pre class=""lang-go prettyprint-override""><code>stmt, err := conn.Prepare(`INSERT OR IGNORE INTO master(id, username, password) VALUES(?, ?, ?);`)

...

id, err := uuid.New().MarshalBinary()
</code></pre>
<p>Query:</p>
<pre class=""lang-go prettyprint-override""><code>// id = uuid.UUID
// db = *goqu.Database

idBin, err := id.MarshalBinary()

...

sqlString, args, err := db.From(&quot;master&quot;).Select(&quot;id&quot;, &quot;username&quot;).Where(goqu.Ex{&quot;id&quot;: idBin}).ToSQL()

...

stmt, err := db.Prepare(sqlString)

...

err = stmt.QueryRow(args...).Scan(&amp;idBytes, &amp;username) // always: &quot;sql: no rows in result set&quot;
</code></pre>
",1,1,0,2025-12-06T10:27:56+00:00,1,119,True
79840182,1127407,,sqlite,How to add a new collation for NOCASE in SQLitestudio,"<p>I' have just installed a new computer and added SQLiteStudio3. Tried to open a database created by a mORMot application and got the error: &quot;no such collation sequence: SYSTEMNOCASE&quot;.</p>
<p>I have fixed this once before but that was for some years ago so I have totally forgot how I did it.</p>
<p>It was a short javascript and that is all I remember.</p>
",0,0,0,2025-12-07T10:09:36+00:00,0,61,False
79846840,32006481,,sqlite,Why am I getting a foreign key mismatch when trying to join 2 tables using foreign keys?,"<p>I am doing <a href=""https://automatetheboringstuff.com/3e/chapter16.html"" rel=""nofollow noreferrer"">Chapter 16 - SQLite Databases of Automate the Boring Stuff with Python</a> By Al. Sweigart and I ran into a problem in the &quot;Joining Multiple Tables with Foreign Keys&quot; part, let it be known that I just started to get into python and I am for the first time learning about databases and SQLite.</p>
<p>I have a file called sweigartcats.db containing a table called &quot;cats&quot;, these are its columns:</p>
<pre><code>&gt;&gt;&gt; conn.execute('PRAGMA TABLE_INFO(cats)').fetchall()
[(0, 'name', 'TEXT', 1, None, 0), (1, 'birthdate', 'TEXT', 0, None, 0), (2, 'fur', 'TEXT', 0, None, 0), (3, 'weight_kg', 'REAL', 0, None, 0)]
</code></pre>
<p>The tutorial is showing how to create another table called vaccinations (which is successfully created) with the following structure:</p>
<pre><code>&gt;&gt;&gt; conn.execute('CREATE TABLE IF NOT EXISTS vaccinations (vaccine TEXT,
date_administered TEXT, administered_by TEXT, cat_id INTEGER,
FOREIGN KEY(cat_id) REFERENCES cats(rowid)) STRICT')
</code></pre>
<p>The values inserted into the vaccinations table use the rowid of the cats table to connect vaccination data to a selected cat, in this case the cat &quot;Zophie&quot; has the rowid of 1:</p>
<pre><code>&gt;&gt;&gt; conn.execute('INSERT INTO vaccinations VALUES (&quot;rabies&quot;, &quot;2023-06-06&quot;, &quot;Dr. Echo&quot;, 1)')
</code></pre>
<p>However, instead of returning a Cursor object, it gives me an error:</p>
<pre><code>sqlite3.OperationalError: foreign key mismatch - &quot;vaccinations&quot; referencing &quot;cats&quot;
</code></pre>
<p>Earlier today for whatever reason that error didn't happen when I inserted the values (I don't remember doing anything different) and I managed to use the following code to return a tuple that joined the rowid cat and its vaccines data:</p>
<pre><code>&gt;&gt;&gt; conn.execute('SELECT * FROM cats INNER JOIN vaccinations ON cats.rowid = 
vaccinations.cat_id').fetchall()
</code></pre>
<p>But that didn't last as the foreign key mismatch error happened right next to it when i tried to use iterdumps() to copy the database to a .txt file, I tried redoing it using the original file many times but the error always happens and I can no longer insert anything into the vaccinations table (even if i copy the code straight from the website to make sure it's not a typo).</p>
<p>I'm not sure if it's needed but here are the queries used to create the original file: <a href=""https://pastebin.com/neG0q4Ai"" rel=""nofollow noreferrer"">https://pastebin.com/neG0q4Ai</a></p>
<p>I checked the sqlite3 docs and some questions here and apparently there might be an issue with using rowid as a foreign key, but I'm not very familiar with it so I can't be sure, also the fact that it is in the book with no regards to any errors and I couldn't find anyone else with the same error in this specific lesson could mean I am getting something wrong.</p>
<p>One of the practice programs at the end of that chapter involves checking the vaccinations of the cats so I can only do it if I can make this work first.</p>
<p>I apologize if I forgot to add an important piece of information and I'll add it if needed.</p>
",1,1,0,2025-12-14T02:13:23+00:00,0,115,False
79852308,2962393,,sqlite,Recursive common table expression initial select search with primary key,"<p>I have a question about initial select in a recursive common table expression
(CTE) on SQLite and how to interpret the output of <code>EXPLAIN QUERY PLAN</code>.</p>
<p>I have constructed a linked list out of <code>node</code> and <code>link</code> tables. I query
a chain of linked nodes using a CTE query <code>chain</code>. I expected the initial
select in the SETUP phase to search the node table using primary key, but
instead it's showing SCAN.</p>
<p>Does that really mean it's potentially scanning the whole node table? What can I do to make it search with primary key?</p>
<p>Here is the query plan output with an annotation:</p>
<pre><code>QUERY PLAN
|--CO-ROUTINE chain
|  |--SETUP
|  |  `--SCAN node  &lt;———— Why is this SCAN?
|  `--RECURSIVE STEP
|     |--SCAN chain
|     |--SEARCH link USING PRIMARY KEY (id_src=?)
|     `--SEARCH node USING INTEGER PRIMARY KEY (rowid=?)
`--SCAN chain
</code></pre>
<p>Here is the SQL code:</p>
<pre><code>CREATE TABLE node
(
    id INTEGER NOT NULL PRIMARY KEY,
    content TEXT
);

CREATE TABLE link
(
    id_src INTEGER NOT NULL,
    id_dst INTEGER NOT NULL,
    PRIMARY KEY (id_src, id_dst),
    FOREIGN KEY (id_src) REFERENCES node (id),
    FOREIGN KEY (id_dst) REFERENCES node (id)
) WITHOUT ROWID;

INSERT INTO node VALUES (1, 'a');
INSERT INTO node VALUES (2, 'b');
INSERT INTO node VALUES (3, 'c');
INSERT INTO node VALUES (4, 'd');
INSERT INTO node VALUES (5, 'e');
INSERT INTO node VALUES (6, 'f');
INSERT INTO node VALUES (7, 'g');
INSERT INTO node VALUES (8, 'h');

INSERT INTO link VALUES (1, 3), (3, 5), (5, 7);
INSERT INTO link VALUES (2, 4), (4, 6), (6, 8);

EXPLAIN QUERY PLAN
WITH RECURSIVE
chain AS
(
    SELECT id AS id_first, id, content
    FROM node

    UNION ALL

    SELECT chain.id_first, node.id, node.content
    FROM chain
    INNER JOIN link ON link.id_src = chain.id
    INNER JOIN node ON node.id = link.id_dst
)
SELECT * FROM chain WHERE id_first = 1;
</code></pre>
",1,1,0,2025-12-21T15:29:09+00:00,1,106,True
79853954,32091931,,sqlite,SQLite fails to find newly inserted row,"<p>I'm using SQLite to track Fabrics and create projects that use those Fabrics. Two tables point to each other so Fabrics knows what projects they belong to and projects know what Fabrics are a part of it. While adding a new project I want to cycle through the Fabrics that are added and append the new project ID. However, when I attempt to get the ID from the newly created project, SQLite can't find a column with the name.</p>
<p>I use the following button press for creating a new project where the <code>Name</code> column in the <code>Project</code> table needs to be unique:</p>
<pre><code>projectButton.setOnClickListener {
    if(!projectName.text.isEmpty()){
        val fabricToProject = buildChecked(checkedList).toTypedArray()
        val result: Long = fabricDB.projectCreate(fabricToProject,projectName.text.toString())
        val projectID = fabricDB.getProjectID(projectName.text.toString())
        for(x in fabricToProject){
            fabricDB.fabricUpdate(x,projectID)
        }

        if(result.toInt() == -1){
            Toast.makeText(applicationContext,&quot;Project create failed, is your project name unique?&quot;,Toast.LENGTH_LONG).show()
        }else{
            Toast.makeText(applicationContext,&quot;Project created!&quot;,Toast.LENGTH_SHORT).show()
        }
    }
    else{
        Toast.makeText(applicationContext,&quot;Please enter a project name.&quot;,Toast.LENGTH_LONG).show()
    }
}
</code></pre>
<p><code>projectCreate()</code> works; when I load my activity that loads them into views, I can see all projects:</p>
<pre><code>fun projectCreate(fabricIDs: Array&lt;Int&gt;, name: String): Long {
    val dbW: SQLiteDatabase=this.writableDatabase
    val dbR: SQLiteDatabase=this.readableDatabase
    var idString = &quot;&quot;
    val contentValuesProject = ContentValues()
    contentValuesProject.put(Pcol4,name)
    for(x in fabricIDs){
        idString += &quot;$x,&quot;
    }
    idString = idString.dropLast(1)
    contentValuesProject.put(Pcol2,idString)
    val result: Long = dbW.insert(PtableName,null,contentValuesProject)
    return result
}
</code></pre>
<p>I attempted to pull the project's ID from within <code>projectCreate()</code> and add it to the <code>Fabric</code> table's rows, but when that could not find the column I created <code>getProjectID()</code> to run a <code>SELECT</code> query:</p>
<pre><code>fun getProjectID(name:String): Int{
    val dbR: SQLiteDatabase=this.readableDatabase
    val projectID = DatabaseUtils.stringForQuery(dbR,&quot;SELECT ID FROM Project WHERE Name is $name&quot;,null)
    return projectID.toInt()
}
</code></pre>
<p>If I add Project1 it crashes the activity:</p>
<blockquote>
<p>(1) no such column: Project1 in &quot;SELECT ID FROM Project WHERE Name is Project1&quot;</p>
</blockquote>
<p>If I then open my project list activity (which loads from the same table) I can see Project1. The difference is my project list activity uses a cursor since I need to loop through the table, whereas <code>getProjectID()</code> uses <code>readableDatabase()</code> since I only pull a single value.</p>
",0,1,1,2025-12-23T20:03:13+00:00,1,78,True
